what,kind,bundleId,bundleVersion,description,time
started,bundle,org.eclipse.equinox.p2.updatesite,1.0.300.v20110815-1419,"org.eclipse.equinox.p2.updatesite",1369114139718
started,bundle,org.eclipse.equinox.preferences,3.4.2.v20120111-2020,"org.eclipse.equinox.preferences",1369114139718
started,bundle,org.eclipse.equinox.registry,3.5.101.R37x_v20110810-1611,"org.eclipse.equinox.registry",1369114139718
started,bundle,org.eclipse.equinox.security,1.1.1.R37x_v20110822-1018,"org.eclipse.equinox.security",1369114139718
started,bundle,org.eclipse.equinox.simpleconfigurator.manipulator,2.0.0.v20110815-1438,"org.eclipse.equinox.simpleconfigurator.manipulator",1369114139718
started,bundle,org.eclipse.equinox.util,1.0.300.v20110502,"org.eclipse.equinox.util",1369114139718
started,bundle,org.eclipse.help,3.5.100.v20110426,"org.eclipse.help",1369114139718
started,bundle,org.eclipse.jdt.core,3.7.3.v20120119-1537,"org.eclipse.jdt.core",1369114139718
started,bundle,org.eclipse.jdt.core.manipulation,1.4.0.v20110928-1453,"org.eclipse.jdt.core.manipulation",1369114139718
started,bundle,org.eclipse.jdt.launching,3.6.1.v20111006_r372,"org.eclipse.jdt.launching",1369114139718
started,bundle,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui",1369114139718
started,bundle,org.eclipse.jem,2.0.501.v201111030500,"org.eclipse.jem",1369114139718
started,bundle,org.eclipse.jem.util,2.1.100.v201103021400,"org.eclipse.jem.util",1369114139718
started,bundle,org.eclipse.jface,3.7.0.v20110928-1505,"org.eclipse.jface",1369114139718
started,bundle,org.eclipse.jsch.core,1.1.300.I20110514-0800,"org.eclipse.jsch.core",1369114139718
started,bundle,org.eclipse.jst.common.project.facet.core,1.4.200.v201103170302,"org.eclipse.jst.common.project.facet.core",1369114139718
started,bundle,org.eclipse.jst.j2ee,1.1.503.v201202090300,"org.eclipse.jst.j2ee",1369114139718
started,bundle,org.eclipse.jst.j2ee.core,1.2.102.v201202090300,"org.eclipse.jst.j2ee.core",1369114139718
started,bundle,org.eclipse.jst.j2ee.navigator.ui,1.1.501.v201107261503,"org.eclipse.jst.j2ee.navigator.ui",1369114139718
started,bundle,org.eclipse.jst.j2ee.ui,1.1.503.v201201190400,"org.eclipse.jst.j2ee.ui",1369114139718
started,bundle,org.eclipse.jst.j2ee.webservice,1.1.400.v201004110600,"org.eclipse.jst.j2ee.webservice",1369114139718
started,bundle,org.eclipse.jst.j2ee.webservice.ui,1.1.500.v201105122000,"org.eclipse.jst.j2ee.webservice.ui",1369114139718
started,bundle,org.eclipse.jst.jee.ui,1.0.401.v201202011107,"org.eclipse.jst.jee.ui",1369114139718
started,bundle,org.eclipse.jst.ws.jaxrs.core,1.0.301.v201108240656,"org.eclipse.jst.ws.jaxrs.core",1369114139718
started,bundle,org.eclipse.jst.ws.jaxws.utils,1.0.100.v201201031437,"org.eclipse.jst.ws.jaxws.utils",1369114139718
started,bundle,org.eclipse.mylyn.bugzilla.core,3.6.5.v20120123-0100,"org.eclipse.mylyn.bugzilla.core",1369114139718
started,bundle,org.eclipse.mylyn.bugzilla.ui,3.6.0.v20110608-1400,"org.eclipse.mylyn.bugzilla.ui",1369114139718
started,bundle,org.eclipse.mylyn.commons.identity,0.8.0.v20110608-1400,"org.eclipse.mylyn.commons.identity",1369114139718
started,bundle,org.eclipse.mylyn.commons.net,3.6.0.v20110608-1400,"org.eclipse.mylyn.commons.net",1369114139718
started,bundle,org.eclipse.mylyn.commons.ui,3.6.1.v20110720-0100,"org.eclipse.mylyn.commons.ui",1369114139718
started,bundle,org.eclipse.mylyn.context.core,3.6.1.v20110720-0100,"org.eclipse.mylyn.context.core",1369114139718
started,bundle,org.eclipse.mylyn.context.ui,3.6.1.v20120112-0100,"org.eclipse.mylyn.context.ui",1369114139718
started,bundle,org.eclipse.mylyn.monitor.ui,3.6.0.v20110608-1400,"org.eclipse.mylyn.monitor.ui",1369114139718
started,bundle,org.eclipse.mylyn.tasks.ui,3.6.5.v20120215-0100,"org.eclipse.mylyn.tasks.ui",1369114139718
started,bundle,org.eclipse.mylyn.team.ui,3.6.1.v20110825-0100,"org.eclipse.mylyn.team.ui",1369114139718
started,bundle,org.eclipse.search,3.7.0.v20110928-1504,"org.eclipse.search",1369114139718
started,bundle,org.eclipse.team.core,3.6.0.I20110525-0800,"org.eclipse.team.core",1369114139718
started,bundle,org.eclipse.team.cvs.core,3.3.400.I20110510-0800,"org.eclipse.team.cvs.core",1369114139718
started,bundle,org.eclipse.team.cvs.ui,3.3.401.v20120126-1227,"org.eclipse.team.cvs.ui",1369114139718
started,bundle,org.eclipse.team.ui,3.6.101.R37x_v20111109-0800,"org.eclipse.team.ui",1369114139718
started,bundle,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui",1369114139718
started,bundle,org.eclipse.ui.console,3.5.100.v20111007_r372,"org.eclipse.ui.console",1369114139718
started,bundle,org.eclipse.ui.editors,3.7.0.v20110928-1504,"org.eclipse.ui.editors",1369114139718
started,bundle,org.eclipse.ui.forms,3.5.101.v20111011-1919,"org.eclipse.ui.forms",1369114139718
started,bundle,org.eclipse.ui.ide,3.7.0.v20110928-1505,"org.eclipse.ui.ide",1369114139718
started,bundle,org.eclipse.ui.navigator,3.5.101.v20120106-1355,"org.eclipse.ui.navigator",1369114139718
started,bundle,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.resources",1369114139718
started,bundle,org.eclipse.ui.net,1.2.100.v20111208-1155,"org.eclipse.ui.net",1369114139718
started,bundle,org.eclipse.ui.views,3.6.0.v20110928-1505,"org.eclipse.ui.views",1369114139718
started,bundle,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"org.eclipse.ui.workbench",1369114139718
started,bundle,org.eclipse.ui.workbench.texteditor,3.7.0.v20110928-1504,"org.eclipse.ui.workbench.texteditor",1369114139719
started,bundle,org.eclipse.update.configurator,3.3.100.v20100512,"org.eclipse.update.configurator",1369114139719
started,bundle,org.eclipse.update.core,3.2.500.v20110330,"org.eclipse.update.core",1369114139719
started,bundle,org.eclipse.update.scheduler,3.2.300.v20100512,"org.eclipse.update.scheduler",1369114139719
started,bundle,org.eclipse.wst.common.core,1.2.0.v200908252030,"org.eclipse.wst.common.core",1369114139719
started,bundle,org.eclipse.wst.common.emf,1.2.100.v201101101900,"org.eclipse.wst.common.emf",1369114139719
started,bundle,org.eclipse.wst.common.emfworkbench.integration,1.2.101.v201107140600,"org.eclipse.wst.common.emfworkbench.integration",1369114139719
started,bundle,org.eclipse.wst.common.frameworks,1.2.102.v201201190400,"org.eclipse.wst.common.frameworks",1369114139719
started,bundle,org.eclipse.wst.common.frameworks.ui,1.2.100.v201105122000,"org.eclipse.wst.common.frameworks.ui",1369114139719
started,bundle,org.eclipse.wst.common.modulecore,1.2.103.v201201190400,"org.eclipse.wst.common.modulecore",1369114139719
started,bundle,org.eclipse.wst.common.project.facet.core,1.4.201.v201201101553,"org.eclipse.wst.common.project.facet.core",1369114139719
started,bundle,org.eclipse.wst.validation,1.2.303.v201202090300,"org.eclipse.wst.validation",1369114139719
started,bundle,org.eclipse.wst.web,1.1.501.v201201190400,"org.eclipse.wst.web",1369114139719
started,bundle,org.eclipse.wst.xml.core,1.1.602.v201201091944,"org.eclipse.wst.xml.core",1369114139719
started,bundle,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse",1369114139719
os,sysinfo,,,"linux",1369114139724
arch,sysinfo,,,"x86_64",1369114139724
ws,sysinfo,,,"gtk",1369114139724
locale,sysinfo,,,"zh_CN",1369114139724
processors,sysinfo,,,"4",1369114139724
java.runtime.name,sysinfo,,,"OpenJDK Runtime Environment",1369114139724
java.runtime.version,sysinfo,,,"1.6.0_27-b27",1369114139724
java.specification.name,sysinfo,,,"Java Platform API Specification",1369114139724
java.specification.vendor,sysinfo,,,"Sun Microsystems Inc.",1369114139724
java.specification.version,sysinfo,,,"1.6",1369114139724
java.vendor,sysinfo,,,"Sun Microsystems Inc.",1369114139724
java.version,sysinfo,,,"1.6.0_27",1369114139724
java.vm.info,sysinfo,,,"mixed mode",1369114139724
java.vm.name,sysinfo,,,"OpenJDK 64-Bit Server VM",1369114139724
java.vm.specification.name,sysinfo,,,"Java Virtual Machine Specification",1369114139724
java.vm.specification.vendor,sysinfo,,,"Sun Microsystems Inc.",1369114139724
java.vm.specification.version,sysinfo,,,"1.0",1369114139724
java.vm.vendor,sysinfo,,,"Sun Microsystems Inc.",1369114139724
java.vm.version,sysinfo,,,"20.0-b12",1369114139724
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114151562
started,bundle,org.eclipse.ui.browser,3.3.101.v20111019-1723,"org.eclipse.ui.browser",1369114156265
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114161529
opened,view,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse.view.servers",1369114163405
activated,view,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse.view.servers",1369114163426
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.views.showView",1369114163428
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.views.showView",1369114163428
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114163451
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114163459
activated,view,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse.view.servers",1369114165089
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114166586
started,bundle,org.eclipse.jst.ws.jaxws.dom.integration,1.0.101.v201201031437,"org.eclipse.jst.ws.jaxws.dom.integration",1369114198588
started,bundle,org.eclipse.jst.ws.jaxws.dom.ui,1.0.0.v201004171919,"org.eclipse.jst.ws.jaxws.dom.ui",1369114198593
started,bundle,org.eclipse.jst.servlet.ui,1.1.502.v201202011054,"org.eclipse.jst.servlet.ui",1369114198664
started,bundle,org.eclipse.ltk.core.refactoring,3.5.201.r372_v20111101-0700,"org.eclipse.ltk.core.refactoring",1369114537413
started,bundle,org.eclipse.ltk.ui.refactoring,3.6.0.v20110928-1453,"org.eclipse.ltk.ui.refactoring",1369114537414
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114540671
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114727909
started,bundle,org.eclipse.compare.core,3.5.200.I20110208-0800,"org.eclipse.compare.core",1369114731476
opened,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369114731553
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369114731717
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114731772
started,bundle,org.eclipse.jdt.apt.core,3.3.500.v20110420-1015,"org.eclipse.jdt.apt.core",1369114732550
started,bundle,org.eclipse.jdt.apt.pluggable.core,1.0.400.v20110305-1450,"org.eclipse.jdt.apt.pluggable.core",1369114732557
started,bundle,org.eclipse.core.variables,3.2.500.v20110928-1503,"org.eclipse.core.variables",1369114732599
activated,view,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse.view.servers",1369114736020
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114736065
activated,view,org.eclipse.ui.views,3.6.0.v20110928-1505,"org.eclipse.ui.views.ContentOutline",1369114738188
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114738218
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369114739345
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114742501
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114744389
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114767456
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369114935580
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114947410
activated,view,org.apache.hadoop.eclipse,0.18,"org.apache.hadoop.eclipse.view.servers",1369114950539
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114953166
error,log,,,"Unhandled event loop exception",1369114964861
error,log,,,"Unhandled event loop exception",1369114967544
error,log,,,"Unhandled event loop exception",1369114967915
error,log,,,"Unhandled event loop exception",1369114968119
error,log,,,"Unhandled event loop exception",1369114968898
error,log,,,"Unhandled event loop exception",1369114969171
error,log,,,"Unhandled event loop exception",1369114969284
error,log,,,"Unhandled event loop exception",1369114976847
error,log,,,"Unhandled event loop exception",1369114977544
error,log,,,"Unhandled event loop exception",1369114977719
error,log,,,"Unhandled event loop exception",1369114977865
error,log,,,"Unhandled event loop exception",1369114978014
error,log,,,"Unhandled event loop exception",1369114979054
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369114981350
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114981358
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369114991760
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369114994330
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115019562
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115022151
started,bundle,org.eclipse.compare,3.5.202.R37x_v20111109-0800,"org.eclipse.compare",1369115024137
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115027314
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115037628
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115050561
opened,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115050931
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115051070
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369115052548
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115054742
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115058675
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115067936
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115070362
started,bundle,org.eclipse.mylyn.java.tasks,3.6.0.v20110608-1400,"org.eclipse.mylyn.java.tasks",1369115070464
started,bundle,org.eclipse.jdt.junit,3.7.0.v20110928-1453,"org.eclipse.jdt.junit",1369115070981
started,bundle,org.eclipse.pde.launching,3.6.0.v20110506,"org.eclipse.pde.launching",1369115079510
started,bundle,org.eclipse.ui.views.log,1.0.200.v20110404,"org.eclipse.ui.views.log",1369115079513
started,bundle,org.eclipse.pde.ui,3.6.100.v20120103_r372,"org.eclipse.pde.ui",1369115079514
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.file.save",1369115088980
started,bundle,org.eclipse.mylyn.ide.ui,3.6.0.v20110608-1400,"org.eclipse.mylyn.ide.ui",1369115103298
started,bundle,org.eclipse.jpt.jaxb.ui,1.1.2.v201201260000,"org.eclipse.jpt.jaxb.ui",1369115103320
started,bundle,org.eclipse.jpt.jpa.ui,3.0.2.v201201243020,"org.eclipse.jpt.jpa.ui",1369115103333
started,bundle,org.eclipse.jpt.common.core,1.0.2.v201111103010,"org.eclipse.jpt.common.core",1369115103339
started,bundle,org.eclipse.jpt.jpa.core,3.0.2.v201201243020,"org.eclipse.jpt.jpa.core",1369115103342
started,bundle,org.eclipse.jst.ws.jaxws.ui,1.0.100.v201104032053,"org.eclipse.jst.ws.jaxws.ui",1369115103363
started,bundle,org.eclipse.mylyn.java.ui,3.6.0.v20110608-1400,"org.eclipse.mylyn.java.ui",1369115103369
started,bundle,org.eclipse.pde.api.tools,1.0.301.v20111129-2053,"org.eclipse.pde.api.tools",1369115103404
started,bundle,org.eclipse.pde.api.tools.ui,1.0.301.v20110803_r371,"org.eclipse.pde.api.tools.ui",1369115103406
started,bundle,org.eclipse.jpt.jaxb.core,1.0.2.v201111103010,"org.eclipse.jpt.jaxb.core",1369115103782
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115156014
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115186061
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.file.save",1369115189568
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115193636
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115220895
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115222151
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.file.save",1369115242772
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115245207
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115248708
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369115257517
started,bundle,org.eclipse.mylyn.resources.ui,3.6.0.v20110608-1400,"org.eclipse.mylyn.resources.ui",1369115259796
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115263368
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115272324
opened,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115272460
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115272537
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115274554
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115288575
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.delete",1369115292152
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115292888
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.file.save",1369115295411
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115297878
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115327519
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369115335634
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115337059
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115364002
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115370901
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115377793
opened,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115377921
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115377979
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115383083
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.file.save",1369115396237
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115398694
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115421389
activated,view,org.eclipse.ui.navigator.resources,3.4.300.v20110928-1505,"org.eclipse.ui.navigator.ProjectExplorer",1369115442262
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115445896
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115492753
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115502446
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115509292
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115514292
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115519513
opened,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115519781
activated,editor,org.eclipse.jdt.ui,3.7.2.v20120109-1427,"org.eclipse.jdt.ui.CompilationUnitEditor",1369115519868
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115525374
error,log,,,"Exception occurred during compilation unit conversion:\n----------------------------------- SOURCE BEGIN -------------------------------------\npackage ch02;\n\nHadoop: The Definitive Guide\nTom White\nforeword by Doug Cutting\nBeijing • Cambridge • Farnham • Köln • Sebastopol • Taipei • TokyoHadoop: The Definitive Guide\nby Tom White\nCopyright © 2009 Tom White. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\nare also available for most titles (http://my.safaribooksonline.com). For more information, contact our\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\nEditor: Mike Loukides\nProduction Editor: Loranah Dimant\nProofreader: Nancy Kotary\nIndexer: Ellen Troutman Zaig\nCover Designer: Karen Montgomery\nInterior Designer: David Futato\nIllustrator: Robert Romano\nPrinting History:\nJune 2009:\nFirst Edition.\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\nO’Reilly Media, Inc. Hadoop: The Definitive Guide, the image of an African elephant, and related trade\ndress are trademarks of O’Reilly Media, Inc.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\ntrademark claim, the designations have been printed in caps or initial caps.\nWhile every precaution has been taken in the preparation of this book, the publisher and author assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\ntained herein.\nTM\nThis book uses RepKoverTM, a durable and flexible lay-flat binding.\nISBN: 978-0-596-52197-4\n[M]\n1243455573For Eliane, Emilia, and LottieTable of Contents\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\n1. Meet Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nData!\nData Storage and Analysis\nComparison with Other Systems\nRDBMS\nGrid Computing\nVolunteer Computing\nA Brief History of Hadoop\nThe Apache Hadoop Project\n1\n3\n4\n4\n6\n8\n9\n12\n2. MapReduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA Weather Dataset\nData Format\nAnalyzing the Data with Unix Tools\nAnalyzing the Data with Hadoop\nMap and Reduce\nJava MapReduce\nScaling Out\nData Flow\nCombiner Functions\nRunning a Distributed MapReduce Job\nHadoop Streaming\nRuby\nPython\nHadoop Pipes\nCompiling and Running\n15\n15\n17\n18\n18\n20\n27\n27\n29\n32\n32\n33\n35\n36\n38\nv3. The Hadoop Distributed Filesystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nThe Design of HDFS\nHDFS Concepts\nBlocks\nNamenodes and Datanodes\nThe Command-Line Interface\nBasic Filesystem Operations\nHadoop Filesystems\nInterfaces\nThe Java Interface\nReading Data from a Hadoop URL\nReading Data Using the FileSystem API\nWriting Data\nDirectories\nQuerying the Filesystem\nDeleting Data\nData Flow\nAnatomy of a File Read\nAnatomy of a File Write\nCoherency Model\nParallel Copying with distcp\nKeeping an HDFS Cluster Balanced\nHadoop Archives\nUsing Hadoop Archives\nLimitations\n41\n42\n42\n44\n45\n45\n47\n49\n51\n51\n52\n56\n57\n58\n62\n63\n63\n66\n68\n70\n71\n71\n72\n73\n4. Hadoop I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nData Integrity\nData Integrity in HDFS\nLocalFileSystem\nChecksumFileSystem\nCompression\nCodecs\nCompression and Input Splits\nUsing Compression in MapReduce\nSerialization\nThe Writable Interface\nWritable Classes\nImplementing a Custom Writable\nSerialization Frameworks\nFile-Based Data Structures\nSequenceFile\nMapFile\nvi | Table of Contents\n75\n75\n76\n77\n77\n79\n83\n84\n86\n87\n89\n96\n101\n103\n103\n1105. Developing a MapReduce Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nThe Configuration API\nCombining Resources\nVariable Expansion\nConfiguring the Development Environment\nManaging Configuration\nGenericOptionsParser, Tool, and ToolRunner\nWriting a Unit Test\nMapper\nReducer\nRunning Locally on Test Data\nRunning a Job in a Local Job Runner\nTesting the Driver\nRunning on a Cluster\nPackaging\nLaunching a Job\nThe MapReduce Web UI\nRetrieving the Results\nDebugging a Job\nUsing a Remote Debugger\nTuning a Job\nProfiling Tasks\nMapReduce Workflows\nDecomposing a Problem into MapReduce Jobs\nRunning Dependent Jobs\n116\n117\n117\n118\n118\n121\n123\n124\n126\n127\n127\n130\n132\n132\n132\n134\n136\n138\n144\n145\n146\n149\n149\n151\n6. How MapReduce Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nAnatomy of a MapReduce Job Run\nJob Submission\nJob Initialization\nTask Assignment\nTask Execution\nProgress and Status Updates\nJob Completion\nFailures\nTask Failure\nTasktracker Failure\nJobtracker Failure\nJob Scheduling\nThe Fair Scheduler\nShuffle and Sort\nThe Map Side\nThe Reduce Side\n153\n153\n155\n155\n156\n156\n158\n159\n159\n161\n161\n161\n162\n163\n163\n164\nTable of Contents | viiConfiguration Tuning\nTask Execution\nSpeculative Execution\nTask JVM Reuse\nSkipping Bad Records\nThe Task Execution Environment\n166\n168\n169\n170\n171\n172\n7. MapReduce Types and Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nMapReduce Types\nThe Default MapReduce Job\nInput Formats\nInput Splits and Records\nText Input\nBinary Input\nMultiple Inputs\nDatabase Input (and Output)\nOutput Formats\nText Output\nBinary Output\nMultiple Outputs\nLazy Output\nDatabase Output\n175\n178\n184\n185\n196\n199\n200\n201\n202\n202\n203\n203\n210\n210\n8. MapReduce Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\nCounters\nBuilt-in Counters\nUser-Defined Java Counters\nUser-Defined Streaming Counters\nSorting\nPreparation\nPartial Sort\nTotal Sort\nSecondary Sort\nJoins\nMap-Side Joins\nReduce-Side Joins\nSide Data Distribution\nUsing the Job Configuration\nDistributed Cache\nMapReduce Library Classes\n211\n211\n213\n218\n218\n218\n219\n223\n227\n233\n233\n235\n238\n238\n239\n243\n9. Setting Up a Hadoop Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\nCluster Specification\nviii | Table of Contents\n245Network Topology\nCluster Setup and Installation\nInstalling Java\nCreating a Hadoop User\nInstalling Hadoop\nTesting the Installation\nSSH Configuration\nHadoop Configuration\nConfiguration Management\nEnvironment Settings\nImportant Hadoop Daemon Properties\nHadoop Daemon Addresses and Ports\nOther Hadoop Properties\nPost Install\nBenchmarking a Hadoop Cluster\nHadoop Benchmarks\nUser Jobs\nHadoop in the Cloud\nHadoop on Amazon EC2\n247\n249\n249\n250\n250\n250\n251\n251\n252\n254\n258\n263\n264\n266\n266\n267\n269\n269\n269\n10. Administering Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\nHDFS\nPersistent Data Structures\nSafe Mode\nAudit Logging\nTools\nMonitoring\nLogging\nMetrics\nJava Management Extensions\nMaintenance\nRoutine Administration Procedures\nCommissioning and Decommissioning Nodes\nUpgrades\n273\n273\n278\n280\n280\n285\n285\n286\n289\n292\n292\n293\n296\n11. Pig . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nInstalling and Running Pig\nExecution Types\nRunning Pig Programs\nGrunt\nPig Latin Editors\nAn Example\nGenerating Examples\n302\n302\n304\n304\n305\n305\n307\nTable of Contents | ixComparison with Databases\nPig Latin\nStructure\nStatements\nExpressions\nTypes\nSchemas\nFunctions\nUser-Defined Functions\nA Filter UDF\nAn Eval UDF\nA Load UDF\nData Processing Operators\nLoading and Storing Data\nFiltering Data\nGrouping and Joining Data\nSorting Data\nCombining and Splitting Data\nPig in Practice\nParallelism\nParameter Substitution\n308\n309\n310\n311\n314\n315\n317\n320\n322\n322\n325\n327\n331\n331\n331\n334\n338\n339\n340\n340\n341\n12. HBase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\nHBasics\nBackdrop\nConcepts\nWhirlwind Tour of the Data Model\nImplementation\nInstallation\nTest Drive\nClients\nJava\nREST and Thrift\nExample\nSchemas\nLoading Data\nWeb Queries\nHBase Versus RDBMS\nSuccessful Service\nHBase\nUse Case: HBase at streamy.com\nPraxis\nVersions\nx | Table of Contents\n343\n344\n344\n344\n345\n348\n349\n350\n351\n353\n354\n354\n355\n358\n361\n362\n363\n363\n365\n365Love and Hate: HBase and HDFS\nUI\nMetrics\nSchema Design\n366\n367\n367\n367\n13. ZooKeeper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\nInstalling and Running ZooKeeper\nAn Example\nGroup Membership in ZooKeeper\nCreating the Group\nJoining a Group\nListing Members in a Group\nDeleting a Group\nThe ZooKeeper Service\nData Model\nOperations\nImplementation\nConsistency\nSessions\nStates\nBuilding Applications with ZooKeeper\nA Configuration Service\nThe Resilient ZooKeeper Application\nA Lock Service\nMore Distributed Data Structures and Protocols\nZooKeeper in Production\nResilience and Performance\nConfiguration\n370\n371\n372\n372\n374\n376\n378\n378\n379\n380\n384\n386\n388\n389\n391\n391\n394\n398\n400\n401\n401\n402\n14. Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\nHadoop Usage at Last.fm\nLast.fm: The Social Music Revolution\nHadoop at Last.fm\nGenerating Charts with Hadoop\nThe Track Statistics Program\nSummary\nHadoop and Hive at Facebook\nIntroduction\nHadoop at Facebook\nHypothetical Use Case Studies\nHive\nProblems and Future Work\nNutch Search Engine\n405\n405\n405\n406\n407\n414\n414\n414\n414\n417\n420\n424\n425\nTable of Contents | xiBackground\nData Structures\nSelected Examples of Hadoop Data Processing in Nutch\nSummary\nLog Processing at Rackspace\nRequirements/The Problem\nBrief History\nChoosing Hadoop\nCollection and Storage\nMapReduce for Logs\nCascading\nFields, Tuples, and Pipes\nOperations\nTaps, Schemes, and Flows\nCascading in Practice\nFlexibility\nHadoop and Cascading at ShareThis\nSummary\nTeraByte Sort on Apache Hadoop\n425\n426\n429\n438\n439\n439\n440\n440\n440\n442\n447\n448\n451\n452\n454\n456\n457\n461\n461\nA. Installing Apache Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\nB. Cloudera’s Distribution for Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\nC. Preparing the NCDC Weather Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479\nxii | Table of ContentsForeword\nHadoop got its start in Nutch. A few of us were attempting to build an open source\nweb search engine and having trouble managing computations running on even a\nhandful of computers. Once Google published its GFS and MapReduce papers, the\nroute became clear. They’d devised systems to solve precisely the problems we were\nhaving with Nutch. So we started, two of us, half-time, to try to recreate these systems\nas a part of Nutch.\nWe managed to get Nutch limping along on 20 machines, but it soon became clear that\nto handle the Web’s massive scale, we’d need to run it on thousands of machines and,\nmoreover, that the job was bigger than two half-time developers could handle.\nAround that time, Yahoo! got interested, and quickly put together a team that I joined.\nWe split off the distributed computing part of Nutch, naming it Hadoop. With the help\nof Yahoo!, Hadoop soon grew into a technology that could truly scale to the Web.\nIn 2006, Tom White started contributing to Hadoop. I already knew Tom through an\nexcellent article he’d written about Nutch, so I knew he could present complex ideas\nin clear prose. I soon learned that he could also develop software that was as pleasant\nto read as his prose.\nFrom the beginning, Tom’s contributions to Hadoop showed his concern for users and\nfor the project. Unlike most open source contributors, Tom is not primarily interested\nin tweaking the system to better meet his own needs, but rather in making it easier for\nanyone to use.\nInitially, Tom specialized in making Hadoop run well on Amazon’s EC2 and S3 serv-\nices. Then he moved on to tackle a wide variety of problems, including improving the\nMapReduce APIs, enhancing the website, and devising an object serialization frame-\nwork. In all cases, Tom presented his ideas precisely. In short order, Tom earned the\nrole of Hadoop committer and soon thereafter became a member of the Hadoop Project\nManagement Committee.\nTom is now a respected senior member of the Hadoop developer community. Though\nhe’s an expert in many technical corners of the project, his specialty is making Hadoop\neasier to use and understand.\nxiiiGiven this, I was very pleased when I learned that Tom intended to write a book about\nHadoop. Who could be better qualified? Now you have the opportunity to learn about\nHadoop from a master—not only of the technology, but also of common sense and\nplain talk.\n—Doug Cutting\nShed in the Yard, California\nxiv | ForewordPreface\nMartin Gardner, the mathematics and science writer, once said in an interview:\nBeyond calculus, I am lost. That was the secret of my column’s success. It took me so\nlong to understand what I was writing about that I knew how to write in a way most\nreaders would understand.*\nIn many ways, this is how I feel about Hadoop. Its inner workings are complex, resting\nas they do on a mixture of distributed systems theory, practical engineering, and com-\nmon sense. And to the uninitiated, Hadoop can appear alien.\nBut it doesn’t need to be like this. Stripped to its core, the tools that Hadoop provides\nfor building distributed systems—for data storage, data analysis, and coordination—\nare simple. If there’s a common theme, it is about raising the level of abstraction—to\ncreate building blocks for programmers who just happen to have lots of data to store,\nor lots of data to analyze, or lots of machines to coordinate, and who don’t have the\ntime, the skill, or the inclination to become distributed systems experts to build the\ninfrastructure to handle it.\nWith such a simple and generally applicable feature set, it seemed obvious to me when\nI started using it that Hadoop deserved to be widely used. However, at the time (in\nearly 2006), setting up, configuring, and writing programs to use Hadoop was an art.\nThings have certainly improved since then: there is more documentation, there are\nmore examples, and there are thriving mailing lists to go to when you have questions.\nAnd yet the biggest hurdle for newcomers is understanding what this technology is\ncapable of, where it excels, and how to use it. That is why I wrote this book.\nThe Apache Hadoop community has come a long way. Over the course of three years,\nthe Hadoop project has blossomed and spun off half a dozen subprojects. In this time,\nthe software has made great leaps in performance, reliability, scalability, and manage-\nability. To gain even wider adoption, however, I believe we need to make Hadoop even\neasier to use. This will involve writing more tools; integrating with more systems; and\n* “The science of fun,” Alex Bellos, The Guardian, May 31, 2008, http://www.guardian.co.uk/science/\n2008/may/31/maths.science.\nxvwriting new, improved APIs. I’m looking forward to being a part of this, and I hope\nthis book will encourage and enable others to do so, too.\nAdministrative Notes\nDuring discussion of a particular Java class in the text, I often omit its package name,\nto reduce clutter. If you need to know which package a class is in, you can easily look\nit up in Hadoop’s Java API documentation for the relevant subproject, linked to from\nthe Apache Hadoop home page at http://hadoop.apache.org/. Or if you’re using an IDE,\nit can help using its auto-complete mechanism.\nSimilarly, although it deviates from usual style guidelines, program listings that import\nmultiple classes from the same package may use the asterisk wildcard character to save\nspace (for example: import org.apache.hadoop.io.*).\nThe sample programs in this book are available for download from the website that\naccompanies this book: http://www.hadoopbook.com/. You will also find instructions\nthere for obtaining the datasets that are used in examples throughout the book, as well\nas further notes for running the programs in the book, and links to updates, additional\nresources, and my blog.\nWhat’s in This Book?\nThe rest of this book is organized as follows. Chapter 2 provides an introduction to\nMapReduce. Chapter 3 looks at Hadoop filesystems, and in particular HDFS, in depth.\nChapter 4 covers the fundamentals of I/O in Hadoop: data integrity, compression,\nserialization, and file-based data structures.\nThe next four chapters cover MapReduce in depth. Chapter 5 goes through the practical\nsteps needed to develop a MapReduce application. Chapter 6 looks at how MapReduce\nis implemented in Hadoop, from the point of view of a user. Chapter 7 is about the\nMapReduce programming model, and the various data formats that MapReduce can\nwork with. Chapter 8 is on advanced MapReduce topics, including sorting and joining\ndata.\nChapters 9 and 10 are for Hadoop administrators, and describe how to set up and\nmaintain a Hadoop cluster running HDFS and MapReduce.\nChapters 11, 12, and 13 present Pig, HBase, and ZooKeeper, respectively.\nFinally, Chapter 14 is a collection of case studies contributed by members of the Apache\nHadoop community.\nxvi | PrefaceConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, databases, data types, environment variables,\nstatements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter-\nmined by context.\nThis icon signifies a tip, suggestion, or general note.\nThis icon indicates a warning or caution.\nUsing Code Examples\nThis book is here to help you get your job done. In general, you may use the code in\nthis book in your programs and documentation. You do not need to contact us for\npermission unless you’re reproducing a significant portion of the code. For example,\nwriting a program that uses several chunks of code from this book does not require\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\nrequire permission. Answering a question by citing this book and quoting example\ncode does not require permission. Incorporating a significant amount of example code\nfrom this book into your product’s documentation does require permission.\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\nauthor, publisher, and ISBN. For example: “Hadoop: The Definitive Guide, by Tom\nWhite. Copyright 2009 Tom White, 978-0-596-52197-4.”\nIf you feel your use of code examples falls outside fair use or the permission given above,\nfeel free to contact us at permissions@oreilly.com.\nPreface | xviiSafari® Books Online\nWhen you see a Safari® Books Online icon on the cover of your favorite\ntechnology book, that means the book is available online through the\nO’Reilly Network Safari Bookshelf.\nSafari offers a solution that’s better than e-books. It’s a virtual library that lets you easily\nsearch thousands of top tech books, cut and paste code samples, download chapters,\nand find quick answers when you need the most accurate, current information. Try it\nfor free at http://my.safaribooksonline.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at:\nhttp://www.oreilly.com/catalog/9780596521974\nThe author also has a site for this book at:\nhttp://www.hadoopbook.com/\nTo comment or ask technical questions about this book, send email to:\nbookquestions@oreilly.com\nFor more information about our books, conferences, Resource Centers, and the\nO’Reilly Network, see our website at:\nhttp://www.oreilly.com\nAcknowledgments\nI have relied on many people, both directly and indirectly, in writing this book. I would\nlike to thank the Hadoop community, from whom I have learned, and continue to learn,\na great deal.\nIn particular, I would like to thank Michael Stack and Jonathan Gray for writing the\nchapter on HBase. Also thanks go to Adrian Woodhead, Marc de Palol, Joydeep Sen\nSarma, Ashish Thusoo, Andrzej Białecki, Stu Hood, Chris K Wensel, and Owen\nxviii | PrefaceO’Malley for contributing case studies for Chapter 14. Matt Massie and Todd Lipcon\nwrote Appendix B, for which I am very grateful.\nI would like to thank the following reviewers who contributed many helpful suggestions\nand improvements to my drafts: Raghu Angadi, Matt Biddulph, Christophe Bisciglia,\nRyan Cox, Devaraj Das, Alex Dorman, Chris Douglas, Alan Gates, Lars George, Patrick\nHunt, Aaron Kimball, Peter Krey, Hairong Kuang, Simon Maxen, Olga Natkovich,\nBenjamin Reed, Konstantin Shvachko, Allen Wittenauer, Matei Zaharia, and Philip\nZeyliger. Ajay Anand kept the review process flowing smoothly. Philip (“flip”) Kromer\nkindly helped me with the NCDC weather dataset featured in the examples in this book.\nSpecial thanks to Owen O’Malley and Arun C Murthy for explaining the intricacies of\nthe MapReduce shuffle to me. Any errors that remain are, of course, to be laid at my\ndoor.\nI am particularly grateful to Doug Cutting for his encouragement, support, and friend-\nship, and for contributing the foreword.\nThanks also go to the many others with whom I have had conversations or email\ndiscussions over the course of writing the book.\nHalfway through writing this book, I joined Cloudera, and I want to thank my\ncolleagues for being incredibly supportive in allowing me the time to write, and to get\nit finished promptly.\nI am grateful to my editor, Mike Loukides, and his colleagues at O’Reilly for their help\nin the preparation of this book. Mike has been there throughout to answer my ques-\ntions, to read my first drafts, and to keep me on schedule.\nFinally, the writing of this book has been a great deal of work, and I couldn’t have done\nit without the constant support of my family. My wife, Eliane, not only kept the home\ngoing, but also stepped in to help review, edit, and chase case studies. My daughters,\nEmilia and Lottie, have been very understanding, and I’m looking forward to spending\nlots more time with all of them.\nPreface | xixCHAPTER 1\nMeet Hadoop\nIn pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log,\nthey didn’t try to grow a larger ox. We shouldn’t be trying for bigger computers, but for\nmore systems of computers.\n—Grace Hopper\nData!\nWe live in the data age. It’s not easy to measure the total volume of data stored elec-\ntronically, but an IDC estimate put the size of the “digital universe” at 0.18 zettabytes\nin 2006, and is forecasting a tenfold growth by 2011 to 1.8 zettabytes.* A zettabyte is\n1021 bytes, or equivalently one thousand exabytes, one million petabytes, or one billion\nterabytes. That’s roughly the same order of magnitude as one disk drive for every person\nin the world.\nThis flood of data is coming from many sources. Consider the following:†\n• The New York Stock Exchange generates about one terabyte of new trade data per\nday.\n• Facebook hosts approximately 10 billion photos, taking up one petabyte of storage.\n• Ancestry.com, the genealogy site, stores around 2.5 petabytes of data.\n• The Internet Archive stores around 2 petabytes of data, and is growing at a rate of\n20 terabytes per month.\n• The Large Hadron Collider near Geneva, Switzerland, will produce about 15\npetabytes of data per year.\n* From Gantz et al., “The Diverse and Exploding Digital Universe,” March 2008 (http://www.emc.com/\ncollateral/analyst-reports/diverse-exploding-digital-universe.pdf).\n† http://www.intelligententerprise.com/showArticle.jhtml?articleID=207800705, http://mashable.com/2008/10/\n15/facebook-10-billion-photos/, http://blog.familytreemagazine.com/insider/Inside+Ancestrycoms+TopSecret\n+Data+Center.aspx, and http://www.archive.org/about/faqs.php, http://www.interactions.org/cms/?pid=\n1027032.\n1So there’s a lot of data out there. But you are probably wondering how it affects you.\nMost of the data is locked up in the largest web properties (like search engines), or\nscientific or financial institutions, isn’t it? Does the advent of “Big Data,” as it is being\ncalled, affect smaller organizations or individuals?\nI argue that it does. Take photos, for example. My wife’s grandfather was an avid\nphotographer, and took photographs throughout his adult life. His entire corpus of\nmedium format, slide, and 35mm film, when scanned in at high-resolution, occupies\naround 10 gigabytes. Compare this to the digital photos that my family took last year,\nwhich take up about 5 gigabytes of space. My family is producing photographic data\nat 35 times the rate my wife’s grandfather’s did, and the rate is increasing every year as\nit becomes easier to take more and more photos.\nMore generally, the digital streams that individuals are producing are growing apace.\nMicrosoft Research’s MyLifeBits project gives a glimpse of archiving of personal infor-\nmation that may become commonplace in the near future. MyLifeBits was an experi-\nment where an individual’s interactions—phone calls, emails, documents—were cap-\ntured electronically and stored for later access. The data gathered included a photo\ntaken every minute, which resulted in an overall data volume of one gigabyte a\nmonth. When storage costs come down enough to make it feasible to store continuous\naudio and video, the data volume for a future MyLifeBits service will be many times that.\nThe trend is for every individual’s data footprint to grow, but perhaps more importantly\nthe amount of data generated by machines will be even greater than that generated by\npeople. Machine logs, RFID readers, sensor networks, vehicle GPS traces, retail\ntransactions—all of these contribute to the growing mountain of data.\nThe volume of data being made publicly available increases every year too. Organiza-\ntions no longer have to merely manage their own data: success in the future will be\ndictated to a large extent by their ability to extract value from other organizations’ data.\nInitiatives such as Public Data Sets on Amazon Web Services, Infochimps.org, and\ntheinfo.org exist to foster the “information commons,” where data can be freely (or in\nthe case of AWS, for a modest price) shared for anyone to download and analyze.\nMashups between different information sources make for unexpected and hitherto\nunimaginable applications.\nTake, for example, the Astrometry.net project, which watches the Astrometry group\non Flickr for new photos of the night sky. It analyzes each image, and identifies which\npart of the sky it is from, and any interesting celestial bodies, such as stars or galaxies.\nAlthough it’s still a new and experimental service, it shows the kind of things that are\npossible when data (in this case, tagged photographic images) is made available and\nused for something (image analysis) that was not anticipated by the creator.\nIt has been said that “More data usually beats better algorithms,” which is to say that\nfor some problems (such as recommending movies or music based on past preferences),\n2 | Chapter 1: Meet Hadoophowever fiendish your algorithms are, they can often be beaten simply by having more\ndata (and a less sophisticated algorithm).‡\nThe good news is that Big Data is here. The bad news is that we are struggling to store\nand analyze it.\nData Storage and Analysis\nThe problem is simple: while the storage capacities of hard drives have increased mas-\nsively over the years, access speeds—the rate at which data can be read from drives—\nhave not kept up. One typical drive from 1990 could store 1370 MB of data and had a\ntransfer speed of 4.4 MB/s,§ so you could read all the data from a full drive in around\nfive minutes. Almost 20 years later one terabyte drives are the norm, but the transfer\nspeed is around 100 MB/s, so it takes more than two and a half hours to read all the\ndata off the disk.\nThis is a long time to read all data on a single drive—and writing is even slower. The\nobvious way to reduce the time is to read from multiple disks at once. Imagine if we\nhad 100 drives, each holding one hundredth of the data. Working in parallel, we could\nread the data in under two minutes.\nOnly using one hundredth of a disk may seem wasteful. But we can store one hundred\ndatasets, each of which is one terabyte, and provide shared access to them. We can\nimagine that the users of such a system would be happy to share access in return for\nshorter analysis times, and, statistically, that their analysis jobs would be likely to be\nspread over time, so they wouldn’t interfere with each other too much.\nThere’s more to being able to read and write data in parallel to or from multiple disks,\nthough.\nThe first problem to solve is hardware failure: as soon as you start using many pieces\nof hardware, the chance that one will fail is fairly high. A common way of avoiding data\nloss is through replication: redundant copies of the data are kept by the system so that\nin the event of failure, there is another copy available. This is how RAID works, for\ninstance, although Hadoop’s filesystem, the Hadoop Distributed Filesystem (HDFS),\ntakes a slightly different approach, as you shall see later.\nThe second problem is that most analysis tasks need to be able to combine the data in\nsome way; data read from one disk may need to be combined with the data from any\nof the other 99 disks. Various distributed systems allow data to be combined from\nmultiple sources, but doing this correctly is notoriously challenging. MapReduce pro-\nvides a programming model that abstracts the problem from disk reads and writes,\n‡ The quote is from Anand Rajaraman writing about the Netflix Challenge (http://anand.typepad.com/\ndatawocky/2008/03/more-data-usual.html).\n§ These specifications are for the Seagate ST-41600n.\nData Storage and Analysis | 3transforming it into a computation over sets of keys and values. We will look at the\ndetails of this model in later chapters, but the important point for the present discussion\nis that there are two parts to the computation, the map and the reduce, and it’s the\ninterface between the two where the “mixing” occurs. Like HDFS, MapReduce has\nreliability built-in.\nThis, in a nutshell, is what Hadoop provides: a reliable shared storage and analysis\nsystem. The storage is provided by HDFS, and analysis by MapReduce. There are other\nparts to Hadoop, but these capabilities are its kernel.\nComparison with Other Systems\nThe approach taken by MapReduce may seem like a brute-force approach. The premise\nis that the entire dataset—or at least a good portion of it—is processed for each query.\nBut this is its power. MapReduce is a batch query processor, and the ability to run an\nad hoc query against your whole dataset and get the results in a reasonable time is\ntransformative. It changes the way you think about data, and unlocks data that was\npreviously archived on tape or disk. It gives people the opportunity to innovate with\ndata. Questions that took too long to get answered before can now be answered, which\nin turn leads to new questions and new insights.\nFor example, Mailtrust, Rackspace’s mail division, used Hadoop for processing email\nlogs. One ad hoc query they wrote was to find the geographic distribution of their users.\nIn their words:\nThis data was so useful that we’ve scheduled the MapReduce job to run monthly and we\nwill be using this data to help us decide which Rackspace data centers to place new mail\nservers in as we grow.‖\nBy bringing several hundred gigabytes of data together and having the tools to analyze\nit, the Rackspace engineers were able to gain an understanding of the data that they\notherwise would never have had, and, furthermore, they were able to use what they\nhad learned to improve the service for their customers. You can read more about how\nRackspace uses Hadoop in Chapter 14.\nRDBMS\nWhy can’t we use databases with lots of disks to do large-scale batch analysis? Why is\nMapReduce needed?\nThe answer to these questions comes from another trend in disk drives: seek time is\nimproving more slowly than transfer rate. Seeking is the process of moving the disk’s\nhead to a particular place on the disk to read or write data. It characterizes the latency\nof a disk operation, whereas the transfer rate corresponds to a disk’s bandwidth.\n‖ http://blog.racklabs.com/?p=66\n4 | Chapter 1: Meet HadoopIf the data access pattern is dominated by seeks, it will take longer to read or write large\nportions of the dataset than streaming through it, which operates at the transfer rate.\nOn the other hand, for updating a small proportion of records in a database, a tradi-\ntional B-Tree (the data structure used in relational databases, which is limited by the\nrate it can perform seeks) works well. For updating the majority of a database, a B-Tree\nis less efficient than MapReduce, which uses Sort/Merge to rebuild the database.\nIn many ways, MapReduce can be seen as a complement to an RDBMS. (The differences\nbetween the two systems are shown in Table 1-1.) MapReduce is a good fit for problems\nthat need to analyze the whole dataset, in a batch fashion, particularly for ad hoc anal-\nysis. An RDBMS is good for point queries or updates, where the dataset has been in-\ndexed to deliver low-latency retrieval and update times of a relatively small amount of\ndata. MapReduce suits applications where the data is written once, and read many\ntimes, whereas a relational database is good for datasets that are continually updated.\nTable 1-1. RDBMS compared to MapReduce\nTraditional RDBMS MapReduce\nData size Gigabytes Petabytes\nAccess Interactive and batch Batch\nUpdates Read and write many times Write once, read many times\nStructure Static schema Dynamic schema\nIntegrity High Low\nScaling Nonlinear Linear\nAnother difference between MapReduce and an RDBMS is the amount of structure in\nthe datasets that they operate on. Structured data is data that is organized into entities\nthat have a defined format, such as XML documents or database tables that conform\nto a particular predefined schema. This is the realm of the RDBMS. Semi-structured\ndata, on the other hand, is looser, and though there may be a schema, it is often ignored,\nso it may be used only as a guide to the structure of the data: for example, a spreadsheet,\nin which the structure is the grid of cells, although the cells themselves may hold any\nform of data. Unstructured data does not have any particular internal structure: for\nexample, plain text or image data. MapReduce works well on unstructured or semi-\nstructured data, since it is designed to interpret the data at processing time. In other\nwords, the input keys and values for MapReduce are not an intrinsic property of the\ndata, but they are chosen by the person analyzing the data.\nComparison with Other Systems | 5Relational data is often normalized to retain its integrity, and remove redundancy.\nNormalization poses problems for MapReduce, since it makes reading a record a non-\nlocal operation, and one of the central assumptions that MapReduce makes is that it\nis possible to perform (high-speed) streaming reads and writes.\nA web server log is a good example of a set of records that is not normalized (for ex-\nample, the client hostnames are specified in full each time, even though the same client\nmay appear many times), and this is one reason that logfiles of all kinds are particularly\nwell-suited to analysis with MapReduce.\nMapReduce is a linearly scalable programming model. The programmer writes two\nfunctions—a map function and a reduce function—each of which defines a mapping\nfrom one set of key-value pairs to another. These functions are oblivious to the size of\nthe data or the cluster that they are operating on, so they can be used unchanged for a\nsmall dataset and for a massive one. More importantly, if you double the size of the\ninput data, a job will run twice as slow. But if you also double the size of the cluster, a\njob will run as fast as the original one. This is not generally true of SQL queries.\nOver time, however, the differences between relational databases and MapReduce sys-\ntems are likely to blur. Both as relational databases start incorporating some of the ideas\nfrom MapReduce (such as Aster Data’s and Greenplum’s databases), and, from the\nother direction, as higher-level query languages built on MapReduce (such as Pig and\nHive) make MapReduce systems more approachable to traditional database\nprogrammers.#\nGrid Computing\nThe High Performance Computing (HPC) and Grid Computing communities have\nbeen doing large-scale data processing for years, using such APIs as Message Passing\nInterface (MPI). Broadly, the approach in HPC is to distribute the work across a cluster\nof machines, which access a shared filesystem, hosted by a SAN. This works well for\npredominantly compute-intensive jobs, but becomes a problem when nodes need to\naccess larger data volumes (hundreds of gigabytes, the point at which MapReduce really\nstarts to shine), since the network bandwidth is the bottleneck, and compute nodes\nbecome idle.\n# In January 2007, David J. DeWitt and Michael Stonebraker caused a stir by publishing “MapReduce: A major\nstep backwards” (http://www.databasecolumn.com/2008/01/mapreduce-a-major-step-back.html), in which\nthey criticized MapReduce for being a poor substitute for relational databases. Many commentators argued\nthat it was a false comparison (see, for example, Mark C. Chu-Carroll’s “Databases are hammers; MapReduce\nis a screwdriver,” http://scienceblogs.com/goodmath/2008/01/databases_are_hammers_mapreduc.php), and\nDeWitt and Stonebraker followed up with “MapReduce II” (http://www.databasecolumn.com/2008/01/\nmapreduce-continued.html), where they addressed the main topics brought up by others.\n6 | Chapter 1: Meet HadoopMapReduce tries to colocate the data with the compute node, so data access is fast\nsince it is local.* This feature, known as data locality, is at the heart of MapReduce and\nis the reason for its good performance. Recognizing that network bandwidth is the most\nprecious resource in a data center environment (it is easy to saturate network links by\ncopying data around), MapReduce implementations go to great lengths to preserve it\nby explicitly modelling network topology. Notice that this arrangement does not pre-\nclude high-CPU analyses in MapReduce.\nMPI gives great control to the programmer, but requires that he or she explicitly handle\nthe mechanics of the data flow, exposed via low-level C routines and constructs, such\nas sockets, as well as the higher-level algorithm for the analysis. MapReduce operates\nonly at the higher level: the programmer thinks in terms of functions of key and value\npairs, and the data flow is implicit.\nCoordinating the processes in a large-scale distributed computation is a challenge. The\nhardest aspect is gracefully handling partial failure—when you don’t know if a remote\nprocess has failed or not—and still making progress with the overall computation.\nMapReduce spares the programmer from having to think about failure, since the\nimplementation detects failed map or reduce tasks and reschedules replacements on\nmachines that are healthy. MapReduce is able to do this since it is a shared-nothing\narchitecture, meaning that tasks have no dependence on one other. (This is a slight\noversimplification, since the output from mappers is fed to the reducers, but this is\nunder the control of the MapReduce system; in this case, it needs to take more care\nrerunning a failed reducer than rerunning a failed map, since it has to make sure it can\nretrieve the necessary map outputs, and if not, regenerate them by running the relevant\nmaps again.) So from the programmer’s point of view, the order in which the tasks run\ndoesn’t matter. By contrast, MPI programs have to explicitly manage their own check-\npointing and recovery, which gives more control to the programmer, but makes them\nmore difficult to write.\nMapReduce might sound like quite a restrictive programming model, and in a sense it\nis: you are limited to key and value types that are related in specified ways, and mappers\nand reducers run with very limited coordination between one another (the mappers\npass keys and values to reducers). A natural question to ask is: can you do anything\nuseful or nontrivial with it?\nThe answer is yes. MapReduce was invented by engineers at Google as a system for\nbuilding production search indexes because they found themselves solving the same\nproblem over and over again (and MapReduce was inspired by older ideas from the\nfunctional programming, distributed computing, and database communities), but it\nhas since been used for many other applications in many other industries. It is pleasantly\nsurprising to see the range of algorithms that can be expressed in MapReduce, from\n* Jim Gray was an early advocate of putting the computation near the data. See “Distributed Computing\nEconomics,” March 2003, http://research.microsoft.com/apps/pubs/default.aspx?id=70001.\nComparison with Other Systems | 7image analysis, to graph-based problems, to machine learning algorithms.† It can’t\nsolve every problem, of course, but it is a general data-processing tool.\nYou can see a sample of some of the applications that Hadoop has been used for in\nChapter 14.\nVolunteer Computing\nWhen people first hear about Hadoop and MapReduce, they often ask, “How is it\ndifferent from SETI@home?” SETI, the Search for Extra-Terrestrial Intelligence, runs\na project called SETI@home in which volunteers donate CPU time from their otherwise\nidle computers to analyze radio telescope data for signs of intelligent life outside earth.\nSETI@home is the most well-known of many volunteer computing projects; others in-\nclude the Great Internet Mersenne Prime Search (to search for large prime numbers)\nand Folding@home (to understand protein folding, and how it relates to disease).\nVolunteer computing projects work by breaking the problem they are trying to solve\ninto chunks called work units, which are sent to computers around the world to be\nanalyzed. For example, a SETI@home work unit is about 0.35 MB of radio telescope\ndata, and takes hours or days to analyze on a typical home computer. When the analysis\nis completed, the results are sent back to the server, and the client gets another work\nunit. As a precaution to combat cheating, each work unit is sent to three different\nmachines, and needs at least two results to agree to be accepted.\nAlthough SETI@home may be superficially similar to MapReduce (breaking a problem\ninto independent pieces to be worked on in parallel), there are some significant differ-\nences. The SETI@home problem is very CPU-intensive, which makes it suitable for\nrunning on hundreds of thousands of computers across the world,‡ since the time to\ntransfer the work unit is dwarfed by the time to run the computation on it. Volunteers\nare donating CPU cycles, not bandwidth.\nMapReduce is designed to run jobs that last minutes or hours on trusted, dedicated\nhardware running in a single data center with very high aggregate bandwidth inter-\nconnects. By contrast, SETI@home runs a perpetual computation on untrusted ma-\nchines on the Internet with highly variable connection speeds and no data locality.\n† Apache Mahout (http://lucene.apache.org/mahout/) is a project to build machine learning libraries (such as\nclassification and clustering algorithms) that run on Hadoop.\n‡ In January 2008, SETI@home was reported at http://www.planetary.org/programs/projects/setiathome/\nsetiathome_20080115.html to be processing 300 gigabytes a day, using 320,000 computers (most of which\nare not dedicated to SETI@home; they are used for other things, too).\n8 | Chapter 1: Meet HadoopA Brief History of Hadoop\nHadoop was created by Doug Cutting, the creator of Apache Lucene, the widely used\ntext search library. Hadoop has its origins in Apache Nutch, an open source web search\nengine, itself a part of the Lucene project.\nThe Origin of the Name “Hadoop”\nThe name Hadoop is not an acronym; it’s a made-up name. The project’s creator, Doug\nCutting, explains how the name came about:\nThe name my kid gave a stuffed yellow elephant. Short, relatively easy to spell and\npronounce, meaningless, and not used elsewhere: those are my naming criteria.\nKids are good at generating such. Googol is a kid’s term.\nSubprojects and “contrib” modules in Hadoop also tend to have names that are unre-\nlated to their function, often with an elephant or other animal theme (“Pig,” for exam-\nple). Smaller components are given more descriptive (and therefore more mundane)\nnames. This is a good principle, as it means you can generally work out what something\ndoes from its name. For example, the jobtracker§ keeps track of MapReduce jobs.\nBuilding a web search engine from scratch was an ambitious goal, for not only is the\nsoftware required to crawl and index websites complex to write, but it is also a challenge\nto run without a dedicated operations team, since there are so many moving parts. It’s\nexpensive too: Mike Cafarella and Doug Cutting estimated a system supporting a 1-\nbillion-page index would cost around half a million dollars in hardware, with a monthly\nrunning cost of $30,000.‖ Nevertheless, they believed it was a worthy goal, as it would\nopen up and ultimately democratize search engine algorithms.\nNutch was started in 2002, and a working crawler and search system quickly emerged.\nHowever, they realized that their architecture wouldn’t scale to the billions of pages on\nthe Web. Help was at hand with the publication of a paper in 2003 that described the\narchitecture of Google’s distributed filesystem, called GFS, which was being used in\nproduction at Google.# GFS, or something like it, would solve their storage needs for\nthe very large files generated as a part of the web crawl and indexing process. In par-\nticular, GFS would free up time being spent on administrative tasks such as managing\nstorage nodes. In 2004, they set about writing an open source implementation, the\nNutch Distributed Filesystem (NDFS).\n§ In this book, we use the lowercase form, “jobtracker,” to denote the entity when it’s being referred to\ngenerally, and the CamelCase form JobTracker to denote the Java class that implements it.\n‖ Mike Cafarella and Doug Cutting, “Building Nutch: Open Source Search,” ACM Queue, April 2004, http://\nqueue.acm.org/detail.cfm?id=988408.\n# Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, “The Google File System,” October 2003, http:\n//labs.google.com/papers/gfs.html.\nA Brief History of Hadoop | 9In 2004, Google published the paper that introduced MapReduce to the world.* Early\nin 2005, the Nutch developers had a working MapReduce implementation in Nutch,\nand by the middle of that year all the major Nutch algorithms had been ported to run\nusing MapReduce and NDFS.\nNDFS and the MapReduce implementation in Nutch were applicable beyond the realm\nof search, and in February 2006 they moved out of Nutch to form an independent\nsubproject of Lucene called Hadoop. At around the same time, Doug Cutting joined\nYahoo!, which provided a dedicated team and the resources to turn Hadoop into a\nsystem that ran at web scale (see sidebar). This was demonstrated in February 2008\nwhen Yahoo! announced that its production search index was being generated by a\n10,000-core Hadoop cluster.†\nIn January 2008, Hadoop was made its own top-level project at Apache, confirming its\nsuccess and its diverse, active community. By this timem Hadoop was being used by\nmany other companies besides Yahoo!, such as Last.fm, Facebook, and the New York\nTimes (some applications are covered in the case studies in Chapter 14 and on the\nHadoop wiki.\nIn one well-publicized feat, the New York Times used Amazon’s EC2 compute cloud\nto crunch through four terabytes of scanned archives from the paper converting them\nto PDFs for the Web.‡ The processing took less than 24 hours to run using 100 ma-\nchines, and the project probably wouldn’t have been embarked on without the com-\nbination of Amazon’s pay-by-the-hour model (which allowed the NYT to access a large\nnumber of machines for a short period), and Hadoop’s easy-to-use parallel program-\nming model.\nIn April 2008, Hadoop broke a world record to become the fastest system to sort a\nterabyte of data. Running on a 910-node cluster, Hadoop sorted one terabyte in 209\nseconds (just under 31⁄2 minutes), beating the previous year’s winner of 297 seconds\n(described in detail in “TeraByte Sort on Apache Hadoop” on page 461). In November\nof the same year, Google reported that its MapReduce implementation sorted one ter-\nabyte in 68 seconds.§ As this book was going to press (May 2009), it was announced\nthat a team at Yahoo! used Hadoop to sort one terabyte in 62 seconds.\n* Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters ,” December\n2004, http://labs.google.com/papers/mapreduce.html.\n† “Yahoo! Launches World’s Largest Hadoop Production Application,” 19 February 2008, http://developer\n.yahoo.net/blogs/hadoop/2008/02/yahoo-worlds-largest-production-hadoop.html.\n‡ Derek Gottfrid, “Self-service, Prorated Super Computing Fun!,” 1 November 2007, http://open.blogs.nytimes\n.com/2007/11/01/self-service-prorated-super-computing-fun/.\n§ “Sorting 1PB with MapReduce,” 21 November 2008, http://googleblog.blogspot.com/2008/11/sorting-1pb\n-with-mapreduce.html.\n10 | Chapter 1: Meet HadoopHadoop at Yahoo!\nBuilding Internet-scale search engines requires huge amounts of data and therefore\nlarge numbers of machines to process it. Yahoo! Search consists of four primary com-\nponents: the Crawler, which downloads pages from web servers; the WebMap, which\nbuilds a graph of the known Web; the Indexer, which builds a reverse index to the best\npages; and the Runtime, which answers users’ queries. The WebMap is a graph that\nconsists of roughly 1 trillion (1012) edges each representing a web link and 100 billion\n(1011) nodes each representing distinct URLs. Creating and analyzing such a large graph\nrequires a large number of computers running for many days. In early 2005, the infra-\nstructure for the WebMap, named Dreadnaught, needed to be redesigned to scale up\nto more nodes. Dreadnaught had successfully scaled from 20 to 600 nodes, but required\na complete redesign to scale up further. Dreadnaught is similar to MapReduce in many\nways, but provides more flexibility and less structure. In particular, each fragment in a\nDreadnaught job can send output to each of the fragments in the next stage of the job,\nbut the sort was all done in library code. In practice, most of the WebMap phases were\npairs that corresponded to MapReduce. Therefore, the WebMap applications would\nnot require extensive refactoring to fit into MapReduce.\nEric Baldeschwieler (Eric14) created a small team and we starting designing and\nprototyping a new framework written in C++ modeled after GFS and MapReduce to\nreplace Dreadnaught. Although the immediate need was for a new framework for\nWebMap, it was clear that standardization of the batch platform across Yahoo! Search\nwas critical and by making the framework general enough to support other users, we\ncould better leverage investment in the new platform.\nAt the same time, we were watching Hadoop, which was part of Nutch, and its progress.\nIn January 2006, Yahoo! hired Doug Cutting, and a month later we decided to abandon\nour prototype and adopt Hadoop. The advantage of Hadoop over our prototype and\ndesign was that it was already working with a real application (Nutch) on 20 nodes.\nThat allowed us to bring up a research cluster two months later and start helping real\ncustomers use the new framework much sooner than we could have otherwise. Another\nadvantage, of course, was that since Hadoop was already open source, it was easier\n(although far from easy!) to get permission from Yahoo!’s legal department to work in\nopen source. So we set up a 200-node cluster for the researchers in early 2006 and put\nthe WebMap conversion plans on hold while we supported and improved Hadoop for\nthe research users.\nHere’s a quick timeline of how things have progressed:\n• 2004—Initial versions of what is now Hadoop Distributed Filesystem and Map-\nReduce implemented by Doug Cutting and Mike Cafarella.\n• December 2005—Nutch ported to the new framework. Hadoop runs reliably on\n20 nodes.\n• January 2006—Doug Cutting joins Yahoo!.\n• February 2006—Apache Hadoop project officially started to support the stand-\nalone development of MapReduce and HDFS.\nA Brief History of Hadoop | 11• February 2006—Adoption of Hadoop by Yahoo! Grid team.\n• April 2006—Sort benchmark (10 GB/node) run on 188 nodes in 47.9 hours.\n• May 2006—Yahoo! set up a Hadoop research cluster—300 nodes.\n• May 2006—Sort benchmark run on 500 nodes in 42 hours (better hardware than\nApril benchmark).\n• October 2006—Research cluster reaches 600 nodes.\n• December 2006—Sort benchmark run on 20 nodes in 1.8 hours, 100 nodes in 3.3\nhours, 500 nodes in 5.2 hours, 900 nodes in 7.8 hours.\n• January 2007—Research cluster reaches 900 nodes.\n• April 2007—Research clusters—2 clusters of 1000 nodes.\n• April 2008—Won the 1 terabyte sort benchmark in 209 seconds on 900 nodes.\n• October 2008—Loading 10 terabytes of data per a day on to research clusters.\n• March 2009—17 clusters with a total of 24,000 nodes.\n• April 2009—Won the minute sort by sorting 500 GB in 59 seconds (on 1400 nodes)\nand the 100 terabyte sort in 173 minutes (on 3400 nodes).\n—Owen O’Malley\nThe Apache Hadoop Project\nToday, Hadoop is a collection of related subprojects that fall under the umbrella of\ninfrastructure for distributed computing. These projects are hosted by the Apache Soft\nware Foundation, which provides support for a community of open source software\nprojects. Although Hadoop is best known for MapReduce and its distributed filesystem\n(HDFS, renamed from NDFS), the other subprojects provide complementary services,\nor build on the core to add higher-level abstractions. The subprojects, and where they\nsit in the technology stack, are shown in Figure 1-1 and described briefly here:\nCore\nA set of components and interfaces for distributed filesystems and general I/O\n(serialization, Java RPC, persistent data structures).\nAvro\nA data serialization system for efficient, cross-language RPC, and persistent data\nstorage. (At the time of this writing, Avro had been created only as a new subpro-\nject, and no other Hadoop subprojects were using it yet.)\nMapReduce\nA distributed data processing model and execution environment that runs on large\nclusters of commodity machines.\nHDFS\nA distributed filesystem that runs on large clusters of commodity machines.\n12 | Chapter 1: Meet HadoopPig\nA data flow language and execution environment for exploring very large datasets.\nPig runs on HDFS and MapReduce clusters.\nHBase\nA distributed, column-oriented database. HBase uses HDFS for its underlying\nstorage, and supports both batch-style computations using MapReduce and point\nqueries (random reads).\nZooKeeper\nA distributed, highly available coordination service. ZooKeeper provides primitives\nsuch as distributed locks that can be used for building distributed applications.\nHive\nA distributed data warehouse. Hive manages data stored in HDFS and provides a\nquery language based on SQL (and which is translated by the runtime engine to\nMapReduce jobs) for querying the data.\nChukwa\nA distributed data collection and analysis system. Chukwa runs collectors that\nstore data in HDFS, and it uses MapReduce to produce reports. (At the time of this\nwriting, Chukwa had only recently graduated from a “contrib” module in Core to\nits own subproject.)\nFigure 1-1. Hadoop subprojects\nThe Apache Hadoop Project | 13CHAPTER 2\nMapReduce\nMapReduce is a programming model for data processing. The model is simple, yet not\ntoo simple to express useful programs in. Hadoop can run MapReduce programs writ-\nten in various languages; in this chapter, we shall look at the same program expressed\nin Java, Ruby, Python, and C++. Most importantly, MapReduce programs are inher-\nently parallel, thus putting very large-scale data analysis into the hands of anyone with\nenough machines at their disposal. MapReduce comes into its own for large datasets,\nso let’s start by looking at one.\nA Weather Dataset\nFor our example, we will write a program that mines weather data. Weather sensors\ncollecting data every hour at many locations across the globe gather a large volume of\nlog data, which is a good candidate for analysis with MapReduce, since it is semi-\nstructured and record-oriented.\nData Format\nThe data we will use is from the National Climatic Data Center (NCDC, http://www\n.ncdc.noaa.gov/). The data is stored using a line-oriented ASCII format, in which each\nline is a record. The format supports a rich set of meteorological elements, many of\nwhich are optional or with variable data lengths. For simplicity, we shall focus on the\nbasic elements, such as temperature, which are always present and are of fixed width.\nExample 2-1 shows a sample line with some of the salient fields highlighted. The line\nhas been split into multiple lines to show each field: in the real file, fields are packed\ninto one line with no delimiters.\n15Example 2-1. Format of a National Climate Data Center record\n0057\n332130\n99999\n19500101\n0300\n4\n+51317\n+028783\nFM-12\n+0171\n99999\nV020\n320\n1\nN\n0072\n1\n00450\n1\nC\nN\n010000\n1\nN\n9\n-0128\n1\n-0139\n1\n10268\n1\n#\n#\n#\n#\nUSAF weather station identifier\nWBAN weather station identifier\nobservation date\nobservation time\n# latitude (degrees x 1000)\n# longitude (degrees x 1000)\n# elevation (meters)\n# wind direction (degrees)\n# quality code\n# sky ceiling height (meters)\n# quality code\n# visibility distance (meters)\n# quality code\n#\n#\n#\n#\n#\n#\nair temperature (degrees Celsius x 10)\nquality code\ndew point temperature (degrees Celsius x 10)\nquality code\natmospheric pressure (hectopascals x 10)\nquality code\nData files are organized by date and weather station. There is a directory for each year\nfrom 1901 to 2001, each containing a gzipped file for each weather station with its\nreadings for that year. For example, here are the first entries for 1990:\n% ls raw/1990 | head\n010010-99999-1990.gz\n010014-99999-1990.gz\n010015-99999-1990.gz\n010016-99999-1990.gz\n010017-99999-1990.gz\n010030-99999-1990.gz\n010040-99999-1990.gz\n010080-99999-1990.gz\n010100-99999-1990.gz\n010150-99999-1990.gz\nSince there are tens of thousands of weather stations, the whole dataset is made up of\na large number of relatively small files. It’s generally easier and more efficient to process\na smaller number of relatively large files, so the data was preprocessed so that each\n16 | Chapter 2: MapReduceyear’s readings were concatenated into a single file. (The means by which this was\ncarried out is described in Appendix C.)\nAnalyzing the Data with Unix Tools\nWhat’s the highest recorded global temperature for each year in the dataset? We will\nanswer this first without using Hadoop, as this information will provide a performance\nbaseline, as well as a useful means to check our results.\nThe classic tool for processing line-oriented data is awk. Example 2-2 is a small script\nto calculate the maximum temperature for each year.\nExample 2-2. A program for finding the maximum recorded temperature by year from NCDC weather\nrecords\n#!/usr/bin/env bash\nfor year in all/*\ndo\necho -ne `basename $year .gz`""\\t""\ngunzip -c $year | \\\nawk '{ temp = substr($0, 88, 5) + 0;\nq = substr($0, 93, 1);\nif (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp }\nEND { print max }'\ndone\nThe script loops through the compressed year files, first printing the year, and then\nprocessing each file using awk. The awk script extracts two fields from the data: the air\ntemperature and the quality code. The air temperature value is turned into an integer\nby adding 0. Next, a test is applied to see if the temperature is valid (the value 9999\nsignifies a missing value in the NCDC dataset), and if the quality code indicates that\nthe reading is not suspect or erroneous. If the reading is OK, the value is compared with\nthe maximum value seen so far, which is updated if a new maximum is found. The\nEND block is executed after all the lines in the file have been processed, and prints the\nmaximum value.\nHere is the beginning of a run:\n% ./max_temperature.sh\n1901\n317\n1902\n244\n1903\n289\n1904\n256\n1905\n283\n...\nThe temperature values in the source file are scaled by a factor of 10, so this works out\nas a maximum temperature of 31.7°C for 1901 (there were very few readings at the\nbeginning of the century, so this is plausible). The complete run for the century took\n42 minutes in one run on a single EC2 High-CPU Extra Large Instance.\nAnalyzing the Data with Unix Tools | 17To speed up the processing, we need to run parts of the program in parallel. In theory,\nthis is straightforward: we could process different years in different processes, using all\nthe available hardware threads on a machine. There are a few problems with this,\nhowever.\nFirst, dividing the work into equal-size pieces isn’t always easy or obvious. In this case,\nthe file size for different years varies widely, so some processes will finish much earlier\nthan others. Even if they pick up further work, the whole run is dominated by the\nlongest file. An alternative approach is to split the input into fixed-size chunks and\nassign each chunk to a process.\nSecond, combining the results from independent processes can need further processing.\nIn this case, the result for each year is independent of other years and may be combined\nby concatenating all the results, and sorting by year. If using the fixed-size chunk ap-\nproach, the combination is more delicate. For this example, data for a particular year\nwill typically be split into several chunks, each processed independently. We’ll end up\nwith the maximum temperature for each chunk, so the final step is to look for the\nhighest of these maximums, for each year.\nThird, you are still limited by the processing capacity of a single machine. If the best\ntime you can achieve is 20 minutes with the number of processors you have, then that’s\nit. You can’t make it go faster. Also, some datasets grow beyond the capacity of a single\nmachine. When we start using multiple machines, a whole host of other factors come\ninto play, mainly falling in the category of coordination and reliability. Who runs the\noverall job? How do we deal with failed processes?\nSo, though it’s feasible to parallelize the processing, in practice it’s messy. Using a\nframework like Hadoop to take care of these issues is a great help.\nAnalyzing the Data with Hadoop\nTo take advantage of the parallel processing that Hadoop provides, we need to express\nour query as a MapReduce job. After some local, small-scale testing, we will be able to\nrun it on a cluster of machines.\nMap and Reduce\nMapReduce works by breaking the processing into two phases: the map phase and the\nreduce phase. Each phase has key-value pairs as input and output, the types of which\nmay be chosen by the programmer. The programmer also specifies two functions: the\nmap function and the reduce function.\nThe input to our map phase is the raw NCDC data. We choose a text input format that\ngives us each line in the dataset as a text value. The key is the offset of the beginning\nof the line from the beginning of the file, but as we have no need for this, we ignore it.\n18 | Chapter 2: MapReduceOur map function is simple. We pull out the year and the air temperature, since these\nare the only fields we are interested in. In this case, the map function is just a data\npreparation phase, setting up the data in such a way that the reducer function can do\nits work on it: finding the maximum temperature for each year. The map function is\nalso a good place to drop bad records: here we filter out temperatures that are missing,\nsuspect, or erroneous.\nTo visualize the way the map works, consider the following sample lines of input data\n(some unused columns have been dropped to fit the page, indicated by ellipses):\n0067011990999991950051507004...9999999N9+00001+99999999999...\n0043011990999991950051512004...9999999N9+00221+99999999999...\n0043011990999991950051518004...9999999N9-00111+99999999999...\n0043012650999991949032412004...0500001N9+01111+99999999999...\n0043012650999991949032418004...0500001N9+00781+99999999999...\nThese lines are presented to the map function as the key-value pairs:\n(0, 0067011990999991950051507004...9999999N9+00001+99999999999...)\n(106, 0043011990999991950051512004...9999999N9+00221+99999999999...)\n(212, 0043011990999991950051518004...9999999N9-00111+99999999999...)\n(318, 0043012650999991949032412004...0500001N9+01111+99999999999...)\n(424, 0043012650999991949032418004...0500001N9+00781+99999999999...)\nThe keys are the line offsets within the file, which we ignore in our map function. The\nmap function merely extracts the year and the air temperature (indicated in bold text),\nand emits them as its output. (The temperature values have been interpreted as\nintegers.)\n(1950,\n(1950,\n(1950,\n(1949,\n(1949,\n0)\n22)\n−11)\n111)\n78)\nThe output from the map function is processed by the MapReduce framework before\nbeing sent to the reduce function. This processing sorts and groups the key-value pairs\nby key. So, continuing the example, our reduce function sees the following input:\n(1949, [111, 78])\n(1950, [0, 22, −11])\nEach year appears with a list of all its air temperature readings. All the reduce function\nhas to do now is iterate through the list and pick up the maximum reading:\n(1949, 111)\n(1950, 22)\nThis is the final output: the maximum global temperature recorded in each year.\nThe whole data flow is illustrated in Figure 2-1. At the bottom of the diagram is a Unix\npipeline, which mimics the whole MapReduce flow, and which we will see again later\nin the chapter when we look at Hadoop Streaming.\nAnalyzing the Data with Hadoop | 19Figure 2-1. MapReduce logical data flow\nJava MapReduce\nHaving run through how the MapReduce program works, the next step is to express it\nin code. We need three things: a map function, a reduce function, and some code to\nrun the job. The map function is represented by an implementation of the Mapper\ninterface, which declares a map() method. Example 2-3 shows the implementation of\nour map function.\nExample 2-3. Mapper for maximum temperature example\nimport java.io.IOException;\nimport\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.LongWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.MapReduceBase;\norg.apache.hadoop.mapred.Mapper;\norg.apache.hadoop.mapred.OutputCollector;\norg.apache.hadoop.mapred.Reporter;\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate static final int MISSING = 9999;\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature;\nif (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs\nairTemperature = Integer.parseInt(line.substring(88, 92));\n} else {\nairTemperature = Integer.parseInt(line.substring(87, 92));\n}\nString quality = line.substring(92, 93);\nif (airTemperature != MISSING && quality.matches(""[01459]"")) {\noutput.collect(new Text(year), new IntWritable(airTemperature));\n}\n20 | Chapter 2: MapReduceThe Mapper interface is a generic type, with four formal type parameters that specify the\ninput key, input value, output key, and output value types of the map function. For the\npresent example, the input key is a long integer offset, the input value is a line of text,\nthe output key is a year, and the output value is an air temperature (an integer). Rather\nthan use built-in Java types, Hadoop provides its own set of basic types that are opti-\nmized for network serialization. These are found in the org.apache.hadoop.io package.\nHere we use LongWritable, which corresponds to a Java Long, Text (like Java String),\nand IntWritable (like Java Integer).\nThe map() method is passed a key and a value. We convert the Text value containing\nthe line of input into a Java String, then use its substring() method to extract the\ncolumns we are interested in.\nThe map() method also provides an instance of OutputCollector to write the output to.\nIn this case, we write the year as a Text object (since we are just using it as a key), and\nthe temperature wrapped in an IntWritable. We write an output record only if the\ntemperature is present and the quality code indicates the temperature reading is OK.\nThe reduce function is similarly defined using a Reducer, as illustrated in Example 2-4.\nExample 2-4. Reducer for maximum temperature example\nimport java.io.IOException;\nimport java.util.Iterator;\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.MapReduceBase;\norg.apache.hadoop.mapred.OutputCollector;\norg.apache.hadoop.mapred.Reducer;\norg.apache.hadoop.mapred.Reporter;\npublic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nint maxValue = Integer.MIN_VALUE;\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(key, new IntWritable(maxValue));\nAgain, four formal type parameters are used to specify the input and output types, this\ntime for the reduce function. The input types of the reduce function must match the\noutput type of the map function: Text and IntWritable. And in this case, the output\ntypes of the reduce function are Text and IntWritable, for a year and its maximum\nAnalyzing the Data with Hadoop | 21temperature, which we find by iterating through the temperatures and comparing each\nwith a record of the highest found so far.\nThe third piece of code runs the MapReduce job (see Example 2-5).\nExample 2-5. Application to find the maximum temperature in the weather dataset\nimport java.io.IOException;\nimport\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.fs.Path;\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.FileInputFormat;\norg.apache.hadoop.mapred.FileOutputFormat;\norg.apache.hadoop.mapred.JobClient;\norg.apache.hadoop.mapred.JobConf;\npublic class MaxTemperature {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperature <input path> <output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperature.class);\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\n}\n}\nJobClient.runJob(conf);\nA JobConf object forms the specification of the job. It gives you control over how the\njob is run. When we run this job on a Hadoop cluster, we will package the code into a\nJAR file (which Hadoop will distribute round the cluster). Rather than explicitly specify\nthe name of the JAR file, we can pass a class in the JobConf constructor, which Hadoop\nwill use to locate the relevant JAR file by looking for the JAR file containing this class.\nHaving constructed a JobConf object, we specify the input and output paths. An input\npath is specified by calling the static addInputPath() method on FileInputFormat, and\nit can be a single file, a directory (in which case, the input forms all the files in that\ndirectory), or a file pattern. As the name suggests, addInputPath() can be called more\nthan once to use input from multiple paths.\n22 | Chapter 2: MapReduceThe output path (of which there is only one) is specified by the static setOutput\nPath() method on FileOutputFormat. It specifies a directory where the output files from\nthe reducer functions are written. The directory shouldn’t exist before running the job,\nas Hadoop will complain and not run the job. This precaution is to prevent data loss\n(it can be very annoying to accidentally overwrite the output of a long job with\nanother).\nNext, we specify the map and reduce types to use via the setMapperClass() and\nsetReducerClass() methods.\nThe setOutputKeyClass() and setOutputValueClass() methods control the output types\nfor the map and the reduce functions, which are often the same, as they are in our case.\nIf they are different, then the map output types can be set using the methods\nsetMapOutputKeyClass() and setMapOutputValueClass().\nThe input types are controlled via the input format, which we have not explicitly set\nsince we are using the default TextInputFormat.\nAfter setting the classes that define the map and reduce functions, we are ready to run\nthe job. The static runJob() method on JobClient submits the job and waits for it to\nfinish, writing information about its progress to the console.\nA test run\nAfter writing a MapReduce job, it’s normal to try it out on a small dataset to flush out\nany immediate problems with the code. First install Hadoop in standalone mode—\nthere are instructions for how to do this in Appendix A. This is the mode in which\nHadoop runs using the local filesystem with a local job runner. Let’s test it on the five-\nline sample discussed earlier (the output has been slightly reformatted to fit the page):\n% export HADOOP_CLASSPATH=build/classes\n% hadoop MaxTemperature input/ncdc/sample.txt output\n09/04/07 12:34:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=Job\nTracker, sessionId=\n09/04/07 12:34:35 WARN mapred.JobClient: Use GenericOptionsParser for parsing the\narguments. Applications should implement Tool for the same.\n09/04/07 12:34:35 WARN mapred.JobClient: No job jar file set. User classes may not\nbe found. See JobConf(Class) or JobConf#setJar(String).\n09/04/07 12:34:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n09/04/07 12:34:35 INFO mapred.JobClient: Running job: job_local_0001\n09/04/07 12:34:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n09/04/07 12:34:35 INFO mapred.MapTask: numReduceTasks: 1\n09/04/07 12:34:35 INFO mapred.MapTask: io.sort.mb = 100\n09/04/07 12:34:35 INFO mapred.MapTask: data buffer = 79691776/99614720\n09/04/07 12:34:35 INFO mapred.MapTask: record buffer = 262144/327680\n09/04/07 12:34:35 INFO mapred.MapTask: Starting flush of map output\n09/04/07 12:34:36 INFO mapred.MapTask: Finished spill 0\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task:attempt_local_0001_m_000000_0 is\ndone. And is in the process of commiting\n09/04/07 12:34:36 INFO mapred.LocalJobRunner: file:/Users/tom/workspace/htdg/input/n\ncdc/sample.txt:0+529\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task 'attempt_local_0001_m_000000_0' done.\nAnalyzing the Data with Hadoop | 2309/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.Merger: Merging 1 sorted segments\n09/04/07 12:34:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments\nleft of total size: 57 bytes\n09/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task:attempt_local_0001_r_000000_0 is done\n. And is in the process of commiting\n09/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task attempt_local_0001_r_000000_0 is\nallowed to commit now\n09/04/07 12:34:36 INFO mapred.FileOutputCommitter: Saved output of task\n'attempt_local_0001_r_000000_0' to file:/Users/tom/workspace/htdg/output\n09/04/07 12:34:36 INFO mapred.LocalJobRunner: reduce > reduce\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task 'attempt_local_0001_r_000000_0' done.\n09/04/07 12:34:36 INFO mapred.JobClient: map 100% reduce 100%\n09/04/07 12:34:36 INFO mapred.JobClient: Job complete: job_local_0001\n09/04/07 12:34:36 INFO mapred.JobClient: Counters: 13\n09/04/07 12:34:36 INFO mapred.JobClient: FileSystemCounters\n09/04/07 12:34:36 INFO mapred.JobClient:\nFILE_BYTES_READ=27571\n09/04/07 12:34:36 INFO mapred.JobClient:\nFILE_BYTES_WRITTEN=53907\n09/04/07 12:34:36 INFO mapred.JobClient: Map-Reduce Framework\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce input groups=2\n09/04/07 12:34:36 INFO mapred.JobClient:\nCombine output records=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap input records=5\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce shuffle bytes=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce output records=2\n09/04/07 12:34:36 INFO mapred.JobClient:\nSpilled Records=10\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap output bytes=45\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap input bytes=529\n09/04/07 12:34:36 INFO mapred.JobClient:\nCombine input records=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap output records=5\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce input records=5\nWhen the hadoop command is invoked with a classname as the first argument, it\nlaunches a JVM to run the class. It is more convenient to use hadoop than straight\njava since the former adds the Hadoop libraries (and their dependencies) to the class-\npath, and picks up the Hadoop configuration too. To add the application classes to the\nclasspath, we’ve defined an environment variable called HADOOP_CLASSPATH, which the\nhadoop script picks up.\nWhen running in local (standalone) mode, the programs in this book\nall assume that you have set the HADOOP_CLASSPATH in this way. The com-\nmands should be run from the directory that the example code is in-\nstalled in.\nThe output from running the job provides some useful information. (The warning\nabout the job JAR file not being found is expected, since we are running in local mode\nwithout a JAR. We won’t see this warning when we run on a cluster.) For example, we\ncan see that the job was given an ID of job_local_0001, and it ran one map task and\none reduce task (with the IDs attempt_local_0001_m_000000_0 and\n24 | Chapter 2: MapReduceattempt_local_0001_r_000000_0). Knowing the job and task IDs can be very useful when\ndebugging MapReduce jobs.\nThe last section of the output, entitled “Counters,” shows the statistics that Hadoop\ngenerates for each job it runs. These are very useful for checking whether the amount\nof data processed is what you expected. For example, we can follow the number of\nrecords that went through the system: five map inputs produced five map outputs, then\nfive reduce inputs in two groups produced two reduce outputs.\nThe output was written to the output directory, which contains one output file per\nreducer. The job had a single reducer, so we find a single file, named part-00000:\n% cat output/part-00000\n1949\n111\n1950\n22\nThis result is the same as when we went through it by hand earlier. We interpret this\nas saying that the maximum temperature recorded in 1949 was 11.1°C, and in 1950 it\nwas 2.2°C.\nThe new Java MapReduce API\nRelease 0.20.0 of Hadoop included a new Java MapReduce API, sometimes referred to\nas “Context Objects,” designed to make the API easier to evolve in the future. The new\nAPI is type-incompatible with the old, however, so applications need to be rewritten\nto take advantage of it.*\nThere are several notable differences between the two APIs:\n• The new API favors abstract classes over interfaces, since these are easier to evolve.\nFor example, you can add a method (with a default implementation) to an abstract\nclass without breaking old implementations of the class. In the new API, the\nMapper and Reducer interfaces are now abstract classes.\n• The new API is in the org.apache.hadoop.mapreduce package (and subpackages).\nThe old API is found in org.apache.hadoop.mapred.\n• The new API makes extensive use of context objects that allow the user code to\ncommunicate with the MapReduce system. The MapContext, for example, essen-\ntially unifies the role of the JobConf, the OutputCollector, and the Reporter.\n• The new API supports both a “push” and a “pull” style of iteration. In both APIs,\nkey-value record pairs are pushed to the mapper, but in addition, the new API\nallows a mapper to pull records from within the map() method. The same goes for\nthe reducer. An example of how the “pull” style can be useful is processing records\nin batches, rather than one by one.\n* At the time of this writing, not all of the MapReduce libraries in Hadoop have been ported to work with the\nnew API. This book uses the old API for this reason. However, a copy of all of the examples in this book,\nrewritten to use the new API, will be made available on the book’s website.\nAnalyzing the Data with Hadoop | 25• Configuration has been unified. The old API has a special JobConf object for job\nconfiguration, which is an extension of Hadoop’s vanilla Configuration object\n(used for configuring daemons; see “The Configuration API” on page 116). In the\nnew API, this distinction is dropped, so job configuration is done through a\nConfiguration.\n• Job control is performed through the Job class, rather than JobClient, which no\nlonger exists in the new API.\nExample 2-6 shows the MaxTemperature application rewritten to use the new API. The\ndifferences are highlighted in bold.\nExample 2-6. Application to find the maximum temperature in the weather dataset using the new\ncontext objects MapReduce API\npublic class NewMaxTemperature {\nstatic class NewMaxTemperatureMapper\nextends Mapper<LongWritable, Text, Text, IntWritable> {\nprivate static final int MISSING = 9999;\npublic void map(LongWritable key, Text value, Context context)\nthrows IOException, InterruptedException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature;\nif (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs\nairTemperature = Integer.parseInt(line.substring(88, 92));\n} else {\nairTemperature = Integer.parseInt(line.substring(87, 92));\n}\nString quality = line.substring(92, 93);\nif (airTemperature != MISSING && quality.matches(""[01459]"")) {\ncontext.write(new Text(year), new IntWritable(airTemperature));\n}\nstatic class NewMaxTemperatureReducer\nextends Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterable<IntWritable> values,\nContext context)\nthrows IOException, InterruptedException {\n}\n}\nint maxValue = Integer.MIN_VALUE;\nfor (IntWritable value : values) {\nmaxValue = Math.max(maxValue, value.get());\n}\ncontext.write(key, new IntWritable(maxValue));\n26 | Chapter 2: MapReducepublic static void main(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.println(""Usage: NewMaxTemperature <input path> <output path>"");\nSystem.exit(-1);\n}\nJob job = new Job();\njob.setJarByClass(NewMaxTemperature.class);\nFileInputFormat.addInputPath(job, new Path(args[0]));\nFileOutputFormat.setOutputPath(job, new Path(args[1]));\njob.setMapperClass(NewMaxTemperatureMapper.class);\njob.setReducerClass(NewMaxTemperatureReducer.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(IntWritable.class);\n}\n}\nSystem.exit(job.waitForCompletion(true) ? 0 : 1);\nScaling Out\nYou’ve seen how MapReduce works for small inputs; now it’s time to take a bird’s-eye\nview of the system and look at the data flow for large inputs. For simplicity, the exam-\nples so far have used files on the local filesystem. However, to scale out, we need to\nstore the data in a distributed filesystem, typically HDFS (which you’ll learn about in\nthe next chapter), to allow Hadoop to move the MapReduce computation to each\nmachine hosting a part of the data. Let’s see how this works.\nData Flow\nFirst, some terminology. A MapReduce job is a unit of work that the client wants to be\nperformed: it consists of the input data, the MapReduce program, and configuration\ninformation. Hadoop runs the job by dividing it into tasks, of which there are two types:\nmap tasks and reduce tasks.\nThere are two types of nodes that control the job execution process: a jobtracker and\na number of tasktrackers. The jobtracker coordinates all the jobs run on the system by\nscheduling tasks to run on tasktrackers. Tasktrackers run tasks and send progress re-\nports to the jobtracker, which keeps a record of the overall progress of each job. If a\ntasks fails, the jobtracker can reschedule it on a different tasktracker.\nHadoop divides the input to a MapReduce job into fixed-size pieces called input\nsplits, or just splits. Hadoop creates one map task for each split, which runs the user-\ndefined map function for each record in the split.\nScaling Out | 27Having many splits means the time taken to process each split is small compared to the\ntime to process the whole input. So if we are processing the splits in parallel, the pro-\ncessing is better load-balanced if the splits are small, since a faster machine will be able\nto process proportionally more splits over the course of the job than a slower machine.\nEven if the machines are identical, failed processes or other jobs running concurrently\nmake load balancing desirable, and the quality of the load balancing increases as the\nsplits become more fine-grained.\nOn the other hand, if splits are too small, then the overhead of managing the splits and\nof map task creation begins to dominate the total job execution time. For most jobs, a\ngood split size tends to be the size of a HDFS block, 64 MB by default, although this\ncan be changed for the cluster (for all newly created files), or specified when each file\nis created.\nHadoop does its best to run the map task on a node where the input data resides in\nHDFS. This is called the data locality optimization. It should now be clear why the\noptimal split size is the same as the block size: it is the largest size of input that can be\nguaranteed to be stored on a single node. If the split spanned two blocks, it would be\nunlikely that any HDFS node stored both blocks, so some of the split would have to be\ntransferred across the network to the node running the map task, which is clearly less\nefficient than running the whole map task using local data.\nMap tasks write their output to local disk, not to HDFS. Why is this? Map output is\nintermediate output: it’s processed by reduce tasks to produce the final output, and\nonce the job is complete the map output can be thrown away. So storing it in HDFS,\nwith replication, would be overkill. If the node running the map task fails before the\nmap output has been consumed by the reduce task, then Hadoop will automatically\nrerun the map task on another node to recreate the map output.\nReduce tasks don’t have the advantage of data locality—the input to a single reduce\ntask is normally the output from all mappers. In the present example, we have a single\nreduce task that is fed by all of the map tasks. Therefore the sorted map outputs have\nto be transferred across the network to the node where the reduce task is running, where\nthey are merged and then passed to the user-defined reduce function. The output of\nthe reduce is normally stored in HDFS for reliability. As explained in Chapter 3, for\neach HDFS block of the reduce output, the first replica is stored on the local node, with\nother replicas being stored on off-rack nodes. Thus, writing the reduce output does\nconsume network bandwidth, but only as much as a normal HDFS write pipeline\nconsumes.\nThe whole data flow with a single reduce task is illustrated in Figure 2-2. The dotted\nboxes indicate nodes, the light arrows show data transfers on a node, and the heavy\narrows show data transfers between nodes.\nThe number of reduce tasks is not governed by the size of the input, but is specified\nindependently. In “The Default MapReduce Job” on page 178, you will see how to\nchoose the number of reduce tasks for a given job.\n28 | Chapter 2: MapReduceFigure 2-2. MapReduce data flow with a single reduce task\nWhen there are multiple reducers, the map tasks partition their output, each creating\none partition for each reduce task. There can be many keys (and their associated values)\nin each partition, but the records for every key are all in a single partition. The parti-\ntioning can be controlled by a user-defined partitioning function, but normally the\ndefault partitioner—which buckets keys using a hash function—works very well.\nThe data flow for the general case of multiple reduce tasks is illustrated in Figure 2-3.\nThis diagram makes it clear why the data flow between map and reduce tasks is collo-\nquially known as “the shuffle,” as each reduce task is fed by many map tasks. The\nshuffle is more complicated than this diagram suggests, and tuning it can have a big\nimpact on job execution time, as you will see in “Shuffle and Sort” on page 163.\nFinally, it’s also possible to have zero reduce tasks. This can be appropriate when you\ndon’t need the shuffle since the processing can be carried out entirely in parallel (a few\nexamples are discussed in “NLineInputFormat” on page 198). In this case, the only\noff-node data transfer is when the map tasks write to HDFS (see Figure 2-4).\nCombiner Functions\nMany MapReduce jobs are limited by the bandwidth available on the cluster, so it pays\nto minimize the data transferred between map and reduce tasks. Hadoop allows the\nuser to specify a combiner function to be run on the map output—the combiner func-\ntion’s output forms the input to the reduce function. Since the combiner function is an\noptimization, Hadoop does not provide a guarantee of how many times it will call it\nfor a particular map output record, if at all. In other words, calling the combiner func-\ntion zero, one, or many times should produce the same output from the reducer.\nScaling Out | 29Figure 2-3. MapReduce data flow with multiple reduce tasks\nFigure 2-4. MapReduce data flow with no reduce tasks\nThe contract for the combiner function constrains the type of function that may be\nused. This is best illustrated with an example. Suppose that for the maximum temper-\nature example, readings for the year 1950 were processed by two maps (because they\nwere in different splits). Imagine the first map produced the output:\n30 | Chapter 2: MapReduce(1950, 0)\n(1950, 20)\n(1950, 10)\nAnd the second produced:\n(1950, 25)\n(1950, 15)\nThe reduce function would be called with a list of all the values:\n(1950, [0, 20, 10, 25, 15])\nwith output:\n(1950, 25)\nsince 25 is the maximum value in the list. We could use a combiner function that, just\nlike the reduce function, finds the maximum temperature for each map output. The\nreduce would then be called with:\n(1950, [20, 25])\nand the reduce would produce the same output as before. More succinctly, we may\nexpress the function calls on the temperature values in this case as follows:\nmax(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25\nNot all functions possess this property.† For example, if we were calculating mean\ntemperatures, then we couldn’t use the mean as our combiner function, since:\nmean(0, 20, 10, 25, 15) = 14\nbut:\nmean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15\nThe combiner function doesn’t replace the reduce function. (How could it? The reduce\nfunction is still needed to process records with the same key from different maps.) But\nit can help cut down the amount of data shuffled between the maps and the reduces,\nand for this reason alone it is always worth considering whether you can use a combiner\nfunction in your MapReduce job.\nSpecifying a combiner function\nGoing back to the Java MapReduce program, the combiner function is defined using\nthe Reducer interface, and for this application, it is the same implementation as the\nreducer function in MaxTemperatureReducer. The only change we need to make is to set\nthe combiner class on the JobConf (see Example 2-7).\n† Functions with this property are called distributive in the paper “Data Cube: A Relational Aggregation\nOperator Generalizing Group-By, Cross-Tab, and Sub-Totals,” Gray et al. (1995).\nScaling Out | 31Example 2-7. Application to find the maximum temperature, using a combiner function for efficiency\npublic class MaxTemperatureWithCombiner {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperatureWithCombiner <input path> "" +\n""<output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperatureWithCombiner.class);\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\n}\n}\nJobClient.runJob(conf);\nRunning a Distributed MapReduce Job\nThe same program will run, without alteration, on a full dataset. This is the point of\nMapReduce: it scales to the size of your data and the size of your hardware. Here’s one\ndata point: on a 10-node EC2 cluster running High-CPU Extra Large Instances, the\nprogram took six minutes to run.‡\nWe’ll go through the mechanics of running programs on a cluster in Chapter 5.\nHadoop Streaming\nHadoop provides an API to MapReduce that allows you to write your map and reduce\nfunctions in languages other than Java. Hadoop Streaming uses Unix standard streams\nas the interface between Hadoop and your program, so you can use any language that\ncan read standard input and write to standard output to write your MapReduce\nprogram.\n‡ This is a factor of seven faster than the serial run on one machine using awk. The main reason it wasn’t\nproportionately faster is because the input data wasn’t evenly partitioned. For convenience, the input files\nwere gzipped by year, resulting in large files for the later years in dataset, when the number of weather records\nwas much higher.\n32 | Chapter 2: MapReduceStreaming is naturally suited for text processing (although as of version 0.21.0 it can\nhandle binary streams, too), and when used in text mode, it has a line-oriented view of\ndata. Map input data is passed over standard input to your map function, which pro-\ncesses it line by line and writes lines to standard output. A map output key-value pair\nis written as a single tab-delimited line. Input to the reduce function is in the same\nformat—a tab-separated key-value pair—passed over standard input. The reduce func-\ntion reads lines from standard input, which the framework guarantees are sorted by\nkey, and writes its results to standard output.\nLet’s illustrate this by rewriting our MapReduce program for finding maximum tem-\nperatures by year in Streaming.\nRuby\nThe map function can be expressed in Ruby as shown in Example 2-8.\nExample 2-8. Map function for maximum temperature in Ruby\n#!/usr/bin/env ruby\nSTDIN.each_line do |line|\nval = line\nyear, temp, q = val[15,4], val[87,5], val[92,1]\nputs ""#{year}\\t#{temp}"" if (temp != ""+9999"" && q =~ /[01459]/)\nend\nThe program iterates over lines from standard input by executing a block for each line\nfrom STDIN (a global constant of type IO). The block pulls out the relevant fields from\neach input line, and, if the temperature is valid, writes the year and the temperature\nseparated by a tab character \\t to standard output (using puts).\nIt’s worth drawing out a design difference between Streaming and the\nJava MapReduce API. The Java API is geared toward processing your\nmap function one record at a time. The framework calls the map()\nmethod on your Mapper for each record in the input, whereas with\nStreaming the map program can decide how to process the input—for\nexample, it could easily read and process multiple lines at a time since\nit’s in control of the reading. The user’s Java map implementation is\n“pushed” records, but it’s still possible to consider multiple lines at a\ntime by accumulating previous lines in an instance variable in the\nMapper.§ In this case, you need to implement the close() method so that\nyou know when the last record has been read, so you can finish pro-\ncessing the last group of lines.\n§ Alternatively, you could use “pull” style processing in the new MapReduce API—see “The new\nJava MapReduce API” on page 25.\nHadoop Streaming | 33Since the script just operates on standard input and output, it’s trivial to test the script\nwithout using Hadoop, simply using Unix pipes:\n% cat input/ncdc/sample.txt | src/main/ch02/ruby/max_temperature_map.rb\n1950\n+0000\n1950\n+0022\n1950\n-0011\n1949\n+0111\n1949\n+0078\nThe reduce function shown in Example 2-9 is a little more complex.\nExample 2-9. Reduce function for maximum temperature in Ruby\n#!/usr/bin/env ruby\nlast_key, max_val = nil, 0\nSTDIN.each_line do |line|\nkey, val = line.split(""\\t"")\nif last_key && last_key != key\nputs ""#{last_key}\\t#{max_val}""\nlast_key, max_val = key, val.to_i\nelse\nlast_key, max_val = key, [max_val, val.to_i].max\nend\nend\nputs ""#{last_key}\\t#{max_val}"" if last_key\nAgain, the program iterates over lines from standard input, but this time we have to\nstore some state as we process each key group. In this case, the keys are weather station\nidentifiers, and we store the last key seen and the maximum temperature seen so far\nfor that key. The MapReduce framework ensures that the keys are ordered, so we know\nthat if a key is different from the previous one, we have moved into a new key group.\nIn contrast to the Java API, where you are provided an iterator over each key group, in\nStreaming you have to find key group boundaries in your program.\nFor each line we pull out the key and value, then if we’ve just finished a group (last_key\n&& last_key != key), we write the key and the maximum temperature for that group,\nseparated by a tab character, before resetting the maximum temperature for the new\nkey. If we haven’t just finished a group, we just update the maximum temperature for\nthe current key.\nThe last line of the program ensures that a line is written for the last key group in the\ninput.\nWe can now simulate the whole MapReduce pipeline with a Unix pipeline (which is\nequivalent to the Unix pipeline shown in Figure 2-1):\n% cat input/ncdc/sample.txt | src/main/ch02/ruby/max_temperature_map.rb | \\\nsort | src/main/ch02/ruby/max_temperature_reduce.rb\n1949\n111\n1950\n22\n34 | Chapter 2: MapReduceThe output is the same as the Java program, so the next step is to run it using Hadoop\nitself.\nThe hadoop command doesn’t support a Streaming option; instead, you specify the\nStreaming JAR file along with the jar option. Options to the Streaming program specify\nthe input and output paths, and the map and reduce scripts. This is what it looks like:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-mapper src/main/ch02/ruby/max_temperature_map.rb \\\n-reducer src/main/ch02/ruby/max_temperature_reduce.rb\nWhen running on a large dataset on a cluster, we should set the combiner, using the\n-combiner option.\nFrom release 0.21.0, the combiner can be any Streaming command. For earlier releases,\nthe combiner had to be written in Java, so as a workaround it was common to do manual\ncombining in the mapper, without having to resort to Java. In this case, we could change\nthe mapper to be a pipeline:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/all \\\n-output output \\\n-mapper ""ch02/ruby/max_temperature_map.rb | sort | ch02/ruby/max_temperature_reduce.rb"" \\\n-reducer src/main/ch02/ruby/max_temperature_reduce.rb \\\n-file src/main/ch02/ruby/max_temperature_map.rb \\\n-file src/main/ch02/ruby/max_temperature_reduce.rb\nNote also the use of -file, which we use when running Streaming programs on the\ncluster to ship the scripts to the cluster.\nPython\nStreaming supports any programming language that can read from standard input, and\nwrite to standard output, so for readers more familiar with Python, here’s the same\nexample again.‖ The map script is in Example 2-10, and the reduce script is in Exam-\nple 2-11.\nExample 2-10. Map function for maximum temperature in Python\n#!/usr/bin/env python\nimport re\nimport sys\nfor line in sys.stdin:\nval = line.strip()\n(year, temp, q) = (val[15:19], val[87:92], val[92:93])\n‖ As an alternative to Streaming, Python programmers should consider Dumbo (http://www.last.fm/dumbo),\nwhich makes the Streaming MapReduce interface more Pythonic, and easier to use.\nHadoop Streaming | 35if (temp != ""+9999"" and re.match(""[01459]"", q)):\nprint ""%s\\t%s"" % (year, temp)\nExample 2-11. Reduce function for maximum temperature in Python\n#!/usr/bin/env python\nimport sys\n(last_key, max_val) = (None, 0)\nfor line in sys.stdin:\n(key, val) = line.strip().split(""\\t"")\nif last_key and last_key != key:\nprint ""%s\\t%s"" % (last_key, max_val)\n(last_key, max_val) = (key, int(val))\nelse:\n(last_key, max_val) = (key, max(max_val, int(val)))\nif last_key:\nprint ""%s\\t%s"" % (last_key, max_val)\nWe can test the programs and run the job in the same way we did in Ruby. For example,\nto run a test:\n% cat input/ncdc/sample.txt | src/main/ch02/python/max_temperature_map.py | \\\nsort | src/main/ch02/python/max_temperature_reduce.py\n1949\n111\n1950\n22\nHadoop Pipes\nHadoop Pipes is the name of the C++ interface to Hadoop MapReduce. Unlike Stream-\ning, which uses standard input and output to communicate with the map and reduce\ncode, Pipes uses sockets as the channel over which the tasktracker communicates with\nthe process running the C++ map or reduce function. JNI is not used.\nWe’ll rewrite the example running through the chapter in C++, and then we’ll see how\nto run it using Pipes. Example 2-12 shows the source code for the map and reduce\nfunctions in C++.\nExample 2-12. Maximum temperature in C++\n#include <algorithm>\n#include <limits>\n#include <string>\n#include ""hadoop/Pipes.hh""\n#include ""hadoop/TemplateFactory.hh""\n#include ""hadoop/StringUtils.hh""\nclass MaxTemperatureMapper : public HadoopPipes::Mapper {\npublic:\nMaxTemperatureMapper(HadoopPipes::TaskContext& context) {\n36 | Chapter 2: MapReduce}\nvoid map(HadoopPipes::MapContext& context) {\nstd::string line = context.getInputValue();\nstd::string year = line.substr(15, 4);\nstd::string airTemperature = line.substr(87, 5);\nstd::string q = line.substr(92, 1);\nif (airTemperature != ""+9999"" &&\n(q == ""0"" || q == ""1"" || q == ""4"" || q == ""5"" || q == ""9"")) {\ncontext.emit(year, airTemperature);\n}\n}\n};\nclass MapTemperatureReducer : public HadoopPipes::Reducer {\npublic:\nMapTemperatureReducer(HadoopPipes::TaskContext& context) {\n}\nvoid reduce(HadoopPipes::ReduceContext& context) {\nint maxValue = INT_MIN;\nwhile (context.nextValue()) {\nmaxValue = std::max(maxValue, HadoopUtils::toInt(context.getInputValue()));\n}\ncontext.emit(context.getInputKey(), HadoopUtils::toString(maxValue));\n}\n};\nint main(int argc, char *argv[]) {\nreturn HadoopPipes::runTask(HadoopPipes::TemplateFactory<MaxTemperatureMapper,\nMapTemperatureReducer>());\n}\nThe application links against the Hadoop C++ library, which is a thin wrapper for\ncommunicating with the tasktracker child process. The map and reduce functions are\ndefined by extending the Mapper and Reducer classes defined in the HadoopPipes name-\nspace and providing implementations of the map() and reduce() methods in each case.\nThese methods take a context object (of type MapContext or ReduceContext), which\nprovides the means for reading input and writing output, as well as accessing job con-\nfiguration information via the JobConf class. The processing in this example is very\nsimilar to the Java equivalent.\nUnlike the Java interface, keys and values in the C++ interface are byte buffers, repre-\nsented as Standard Template Library (STL) strings. This makes the interface simpler,\nalthough it does put a slightly greater burden on the application developer, who has to\nconvert to and from richer domain-level types. This is evident in MapTemperatureRe\nducer where we have to convert the input value into an integer (using a convenience\nmethod in HadoopUtils) and then the maximum value back into a string before it’s\nwritten out. In some cases, we can save on doing the conversion, such as in MaxTemper\natureMapper where the airTemperature value is never converted to an integer since it is\nnever processed as a number in the map() method.\nHadoop Pipes | 37The main() method is the application entry point. It calls HadoopPipes::runTask, which\nconnects to the Java parent process and marshals data to and from the Mapper or\nReducer. The runTask() method is passed a Factory so that it can create instances of the\nMapper or Reducer. Which one it creates is controlled by the Java parent over the socket\nconnection. There are overloaded template factory methods for setting a combiner,\npartitioner, record reader, or record writer.\nCompiling and Running\nNow we can compile and link our program using the Makefile in Example 2-13.\nExample 2-13. Makefile for C++ MapReduce program\nCC = g++\nCPPFLAGS = -m32 -I$(HADOOP_INSTALL)/c++/$(PLATFORM)/include\nmax_temperature: max_temperature.cpp\n$(CC) $(CPPFLAGS) $< -Wall -L$(HADOOP_INSTALL)/c++/$(PLATFORM)/lib -lhadooppipes \\\n-lhadooputils -lpthread -g -O2 -o $@\nThe Makefile expects a couple of environment variables to be set. Apart from\nHADOOP_INSTALL (which you should already have set if you followed the installation\ninstructions in Appendix A), you need to define PLATFORM, which specifies the operating\nsystem, architecture, and data model (e.g., 32- or 64-bit). I ran it on a 32-bit Linux\nsystem with the following:\n% export PLATFORM=Linux-i386-32\n% make\nOn successful completion, you’ll find the max_temperature executable in the current\ndirectory.\nTo run a Pipes job, we need to run Hadoop in pseudo-distributed mode (where all the\ndaemons run on the local machine), for which there are setup instructions in Appen-\ndix A. Pipes doesn’t run in standalone (local) mode, since it relies on Hadoop’s dis-\ntributed cache mechanism, which works only when HDFS is running.\nWith the Hadoop daemons now running, the first step is to copy the executable to\nHDFS so that it can be picked up by tasktrackers when they launch map and reduce\ntasks:\n% hadoop fs -put max_temperature bin/max_temperature\nThe sample data also needs to be copied from the local filesystem into HDFS:\n% hadoop fs -put input/ncdc/sample.txt sample.txt\n38 | Chapter 2: MapReduceNow we can run the job. For this, we use the Hadoop pipes command, passing the URI\nof the executable in HDFS using the -program argument:\n% hadoop pipes \\\n-D hadoop.pipes.java.recordreader=true \\\n-D hadoop.pipes.java.recordwriter=true \\\n-input sample.txt \\\n-output output \\\n-program bin/max_temperature\nWe specify two properties using the -D option: hadoop.pipes.java.recordreader and\nhadoop.pipes.java.recordwriter, setting both to true to say that we have not specified\na C++ record reader or writer, but that we want to use the default Java ones (which are\nfor text input and output). Pipes also allows you to set a Java mapper, reducer,\ncombiner, or partitioner. In fact, you can have a mixture of Java or C++ classes within\nany one job.\nThe result is the same as the other versions of the same program that we ran.\nHadoop Pipes | 39CHAPTER 3\nThe Hadoop Distributed Filesystem\nWhen a dataset outgrows the storage capacity of a single physical machine, it becomes\nnecessary to partition it across a number of separate machines. Filesystems that manage\nthe storage across a network of machines are called distributed filesystems. Since they\nare network-based, all the complications of network programming kick in, thus making\ndistributed filesystems more complex than regular disk filesystems. For example, one\nof the biggest challenges is making the filesystem tolerate node failure without suffering\ndata loss.\nHadoop comes with a distributed filesystem called HDFS, which stands for Hadoop\nDistributed Filesystem. (You may sometimes see references to “DFS”—informally or in\nolder documentation or configuration—which is the same thing.) HDFS is Hadoop’s\nflagship filesystem and is the focus of this chapter, but Hadoop actually has a general-\npurpose filesystem abstraction, so we’ll see along the way how Hadoop integrates with\nother storage systems (such as the local filesystem and Amazon S3).\nThe Design of HDFS\nHDFS is a filesystem designed for storing very large files with streaming data access\npatterns, running on clusters on commodity hardware. Let’s examine this statement in\nmore detail:\nVery large files\n“Very large” in this context means files that are hundreds of megabytes, gigabytes,\nor terabytes in size. There are Hadoop clusters running today that store petabytes\nof data.*\nStreaming data access\nHDFS is built around the idea that the most efficient data processing pattern is a\nwrite-once, read-many-times pattern. A dataset is typically generated or copied\n* “Scaling Hadoop to 4000 nodes at Yahoo!,” http://developer.yahoo.net/blogs/hadoop/2008/09/scaling_hadoop\n_to_4000_nodes_a.html.\n41from source, then various analyses are performed on that dataset over time. Each\nanalysis will involve a large proportion, if not all, of the dataset, so the time to read\nthe whole dataset is more important than the latency in reading the first record.\nCommodity hardware\nHadoop doesn’t require expensive, highly reliable hardware to run on. It’s designed\nto run on clusters of commodity hardware (commonly available hardware available\nfrom multiple vendors†) for which the chance of node failure across the cluster is\nhigh, at least for large clusters. HDFS is designed to carry on working without a\nnoticeable interruption to the user in the face of such failure.\nIt is also worth examining the applications for which using HDFS does not work so\nwell. While this may change in the future, these are areas where HDFS is not a good fit\ntoday:\nLow-latency data access\nApplications that require low-latency access to data, in the tens of milliseconds\nrange, will not work well with HDFS. Remember HDFS is optimized for delivering\na high throughput of data, and this may be at the expense of latency. HBase\n(Chapter 12) is currently a better choice for low-latency access.\nLots of small files\nSince the namenode holds filesystem metadata in memory, the limit to the number\nof files in a filesystem is governed by the amount of memory on the namenode. As\na rule of thumb, each file, directory, and block takes about 150 bytes. So, for ex-\nample, if you had one million files, each taking one block, you would need at least\n300 MB of memory. While storing millions of files is feasible, billions is beyond\nthe capability of current hardware.\nMultiple writers, arbitrary file modifications\nFiles in HDFS may be written to by a single writer. Writes are always made at the\nend of the file. There is no support for multiple writers, or for modifications at\narbitrary offsets in the file. (These might be supported in the future, but they are\nlikely to be relatively inefficient.)\nHDFS Concepts\nBlocks\nA disk has a block size, which is the minimum amount of data that it can read or write.\nFilesystems for a single disk build on this by dealing with data in blocks, which are an\nintegral multiple of the disk block size. Filesystem blocks are typically a few kilobytes\nin size, while disk blocks are normally 512 bytes. This is generally transparent to the\n† See Chapter 9 for a typical machine specification.\n42 | Chapter 3: The Hadoop Distributed Filesystemfilesystem user who is simply reading or writing a file—of whatever length. However,\nthere are tools to do with filesystem maintenance, such as df and fsck, that operate on\nthe filesystem block level.\nHDFS too has the concept of a block, but it is a much larger unit—64 MB by default.\nLike in a filesystem for a single disk, files in HDFS are broken into block-sized chunks,\nwhich are stored as independent units. Unlike a filesystem for a single disk, a file in\nHDFS that is smaller than a single block does not occupy a full block’s worth of un-\nderlying storage. When unqualified, the term “block” in this book refers to a block in\nHDFS.\nWhy Is a Block in HDFS So Large?\nHDFS blocks are large compared to disk blocks, and the reason is to minimize the cost\nof seeks. By making a block large enough, the time to transfer the data from the disk\ncan be made to be significantly larger than the time to seek to the start of the block.\nThus the time to transfer a large file made of multiple blocks operates at the disk transfer\nrate.\nA quick calculation shows that if the seek time is around 10ms, and the transfer rate is\n100 MB/s, then to make the seek time 1% of the transfer time, we need to make the\nblock size around 100 MB. The default is actually 64 MB, although many HDFS in-\nstallations use 128 MB blocks. This figure will continue to be revised upward as transfer\nspeeds grow with new generations of disk drives.\nThis argument shouldn’t be taken too far, however. Map tasks in MapReduce normally\noperate on one block at a time, so if you have too few tasks (fewer than nodes in the\ncluster), your jobs will run slower than they could otherwise.\nHaving a block abstraction for a distributed filesystem brings several benefits. The first\nbenefit is the most obvious: a file can be larger than any single disk in the network.\nThere’s nothing that requires the blocks from a file to be stored on the same disk, so\nthey can take advantage of any of the disks in the cluster. In fact, it would be possible,\nif unusual, to store a single file on an HDFS cluster whose blocks filled all the disks in\nthe cluster.\nSecond, making the unit of abstraction a block rather than a file simplifies the storage\nsubsystem. Simplicity is something to strive for all in all systems, but is important for\na distributed system in which the failure modes are so varied. The storage subsystem\ndeals with blocks, simplifying storage management (since blocks are a fixed size, it is\neasy to calculate how many can be stored on a given disk), and eliminating metadata\nconcerns (blocks are just a chunk of data to be stored—file metadata such as permis-\nsions information does not need to be stored with the blocks, so another system can\nhandle metadata orthogonally).\nFurthermore, blocks fit well with replication for providing fault tolerance and availa-\nbility. To insure against corrupted blocks and disk and machine failure, each block is\nHDFS Concepts | 43replicated to a small number of physically separate machines (typically three). If a block\nbecomes unavailable, a copy can be read from another location in a way that is trans-\nparent to the client. A block that is no longer available due to corruption or machine\nfailure can be replicated from their alternative locations to other live machines to bring\nthe replication factor back to the normal level. (See “Data Integrity” on page 75 for\nmore on guarding against corrupt data.) Similarly, some applications may choose to\nset a high replication factor for the blocks in a popular file to spread the read load on\nthe cluster.\nLike its disk filesystem cousin, HDFS’s fsck command understands blocks. For exam-\nple, running:\n% hadoop fsck -files -blocks\nwill list the blocks that make up each file in the filesystem. (See also “Filesystem check\n(fsck)” on page 281.)\nNamenodes and Datanodes\nA HDFS cluster has two types of node operating in a master-worker pattern: a name-\nnode (the master) and a number of datanodes (workers). The namenode manages the\nfilesystem namespace. It maintains the filesystem tree and the metadata for all the files\nand directories in the tree. This information is stored persistently on the local disk in\nthe form of two files: the namespace image and the edit log. The namenode also knows\nthe datanodes on which all the blocks for a given file are located, however, it does not\nstore block locations persistently, since this information is reconstructed from\ndatanodes when the system starts.\nA client accesses the filesystem on behalf of the user by communicating with the name-\nnode and datanodes. The client presents a POSIX-like filesystem interface, so the user\ncode does not need to know about the namenode and datanode to function.\nDatanodes are the work horses of the filesystem. They store and retrieve blocks when\nthey are told to (by clients or the namenode), and they report back to the namenode\nperiodically with lists of blocks that they are storing.\nWithout the namenode, the filesystem cannot be used. In fact, if the machine running\nthe namenode were obliterated, all the files on the filesystem would be lost since there\nwould be no way of knowing how to reconstruct the files from the blocks on the\ndatanodes. For this reason, it is important to make the namenode resilient to failure,\nand Hadoop provides two mechanisms for this.\nThe first way is to back up the files that make up the persistent state of the filesystem\nmetadata. Hadoop can be configured so that the namenode writes its persistent state\nto multiple filesystems. These writes are synchronous and atomic. The usual configu-\nration choice is to write to local disk as well as a remote NFS mount.\n44 | Chapter 3: The Hadoop Distributed FilesystemIt is also possible to run a secondary namenode, which despite its name does not act as\na namenode. Its main role is to periodically merge the namespace image with the edit\nlog to prevent the edit log from becoming too large. The secondary namenode usually\nruns on a separate physical machine, since it requires plenty of CPU and as much\nmemory as the namenode to perform the merge. It keeps a copy of the merged name-\nspace image, which can be used in the event of the namenode failing. However, the\nstate of the secondary namenode lags that of the primary, so in the event of total failure\nof the primary data, loss is almost guaranteed. The usual course of action in this case\nis to copy the namenode’s metadata files that are on NFS to the secondary and run it\nas the new primary.\nSee “The filesystem image and edit log” on page 274 for more details.\nThe Command-Line Interface\nWe’re going to have a look at HDFS by interacting with it from the command line.\nThere are many other interfaces to HDFS, but the command line is one of the simplest,\nand to many developers the most familiar.\nWe are going to run HDFS on one machine, so first follow the instructions for setting\nup Hadoop in pseudo-distributed mode in Appendix A. Later you’ll see how to run on\na cluster of machines to give us scalability and fault tolerance.\nThere are two properties that we set in the pseudo-distributed configuration that de-\nserve further explanation. The first is fs.default.name, set to hdfs://localhost/, which is\nused to set a default filesystem for Hadoop. Filesystems are specified by a URI, and\nhere we have used a hdfs URI to configure Hadoop to use HDFS by default. The HDFS\ndaemons will use this property to determine the host and port for the HDFS namenode.\nWe’ll be running it on localhost, on the default HDFS port, 8020. And HDFS clients\nwill use this property to work out where the namenode is running so they can connect\nto it.\nWe set the second property, dfs.replication, to one so that HDFS doesn’t replicate\nfilesystem blocks by the usual default of three. When running with a single datanode,\nHDFS can’t replicate blocks to three datanodes, so it would perpetually warn about\nblocks being under-replicated. This setting solves that problem.\nBasic Filesystem Operations\nThe filesystem is ready to be used, and we can do all of the usual filesystem operations\nsuch as reading files, creating directories, moving files, deleting data, and listing direc-\ntories. You can type hadoop fs -help to get detailed help on every command.\nStart by copying a file from the local filesystem to HDFS:\n% hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/tom/quangle.txt\nThe Command-Line Interface | 45This command invokes Hadoop’s filesystem shell command fs, which supports a\nnumber of subcommands—in this case, we are running -copyFromLocal. The local file\nquangle.txt is copied to the file /user/tom/quangle.txt on the HDFS instance running on\nlocalhost. In fact, we could have omitted the scheme and host of the URI and picked\nup the default, hdfs://localhost, as specified in core-site.xml.\n% hadoop fs -copyFromLocal input/docs/quangle.txt /user/tom/quangle.txt\nWe could also have used a relative path, and copied the file to our home directory in\nHDFS, which in this case is /user/tom:\n% hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt\nLet’s copy the file back to the local filesystem and check whether it’s the same:\n% hadoop fs -copyToLocal quangle.txt quangle.copy.txt\n% md5 input/docs/quangle.txt quangle.copy.txt\nMD5 (input/docs/quangle.txt) = a16f231da6b05e2ba7a339320e7dacd9\nMD5 (quangle.copy.txt) = a16f231da6b05e2ba7a339320e7dacd9\nThe MD5 digests are the same, showing that the file survived its trip to HDFS and is\nback intact.\nFinally, let’s look at an HDFS file listing. We create a directory first just to see how it\nis displayed in the listing:\n% hadoop fs -mkdir books\n% hadoop fs -ls .\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\n-rw-r--r--\n1 tom supergroup\n0 2009-04-02 22:41 /user/tom/books\n118 2009-04-02 22:29 /user/tom/quangle.txt\nThe information returned is very similar to the Unix command ls -l, with a few minor\ndifferences. The first column shows the file mode. The second column is the replication\nfactor of the file (something a traditional Unix filesystems does not have). Remember\nwe set the default replication factor in the site-wide configuration to be 1, which is why\nwe see the same value here. The entry in this column is empty for directories since the\nconcept of replication does not apply to them—directories are treated as metadata and\nstored by the namenode, not the datanodes. The third and fourth columns show the\nfile owner and group. The fifth column is the size of the file in bytes, or zero for direc-\ntories. The six and seventh columns are the last modified date and time. Finally, the\neighth column is the absolute name of the file or directory.\n46 | Chapter 3: The Hadoop Distributed FilesystemFile Permissions in HDFS\nHDFS has a permissions model for files and directories that is much like POSIX.\nThere are three types of permission: the read permission (r), the write permission (w)\nand the execute permission (x). The read permission is required to read files or list the\ncontents of a directory. The write permission is required to write a file, or for a directory,\nto create or delete files or directories in it. The execute permission is ignored for a file\nsince you can’t execute a file on HDFS (unlike POSIX), and for a directory it is required\nto access its children.\nEach file and directory has an owner, a group, and a mode. The mode is made up of the\npermissions for the user who is the owner, the permissions for the users who are mem-\nbers of the group, and the permissions for users who are neither the owner nor members\nof the group.\nA client’s identity is determined by the username and groups of the process it is running\nin. Because clients are remote, this makes it possible to become an arbitrary user, simply\nby creating an account of that name on the remote system. Thus, permissions should\nbe used only in a cooperative community of users, as a mechanism for sharing filesystem\nresources and for avoiding accidental data loss, and not for securing resources in a\nhostile environment. However, despite these drawbacks, it is worthwhile having per-\nmissions enabled (as it is by default; see the dfs.permissions property), to avoid acci-\ndental modification or deletion of substantial parts of the filesystem, either by users or\nby automated tools or programs.\nWhen permissions checking is enabled, the owner permissions are checked if the cli-\nent’s username matches the owner, and the group permissions are checked if the client\nis a member of the group; otherwise, the other permissions are checked.\nThere is a concept of a super-user, which is the identity of the namenode process.\nPermissions checks are not performed for the super-user.\nHadoop Filesystems\nHadoop has an abstract notion of filesystem, of which HDFS is just one implementa-\ntion. The Java abstract class org.apache.hadoop.fs.FileSystem represents a filesystem\nin Hadoop, and there are several concrete implementations, which are described in\nTable 3-1.\nHadoop Filesystems | 47Table 3-1. Hadoop filesystems\nFilesystem URI scheme Java implementation (all under org.apache.hadoop) Description\nLocal file fs.LocalFileSystem A filesystem for a locally connec-\n                             ted disk with client-side check-\n                            sums. Use RawLocalFileSys\n                           tem for a local filesystem with no\n                          checksums. See “LocalFileSys-\n                           tem” on page 76.\nHDFS hdfs hdfs.DistributedFileSystem Hadoop’s distributed filesystem.\n                                      HDFS is designed to work effi-\n                                     ciently in conjunction with Map-\n                                    Reduce.\nHFTP hftp hdfs.HftpFileSystem A filesystem providing read-only\n                             access to HDFS over HTTP. (Despite\n                            its name, HFTP has no connection\n                           with FTP.) Often used with distcp\n                          (“Parallel Copying with\n                           distcp” on page 70) to copy data\n                            between HDFS clusters running\n                           different versions.\nHSFTP hsftp hdfs.HsftpFileSystem A filesystem providing read-only\n                                access to HDFS over HTTPS. (Again,\n                               this has no connection with FTP.)\nHAR har fs.HarFileSystem A filesystem layered on another\n                        filesystem for archiving files. Ha-\n                       doop Archives are typically used\n                      for archiving files in HDFS to reduce\n                     the namenode’s memory usage.\n                      See “Hadoop Ar-\n                       chives” on page 71.\nKFS (Cloud- kfs fs.kfs.KosmosFileSystem CloudStore (formerly Kosmos fil-\nStore) esystem) is a distributed filesys-\n      tem like HDFS or Google’s GFS,\n       written in C++. Find more infor-\n      mation about it at http://kosmosfs\n     .sourceforge.net/.\nFTP ftp fs.ftp.FTPFileSystem A filesystem backed by an FTP\n                            server.\nS3 (native) s3n fs.s3native.NativeS3FileSystem A filesystem backed by Amazon\n                                              S3. See http://wiki.apache.org/ha\n                                             doop/AmazonS3.\nS3 (block- s3 fs.s3.S3FileSystem A filesystem backed by Amazon\nbased) S3, which stores files in blocks\n      (much like HDFS) to overcome S3’s\n       5 GB file size limit.\n48 | Chapter 3: The Hadoop Distributed FilesystemHadoop provides many interfaces to its filesystems, and it generally uses the URI\nscheme to pick the correct filesystem instance to communicate with. For example, the\nfilesystem shell that we met in the previous section operates with all Hadoop filesys-\ntems. To list the files in the root directory of the local filesystem, type:\n% hadoop fs -ls file:///\nAlthough it is possible (and sometimes very convenient) to run MapReduce programs\nthat access any of these filesystems, when you are processing large volumes of data,\nyou should choose a distributed filesystem that has the data locality optimization, such\nas HDFS or KFS (see “Scaling Out” on page 27).\nInterfaces\nHadoop is written in Java, and all Hadoop filesystem interactions are mediated through\nthe Java API.‡ The filesystem shell, for example, is a Java application that uses the Java\nFileSystem class to provide filesystem operations. The other filesystem interfaces are\ndiscussed briefly in this section. These interfaces are most commonly used with HDFS,\nsince the other filesystems in Hadoop typically have existing tools to access the under-\nlying filesystem (FTP clients for FTP, S3 tools for S3, etc.), but many of them will work\nwith any Hadoop filesystem.\nThrift\nBy exposing its filesystem interface as a Java API, Hadoop makes it awkward for non-\nJava applications to access Hadoop filesystems. The Thrift API in the “thriftfs” contrib\nmodule remedies this deficiency by exposing Hadoop filesystems as an Apache Thrift\nservice, making it easy for any language that has Thrift bindings to interact with a\nHadoop filesystem, such as HDFS.\nTo use the Thrift API, run a Java server that exposes the Thrift service, and acts as a\nproxy to the Hadoop filesystem. Your application accesses the Thrift service, which is\ntypically running on the same machine as your application.\nThe Thrift API comes with a number of pregenerated stubs for a variety of languages,\nincluding C++, Perl, PHP, Python, and Ruby. Thrift has support for versioning, so it’s\na good choice if you want to access different versions of a Hadoop filesystem from the\nsame client code (you will need to run a proxy for each version of Hadoop to achieve\nthis, however).\nFor installation and usage instructions, please refer to the documentation in the\nsrc/contrib/thriftfs directory of the Hadoop distribution.\n‡ The RPC interfaces in Hadoop are based on Hadoop’s Writable interface, which is Java-centric. In the future,\nHadoop will adopt another, cross-language, RPC serialization format, which will allow native HDFS clients\nto be written in languages other than Java.\nHadoop Filesystems | 49C\nHadoop provides a C library called libhdfs that mirrors the Java FileSystem interface\n(it was written as a C library for accessing HDFS, but despite its name it can be used\nto access any Hadoop filesystem). It works using the Java Native Interface (JNI) to call\na Java filesystem client.\nThe C API is very similar to the Java one, but it typically lags the Java one, so newer\nfeatures may not be supported. You can find the generated documentation for the C\nAPI in the libhdfs/docs/api directory of the Hadoop distribution.\nHadoop comes with prebuilt libhdfs binaries for 32-bit Linux, but for other platforms,\nyou will need to build them yourself using the instructions at http://wiki.apache.org/\nhadoop/LibHDFS.\nFUSE\nFilesystem in Userspace (FUSE) allows filesystems that are implemented in user space\nto be integrated as a Unix filesystem. Hadoop’s Fuse-DFS contrib module allows any\nHadoop filesystem (but typically HDFS) to be mounted as a standard filesystem. You\ncan then use Unix utilities (such as ls and cat) to interact with the filesystem, as well\nas POSIX libraries to access the filesystem from any programming language.\nFuse-DFS is implemented in C using libhdfs as the interface to HDFS. Documentation\nfor compiling and running Fuse-DFS is located in the src/contrib/fuse-dfs directory of\nthe Hadoop distribution.\nWebDAV\nWebDAV is a set of extensions to HTTP to support editing and updating files. WebDAV\nshares can be mounted as filesystems on most operating systems, so by exposing HDFS\n(or other Hadoop filesystems) over WebDAV, it’s possible to access HDFS as a standard\nfilesystem.\nAt the time of this writing, WebDAV support in Hadoop (which is implemented by\ncalling the Java API to Hadoop) is still under development, and can be tracked at https:\n//issues.apache.org/jira/browse/HADOOP-496.\nOther HDFS Interfaces\nThere are two interfaces that are specific to HDFS:\nHTTP\nHDFS defines a read-only interface for retrieving directory listings and data over\nHTTP. Directory listings are served by the namenode’s embedded web server\n(which runs on port 50070) in XML format, while file data is streamed from\ndatanodes by their web servers (running on port 50075). This protocol is not tied\nto a specific HDFS version, making it possible to write clients that can use HTTP\n50 | Chapter 3: The Hadoop Distributed Filesystemto read data from HDFS clusters that run different versions of Hadoop. HftpFile\nSystem is a such a client: it is a Hadoop filesystem that talks to HDFS over HTTP\n(HsftpFileSystem is the HTTPS variant).\nFTP\nAlthough not complete at the time of this writing (https://issues.apache.org/jira/\nbrowse/HADOOP-3199), there is an FTP interface to HDFS, which permits the use\nof the FTP protocol to interact with HDFS. This interface is a convenient way to\ntransfer data into and out of HDFS using existing FTP clients.\nThe FTP interface to HDFS is not to be confused with FTPFileSystem, which ex-\nposes any FTP server as a Hadoop filesystem.\nThe Java Interface\nIn this section, we dig into the Hadoop’s FileSystem class: the API for interacting with\none of Hadoop’s filesystems.§ While we focus mainly on the HDFS implementation,\nDistributedFileSystem, in general you should strive to write your code against the\nFileSystem abstract class, to retain portability across filesystems. This is very useful\nwhen testing your program, for example, since you can rapidly run tests using data\nstored on the local filesystem.\nReading Data from a Hadoop URL\nOne of the simplest ways to read a file from a Hadoop filesystem is by using a\njava.net.URL object to open a stream to read the data from. The general idiom is:\nInputStream in = null;\ntry {\nin = new URL(""hdfs://host/path"").openStream();\n// process in\n} finally {\nIOUtils.closeStream(in);\n}\nThere’s a little bit more work required to make Java recognize Hadoop’s hdfs URL\nscheme. This is achieved by calling the setURLStreamHandlerFactory method on URL\nwith an instance of FsUrlStreamHandlerFactory. This method can only be called once\nper JVM, so it is typically executed in a static block. This limitation means that if some\nother part of your program—perhaps a third-party component outside your control—\nsets a URLStreamHandlerFactory, you won’t be able to use this approach for reading data\nfrom Hadoop. The next section discusses an alternative.\n§ There is a Java Specification Request (JSR 203, “NIO.2,” http://jcp.org/en/jsr/detail?id=203) for an improved\nfilesystem interface in Java, with the explicit goal of providing pluggable filesystem implementations. In the\nlong term, it is possible that this new interface (which is slated to appear in Java 7) will replace Hadoop’s\nFileSystem abstraction.\nThe Java Interface | 51Example 3-1 shows a program for displaying files from Hadoop filesystems on standard\noutput, like the Unix cat command.\nExample 3-1. Displaying files from a Hadoop filesystem on standard output using a\nURLStreamHandler\npublic class URLCat {\nstatic {\nURL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\n}\n}\npublic static void main(String[] args) throws Exception {\nInputStream in = null;\ntry {\nin = new URL(args[0]).openStream();\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n}\nWe make use of the handy IOUtils class that comes with Hadoop for closing the stream\nin the finally clause, and also for copying bytes between the input stream and the\noutput stream (System.out in this case). The last two arguments to the copyBytes\nmethod are the buffer size used for copying, and whether to close the streams when the\ncopy is complete. We close the input stream ourselves, and System.out doesn’t need to\nbe closed.\nHere’s a sample run:‖\n% hadoop URLCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nReading Data Using the FileSystem API\nAs the previous section explained, sometimes it is impossible to set a URLStreamHand\nlerFactory for your application. In this case, you will need to use the FileSystem API\nto open an input stream for a file.\nA file in a Hadoop filesystem is represented by a Hadoop Path object (and not a\njava.io.File object, since its semantics are too closely tied to the local filesystem). You\ncan think of a Path as a Hadoop filesystem URI, such as hdfs://localhost/user/tom/quan-\ngle.txt.\n‖ The text is from The Quangle Wangle’s Hat by Edward Lear.\n52 | Chapter 3: The Hadoop Distributed FilesystemFileSystem is a general filesystem API, so the first step is to retrieve an instance for the\nfilesystem we want to use—HDFS in this case. There are two static factory methods\nfor getting a FileSystem instance:\npublic static FileSystem get(Configuration conf) throws IOException\npublic static FileSystem get(URI uri, Configuration conf) throws IOException\nA Configuration object encapsulates a client or server’s configuration, which is set using\nconfiguration files read from the classpath, such as conf/core-site.xml. The first method\nreturns the default filesystem (as specified in the file conf/core-site.xml, or the default\nlocal filesystem if not specified there). The second uses the given URI’s scheme and\nauthority to determine the filesystem to use, falling back to the default filesystem if no\nscheme is specified in the given URI.\nWith a FileSystem instance in hand, we invoke an open() method to get the input stream\nfor a file:\npublic FSDataInputStream open(Path f) throws IOException\npublic abstract FSDataInputStream open(Path f, int bufferSize) throws IOException\nThe first method uses a default buffer size of 4 K.\nPutting this together, we can rewrite Example 3-1 as shown in Example 3-2.\nExample 3-2. Displaying files from a Hadoop filesystem on standard output by using the FileSystem\ndirectly\npublic class FileSystemCat {\n}\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nInputStream in = null;\ntry {\nin = fs.open(new Path(uri));\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n}\nThe program runs as follows:\n% hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nThe Java Interface | 53FSDataInputStream\nThe open() method on FileSystem actually returns a FSDataInputStream rather than a\nstandard java.io class. This class is a specialization of java.io.DataInputStream with\nsupport for random access, so you can read from any part of the stream.\npackage org.apache.hadoop.fs;\npublic class FSDataInputStream extends DataInputStream\nimplements Seekable, PositionedReadable {\n// implementation elided\n}\nThe Seekable interface permits seeking to a position in the file, and a query method for\nthe current offset from the start of the file (getPos()):\npublic interface Seekable {\nvoid seek(long pos) throws IOException;\nlong getPos() throws IOException;\nboolean seekToNewSource(long targetPos) throws IOException;\n}\nCalling seek() with a position that is greater than the length of the file will result in an\nIOException. Unlike the skip() method of java.io.InputStream which positions the\nstream at a point later than the current position, seek() can move to an arbitrary, ab-\nsolute position in the file.\nThe seekToNewSource() method is not normally used by application writers. It attempts\nto find another copy of the data and seek to the offset targetPos in the new copy. This\nis used internally in HDFS to provide a reliable input stream of data to the client in the\nface of datanode failure.\nExample 3-3 is a simple extension of Example 3-2 that writes a file to standard out\ntwice: after writing it once, it seeks to the start of the file and streams through it once\nagain.\nExample 3-3. Displaying files from a Hadoop filesystem on standard output twice, by using seek\npublic class FileSystemDoubleCat {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nFSDataInputStream in = null;\ntry {\nin = fs.open(new Path(uri));\nIOUtils.copyBytes(in, System.out, 4096, false);\nin.seek(0); // go back to the start of the file\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n54 | Chapter 3: The Hadoop Distributed Filesystem}\n}\nHere’s the result of running it on a small file:\n% hadoop FileSystemDoubleCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nFSDataInputStream also implements the PositionedReadable interface for reading parts\nof a file at a given offset:\npublic interface PositionedReadable {\npublic int read(long position, byte[] buffer, int offset, int length)\nthrows IOException;\npublic void readFully(long position, byte[] buffer, int offset, int length)\nthrows IOException;\n}\npublic void readFully(long position, byte[] buffer) throws IOException;\nThe read() method reads up to length bytes from the given position in the file into the\nbuffer at the given offset in the buffer. The return value is the number of bytes actually\nread: callers should check this value as it may be less than length. The readFully()\nmethods will read length bytes into the buffer (or buffer.length bytes for the version\nthat just takes a byte array buffer), unless the end of the file is reached, in which case\nan EOFException is thrown.\nAll of these methods preserve the current offset in the file and are thread-safe, so they\nprovide a convenient way to access another part of the file—metadata perhaps—while\nreading the main body of the file. In fact, they are just implemented using the\nSeekable interface using the following pattern:\nlong oldPos = getPos();\ntry {\nseek(position);\n// read data\n} finally {\nseek(oldPos);\n}\nFinally, bear in mind that calling seek() is a relatively expensive operation, and should\nbe used sparingly. You should structure your application access patterns to rely on\nstreaming data, (by using MapReduce, for example) rather than performing a large\nnumber of seeks.\nThe Java Interface | 55Writing Data\nThe FileSystem class has a number of methods for creating a file. The simplest is the\nmethod that takes a Path object for the file to be created and returns an output stream\nto write to:\npublic FSDataOutputStream create(Path f) throws IOException\nThere are overloaded versions of this method that allow you to specify whether to\nforcibly overwrite existing files, the replication factor of the file, the buffer size to use\nwhen writing the file, the block size for the file, and file permissions.\nThe create() methods create any parent directories of the file to be\nwritten that don’t already exist. Though convenient, this behavior may\nbe unexpected. If you want the write to fail if the parent directory doesn’t\nexist, then you should check for the existence of the parent directory\nfirst by calling the exists() method.\nThere’s also an overloaded method for passing a callback interface, Progressable, so\nyour application can be notified of the progress of the data being written to the\ndatanodes:\npackage org.apache.hadoop.util;\npublic interface Progressable {\npublic void progress();\n}\nAs an alternative to creating a new file, you can append to an existing file using the\nappend() method (there are also some other overloaded versions):\npublic FSDataOutputStream append(Path f) throws IOException\nThe append operation allows a single writer to modify an already written file by opening\nit and writing data from the final offset in the file. With this API, applications that\nproduce unbounded files, such as logfiles, can write to an existing file after a restart,\nfor example. The append operation is optional and not implemented by all Hadoop\nfilesystems. For example, HDFS supports append, but S3 filesystems don’t.\nExample 3-4 shows how to copy a local file to a Hadoop filesystem. We illustrate pro-\ngress by printing a period every time the progress() method is called by Hadoop, which\nis after each 64 K packet of data is written to the datanode pipeline. (Note that this\nparticular behavior is not specified by the API, so it is subject to change in later versions\nof Hadoop. The API merely allows you to infer that “something is happening.”)\nExample 3-4. Copying a local file to a Hadoop filesystem, and shows progress\npublic class FileCopyWithProgress {\npublic static void main(String[] args) throws Exception {\nString localSrc = args[0];\nString dst = args[1];\n56 | Chapter 3: The Hadoop Distributed FilesystemInputStream in = new BufferedInputStream(new FileInputStream(localSrc));\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(dst), conf);\nOutputStream out = fs.create(new Path(dst), new Progressable() {\npublic void progress() {\nSystem.out.print(""."");\n}\n});\n}\n}\nIOUtils.copyBytes(in, out, 4096, true);\nTypical usage:\n% hadoop FileCopyWithProgress input/docs/1400-8.txt hdfs://localhost/user/tom/1400-8.txt\n...............\nCurrently, none of the other Hadoop filesystems call progress() during writes. Progress\nis important in MapReduce applications, as you will see in later chapters.\nFSDataOutputStream\nThe create() method on FileSystem returns a FSDataOutputStream, which, like\nFSDataInputStream, has a method for querying the current position in the file:\npackage org.apache.hadoop.fs;\npublic class FSDataOutputStream extends DataOutputStream implements Syncable {\npublic long getPos() throws IOException {\n// implementation elided\n}\n// implementation elided\n}\nHowever, unlike FSDataInputStream, FSDataOutputStream does not permit seeking. This\nis because HDFS allows only sequential writes to an open file, or appends to an already\nwritten file. In other words, there is no support for writing to anywhere other than the\nend of the file, so there is no value in being able to seek while writing.\nDirectories\nFileSystem provides a method to create a directory:\npublic boolean mkdirs(Path f) throws IOException\nThis method creates all of the necessary parent directories if they don’t already exist,\njust like java.io.File’s mkdirs() method. It returns true if the directory (and all parent\ndirectories) was successfully created.\nThe Java Interface | 57Often, you don’t need to explicitly create a directory, since writing a file, by calling\ncreate(), will automatically create any parent directories.\nQuerying the Filesystem\nFile metadata: FileStatus\nAn important feature of any filesystem is the ability to navigate its directory structure\nand retrieve information about the files and directories that it stores. The FileStatus\nclass encapsulates filesystem metadata for files and directories, including file length,\nblock size, replication, modification time, ownership, and permission information.\nThe method getFileStatus() on FileSystem provides a way of getting a FileStatus\nobject for a single file or directory. Example 3-5 shows an example of its use.\nExample 3-5. Demonstrating file status information\npublic class ShowFileStatusTest {\nprivate MiniDFSCluster cluster; // use an in-process HDFS cluster for testing\nprivate FileSystem fs;\n@Before\npublic void setUp() throws IOException {\nConfiguration conf = new Configuration();\nif (System.getProperty(""test.build.data"") == null) {\nSystem.setProperty(""test.build.data"", ""/tmp"");\n}\ncluster = new MiniDFSCluster(conf, 1, true, null);\nfs = cluster.getFileSystem();\nOutputStream out = fs.create(new Path(""/dir/file""));\nout.write(""content"".getBytes(""UTF-8""));\nout.close();\n}\n@After\npublic void tearDown() throws IOException {\nif (fs != null) { fs.close(); }\nif (cluster != null) { cluster.shutdown(); }\n}\n@Test(expected = FileNotFoundException.class)\npublic void throwsFileNotFoundForNonExistentFile() throws IOException {\nfs.getFileStatus(new Path(""no-such-file""));\n}\n@Test\npublic void fileStatusForFile() throws IOException {\nPath file = new Path(""/dir/file"");\nFileStatus stat = fs.getFileStatus(file);\nassertThat(stat.getPath().toUri().getPath(), is(""/dir/file""));\nassertThat(stat.isDir(), is(false));\nassertThat(stat.getLen(), is(7L));\n58 | Chapter 3: The Hadoop Distributed Filesystem}\nassertThat(stat.getModificationTime(),\nis(lessThanOrEqualTo(System.currentTimeMillis())));\nassertThat(stat.getReplication(), is((short) 1));\nassertThat(stat.getBlockSize(), is(64 * 1024 * 1024L));\nassertThat(stat.getOwner(), is(""tom""));\nassertThat(stat.getGroup(), is(""supergroup""));\nassertThat(stat.getPermission().toString(), is(""rw-r--r--""));\n@Test\npublic void fileStatusForDirectory() throws IOException {\nPath dir = new Path(""/dir"");\nFileStatus stat = fs.getFileStatus(dir);\nassertThat(stat.getPath().toUri().getPath(), is(""/dir""));\nassertThat(stat.isDir(), is(true));\nassertThat(stat.getLen(), is(0L));\nassertThat(stat.getModificationTime(),\nis(lessThanOrEqualTo(System.currentTimeMillis())));\nassertThat(stat.getReplication(), is((short) 0));\nassertThat(stat.getBlockSize(), is(0L));\nassertThat(stat.getOwner(), is(""tom""));\nassertThat(stat.getGroup(), is(""supergroup""));\nassertThat(stat.getPermission().toString(), is(""rwxr-xr-x""));\n}\n}\nIf no file or directory exists, a FileNotFoundException is thrown. However, if you are\ninterested only in the existence of a file or directory, then the exists() method on\nFileSystem is more convenient:\npublic boolean exists(Path f) throws IOException\nListing files\nFinding information on a single file or directory is useful, but you also often need to be\nable to list the contents of a directory. That’s what FileSystem’s listStatus() methods\nare for:\npublic\npublic\npublic\npublic\nFileStatus[]\nFileStatus[]\nFileStatus[]\nFileStatus[]\nlistStatus(Path f) throws IOException\nlistStatus(Path f, PathFilter filter) throws IOException\nlistStatus(Path[] files) throws IOException\nlistStatus(Path[] files, PathFilter filter) throws IOException\nWhen the argument is a file, the simplest variant returns an array of FileStatus objects\nof length 1. When the argument is a directory it returns zero or more FileStatus objects\nrepresenting the files and directories contained in the directory.\nOverloaded variants allow a PathFilter to be supplied to restrict the files and directories\nto match—you will see an example in section “PathFilter” on page 61. Finally, if you\nspecify an array of paths the result is a shortcut for calling the equivalent single-path\nlistStatus method for each path in turn and accumulating the FileStatus object arrays\nin a single array. This can be useful for building up lists of input files to process from\nThe Java Interface | 59distinct parts of the filesystem tree. Example 3-6 is a simple demonstration of this idea.\nNote the use of stat2Paths() in FileUtil for turning an array of FileStatus objects to\nan array of Path objects.\nExample 3-6. Showing the file statuses for a collection of paths in a Hadoop filesystem\npublic class ListStatus {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath[] paths = new Path[args.length];\nfor (int i = 0; i < paths.length; i++) {\npaths[i] = new Path(args[i]);\n}\n}\n}\nFileStatus[] status = fs.listStatus(paths);\nPath[] listedPaths = FileUtil.stat2Paths(status);\nfor (Path p : listedPaths) {\nSystem.out.println(p);\n}\nWe can use this program to find the union of directory listings for a collection of paths:\n% hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tom\nhdfs://localhost/user\nhdfs://localhost/user/tom/books\nhdfs://localhost/user/tom/quangle.txt\nFile patterns\nIt is a common requirement to process sets of files in a single operation. For example,\na MapReduce job for log processing might analyze a month worth of files, contained\nin a number of directories. Rather than having to enumerate each file and directory to\nspecify the input, it is convenient to use wildcard characters to match multiple files\nwith a single expression, an operation that is known as globbing. Hadoop provides two\nFileSystem methods for processing globs:\npublic FileStatus[] globStatus(Path pathPattern) throws IOException\npublic FileStatus[] globStatus(Path pathPattern, PathFilter filter) throws IOException\nThe globStatus() methods returns an array of FileStatus objects whose paths match\nthe supplied pattern, sorted by path. An optional PathFilter can be specified to restrict\nthe matches further.\nHadoop supports the same set of glob characters as Unix bash (see Table 3-2).\n60 | Chapter 3: The Hadoop Distributed FilesystemTable 3-2. Glob characters and their meanings\nGlob Name Matches\n* asterisk Matches zero or more characters\n? question mark Matches a single character\n[ab] character class Matches a single character in the set {a, b}\n[^ab] negated character class Matches a single character that is not in the set {a, b}\n[a-b] character range Matches a single character in the (closed) range [a, b], where a is lexicographically\n                     less than or equal to b\n[^a-b] negated character range Matches a single character that is not in the (closed) range [a, b], where a is\n                              lexicographically less than or equal to b\n{a,b} alternation Matches either expression a or b\n\\c escaped character Matches character c when it is a metacharacter\nImagine that logfiles are stored in a directory structure organized hierarchically by date.\nSo, for example, logfiles for the last day of 2007 would go in a directory\nnamed /2007/12/31. Suppose that the full file listing is:\n•\n•\n•\n•\n/2007/12/30\n/2007/12/31\n/2008/01/01\n/2008/01/02\nHere are some file globs and their expansions:\nGlob Expansion\n/* /2007 /2008\n/*/* /2007/12 /2008/01\n/*/12/* /2007/12/30 /2007/12/31\n/200? /2007 /2008\n/200[78] /2007 /2008\n/200[7-8] /2007 /2008\n/200[^01234569] /2007 /2008\n/*/*/{31,01} /2007/12/31 /2008/01/01\n/*/*/3{0,1} /2007/12/30 /2007/12/31\n/*/{12/31,01/01} /2007/12/31 /2008/01/01\nPathFilter\nGlob patterns are not always powerful enough to describe a set of files you want to\naccess. For example, it is not generally possible to exclude a particular file using a glob\nThe Java Interface | 61pattern. The listStatus() and globStatus() methods of FileSystem take an optional\nPathFilter, which allows programmatic control over matching:\npackage org.apache.hadoop.fs;\npublic interface PathFilter {\nboolean accept(Path path);\n}\nPathFilter is the equivalent of java.io.FileFilter for Path objects rather than File\nobjects.\nExample 3-7 shows a PathFilter for excluding paths that match a regular expression.\nExample 3-7. A PathFilter for excluding paths that match a regular expression\npublic class RegexExcludePathFilter implements PathFilter {\nprivate final String regex;\npublic RegexExcludePathFilter(String regex) {\nthis.regex = regex;\n}\n}\npublic boolean accept(Path path) {\nreturn !path.toString().matches(regex);\n}\nThe filter passes only files that don’t match the regular expression. We use the filter in\nconjunction with a glob that picks out an initial set of files to include: the filter is used\nto refine the results. For example:\nfs.globStatus(new Path(""/2007/*/*""), new RegexExcludeFilter(""^.*/2007/12/31$""))\nwill expand to /2007/12/30.\nFilters can only act on a file’s name, as represented by a Path. They can’t use a file’s\nproperties, such as creation time, as the basis of the filter. Nevertheless, they can per-\nform matching that neither glob patterns nor regular expressions can achieve. For ex-\nample, if you store files in a directory structure that is laid out by date (like in the\nprevious section), then you can write a PathFilter to pick out files that fall in a given\ndate range.\nDeleting Data\nUse the delete() method on FileSystem to permanently remove files or directories:\npublic boolean delete(Path f, boolean recursive) throws IOException\nIf f is a file or an empty directory, then the value of recursive is ignored. A nonempty\ndirectory is only deleted, along with its contents, if recursive is true (otherwise an\nIOException is thrown).\n62 | Chapter 3: The Hadoop Distributed FilesystemData Flow\nAnatomy of a File Read\nTo get an idea of how data flows between the client interacting with HDFS, the name-\nnode and the datanode, consider Figure 3-1, which shows the main sequence of events\nwhen reading a file.\nFigure 3-1. A client reading data from HDFS\nThe client opens the file it wishes to read by calling open() on the FileSystem object,\nwhich for HDFS is an instance of DistributedFileSystem (step 1 in Figure 3-1).\nDistributedFileSystem calls the namenode, using RPC, to determine the locations of\nthe blocks for the first few blocks in the file (step 2). For each block, the namenode\nreturns the addresses of the datanodes that have a copy of that block. Furthermore, the\ndatanodes are sorted according to their proximity to the client (according to the top-\nology of the cluster’s network; see “Network Topology and Hadoop” on page 64). If\nthe client is itself a datanode (in the case of a MapReduce task, for instance), then it\nwill read from the local datanode.\nThe DistributedFileSystem returns a FSDataInputStream (an input stream that supports\nfile seeks) to the client for it to read data from. FSDataInputStream in turn wraps a\nDFSInputStream, which manages the datanode and namenode I/O.\nData Flow | 63The client then calls read() on the stream (step 3). DFSInputStream, which has stored\nthe datanode addresses for the first few blocks in the file, then connects to the first\n(closest) datanode for the first block in the file. Data is streamed from the datanode\nback to the client, which calls read() repeatedly on the stream (step 4). When the end\nof the block is reached, DFSInputStream will close the connection to the datanode, then\nfind the best datanode for the next block (step 5). This happens transparently to the\nclient, which from its point of view is just reading a continuous stream.\nBlocks are read in order with the DFSInputStream opening new connections to datanodes\nas the client reads through the stream. It will also call the namenode to retrieve the\ndatanode locations for the next batch of blocks as needed. When the client has finished\nreading, it calls close() on the FSDataInputStream (step 6).\nDuring reading, if the client encounters an error while communicating with a datanode,\nthen it will try the next closest one for that block. It will also remember datanodes that\nhave failed so that it doesn’t needlessly retry them for later blocks. The client also\nverifies checksums for the data transferred to it from the datanode. If a corrupted block\nis found, it is reported to the namenode, before the client attempts to read a replica of\nthe block from another datanode.\nOne important aspect of this design is that the client contacts datanodes directly to\nretrieve data, and is guided by the namenode to the best datanode for each block. This\ndesign allows HDFS to scale to large number of concurrent clients, since the data traffic\nis spread across all the datanodes in the cluster. The namenode meanwhile merely has\nto service block location requests (which it stores in memory, making them very effi-\ncient), and does not, for example, serve data, which would quickly become a bottleneck\nas the number of clients grew.\nNetwork Topology and Hadoop\nWhat does it mean for two nodes in a local network to be “close” to each other? In the\ncontext of high-volume data processing, the limiting factor is the rate at which we can\ntransfer data between nodes—bandwidth is a scarce commodity. The idea is to use the\nbandwidth between two nodes as a measure of distance.\nRather than measuring bandwidth between nodes, which can be difficult to do in prac-\ntice (it requires a quiet cluster, and the number of pairs of nodes in a cluster grows as\nthe square of the number of nodes), Hadoop takes a simple approach in which the\nnetwork is represented as a tree and the distance between two nodes is the sum of their\ndistances to their closest common ancestor. Levels in the tree are not predefined, but\nit is common to have levels that correspond to the data center, the rack, and the node\nthat a process is running on. The idea is that the bandwidth available for each of the\nfollowing scenarios becomes progressively less:\n64 | Chapter 3: The Hadoop Distributed Filesystem• Processes on the same node\n• Different nodes on the same rack\n• Nodes on different racks in the same data center\n• Nodes in different data centers#\nFor example, imagine a node n1 on rack r1 in data center d1. This can be represented\nas /d1/r1/n1. Using this notation, here are the distances for the four scenarios:\n• distance(/d1/r1/n1, /d1/r1/n1) = 0 (processes on the same node)\n• distance(/d1/r1/n1, /d1/r1/n2) = 2 (different nodes on the same rack)\n• distance(/d1/r1/n1, /d1/r2/n3) = 4 (nodes on different racks in the same data center)\n• distance(/d1/r1/n1, /d2/r3/n4) = 6 (nodes in different data centers)\nThis is illustrated schematically in Figure 3-2. (Mathematically inclined readers will\nnotice that this is an example of a distance metric.)\nFinally, it is important to realize that Hadoop cannot divine your network topology for\nyou. It needs some help; we’ll cover how to configure topology in “Network Topol-\nogy” on page 247. By default though, it assumes that the network is flat—a single-level\nhierarchy—or in other words, that all nodes are on a single rack in a single data center.\nFor small clusters, this may actually be the case, and no further configuration is\nrequired.\nFigure 3-2. Network distance in Hadoop\n# At the time of this writing, Hadoop is not suited for running across data centers.\nData Flow | 65Anatomy of a File Write\nNext we’ll look at how files are written to HDFS. Although quite detailed, it is instruc-\ntive to understand the data flow since it clarifies HDFS’s coherency model.\nThe case we’re going to consider is the case of creating a new file, writing data to it,\nthen closing the file. See Figure 3-3.\nFigure 3-3. A client writing data to HDFS\nThe client creates the file by calling create() on DistributedFileSystem (step 1 in\nFigure 3-3). DistributedFileSystem makes an RPC call to the namenode to create a new\nfile in the filesystem’s namespace, with no blocks associated with it (step 2). The name-\nnode performs various checks to make sure the file doesn’t already exist, and that the\nclient has the right permissions to create the file. If these checks pass, the namenode\nmakes a record of the new file; otherwise, file creation fails and the client is thrown an\nIOException. The DistributedFileSystem returns a FSDataOutputStream for the client to\nstart writing data to. Just as in the read case, FSDataOutputStream wraps a DFSOutput\nStream, which handles communication with the datanodes and namenode.\nAs the client writes data (step 3), DFSOutputStream splits it into packets, which it writes\nto an internal queue, called the data queue. The data queue is consumed by the Data\nStreamer, whose responsibility it is to ask the namenode to allocate new blocks by\npicking a list of suitable datanodes to store the replicas. The list of datanodes forms a\npipeline—we’ll assume the replication level is 3, so there are three nodes in the pipeline.\nThe DataStreamer streams the packets to the first datanode in the pipeline, which stores\n66 | Chapter 3: The Hadoop Distributed Filesystemthe packet and forwards it to the second datanode in the pipeline. Similarly, the second\ndatanode stores the packet and forwards it to the third (and last) datanode in the pipe-\nline (step 4).\nDFSOutputStream also maintains an internal queue of packets that are waiting to be\nacknowledged by datanodes, called the ack queue. A packet is removed from the ack\nqueue only when it has been acknowledged by all the datanodes in the pipeline (step 5).\nIf a datanode fails while data is being written to it, then the following actions are taken,\nwhich are transparent to the client writing the data. First the pipeline is closed, and any\npackets in the ack queue are added to the front of the data queue so that datanodes\nthat are downstream from the failed node will not miss any packets. The current block\non the good datanodes is given a new identity, which is communicated to the name-\nnode, so that the partial block on the failed datanode will be deleted if the failed data-\nnode recovers later on. The failed datanode is removed from the pipeline and the\nremainder of the block’s data is written to the two good datanodes in the pipeline. The\nnamenode notices that the block is under-replicated, and it arranges for a further replica\nto be created on another node. Subsequent blocks are then treated as normal.\nIt’s possible, but unlikely, that multiple datanodes fail while a block is being written.\nAs long as dfs.replication.min replicas (default one) are written the write will succeed,\nand the block will be asynchronously replicated across the cluster until its target rep-\nlication factor is reached (dfs.replication, which defaults to three).\nWhen the client has finished writing data it calls close() on the stream (step 6). This\naction flushes all the remaining packets to the datanode pipeline and waits for ac-\nknowledgments before contacting the namenode to signal that the file is complete (step\n7). The namenode already knows which blocks the file is made up of (via Data\nStreamer asking for block allocations), so it only has to wait for blocks to be minimally\nreplicated before returning successfully.\nReplica Placement\nHow does the namenode choose which datanodes to store replicas on? There’s a trade-\noff between reliability and write bandwidth and read bandwidth here. For example,\nplacing all replicas on a single node incurs the lowest write bandwidth penalty since\nthe replication pipeline runs on a single node, but this offers no real redundancy (if the\nnode fails, the data for that block is lost). Also, the read bandwidth is high for off-rack\nreads. At the other extreme, placing replicas in different data centers may maximize\nredundancy, but at the cost of bandwidth. Even in the same data center (which is what\nall Hadoop clusters to date have run in), there are a variety of placement strategies.\nIndeed, Hadoop changed its placement strategy in release 0.17.0 to one that helps keep\na fairly even distribution of blocks across the cluster. (See “balancer” on page 284 for\ndetails on keeping a cluster balanced.)\nHadoop’s strategy is to place the first replica on the same node as the client (for clients\nrunning outside the cluster, a node is chosen at random, although the system tries not\nto pick nodes that are too full or too busy). The second replica is placed on a different\nData Flow | 67rack from the first (off-rack), chosen at random. The third replica is placed on the same\nrack as the second, but on a different node chosen at random. Further replicas are\nplaced on random nodes on the cluster, although the system tries to avoid placing too\nmany replicas on the same rack.\nOnce the replica locations have been chosen, a pipeline is built, taking network topol-\nogy into account. For a replication factor of 3, the pipeline might look like Figure 3-4.\nOverall, this strategy gives a good balance between reliability (blocks are stored on two\nracks), write bandwidth (writes only have to traverse a single network switch), read\nperformance (there’s a choice of two racks to read from), and block distribution across\nthe cluster (clients only write a single block on the local rack).\nFigure 3-4. A typical replica pipeline\nCoherency Model\nA coherency model for a filesystem describes the data visibility of reads and writes for\na file. HDFS trades off some POSIX requirements for performance, so some operations\nmay behave differently than you expect them to.\nAfter creating a file, it is visible in the filesystem namespace, as expected:\nPath p = new Path(""p"");\nfs.create(p);\nassertThat(fs.exists(p), is(true));\nHowever, any content written to the file is not guaranteed to be visible, even if the\nstream is flushed. So the file appears to have a length of zero:\nPath p = new Path(""p"");\nOutputStream out = fs.create(p);\n68 | Chapter 3: The Hadoop Distributed Filesystemout.write(""content"".getBytes(""UTF-8""));\nout.flush();\nassertThat(fs.getFileStatus(p).getLen(), is(0L));\nOnce more than a block’s worth of data has been written, the first block will be visible\nto new readers. This is true of subsequent blocks, too: it is always the current block\nbeing written that is not visible to other readers.\nHDFS provides a method for forcing all buffers to be synchronized to the datanodes\nvia the sync() method on FSDataOutputStream. After a successful return from sync(),\nHDFS guarantees that the data written up to that point in the file is persisted and visible\nto all new readers.* In the event of a crash (of the client or HDFS), the data will not be\nlost:\nPath p = new Path(""p"");\nFSDataOutputStream out = fs.create(p);\nout.write(""content"".getBytes(""UTF-8""));\nout.flush();\nout.sync();\nassertThat(fs.getFileStatus(p).getLen(), is(((long) ""content"".length())));\nThis behavior is similar to the fsync system call in Unix that commits buffered data for\na file descriptor. For example, using the Java API to write a local file, we are guaranteed\nto see the content after flushing the stream and synchronizing:\nFileOutputStream out = new FileOutputStream(localFile);\nout.write(""content"".getBytes(""UTF-8""));\nout.flush(); // flush to operating system\nout.getFD().sync(); // sync to disk\nassertThat(localFile.length(), is(((long) ""content"".length())));\nClosing a file in HDFS performs an implicit sync(), too:\nPath p = new Path(""p"");\nOutputStream out = fs.create(p);\nout.write(""content"".getBytes(""UTF-8""));\nout.close();\nassertThat(fs.getFileStatus(p).getLen(), is(((long) ""content"".length())));\nConsequences for application design\nThis coherency model has implications for the way you design applications. With no\ncalls to sync(), you should be prepared to lose up to a block of data in the event of\nclient or system failure. For many applications, this is unacceptable, so you should call\nsync() at suitable points, such as after writing a certain number of records or number\nof bytes. Though the sync() operation is designed to not unduly tax HDFS, it does have\nsome overhead, so there is a trade-off between data robustness and throughput. What\nis an acceptable trade-off is application-dependent, and suitable values can be selected\nafter measuring your application’s performance with different sync() frequencies.\n* At the time of this writing, the visibility guarantee is not honored. See https://issues.apache.org/jira/browse/\nHADOOP-4379.\nData Flow | 69Parallel Copying with distcp\nThe HDFS access patterns that we have seen so far focus on single-threaded access. It’s\npossible to act on a collection of files, by specifying file globs, for example, but for\nefficient, parallel processing of these files you would have to write a program yourself.\nHadoop comes with a useful program called distcp for copying large amounts of data\nto and from Hadoop filesystems in parallel.\nThe canonical use case for distcp is for transferring data between two HDFS clusters.\nIf the clusters are running identical versions of Hadoop, the hdfs scheme is\nappropriate:\n% hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar\nThis will copy the /foo directory (and its contents) from the first cluster to the /bar\ndirectory on the second cluster, so the second cluster ends up with the directory struc-\nture /bar/foo. If /bar doesn’t exist, it will be created first. You can specify multiple source\npaths, and all will be copied to the destination. Source paths must be absolute.\nBy default, distcp will skip files that already exist in the destination, but they can be\noverwritten by supplying the -overwrite option. You can also update only files that\nhave changed using the -update option.\nUsing either (or both) of -overwrite or -update changes the how the\nsource and destination paths are interpreted. This is best shown with\nan example. If we changed a file in the /foo subtree on the first cluster\nfrom the previous example, then we could synchronize the change with\nthe second cluster by running:\n% hadoop distcp -update hdfs://namenode1/foo hdfs://namenode2/bar/foo\nThe extra trailing /foo subdirectory is needed on the destination, as now\nthe contents of the source directory are copied to the contents of the\ndestination directory. (If you are familiar with rsync, you can think of\nthe -overwrite or -update options as adding an implicit trailing slash to\nthe source.)\nIf you are unsure of the effect of a distcp operation, it is a good idea to\ntry it out on a small test directory tree first.\nThere are more options to control the behavior of distcp, including ones to preserve file\nattributes, ignore failures, and limit the number of files or total data copied. Run it with\nno options to see the usage instructions.\ndistcp is implemented as a MapReduce job where the work of copying is done by the\nmaps that run in parallel across the cluster. There are no reducers. Each file is copied\nby a single map, and distcp tries to give each map approximately the same amount of\ndata, by bucketing files into roughly equal allocations.\n70 | Chapter 3: The Hadoop Distributed FilesystemThe number of maps is decided as follows. Since it’s a good idea to get each map to\ncopy a reasonable amount of data to minimize overheads in task setup, each map copies\nat least 256 MB (unless the total size of the input is less, in which case one map handles\nit all). For example, 1 GB of files will be given four map tasks. When the data size is\nvery large, it becomes necessary to limit the number of maps in order to limit bandwidth\nand cluster utilization. By default, the maximum number of maps is 20 per (tasktracker)\ncluster node. For example, copying 1,000 GB of files to a 100-node cluster will allocate\n2,000 maps (20 per node), so each will copy 512 MB on average. This can be reduced\nby specifying the -m argument to distcp. For example, -m 1000 would allocate 1,000\nmaps, each copying 1 GB on average.\nIf you try to use distcp between two HDFS clusters that are running different versions,\nthe copy will fail if you use the hdfs protocol, since the RPC systems are incompatible.\nTo remedy this, you can use the HTTP-based HFTP filesystem to read from the source.\nThe job must run on the destination cluster so that the HDFS RPC versions are com-\npatible. To repeat the previous example using HFTP:\n% hadoop distcp hftp://namenode1:50070/foo hdfs://namenode2/bar\nNote that you need to specify the namenode’s web port in the source URI. This is\ndetermined by the dfs.http.address property, which defaults to 50070.\nKeeping an HDFS Cluster Balanced\nWhen copying data into HDFS, it’s important to consider cluster balance. HDFS works\nbest when the file blocks are evenly spread across the cluster, so you want to ensure\nthat distcp doesn’t disrupt this. Going back to the 1,000 GB example, by specifying -m\n1 a single map would do the copy, which—apart from being slow and not using the\ncluster resources efficiently—would mean that the first replica of each block would\nreside on the node running the map (until the disk filled up). The second and third\nreplicas would be spread across the cluster, but this one node would be unbalanced.\nBy having more maps than nodes in the cluster, this problem is avoided—for this rea-\nson, it’s best to start by running distcp with the default of 20 maps per node.\nHowever, it’s not always possible to prevent a cluster from becoming unbalanced. Per-\nhaps you want to limit the number of maps so that some of the nodes can be used by\nother jobs. In this case, you can use the balancer tool (see “balancer” on page 284) to\nsubsequently improve the block distribution across the cluster.\nHadoop Archives\nHDFS stores small files inefficiently, since each file is stored in a block, and block\nmetadata is held in memory by the namenode. Thus, a large number of small files can\neat up a lot of memory on the namenode. (Note, however, that small files do not take\nup any more disk space than is required to store the raw contents of the file. For\nHadoop Archives | 71example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not\n128 MB.)\nHadoop Archives, or HAR files, are a file archiving facility that packs files into HDFS\nblocks more efficiently, thereby reducing namenode memory usage while still allowing\ntransparent access to files. In particular, Hadoop Archives can be used as input to\nMapReduce.\nUsing Hadoop Archives\nA Hadoop Archive is created from a collection of files using the archive tool. The tool\nruns a MapReduce job to process the input files in parallel, so to run it, you need a\nMapReduce cluster running to use it. Here are some files in HDFS that we would like\nto archive:\n% hadoop fs -lsr /my/files\n-rw-r--r--\n1 tom supergroup\ndrwxr-xr-x\n- tom supergroup\n-rw-r--r--\n1 tom supergroup\n1 2009-04-09 19:13 /my/files/a\n0 2009-04-09 19:13 /my/files/dir\n1 2009-04-09 19:13 /my/files/dir/b\nNow we can run the archive command:\n% hadoop archive -archiveName files.har /my/files /my\nThe first option is the name of the archive, here files.har. HAR files always have\na .har extension, which is mandatory for reasons you shall see later. Next comes the\nfiles to put in the archive. Here we are archiving only one source tree, the files in /my/\nfiles in HDFS, but the tool accepts multiple source trees. The final argument is the\noutput directory for the HAR file. Let’s see what the archive has created:\n% hadoop fs -ls /my\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\ndrwxr-xr-x\n- tom supergroup\n% hadoop fs -ls /my/files.har\nFound 3 items\n-rw-r--r-- 10 tom supergroup\n-rw-r--r-- 10 tom supergroup\n-rw-r--r--\n1 tom supergroup\n0 2009-04-09 19:13 /my/files\n0 2009-04-09 19:13 /my/files.har\n165 2009-04-09 19:13 /my/files.har/_index\n23 2009-04-09 19:13 /my/files.har/_masterindex\n2 2009-04-09 19:13 /my/files.har/part-0\nThe directory listing shows what a HAR file is made of: two index files and a collection\nof part files—just one in this example. The part files contain the contents of a number\nof the original files concatenated together, and the indexes make it possible to look up\nthe part file that an archived file is contained in, and its offset and length. All these\ndetails are hidden from the application, however, which uses the har URI scheme to\ninteract with HAR files, using a HAR filesystem that is layered on top of the underlying\nfilesystem (HDFS in this case). The following command recursively lists the files in the\narchive:\n% hadoop fs -lsr har:///my/files.har\ndrw-r--r--\n- tom supergroup\ndrw-r--r--\n- tom supergroup\n72 | Chapter 3: The Hadoop Distributed Filesystem\n0 2009-04-09 19:13 /my/files.har/my\n0 2009-04-09 19:13 /my/files.har/my/files-rw-r--r--\ndrw-r--r--\n-rw-r--r--\n10 tom supergroup\n- tom supergroup\n10 tom supergroup\n1 2009-04-09 19:13 /my/files.har/my/files/a\n0 2009-04-09 19:13 /my/files.har/my/files/dir\n1 2009-04-09 19:13 /my/files.har/my/files/dir/b\nThis is quite straightforward if the filesystem that the HAR file is on is the default\nfilesystem. On the other hand, if you want to refer to a HAR file on a different filesystem,\nthen you need to use a different form of the path URI to normal. These two commands\nhave the same effect, for example:\n% hadoop fs -lsr har:///my/files.har/my/files/dir\n% hadoop fs -lsr har://hdfs-localhost:8020/my/files.har/my/files/dir\nNotice in the second form that the scheme is still har to signify a HAR filesystem, but\nthe authority is hdfs to specify the underlying filesystem’s scheme, followed by a dash\nand the HDFS host (localhost) and port (8020). We can now see why HAR files have\nto have a .har extension. The HAR filesystem translates the har URI into a URI for the\nunderlying filesystem, by looking at the authority and path up to and including the\ncomponent with the .har extension. In this case, it is hdfs://localhost:8020/user/tom/\nfiles.har. The remaining part of the path is the path of the file in the archive: /user/tom/\nfiles/dir.\nTo delete a HAR file, you need to use the recursive form of delete, since to the underlying\nfilesystem, the HAR file is a directory.\n% hadoop fs -rmr /my/files.har\nLimitations\nThere are a few limitations to be aware of with HAR files. Creating an archive creates\na copy of the original files, so you need as much disk space as the files you are archiving\nto create the archive (although you can delete the originals once you have created the\narchive). There is currently no support for archive compression, although the files that\ngo into the archive can be compressed (HAR files are like tar files in this respect).\nArchives are immutable once they have been created. To add or remove files, you must\nrecreate the archive. In practice, this is not a problem for files that don’t change after\nbeing written, since they can be archived in batches on a regular basis, such as daily or\nweekly.\nAs noted earlier, HAR files can be used as input to MapReduce. However, there is no\narchive-aware InputFormat that can pack multiple files into a single MapReduce split,\nso processing lots of small files, even in a HAR file, can still be inefficient. “Small files\nand CombineFileInputFormat” on page 190 discusses another approach to this\nproblem.\nHadoop Archives | 73CHAPTER 4\nHadoop I/O\nHadoop comes with a set of primitives for data I/O. Some of these are techniques that\nare more general than Hadoop, such as data integrity and compression, but deserve\nspecial consideration when dealing with multiterabyte datasets. Others are Hadoop\ntools or APIs that form the building blocks for developing distributed systems, such as\nserialization frameworks and on-disk data structures.\nData Integrity\nUsers of Hadoop rightly expect that no data will be lost or corrupted during storage or\nprocessing. However, since every I/O operation on the disk or network carries with it\na small chance of introducing errors into the data that it is reading or writing, when the\nvolumes of data flowing through the system are as large as the ones Hadoop is capable\nof handling, the chance of data corruption occurring is high.\nThe usual way of detecting corrupted data is by computing a checksum for the data\nwhen it first enters the system, and then whenever it is transmitted across a channel\nthat is unreliable and hence capable of corrupting the data. The data is deemed to be\ncorrupt if the newly generated checksum doesn’t exactly match the original. This tech-\nnique doesn’t offer any way to fix the data—merely error detection. (And this is a reason\nfor not using low-end hardware; in particular, be sure to use ECC memory.) Note that\nit is possible that it’s the checksum that is corrupt, not the data, but this is very unlikely,\nsince the checksum is much smaller than the data.\nA commonly used error-detecting code is CRC-32 (cyclic redundancy check), which\ncomputes a 32-bit integer checksum for input of any size.\nData Integrity in HDFS\nHDFS transparently checksums all data written to it and by default verifies checksums\nwhen reading data. A separate checksum is created for every io.bytes.per.checksum\nbytes of data. The default is 512 bytes, and since a CRC-32 checksum is 4 bytes long,\nthe storage overhead is less than 1%.\n75Datanodes are responsible for verifying the data they receive before storing the data\nand its checksum. This applies to data that they receive from clients and from other\ndatanodes during replication. A client writing data sends it to a pipeline of datanodes\n(as explained in Chapter 3), and the last datanode in the pipeline verifies the checksum.\nIf it detects an error, the client receives a ChecksumException, a subclass of IOException.\nWhen clients read data from datanodes, they verify checksums as well, comparing them\nwith the ones stored at the datanode. Each datanode keeps a persistent log of checksum\nverifications, so it knows the last time each of its blocks was verified. When a client\nsuccessfully verifies a block, it tells the datanode, which updates its log. Keeping sta-\ntistics such as these is valuable in detecting bad disks.\nAside from block verification on client reads, each datanode runs a DataBlockScanner\nin a background thread that periodically verifies all the blocks stored on the datanode.\nThis is to guard against corruption due to “bit rot” in the physical storage media. See\n“Datanode block scanner” on page 283 for details on how to access the scanner\nreports.\nSince HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of\nthe good replicas to produce a new, uncorrupt replica. The way this works is that if a\nclient detects an error when reading a block, it reports the bad block and the datanode\nit was trying to read from to the namenode before throwing a ChecksumException. The\nnamenode marks the block replica as corrupt, so it doesn’t direct clients to it, or try to\ncopy this replica to another datanode. It then schedules a copy of the block to be re-\nplicated on another datanode, so its replication factor is back at the expected level.\nOnce this has happened, the corrupt replica is deleted.\nIt is possible to disable verification of checksums by passing false to the setVerify\nChecksum() method on FileSystem, before using the open() method to read a file. The\nsame effect is possible from the shell by using the -ignoreCrc option with the -get or\nthe equivalent -copyToLocal command. This feature is useful if you have a corrupt file\nthat you want to inspect so you can decide what to do with it. For example, you might\nwant to see whether it can be salvaged before you delete it.\nLocalFileSystem\nThe Hadoop LocalFileSystem performs client-side checksumming. This means that\nwhen you write a file called filename, the filesystem client transparently creates a hidden\nfile, .filename.crc, in the same directory containing the checksums for each chunk of\nthe file. Like HDFS, the chunk size is controlled by the io.bytes.per.checksum property,\nwhich defaults to 512 bytes. The chunk size is stored as metadata in the .crc file, so the\nfile can be read back correctly even if the setting for the chunk size has changed.\nChecksums are verified when the file is read, and if an error is detected,\nLocalFileSystem throws a ChecksumException.\n76 | Chapter 4: Hadoop I/OChecksums are fairly cheap to compute (in Java, they are implemented in native code),\ntypically adding a few percent overhead to the time to read or write a file. For most\napplications, this is an acceptable price to pay for data integrity. It is, however, possible\nto disable checksums: the use case here is when the underlying filesystem support\nchecksums natively. This is accomplished by using RawLocalFileSystem in place of\nLocalFileSystem. To do this globally in an application, it suffices to remap the imple-\nmentation for file URIs by setting the property fs.file.impl to the value\norg.apache.hadoop.fs.RawLocalFileSystem. Alternatively, you can directly create a Raw\nLocalFileSystem instance, which may be useful if you want to disable checksum veri-\nfication for only some reads; for example:\nConfiguration conf = ...\nFileSystem fs = new RawLocalFileSystem();\nfs.initialize(null, conf);\nChecksumFileSystem\nLocalFileSystem uses ChecksumFileSystem to do its work, and this class makes it easy\nto add checksumming to other (nonchecksummed) filesystems, as ChecksumFileSys\ntem is just a wrapper around FileSystem. The general idiom is as follows:\nFileSystem rawFs = ...\nFileSystem checksummedFs = new ChecksumFileSystem(rawFs);\nThe underlying filesystem is called the raw filesystem, and may be retrieved using the\ngetRawFileSystem() method on ChecksumFileSystem. ChecksumFileSystem has a few\nmore useful methods for working with checksums, such as getChecksumFile() for get-\nting the path of a checksum file for any file. Check the documentation for the others.\nIf an error is detected by ChecksumFileSystem when reading a file, it will call its\nreportChecksumFailure() method. The default implementation does nothing, but\nLocalFileSystem moves the offending file and its checksum to a side directory on the\nsame device called bad_files. Administrators should periodically check for these bad\nfiles and take action on them.\nCompression\nFile compression brings two major benefits: it reduces the space needed to store files,\nand it speeds up data transfer across the network, or to or from disk. When dealing\nwith large volumes of data, both of these savings can be significant, so it pays to carefully\nconsider how to use compression in Hadoop.\nCompression | 77There are many different compression formats, tools and algorithms, each with differ-\nent characteristics. Table 4-1 lists some of the more common ones that can be used\nwith Hadoop.*\nTable 4-1. A summary of compression formats\nCompression format Tool Algorithm Filename extension Multiple files Splittable\nDEFLATEa N/A DEFLATE .deflate No No\ngzip gzip DEFLATE .gz No No\nZIP zip DEFLATE .zip Yes Yes, at file boundaries\nbzip2 bzip2 bzip2 .bz2 No Yes\nLZO lzop LZO .lzo No No\na DEFLATE is a compression algorithm whose standard implementation is zlib. There is no commonly\navailable command-line tool for producing files in DEFLATE format, as gzip is normally used. (Note that\nthe gzip file format is DEFLATE with extra headers and a footer.) The .deflate filename extension is a\nHadoop convention.\nAll compression algorithms exhibit a space/time trade-off: faster compression and de-\ncompression speeds usually come at the expense of smaller space savings. All of the\ntools listed in Table 4-1 give some control over this trade-off at compression time by\noffering nine different options: –1 means optimize for speed and -9 means optimize for\nspace. For example, the following command creates a compressed file file.gz using the\nfastest compression method:\ngzip -1 file\nThe different tools have very different compression characteristics. Both gzip and ZIP\nare general-purpose compressors, and sit in the middle of the space/time trade-off.\nBzip2 compresses more effectively than gzip or ZIP, but is slower. Bzip2’s decompres-\nsion speed is faster than its compression speed, but it is still slower than the other\nformats. LZO, on the other hand, optimizes for speed: it is faster than gzip or ZIP (or\nany other compression or decompression tool†), but compresses slightly less effectively.\nThe “Splittable” column in Table 4-1 indicates whether the compression format sup-\nports splitting; that is, whether you can seek to any point in the stream and start reading\nfrom some point further on. Splittable compression formats are especially suitable for\nMapReduce; see “Compression and Input Splits” on page 83 for further discussion.\n* At the time of this writing, Hadoop’s ZIP integration is incomplete. See https://issues.apache.org/jira/browse/\nHADOOP-1824.\n† Jeff Gilchrist’s Archive Comparison Test at http://compression.ca/act/act-summary.html contains benchmarks\nfor compression and decompression speed, and compression ratio for a wide range of tools.\n78 | Chapter 4: Hadoop I/OCodecs\nA codec is the implementation of a compression-decompression algorithm. In Hadoop,\na codec is represented by an implementation of the CompressionCodec interface. So, for\nexample, GzipCodec encapsulates the compression and decompression algorithm for\ngzip. Table 4-2 lists the codecs that are available for Hadoop.\nTable 4-2. Hadoop compression codecs\nCompression format Hadoop CompressionCodec\nDEFLATE org.apache.hadoop.io.compress.DefaultCodec\ngzip org.apache.hadoop.io.compress.GzipCodec\nbzip2 org.apache.hadoop.io.compress.BZip2Codec\nLZO com.hadoop.compression.lzo.LzopCodec\nThe LZO libraries are GPL-licensed and may not be included in Apache distributions,\nso for this reason the Hadoop codecs must be downloaded separately from http://code\n.google.com/p/hadoop-gpl-compression/. The LzopCodec is compatible with the lzop tool,\nwhich is essentially the LZO format with extra headers, and is the one you normally\nwant. There is also a LzoCodec for the pure LZO format, which uses the .lzo_deflate\nfilename extension (by analogy with DEFLATE, which is gzip without the headers).\nCompressing and decompressing streams with CompressionCodec\nCompressionCodec has two methods that allow you to easily compress or decompress\ndata. To compress data being written to an output stream, use the createOutput\nStream(OutputStream out) method to create a CompressionOutputStream to which you\nwrite your uncompressed data to have it written in compressed form to the underlying\nstream. Conversely, to decompress data being read from an input stream, call\ncreateInputStream(InputStream in) to obtain a CompressionInputStream, which allows\nyou to read uncompressed data from the underlying stream.\nCompressionOutputStream\nand\nCompressionInputStream\nare\nsimilar\nto\njava.util.zip.DeflaterOutputStream and java.util.zip.DeflaterInputStream, except\nthat both of the former provide the ability to reset their underlying compressor or de-\ncompressor, which is important for applications that compress sections of the data\nstream as separate blocks, such as SequenceFile, described in “Sequence-\nFile” on page 103.\nExample 4-1 illustrates how to use the API to compress data read from standard input\nand write it to standard output.\nExample 4-1. A program to compress data read from standard input and write it to standard output\npublic class StreamCompressor {\npublic static void main(String[] args) throws Exception {\nCompression | 79String codecClassname = args[0];\nClass<?> codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\n}\n}\nCompressionOutputStream out = codec.createOutputStream(System.out);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\nThe application expects the fully qualified name of the CompressionCodec implementa-\ntion as the first command-line argument. We use ReflectionUtils to construct a new\ninstance of the codec, then obtain a compression wrapper around System.out. Then we\ncall the utility method copyBytes() on IOUtils to copy the input to the output, which\nis compressed by the CompressionOutputStream. Finally, we call finish() on\nCompressionOutputStream, which tells the compressor to finish writing to the com-\npressed stream, but doesn’t close the stream. We can try it out with the following\ncommand line, which compresses the string “Text” using the StreamCompressor pro-\ngram with the GzipCodec, then decompresses it from standard input using gunzip:\n% echo ""Text"" | hadoop StreamCompressor org.apache.hadoop.io.compress.GzipCodec \\\n| gunzip -\nText\nInferring CompressionCodecs using CompressionCodecFactory\nIf you are reading a compressed file, you can normally infer the codec to use by looking\nat its filename extension. A file ending in .gz can be read with GzipCodec, and so on.\nThe extension for each compression format is listed in Table 4-1.\nCompressionCodecFactory provides a way of mapping a filename extension to a\nCompressionCodec using its getCodec() method, which takes a Path object for the file in\nquestion. Example 4-2 shows an application that uses this feature to decompress files.\nExample 4-2. A program to decompress a compressed file using a codec inferred from the file's\nextension\npublic class FileDecompressor {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath inputPath = new Path(uri);\nCompressionCodecFactory factory = new CompressionCodecFactory(conf);\nCompressionCodec codec = factory.getCodec(inputPath);\nif (codec == null) {\nSystem.err.println(""No codec found for "" + uri);\nSystem.exit(1);\n80 | Chapter 4: Hadoop I/O}\nString outputUri =\nCompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension());\n}\n}\nInputStream in = null;\nOutputStream out = null;\ntry {\nin = codec.createInputStream(fs.open(inputPath));\nout = fs.create(new Path(outputUri));\nIOUtils.copyBytes(in, out, conf);\n} finally {\nIOUtils.closeStream(in);\nIOUtils.closeStream(out);\n}\nOnce the codec has been found, it is used to strip off the file suffix to form the output\nfilename (via the removeSuffix() static method of CompressionCodecFactory). In this\nway, a file named file.gz is decompressed to file by invoking the program as follows:\n% hadoop FileDecompressor file.gz\nCompressionCodecFactory\nfinds codecs from a list defined by the\nio.compression.codecs configuration property. By default, this lists all the codecs pro-\nvided by Hadoop (see Table 4-3), so you would need to alter it only if you have a custom\ncodec that you wish to register (such as the externally hosted LZO codecs). Each codec\nknows its default filename extension, thus permitting CompressionCodecFactory to\nsearch through the registered codecs to find a match for a given extension (if any).\nTable 4-3. Compression codec properties\nProperty name Type Default value Description\nio.compression.codecs comma-separated org.apache.hadoop.io. A list of the Compres\n                      Class names compress.DefaultCodec, sionCodec classes for\n                                  org.apache.hadoop.io. compression/\n                                  compress.GzipCodec, decompression.\n                                  org.apache.hadoop.io. \n                                  compress.Bzip2Codec \nNative libraries\nFor performance, it is preferable to use a native library for compression and\ndecompression. For example, in one test, using the native gzip libraries reduced de-\ncompression times by up to 50% and compression times by around 10% (compared to\nthe built-in Java implementation). Table 4-4 shows the availability of Java and native\nimplementations for each compression format. Not all formats have native implemen-\ntations (bzip2, for example), whereas others are only available as a native implemen-\ntation (LZO, for example).\nCompression | 81Table 4-4. Compression library implementations\nCompression format Java implementation Native implementation\nDEFLATE Yes Yes\ngzip Yes Yes\nbzip2 Yes No\nLZO No Yes\nHadoop comes with prebuilt native compression libraries for 32- and 64-bit Linux,\nwhich you can find in the lib/native directory. For other platforms, you will need to\ncompile the libraries yourself, following the instructions on the Hadoop wiki at http://\nwiki.apache.org/hadoop/NativeHadoop.\nThe native libraries are picked up using the Java system property java.library.path.\nThe hadoop script in the bin directory sets this property for you, but if you don’t use\nthis script, you will need to set the property in your application.\nBy default Hadoop looks for native libraries for the platform it is running on, and loads\nthem automatically if they are found. This means you don’t have to change any con-\nfiguration settings to use the native libraries. In some circumstances, however, you may\nwish to disable use of native libraries, such as when you are debugging a compression-\nrelated problem. You can achieve this by setting the property hadoop.native.lib to\nfalse, which ensures that the built-in Java equivalents will be used (if they are available).\nCodecPool. If you are using a native library and you are doing a lot of compression or\ndecompression in your application, consider using CodecPool, which allows you to re-\nuse compressors and decompressors, thereby amortizing the cost of creating these\nobjects.\nThe code in Example 4-3 shows the API, although in this program, which only creates\na single Compressor, there is really no need to use a pool.\nExample 4-3. A program to compress data read from standard input and write it to standard output\nusing a pooled compressor\npublic class PooledStreamCompressor {\npublic static void main(String[] args) throws Exception {\nString codecClassname = args[0];\nClass<?> codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\nCompressor compressor = null;\ntry {\ncompressor = CodecPool.getCompressor(codec);\nCompressionOutputStream out =\ncodec.createOutputStream(System.out, compressor);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\n82 | Chapter 4: Hadoop I/O}\n}\n} finally {\nCodecPool.returnCompressor(compressor);\n}\nWe retrieve a Compressor instance from the pool for a given CompressionCodec, which\nwe use in the codec’s overloaded createOutputStream() method. By using a finally\nblock, we ensure that the compressor is returned to the pool even if there is an\nIOException while copying the bytes between the streams.\nCompression and Input Splits\nWhen considering how to compress data that will be processed by MapReduce, it is\nimportant to understand whether the compression format supports splitting. Consider\nan uncompressed file stored in HDFS whose size is 1 GB. With a HDFS block size of\n64 MB, the file will be stored as 16 blocks, and a MapReduce job using this file as input\nwill create 16 input splits, each processed independently as input to a separate map task.\nImagine now the file is a gzip-compressed file whose compressed size is 1 GB. As before,\nHDFS will store the file as 16 blocks. However, creating a split for each block won’t\nwork since it is impossible to start reading at an arbitrary point in the gzip stream, and\ntherefore impossible for a map task to read its split independently of the others. The\ngzip format uses DEFLATE to store the compressed data, and DEFLATE stores data\nas a series of compressed blocks. The problem is that the start of each block is not\ndistinguished in any way that would allow a reader positioned at an arbitrary point in\nthe stream to advance to the beginning of the next block, thereby synchronizing itself\nwith the stream. For this reason, gzip does not support splitting.\nIn this case, MapReduce will do the right thing, and not try to split the gzipped file,\nsince it knows that the input is gzip-compressed (by looking at the filename extension)\nand that gzip does not support splitting. This will work, but at the expense of locality:\na single map will process the 16 HDFS blocks, most of which will not be local to the\nmap. Also, with fewer maps, the job is less granular, and so may take longer to run.\nIf the file in our hypothetical example were an LZO file, we would have the same\nproblem since the underlying compression format does not provide a way for a reader\nto synchronize itself with the stream. A bzip2 file, however, does provide a synchroni-\nzation marker between blocks (a 48-bit approximation of pi), so it does support split-\nting. (Table 4-1 lists whether each compression format supports splitting.)\nFor collections of files, the issues are slightly different. ZIP is an archive format, so it\ncan combine multiple files into a single ZIP archive. Each file is compressed separately,\nand the locations of all the files in the archive are stored in a central directory at the\nend of the ZIP file. This property means that ZIP files support splitting at file bounda-\nries, with each split containing one or more files from the ZIP archive. At the time of\nthis writing, however, Hadoop does not support ZIP files as an input format.\nCompression | 83Which Compression Format Should I Use?\nWhich compression format you should use depends on your application. Do you want\nto maximize the speed of your application or are you more concerned about keeping\nstorage costs down? In general, you should try different strategies for your application,\nand benchmark them with representative datasets to find the best approach.\nFor large, unbounded files, like logfiles, the options are:\n• Store the files uncompressed.\n• Use a compression format that supports splitting, like bzip2.\n• Split the file into chunks in the application and compress each chunk separately\nusing any supported compression format (it doesn’t matter whether it is splittable).\nIn this case, you should choose the chunk size so that the compressed chunks are\napproximately the size of an HDFS block.\n• Use Sequence File, which supports compression and splitting. See “Sequence-\nFile” on page 103.\nFor large files, you should not use a compression format that does not support splitting\non the whole file, since you lose locality and make MapReduce applications very\ninefficient.\nFor archival purposes, consider the Hadoop archive format (see “Hadoop Ar-\nchives” on page 71), although it does not support compression.\nUsing Compression in MapReduce\nAs described in “Inferring CompressionCodecs using CompressionCodecFac-\ntory” on page 80, if your input files are compressed, they will be automatically decom-\npressed as they are read by MapReduce, using the filename extension to determine the\ncodec to use.\nTo compress the output of a MapReduce job, in the job configuration, set the\nmapred.output.compress property to true, and the mapred.output.compression.codec\nproperty to the classname of the compression codec you want to use, as shown in\nExample 4-4.\nExample 4-4. Application to run the maximum temperature job producing compressed output\npublic class MaxTemperatureWithCompression {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperatureWithCompression <input path> "" +\n""<output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperatureWithCompression.class);\n84 | Chapter 4: Hadoop I/Oconf.setJobName(""Max temperature with output compression"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setBoolean(""mapred.output.compress"", true);\nconf.setClass(""mapred.output.compression.codec"", GzipCodec.class,\nCompressionCodec.class);\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\n}\nJobClient.runJob(conf);\nWe run the program over compressed input (which doesn’t have to use the same com-\npression format as the output, although it does in this example) as follows:\n% hadoop MaxTemperatureWithCompression input/ncdc/sample.txt.gz output\nEach part of the final output is compressed; in this case, there is a single part:\n% gunzip -c output/part-00000.gz\n1949\n111\n1950\n22\nIf you are emitting sequence files for your output, then you can set the mapred.out\nput.compression.type property to control the type of compression to use. The default\nis RECORD, which compresses individual records. Changing this to BLOCK, which\ncompresses groups of records, is recommended since it compresses better (see “The\nSequenceFile Format” on page 109).\nCompressing map output\nEven if your MapReduce application reads and writes uncompressed data, it may ben-\nefit from compressing the intermediate output of the map phase. Since the map output\nis written to disk and transferred across the network to the reducer nodes, by using a\nfast compressor such as LZO, you can get performance gains simply because the volume\nof data to transfer is reduced. The configuration properties to enable compression for\nmap outputs and to set the compression format are shown in Table 4-5.\nCompression | 85Table 4-5. Map output compression properties\nProperty name Type Default value Description\nmapred.compress.map.output boolean false Compress map outputs.\nmapred.map.output. Class org.apache.hadoop.io. The compression codec to use for\ncompression.codec compress.DefaultCodec map outputs.\nHere are the lines to add to enable gzip map output compression in your job:\nconf.setCompressMapOutput(true);\nconf.setMapOutputCompressorClass(GzipCodec.class);\nSerialization\nSerialization is the process of turning structured objects into a byte stream for trans-\nmission over a network or for writing to persistent storage. Deserialization is the\nprocess of turning a byte stream back into a series of structured objects.\nSerialization appears in two quite distinct areas of distributed data processing: for\ninterprocess communication and for persistent storage.\nIn Hadoop, interprocess communication between nodes in the system is implemented\nusing remote procedure calls (RPCs). The RPC protocol uses serialization to render the\nmessage into a binary stream to be sent to the remote node, which then deserializes the\nbinary stream into the original message. In general, it is desirable that an RPC seriali-\nzation format is:\nCompact\nA compact format makes the best use of network bandwidth, which is the most\nscarce resource in a data center.\nFast\nInterprocess communication forms the backbone for a distributed system, so it is\nessential that there is as little performance overhead as possible for the serialization\nand deserialization process.\nExtensible\nProtocols change over time to meet new requirements, so it should be\nstraightforward to evolve the protocol in a controlled manner for clients and serv-\ners. For example, it should be possible to add a new argument to a method call,\nand have the new servers accept messages in the old format (without the new ar-\ngument) from old clients.\nInteroperable\nFor some systems, it is desirable to be able to support clients that are written in\ndifferent languages to the server, so the format needs to be designed to make this\npossible.\n86 | Chapter 4: Hadoop I/OOn the face of it, the data format chosen for persistent storage would have different\nrequirements from a serialization framework. After all, the lifespan of an RPC is less\nthan a second, whereas persistent data may be read years after it was written. As it turns\nout, the four desirable properties of an RPC’s serialization format are also crucial for a\npersistent storage format. We want the storage format to be compact (to make efficient\nuse of storage space), fast (so the overhead in reading or writing terabytes of data is\nminimal), extensible (so we can transparently read data written in an older format),\nand interoperable (so we can read or write persistent data using different languages).\nHadoop uses its own serialization format, Writables, which is certainly compact and\nfast (but not so easy to extend, or use from languages other than Java). Since Writables\nare central to Hadoop (MapReduce programs use them for their key and value types),\nwe look at them in some depth in the next section, before briefly turning to other well-\nknown serialization frameworks, like Apache Thrift and Google Protocol Buffers.\nThe Writable Interface\nThe Writable interface defines two methods: one for writing its state to a DataOutput\nbinary stream, and one for reading its state from a DataInput binary stream:\npackage org.apache.hadoop.io;\nimport java.io.DataOutput;\nimport java.io.DataInput;\nimport java.io.IOException;\npublic interface Writable {\nvoid write(DataOutput out) throws IOException;\nvoid readFields(DataInput in) throws IOException;\n}\nLet’s look at a particular Writable to see what we can do with it. We will use\nIntWritable, a wrapper for a Java int. We can create one and set its value using the\nset() method:\nIntWritable writable = new IntWritable();\nwritable.set(163);\nEquivalently, we can use the constructor that takes the integer value:\nIntWritable writable = new IntWritable(163);\nTo examine the serialized form of the IntWritable, we write a small helper method that\nwraps a java.io.ByteArrayOutputStream in a java.io.DataOutputStream (an implemen-\ntation of java.io.DataOutput) to capture the bytes in the serialized stream:\npublic static byte[] serialize(Writable writable) throws IOException {\nByteArrayOutputStream out = new ByteArrayOutputStream();\nDataOutputStream dataOut = new DataOutputStream(out);\nwritable.write(dataOut);\ndataOut.close();\nSerialization | 87}\nreturn out.toByteArray();\nAn integer is written using four bytes (as we see using JUnit 4 assertions):\nbyte[] bytes = serialize(writable);\nassertThat(bytes.length, is(4));\nThe bytes are written in big-endian order (so the most significant byte is written to the\nstream first, this is dictated by the java.io.DataOutput interface), and we can see their\nhexadecimal representation by using a method on Hadoop’s StringUtils:\nassertThat(StringUtils.byteToHexString(bytes), is(""000000a3""));\nLet’s try deserialization. Again, we create a helper method to read a Writable object\nfrom a byte array:\npublic static byte[] deserialize(Writable writable, byte[] bytes)\nthrows IOException {\nByteArrayInputStream in = new ByteArrayInputStream(bytes);\nDataInputStream dataIn = new DataInputStream(in);\nwritable.readFields(dataIn);\ndataIn.close();\nreturn bytes;\n}\nWe construct a new, value-less, IntWritable, then call deserialize() to read from the\noutput data that we just wrote. Then we check that its value, retrieved using the\nget() method, is the original value, 163:\nIntWritable newWritable = new IntWritable();\ndeserialize(newWritable, bytes);\nassertThat(newWritable.get(), is(163));\nWritableComparable and comparators\nIntWritable implements the WritableComparable interface, which is just a subinterface\nof the Writable and java.lang.Comparable interfaces:\npackage org.apache.hadoop.io;\npublic interface WritableComparable<T> extends Writable, Comparable<T> {\n}\nComparison of types is crucial for MapReduce, where there is a sorting phase during\nwhich keys are compared with one another. One optimization that Hadoop provides\nis the RawComparator extension of Java’s Comparator:\npackage org.apache.hadoop.io;\nimport java.util.Comparator;\npublic interface RawComparator<T> extends Comparator<T> {\npublic int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);\n88 | Chapter 4: Hadoop I/O}\nThis interface permits implementors to compare records read from a stream without\ndeserializing them into objects, thereby avoiding any overhead of object creation. For\nexample, the comparator for IntWritables implements the raw compare() method by\nreading an integer from each of the byte arrays b1 and b2 and comparing them directly,\nfrom the given start positions (s1 and s2) and lengths (l1 and l2).\nWritableComparator is a general-purpose implementation of RawComparator for\nWritableComparable classes. It provides two main functions. First, it provides a default\nimplementation of the raw compare() method that deserializes the objects to be com-\npared from the stream and invokes the object compare() method. Second, it acts as a\nfactory for RawComparator instances (that Writable implementations have registered).\nFor example, to obtain a comparator for IntWritable, we just use:\nRawComparator<IntWritable> comparator = WritableComparator.get(IntWritable.class);\nThe comparator can be used to compare two IntWritable objects:\nIntWritable w1 = new IntWritable(163);\nIntWritable w2 = new IntWritable(67);\nassertThat(comparator.compare(w1, w2), greaterThan(0));\nor their serialized representations:\nbyte[] b1 = serialize(w1);\nbyte[] b2 = serialize(w2);\nassertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length),\ngreaterThan(0));\nWritable Classes\nHadoop comes with a large selection of Writable classes in the org.apache.hadoop.io\npackage. They form the class hierarchy shown in Figure 4-1.\nWritable wrappers for Java primitives\nThere are Writable wrappers for all the Java primitive types (see Table 4-6) except\nshort and char (both of which can be stored in an IntWritable). All have a get() and\na set() method for retrieving and storing the wrapped value.\nTable 4-6. Writable wrapper classes for Java primitives\nJava primitive Writable implementation Serialized size (bytes)\nboolean BooleanWritable 1\nbyte ByteWritable 1\nint IntWritable 4\nVIntWritable 1–5\nfloat FloatWritable 4\nSerialization | 89Java primitive Writable implementation Serialized size (bytes)\nlong LongWritable 8\nVLongWritable 1–9\nDoubleWritable 8\ndouble\nFigure 4-1. Writable class hierarchy\nWhen it comes to encoding integers there is a choice between the fixed-length formats\n(IntWritable and LongWritable) and the variable-length formats (VIntWritable and\nVLongWritable). The variable-length formats use only a single byte to encode the value\nif it is small enough (between –112 and 127, inclusive); otherwise, they use the first\nbyte to indicate whether the value is positive or negative, and how many bytes follow.\nFor example, 163 requires two bytes:\n90 | Chapter 4: Hadoop I/Obyte[] data = serialize(new VIntWritable(163));\nassertThat(StringUtils.byteToHexString(data), is(""8fa3""));\nHow do you choose between a fixed-length and a variable-length encoding? Fixed-\nlength encodings are good when the distribution of values is fairly uniform across the\nwhole value space, such as a (well-designed) hash function. Most numeric variables\ntend to have nonuniform distributions, and on average the variable-length encoding\nwill save space. Another advantage of variable-length encodings is that you can switch\nfrom VIntWritable to VLongWritable, since their encodings are actually the same. So by\nchoosing a variable-length representation, you have room to grow without committing\nto an 8-byte long representation from the beginning.\nText\nText is a Writable for UTF-8 sequences. It can be thought of as the Writable equivalent\nof java.lang.String. Text is a replacement for the UTF8 class, which was deprecated\nbecause it didn’t support strings whose encoding was over 32,767 bytes, and because\nit used Java’s modified UTF-8.\nThe Text class uses an int (with a variable-length encoding) to store the number of\nbytes in the string encoding, so the maximum value is 2 GB. Furthermore, Text uses\nstandard UTF-8, which makes it potentially easier to interoperate with other tools that\nunderstand UTF-8.\nIndexing. Because of its emphasis on using standard UTF-8, there are some differences\nbetween Text and the Java String class. Indexing for the Text class is in terms of position\nin the encoded byte sequence, not the Unicode character in the string, or the Java\nchar code unit (as it is for String). For ASCII strings, these three concepts of index\nposition coincide. Here is an example to demonstrate the use of the charAt() method:\nText t = new Text(""hadoop"");\nassertThat(t.getLength(), is(6));\nassertThat(t.getBytes().length, is(6));\nassertThat(t.charAt(2), is((int) 'd'));\nassertThat(""Out of bounds"", t.charAt(100), is(-1));\nNotice that charAt() returns an int representing a Unicode code point, unlike the\nString variant that returns a char. Text also has a find() method, which is analogous\nto String’s indexOf():\nText t = new Text(""hadoop"");\nassertThat(""Find a substring"", t.find(""do""), is(2));\nassertThat(""Finds first 'o'"", t.find(""o""), is(3));\nassertThat(""Finds 'o' from position 4 or later"", t.find(""o"", 4), is(4));\nassertThat(""No match"", t.find(""pig""), is(-1));\nSerialization | 91Unicode. When we start using characters that are encoded with more than a single byte,\nthe differences between Text and String become clear. Consider the Unicode characters\nshown in Table 4-7.‡\nTable 4-7. Unicode characters\nUnicode code point U+0041 U+00DF U+6771 U+10400\nName LATIN CAPITAL LATIN SMALL LET- N/A (a unified Han DESERET CAPITAL LETTER LONG I\n     LETTER A TER SHARP S ideograph) \nUTF-8 code units 41 c3 9f e6 9d b1 f0 90 90 80\nJava representation \\u0041 \\u00DF \\u6771 \\uuD801\\uDC00\nAll but the last character in the table, U+10400, can be expressed using a single Java\nchar. U+10400 is a supplementary character and is represented by two Java chars,\nknown as a surrogate pair. The tests in Example 4-5 show the differences between\nString and Text when processing a string of the four characters from Table 4-7.\nExample 4-5. Tests showing the differences between the String and Text classes\npublic class StringTextComparisonTest {\n@Test\npublic void string() throws UnsupportedEncodingException {\nString s = ""\\u0041\\u00DF\\u6771\\uD801\\uDC00"";\nassertThat(s.length(), is(5));\nassertThat(s.getBytes(""UTF-8"").length, is(10));\nassertThat(s.indexOf(""\\u0041""), is(0));\nassertThat(s.indexOf(""\\u00DF""), is(1));\nassertThat(s.indexOf(""\\u6771""), is(2));\nassertThat(s.indexOf(""\\uD801\\uDC00""), is(3));\nassertThat(s.charAt(0),\nassertThat(s.charAt(1),\nassertThat(s.charAt(2),\nassertThat(s.charAt(3),\nassertThat(s.charAt(4),\n}\nis('\\u0041'));\nis('\\u00DF'));\nis('\\u6771'));\nis('\\uD801'));\nis('\\uDC00'));\nassertThat(s.codePointAt(0),\nassertThat(s.codePointAt(1),\nassertThat(s.codePointAt(2),\nassertThat(s.codePointAt(3),\nis(0x0041));\nis(0x00DF));\nis(0x6771));\nis(0x10400));\n@Test\npublic void text() {\nText t = new Text(""\\u0041\\u00DF\\u6771\\uD801\\uDC00"");\n‡ This example is based on one from the article Supplementary Characters in the Java Platform.\n92 | Chapter 4: Hadoop I/OassertThat(t.getLength(), is(10));\nassertThat(t.find(""\\u0041""), is(0));\nassertThat(t.find(""\\u00DF""), is(1));\nassertThat(t.find(""\\u6771""), is(3));\nassertThat(t.find(""\\uD801\\uDC00""), is(6));\n}\n}\nassertThat(t.charAt(0),\nassertThat(t.charAt(1),\nassertThat(t.charAt(3),\nassertThat(t.charAt(6),\nis(0x0041));\nis(0x00DF));\nis(0x6771));\nis(0x10400));\nThe test confirms that the length of a String is the number of char code units it contains\n(5, one from each of the first three characters in the string, and a surrogate pair from\nthe last), whereas the length of a Text object is the number of bytes in its UTF-8 encoding\n(10 = 1+2+3+4). Similarly, the indexOf() method in String returns an index in char\ncode units, and find() for Text is a byte offset.\nThe charAt() method in String returns the char code unit for the given index, which\nin the case of a surrogate pair will not represent a whole Unicode character. The code\nPointAt() method, indexed by char code unit, is needed to retrieve a single Unicode\ncharacter represented as an int. In fact, the charAt() method in Text is more like the\ncodePointAt() method than its namesake in String. The only difference is that it is\nindexed by byte offset.\nIteration. Iterating over the Unicode characters in Text is complicated by the use of byte\noffsets for indexing, since you can’t just increment the index. The idiom for iteration\nis a little obscure (see Example 4-6): turn the Text object into a java.nio.ByteBuffer,\nthen repeatedly call the bytesToCodePoint() static method on Text with the buffer. This\nmethod extracts the next code point as an int and updates the position in the buffer.\nThe end of the string is detected when bytesToCodePoint() returns –1.\nExample 4-6. Iterating over the characters in a Text object\npublic class TextIterator {\npublic static void main(String[] args) {\nText t = new Text(""\\u0041\\u00DF\\u6771\\uD801\\uDC00"");\n}\n}\nByteBuffer buf = ByteBuffer.wrap(t.getBytes(), 0, t.getLength());\nint cp;\nwhile (buf.hasRemaining() && (cp = Text.bytesToCodePoint(buf)) != -1) {\nSystem.out.println(Integer.toHexString(cp));\n}\nRunning the program prints the code points for the four characters in the string:\nSerialization | 93% hadoop TextIterator\n41\ndf\n6771\n10400\nMutability. Another difference with String is that Text is mutable (like all Writable im-\nplementations in Hadoop, except NullWritable, which is a singleton). You can reuse a\nText instance by calling one of the set() methods on it. For example:\nText t = new Text(""hadoop"");\nt.set(""pig"");\nassertThat(t.getLength(), is(3));\nassertThat(t.getBytes().length, is(3));\nIn some situations, the byte array returned by the getBytes() method\nmay be longer than the length returned by getLength():\nText t = new Text(""hadoop"");\nt.set(new Text(""pig""));\nassertThat(t.getLength(), is(3));\nassertThat(""Byte length not shortened"", t.getBytes().length, is(6));\nThis shows why it is imperative that you always call getLength() when\ncalling getBytes(), so you know how much of the byte array is valid data.\nResorting to String. Text doesn’t have as rich an API for manipulating strings as\njava.lang.String, so in many cases, you need to convert the Text object to a String.\nThis is done in the usual way, using the toString() method.\nassertThat(new Text(""hadoop"").toString(), is(""hadoop""));\nBytesWritable\nBytesWritable is a wrapper for an array of binary data. Its serialized format is an integer\nfield (4 bytes) that specifies the number of bytes to follow, followed by the bytes them-\nselves. For example, the byte array of length two with values 3 and 5 is serialized as a\n4-byte integer (00000002) followed by the two bytes from the array (03 and 05):\nBytesWritable b = new BytesWritable(new byte[] { 3, 5 });\nbyte[] bytes = serialize(b);\nassertThat(StringUtils.byteToHexString(bytes), is(""000000020305""));\nBytesWritable is mutable, and its value may be changed by calling its set() method.\nAs with Text, the size of the byte array returned from the getBytes() method for Byte\nsWritable—the capacity—may not reflect the actual size of the data stored in the\nBytesWritable. You can determine the size of the BytesWritable by calling get\nLength(). To demonstrate:\nb.setCapacity(11);\nassertThat(b.getLength(), is(2));\nassertThat(b.getBytes().length, is(11));\n94 | Chapter 4: Hadoop I/ONullWritable\nNullWritable is a special type of Writable, as it has a zero-length serialization. No bytes\nare written to, or read from, the stream. It is used as a placeholder; for example, in\nMapReduce, a key or a value can be declared as a NullWritable when you don’t need\nto use that position—it effectively stores a constant empty value. NullWritable can also\nbe useful as a key in SequenceFile when you want to store a list of values, as opposed\nto key-value pairs. It is an immutable singleton: the instance can be retrieved by calling\nNullWritable.get().\nObjectWritable and GenericWritable\nObjectWritable is a general-purpose wrapper for the following: Java primitives, String,\nenum, Writable, null, or arrays of any of these types. It is used in Hadoop RPC to marshal\nand unmarshal method arguments and return types.\nObjectWritable is useful when a field can be of more than one type: for example, if the\nvalues in a SequenceFile have multiple types, then you can declare the value type as an\nObjectWritable and wrap each type in an ObjectWritable. Being a general-purpose\nmechanism, it’s fairly wasteful of space since it writes the classname of the wrapped\ntype every time it is serialized. In cases where the number of types is small and known\nahead of time, this can be improved by having a static array of types, and using the\nindex into the array as the serialized reference to the type. This is the approach that\nGenericWritable takes, and you have to subclass it to specify the types to support.\nWritable collections\nThere are four Writable collection types in the org.apache.hadoop.io package: Array\nWritable, TwoDArrayWritable, MapWritable, and SortedMapWritable.\nArrayWritable and TwoDArrayWritable are Writable implementations for arrays and\ntwo-dimensional arrays (array of arrays) of Writable instances. All the elements of an\nArrayWritable or a TwoDArrayWritable must be instances of the same class, which is\nspecified at construction, as follows:\nArrayWritable writable = new ArrayWritable(Text.class);\nIn contexts where the Writable is defined by type, such as in SequenceFile keys or\nvalues, or as input to MapReduce in general, you need to subclass ArrayWritable (or\nTwoDArrayWritable, as appropriate) to set the type statically. For example:\npublic class TextArrayWritable extends ArrayWritable {\npublic TextArrayWritable() {\nsuper(Text.class);\n}\n}\nArrayWritable and TwoDArrayWritable both have get() and set() methods, as well as a\ntoArray() method, which creates a shallow copy of the array (or 2D array).\nSerialization | 95MapWritable and SortedMapWritable are implementations of java.util.Map<Writable,\nWritable> and java.util.SortedMap<WritableComparable, Writable>, respectively. The\ntype of each key and value field is a part of the serialization format for that field. The\ntype is stored as a single byte that acts as an index into an array of types. The array is\npopulated with the standard types in the org.apache.hadoop.io package, but custom\nWritable types are accommodated, too, by writing a header that encodes the type array\nfor nonstandard types. As they are implemented, MapWritable and SortedMapWritable\nuse positive byte values for custom types, so a maximum of 127 distinct nonstandard\nWritable classes can be used in any particular MapWritable or SortedMapWritable in-\nstance. Here’s a demonstration of using a MapWritable with different types for keys and\nvalues:\nMapWritable src = new MapWritable();\nsrc.put(new IntWritable(1), new Text(""cat""));\nsrc.put(new VIntWritable(2), new LongWritable(163));\nMapWritable dest = new MapWritable();\nWritableUtils.cloneInto(dest, src);\nassertThat((Text) dest.get(new IntWritable(1)), is(new Text(""cat"")));\nassertThat((LongWritable) dest.get(new VIntWritable(2)), is(new LongWritable(163)));\nConspicuous by their absence are Writable collection implementations for sets and\nlists. A set can be emulated by using a MapWritable (or a SortedMapWritable for a sorted\nset), with NullWritable values. For lists of a single type of Writable, ArrayWritable is\nadequate, but to store different types of Writable in a single list, you can use\nGenericWritable to wrap the elements in an ArrayWritable. Alternatively, you could\nwrite a general ListWritable using the ideas from MapWritable.\nImplementing a Custom Writable\nHadoop comes with a useful set of Writable implementations that serve most purposes;\nhowever, on occasion, you may need to write your own custom implementation. With\na custom Writable, you have full control over the binary representation and the sort\norder. Because Writables are at the heart of the MapReduce data path, tuning the binary\nrepresentation can have a significant effect on performance. The stock Writable im-\nplementations that come with Hadoop are well-tuned, but for more elaborate struc-\ntures, it is often better to create a new Writable type, rather than compose the stock\ntypes.\nTo demonstrate how to create a custom Writable, we shall write an implementation\nthat represents a pair of strings, called TextPair. The basic implementation is shown\nin Example 4-7.\nExample 4-7. A Writable implementation that stores a pair of Text objects\nimport java.io.*;\nimport org.apache.hadoop.io.*;\n96 | Chapter 4: Hadoop I/Opublic class TextPair implements WritableComparable<TextPair> {\nprivate Text first;\nprivate Text second;\npublic TextPair() {\nset(new Text(), new Text());\n}\npublic TextPair(String first, String second) {\nset(new Text(first), new Text(second));\n}\npublic TextPair(Text first, Text second) {\nset(first, second);\n}\npublic void set(Text first, Text second) {\nthis.first = first;\nthis.second = second;\n}\npublic Text getFirst() {\nreturn first;\n}\npublic Text getSecond() {\nreturn second;\n}\n@Override\npublic void write(DataOutput out) throws IOException {\nfirst.write(out);\nsecond.write(out);\n}\n@Override\npublic void readFields(DataInput in) throws IOException {\nfirst.readFields(in);\nsecond.readFields(in);\n}\n@Override\npublic int hashCode() {\nreturn first.hashCode() * 163 + second.hashCode();\n}\n@Override\npublic boolean equals(Object o) {\nif (o instanceof TextPair) {\nTextPair tp = (TextPair) o;\nreturn first.equals(tp.first) && second.equals(tp.second);\n}\nreturn false;\n}\nSerialization | 97@Override\npublic String toString() {\nreturn first + ""\\t"" + second;\n}\n}\n@Override\npublic int compareTo(TextPair tp) {\nint cmp = first.compareTo(tp.first);\nif (cmp != 0) {\nreturn cmp;\n}\nreturn second.compareTo(tp.second);\n}\nThe first part of the implementation is straightforward: there are two Text instance\nvariables, first and second, and associated constructors, getters, and setters. All\nWritable implementations must have a default constructor so that the MapReduce\nframework can instantiate them, then populate their fields by calling readFields().\nWritable instances are mutable and often reused, so you should take care to avoid\nallocating objects in the write() or readFields() methods.\nTextPair’s write() method serializes each Text object in turn to the output stream, by\ndelegating to the Text objects themselves. Similarly, readFields() deserializes the bytes\nfrom the input stream by delegating to each Text object. The DataOutput and\nDataInput interfaces have a rich set of methods for serializing and deserializing Java\nprimitives, so, in general, you have complete control over the wire format of your\nWritable object.\nJust as you would for any value object you write in Java, you should override the\nhashCode(), equals(), and toString() methods from java.lang.Object. The hash\nCode() method is used by the HashPartitioner (the default partitioner in MapReduce)\nto choose a reduce partition, so you should make sure that you write a good hash\nfunction that mixes well to ensure reduce partitions are of a similar size.\nIf you ever plan to use your custom Writable with TextOutputFormat,\nthen you must implement its toString() method. TextOutputFormat calls\ntoString() on keys and values for their output representation. For Text\nPair, we write the underlying Text objects as strings separated by a tab\ncharacter.\nTextPair is an implementation of WritableComparable, so it provides an implementation\nof the compareTo() method that imposes the ordering you would expect: it sorts by the\nfirst string followed by the second. Notice that TextPair differs from TextArrayWrita\nble from the previous section (apart from the number of Text objects it can store), since\nTextArrayWritable is only a Writable, not a WritableComparable.\n98 | Chapter 4: Hadoop I/OImplementing a RawComparator for speed\nThe code for TextPair in Example 4-7 will work as it stands; however, there is a further\noptimization we can make. As explained in “WritableComparable and compara-\ntors” on page 88, when TextPair is being used as a key in MapReduce, it will have to\nbe deserialized into an object for the compareTo() method to be invoked. What if it were\npossible to compare two TextPair objects just by looking at their serialized\nrepresentations?\nIt turns out that we can do this, since TextPair is the concatenation of two Text objects,\nand the binary representation of a Text object is a variable-length integer containing\nthe number of bytes in the UTF-8 representation of the string, followed by the UTF-8\nbytes themselves. The trick is to read the initial length, so we know how long the first\nText object’s byte representation is; then we can delegate to Text’s RawComparator, and\ninvoke it with the appropriate offsets for the first or second string. Example 4-8 gives\nthe details (note that this code is nested in the TextPair class).\nExample 4-8. A RawComparator for comparing TextPair byte representations\npublic static class Comparator extends WritableComparator {\nprivate static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator();\npublic Comparator() {\nsuper(TextPair.class);\n}\n@Override\npublic int compare(byte[] b1, int s1, int l1,\nbyte[] b2, int s2, int l2) {\n}\n}\ntry {\nint firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1);\nint firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2);\nint cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2);\nif (cmp != 0) {\nreturn cmp;\n}\nreturn TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1,\nb2, s2 + firstL2, l2 - firstL2);\n} catch (IOException e) {\nthrow new IllegalArgumentException(e);\n}\nstatic {\nWritableComparator.define(TextPair.class, new Comparator());\n}\nWe actually subclass WritableComparator rather than implement RawComparator di-\nrectly, since it provides some convenience methods and default implementations. The\nSerialization | 99subtle part of this code is calculating firstL1 and firstL2, the lengths of the first\nText field in each byte stream. Each is made up of the length of the variable-length\ninteger (returned by decodeVIntSize() on WritableUtils), and the value it is encoding\n(returned by readVInt()).\nThe static block registers the raw comparator so that whenever MapReduce sees the\nTextPair class, it knows to use the raw comparator as its default comparator.\nCustom comparators\nAs we can see with TextPair, writing raw comparators takes some care, since you have\nto deal with details at the byte level. It is worth looking at some of the implementations\nof Writable in the org.apache.hadoop.io package for further ideas, if you need to write\nyour own. The utility methods on WritableUtils are very handy too.\nCustom comparators should also be written to be RawComparators, if possible. These\nare comparators that implement a different sort order to the natural sort order defined\nby the default comparator. Example 4-9 shows a comparator for TextPair, called First\nComparator, that considers only the first string of the pair. Note that we override the\ncompare() method that takes objects so both compare() methods have the same\nsemantics.\nWe will make use of this comparator in Chapter 8, when we look at joins and secondary\nsorting in MapReduce (see “Joins” on page 233).\nExample 4-9. A custom RawComparator for comparing the first field of TextPair byte representations\npublic static class FirstComparator extends WritableComparator {\nprivate static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator();\npublic FirstComparator() {\nsuper(TextPair.class);\n}\n@Override\npublic int compare(byte[] b1, int s1, int l1,\nbyte[] b2, int s2, int l2) {\n}\ntry {\nint firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1);\nint firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2);\nreturn TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2);\n} catch (IOException e) {\nthrow new IllegalArgumentException(e);\n}\n@Override\npublic int compare(WritableComparable a, WritableComparable b) {\nif (a instanceof TextPair && b instanceof TextPair) {\nreturn ((TextPair) a).first.compareTo(((TextPair) b).first);\n100 | Chapter 4: Hadoop I/O}\n}\n}\nreturn super.compare(a, b);\nSerialization Frameworks\nAlthough most MapReduce programs use Writable key and value types, this isn’t man-\ndated by the MapReduce API. In fact, any types can be used; the only requirement is\nthat there be a mechanism that translates to and from a binary representation of each\ntype.\nTo support this, Hadoop has an API for pluggable serialization frameworks. A seriali-\nzation framework is represented by an implementation of Serialization (in the\norg.apache.hadoop.io.serializer package). WritableSerialization, for example, is\nthe implementation of Serialization for Writable types.\nA Serialization defines a mapping from types to Serializer instances (for turning an\nobject into a byte stream) and Deserializer instances (for turning a byte stream into\nan object).\nSet the io.serializations property to a comma-separated list of classnames to register\nSerialization implementations. Its default value is org.apache.hadoop.io.serial\nizer.WritableSerialization, which means that only Writable objects can be serialized\nor deserialized out of the box.\nHadoop includes a class called JavaSerialization that uses Java Object Serialization.\nAlthough making it convenient to be able to use standard Java types in MapReduce\nprograms, like Integer or String, Java Object Serialization is not as efficient as Writa-\nbles, so it’s not worth making this trade-off (see the sidebar).\nWhy Not Use Java Object Serialization?\nJava comes with its own serialization mechanism, called Java Object Serialization (often\nreferred to simply as “Java Serialization”), that is tightly integrated with the language,\nso it’s natural to ask why this wasn’t used in Hadoop. Here’s what Doug Cutting said\nin response to that question:\nWhy didn’t I use Serialization when we first started Hadoop? Because it looked\nbig and hairy and I thought we needed something lean and mean, where we had\nprecise control over exactly how objects are written and read, since that is central\nto Hadoop. With Serialization you can get some control, but you have to fight for\nit.\nThe logic for not using RMI was similar. Effective, high-performance inter-process\ncommunications are critical to Hadoop. I felt like we’d need to precisely control\nhow things like connections, timeouts and buffers are handled, and RMI gives you\nlittle control over those.\nThe problem is that Java Serialization doesn’t meet the criteria for a serialization format\nlisted earlier: compact, fast, extensible, and interoperable.\nSerialization | 101Java Serialization is not compact: it writes the classname of each object being written\nto the stream—this is true of classes that implement java.io.Serializable or\njava.io.Externalizable. Subsequent instances of the same class write a reference han-\ndle to the first occurrence, which occupies only 5 bytes. However, reference handles\ndon’t work well with random access, since the referent class may occur at any point in\nthe preceding stream—that is, there is state stored in the stream. Even worse, reference\nhandles play havoc with sorting records in a serialized stream, since the first record of\na particular class is distinguished and must be treated as a special case.\nAll these problems are avoided by not writing the classname to the stream at all, which\nis the approach that Writable takes. This makes the assumption that the client knows\nthe expected type. The result is that the format is considerably more compact than Java\nSerialization, and random access and sorting work as expected since each record is\nindependent of the others (so there is no stream state).\nJava Serialization is a general-purpose mechanism for serializing graphs of objects, so\nit necessarily has some overhead for serialization and deserialization operations. What’s\nmore, the deserialization procedure creates a new instance for each object deserialized\nfrom the stream. Writable objects, on the other hand, can be (and often are) reused.\nFor example, for a MapReduce job, which at its core serializes and deserializes billions\nof records of just a handful of different types, the savings gained by not having to allocate\nnew objects are significant.\nIn terms of extensibility, Java Serialization has some support for evolving a type, but it\nis brittle and hard to use effectively (Writables have no support: the programmer has\nto manage them himself).\nIn principle, other languages could interpret the Java Serialization stream protocol (de-\nfined by the Java Object Serialization Specification), but in practice there are no widely\nused implementations in other languages, so it is a Java-only solution. The situation is\nthe same for Writables.\nSerialization IDL\nThere are a number of other serialization frameworks that approach the problem in a\ndifferent way: rather than defining types through code, you define them in a language-\nneutral, declarative fashion, using an interface description language (IDL). The system\ncan then generate types for different languages, which is good for interoperability. They\nalso typically define versioning schemes that make type evolution straightforward.\nHadoop’s own Record I/O (found in the org.apache.hadoop.record package) has an\nIDL that is compiled into Writable objects, which makes it convenient for generating\ntypes that are compatible with MapReduce. For whatever reason, however, Record\nI/O is not widely used.\nApache Thrift and Google Protocol Buffers are both popular serialization frameworks,\nand they are commonly used as a format for persistent binary data. There is limited\nsupport for these as MapReduce formats;§ however, Thrift is used in parts of Hadoop\n102 | Chapter 4: Hadoop I/Oto provide cross-language APIs, such as the “thriftfs” contrib module, where it is used\nto expose an API to Hadoop filesystems (see “Thrift” on page 49).\nFinally, Avro is a new (at the time of this writing) Hadoop subproject that defines a\nserialization format. The goal is to migrate Hadoop’s RPC mechanism to use Avro.\nAvro will also be suitable as a data format for large files.\nFile-Based Data Structures\nFor some applications, you need a specialized data structure to hold your data. For\ndoing MapReduce-based processing, putting each blob of binary data into its own file\ndoesn’t scale, so Hadoop developed a number of higher-level containers for these\nsituations.\nSequenceFile\nImagine a logfile, where each log record is a new line of text. If you want to log binary\ntypes, plain text isn’t a suitable format. Hadoop’s SequenceFile class fits the bill in this\nsituation, providing a persistent data structure for binary key-value pairs. To use it as\na logfile format, you would choose a key, such as timestamp represented by a LongWrit\nable, and the value is a Writable that represents the quantity being logged.\nSequenceFiles also work well as containers for smaller files. HDFS and MapReduce are\noptimized for large files, so packing files into a SequenceFile makes storing and pro-\ncessing the smaller files more efficient. (“Processing a whole file as a re-\ncord” on page 192 contains a program to pack files into a SequenceFile.‖)\nWriting a SequenceFile\nTo create a SequenceFile, use one of its createWriter() static methods, which returns\na SequenceFile.Writer instance. There are several overloaded versions, but they all\nrequire you to specify a stream to write to (either a FSDataOutputStream or a FileSys\ntem and Path pairing), a Configuration object, and the key and value types. Optional\narguments include the compression type and codec, a Progressable callback to be in-\nformed of write progress, and a Metadata instance to be stored in the SequenceFile\nheader.\nThe keys and values stored in a SequenceFile do not necessarily need to be Writable.\nAny types that can be serialized and deserialized by a Serialization may be used.\n§ You can find the latest status for a Thrift Serialization at https://issues.apache.org/jira/browse/HADOOP\n-3787, and a Protocol Buffers Serialization at https://issues.apache.org/jira/browse/HADOOP-3788.\n‖ In a similar vein, the blog post “A Million Little Files” by Stuart Sierra includes code for converting a tar file\ninto a SequenceFile http://stuartsierra.com/2008/04/24/a-million-little-files.\nFile-Based Data Structures | 103Once you have a SequenceFile.Writer, you then write key-value pairs, using the\nappend() method. Then when you’ve finished you call the close() method (Sequence\nFile.Writer implements java.io.Closeable).\nExample 4-10 shows a short program to write some key-value pairs to a Sequence\nFile, using the API just described.\nExample 4-10. Writing a SequenceFile\npublic class SequenceFileWriteDemo {\nprivate static final String[] DATA = {\n""One, two, buckle my shoe"",\n""Three, four, shut the door"",\n""Five, six, pick up sticks"",\n""Seven, eight, lay them straight"",\n""Nine, ten, a big fat hen""\n};\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath path = new Path(uri);\nIntWritable key = new IntWritable();\nText value = new Text();\nSequenceFile.Writer writer = null;\ntry {\nwriter = SequenceFile.createWriter(fs, conf, path,\nkey.getClass(), value.getClass());\n}\n}\nfor (int i = 0; i < 100; i++) {\nkey.set(100 - i);\nvalue.set(DATA[i % DATA.length]);\nSystem.out.printf(""[%s]\\t%s\\t%s\\n"", writer.getLength(), key, value);\nwriter.append(key, value);\n}\n} finally {\nIOUtils.closeStream(writer);\n}\nThe keys in the sequence file are integers counting down from 100 to 1, represented as\nIntWritable objects. The values are Text objects. Before each record is appended to the\nSequenceFile.Writer, we call the getLength() method to discover the current position\nin the file. (We will use this information about record boundaries in the next section\nwhen we read the file nonsequentially.) We write the position out to the console, along\nwith the key and value pairs. The result of running it is shown here:\n% hadoop SequenceFileWriteDemo numbers.seq\n[128]\n100\nOne, two, buckle my shoe\n[173]\n99\nThree, four, shut the door\n104 | Chapter 4: Hadoop I/O[220]\n[264]\n[314]\n[359]\n[404]\n[451]\n[495]\n[545]\n...\n[1976]\n[2021]\n[2088]\n[2132]\n[2182]\n...\n[4557]\n[4602]\n[4649]\n[4693]\n[4743]\n98 Five, six, pick up sticks\n97 Seven, eight, lay them straight\n96 Nine, ten, a big fat hen\n95 One, two, buckle my shoe\n94 Three, four, shut the door\n93 Five, six, pick up sticks\n92 Seven, eight, lay them straight\n91 Nine, ten, a big fat hen\n60 One, two, buckle my shoe\n59 Three, four, shut the door\n58 Five, six, pick up sticks\n57 Seven, eight, lay them straight\n56 Nine, ten, a big fat hen\n5 One, two, buckle my shoe\n4 Three, four, shut the door\n3 Five, six, pick up sticks\n2 Seven, eight, lay them straight\n1 Nine, ten, a big fat hen\nReading a SequenceFile\nReading sequence files from beginning to end is a matter of creating an instance of\nSequenceFile.Reader, and iterating over records by repeatedly invoking one of the\nnext() methods. Which one you use depends on the serialization framework you are\nusing. If you are using Writable types, you can use the next() method that takes a key\nand a value argument, and reads the next key and value in the stream into these\nvariables:\npublic boolean next(Writable key, Writable val)\nThe return value is true if a key-value pair was read and false if the end of the file has\nbeen reached.\nFor other, non-Writable serialization frameworks (such as Apache Thrift), you should\nuse these two methods:\npublic Object next(Object key) throws IOException\npublic Object getCurrentValue(Object val) throws IOException\nIn this case, you need to make sure that the serialization you want to use has been set\nin the io.serializations property; see “Serialization Frameworks” on page 101.\nIf the next() method returns a non-null object, a key-value pair was read from the\nstream and the value can be retrieved using the getCurrentValue() method. Otherwise,\nif next() returns null, the end of the file has been reached.\nThe program in Example 4-11 demonstrates how to read a sequence file that has\nWritable keys and values. Note how the types are discovered from the Sequence\nFile.Reader via calls to getKeyClass() and getValueClass(), then ReflectionUtils is\nused to create an instance for the key and an instance for the value. By using this tech-\nnique, the program can be used with any sequence file that has Writable keys and values.\nFile-Based Data Structures | 105Example 4-11. Reading a SequenceFile\npublic class SequenceFileReadDemo {\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath path = new Path(uri);\n}\n}\nSequenceFile.Reader reader = null;\ntry {\nreader = new SequenceFile.Reader(fs, path, conf);\nWritable key = (Writable)\nReflectionUtils.newInstance(reader.getKeyClass(), conf);\nWritable value = (Writable)\nReflectionUtils.newInstance(reader.getValueClass(), conf);\nlong position = reader.getPosition();\nwhile (reader.next(key, value)) {\nString syncSeen = reader.syncSeen() ? ""*"" : """";\nSystem.out.printf(""[%s%s]\\t%s\\t%s\\n"", position, syncSeen, key, value);\nposition = reader.getPosition(); // beginning of next record\n}\n} finally {\nIOUtils.closeStream(reader);\n}\nAnother feature of the program is that it displays the position of the sync points in the\nsequence file. A sync point is a point in the stream which can be used to resynchronize\nwith a record boundary if the reader is “lost”—for example, after seeking to an arbitrary\nposition in the stream. Sync points are recorded by SequenceFile.Writer, which inserts\na special entry to mark the sync point every few records as a sequence file is being\nwritten. Such entries are small enough to incur only a modest storage overhead—less\nthan 1%. Sync points always align with record boundaries.\nRunning the program in Example 4-11 shows the sync points in the sequence file as\nasterisks. The first one occurs at position 2021 (the second one occurs at position 4075,\nbut is not shown in the output):\n% hadoop SequenceFileReadDemo numbers.seq\n[128]\n100\nOne, two, buckle my shoe\n[173]\n99\nThree, four, shut the door\n[220]\n98\nFive, six, pick up sticks\n[264]\n97\nSeven, eight, lay them straight\n[314]\n96\nNine, ten, a big fat hen\n[359]\n95\nOne, two, buckle my shoe\n[404]\n94\nThree, four, shut the door\n[451]\n93\nFive, six, pick up sticks\n[495]\n92\nSeven, eight, lay them straight\n[545]\n91\nNine, ten, a big fat hen\n[590]\n90\nOne, two, buckle my shoe\n...\n106 | Chapter 4: Hadoop I/O[1976]\n[2021*]\n[2088]\n[2132]\n[2182]\n...\n[4557]\n[4602]\n[4649]\n[4693]\n[4743]\n60 One, two, buckle my shoe\n59 Three, four, shut the door\n58 Five, six, pick up sticks\n57 Seven, eight, lay them straight\n56 Nine, ten, a big fat hen\n5 One, two, buckle my shoe\n4 Three, four, shut the door\n3 Five, six, pick up sticks\n2 Seven, eight, lay them straight\n1 Nine, ten, a big fat hen\nThere are two ways to seek to a given position in a sequence file. The first is the\nseek() method, which positions the reader at the given point in the file. For example,\nseeking to a record boundary works as expected:\nreader.seek(359);\nassertThat(reader.next(key, value), is(true));\nassertThat(((IntWritable) key).get(), is(95));\nBut if the position in the file is not at a record boundary, the reader fails when the\nnext() method is called:\nreader.seek(360);\nreader.next(key, value); // fails with IOException\nThe second way to find a record boundary makes use of sync points. The sync(long\nposition) method on SequenceFile.Reader positions the reader at the next sync point\nafter position. (If there are no sync points in the file after this position, then the reader\nwill be positioned at the end of the file.) Thus, we can call sync() with any position in\nthe stream—a nonrecord boundary, for example—and the reader will be reestablish\nitself at the next sync point so reading can continue:\nreader.sync(360);\nassertThat(reader.getPosition(), is(2021L));\nassertThat(reader.next(key, value), is(true));\nassertThat(((IntWritable) key).get(), is(59));\nSequenceFile.Writer has a method called sync() for inserting a sync\npoint at the current position in the stream. This is not to be confused\nwith the identically named but otherwise unrelated sync() method de-\nfined by the Syncable interface for synchronizing buffers to the under-\nlying device.\nSync points come into their own when using sequence files as input to MapReduce,\nsince they permit the file to be split, so different portions of it can be processed inde-\npendently by separate map tasks. See “SequenceFileInputFormat” on page 199.\nDisplaying a SequenceFile with the command-line interface\nThe hadoop fs command has a -text option to display sequence files in textual form.\nIt looks at a file’s magic number so that it can attempt to detect the type of the file and\nFile-Based Data Structures | 107appropriately convert it to text. It can recognize gzipped files and sequence files; oth-\nerwise, it assumes the input is plain text.\nFor sequence files, this command is really useful only if the keys and values have a\nmeaningful string representation (as defined by the toString() method). Also, if you\nhave your own key or value classes, then you will need to make sure they are on Ha-\ndoop’s classpath.\nRunning it on the sequence file we created in the previous section gives the following\noutput:\n% hadoop fs -text numbers.seq | head\n100\nOne, two, buckle my shoe\n99\nThree, four, shut the door\n98\nFive, six, pick up sticks\n97\nSeven, eight, lay them straight\n96\nNine, ten, a big fat hen\n95\nOne, two, buckle my shoe\n94\nThree, four, shut the door\n93\nFive, six, pick up sticks\n92\nSeven, eight, lay them straight\n91\nNine, ten, a big fat hen\nSorting and merging SequenceFiles\nThe most powerful way of sorting (and merging) one or more sequence files is to use\nMapReduce. MapReduce is inherently parallel and will let you specify the number of\nreducers to use, which determines the number of output partitions. For example, by\nspecifying one reducer, you get a single output file. We can use the sort example that\ncomes with Hadoop by specifying that the input and output are sequence files, and by\nsetting the key and value types:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort -r 1 \\\n-inFormat org.apache.hadoop.mapred.SequenceFileInputFormat \\\n-outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat \\\n-outKey org.apache.hadoop.io.IntWritable \\\n-outValue org.apache.hadoop.io.Text \\\nnumbers.seq sorted\n% hadoop fs -text sorted/part-00000 | head\n1\nNine, ten, a big fat hen\n2\nSeven, eight, lay them straight\n3\nFive, six, pick up sticks\n4\nThree, four, shut the door\n5\nOne, two, buckle my shoe\n6\nNine, ten, a big fat hen\n7\nSeven, eight, lay them straight\n8\nFive, six, pick up sticks\n9\nThree, four, shut the door\n10\nOne, two, buckle my shoe\nSorting is covered in more detail in “Sorting” on page 218.\n108 | Chapter 4: Hadoop I/OAs an alternative to using MapReduce for sort/merge, there is a SequenceFile.Sorter\nclass that has a number of sort() and merge() methods. These functions predate Map-\nReduce, and are lower-level functions than MapReduce (for example, to get parallelism,\nyou need to partition your data manually), so in general MapReduce is the preferred\napproach to sort and merge sequence files.\nThe SequenceFile Format\nA sequence file consists of a header followed by one or more records (see Figure 4-2).\nThe first three bytes of a sequence file are the bytes SEQ, which acts a magic number,\nfollowed by a single byte representing the version number. The header contains other\nfields including the names of the key and value classes, compression details, user-\ndefined metadata, and the sync marker.# Recall that the sync marker is used to allow\na reader to synchronize to a record boundary from any position in the file. Each file has\na randomly generated sync marker, whose value is stored in the header. Sync markers\nappear between records in the sequence file. They are designed to incur less than a 1%\nstorage overhead, so they don’t necessarily appear between every pair of records (such\nis the case for short records).\nFigure 4-2. The internal structure of a sequence file with no compression and record compression\nThe internal format of the records depends on whether compression is enabled, and if\nit is, whether it is record compression or block compression.\n# Full details of the format of these fields may be found in SequenceFile’s documentation and source code.\nFile-Based Data Structures | 109If no compression is enabled (the default), then each record is made up of the record\nlength (in bytes), the key length, the key and then the value. The length fields are written\nas four-byte integers adhering to the contract of the writeInt() method of java.io.Data\nOutput. Keys and values are serialized using the Serialization defined for the class being\nwritten to the sequence file.\nThe format for record compression is almost identical to no compression, except the\nvalue bytes are compressed using the codec defined in the header. Note that keys are\nnot compressed.\nBlock compression compresses multiple records at once; it is therefore more compact\nthan and should generally be preferred over record compression because it has the\nopportunity to take advantage of similarities between records. (See Figure 4-3.) Records\nare added to a block until it reaches a minimum size in bytes, defined by the io.seq\nfile.compress.blocksize property: the default is 1,000,000 bytes. A sync marker is\nwritten before the start of every block. The format of a block is a field indicating the\nnumber of records in the block, followed by four compressed fields: the key lengths,\nthe keys, the value lengths, and the values.\nFigure 4-3. The internal structure of a sequence file with block compression\nMapFile\nA MapFile is a sorted SequenceFile with an index to permit lookups by key. MapFile can\nbe thought of as a persistent form of java.util.Map (although it doesn’t implement this\ninterface), which is able to grow beyond the size of a Map that is kept in memory.\nWriting a MapFile\nWriting a MapFile is similar to writing a SequenceFile: you create an instance of\nMapFile.Writer, then call the append() method to add entries in order. (Attempting to\nadd entries out of order will result in an IOException.) Keys must be instances of\nWritableComparable, and values must be Writable—contrast this to SequenceFile,\nwhich can use any serialization framework for its entries.\n110 | Chapter 4: Hadoop I/OThe program in Example 4-12 creates a MapFile, and writes some entries to it. It is very\nsimilar to the program in Example 4-10 for creating a SequenceFile.\nExample 4-12. Writing a MapFile\npublic class MapFileWriteDemo {\nprivate static final String[] DATA = {\n""One, two, buckle my shoe"",\n""Three, four, shut the door"",\n""Five, six, pick up sticks"",\n""Seven, eight, lay them straight"",\n""Nine, ten, a big fat hen""\n};\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nIntWritable key = new IntWritable();\nText value = new Text();\nMapFile.Writer writer = null;\ntry {\nwriter = new MapFile.Writer(conf, fs, uri,\nkey.getClass(), value.getClass());\n}\n}\nfor (int i = 0; i < 1024; i++) {\nkey.set(i + 1);\nvalue.set(DATA[i % DATA.length]);\nwriter.append(key, value);\n}\n} finally {\nIOUtils.closeStream(writer);\n}\nLet’s use this program to build a MapFile:\n% hadoop MapFileWriteDemo numbers.map\nIf we look at the MapFile, we see it’s actually a directory containing two files called\ndata and index:\n% ls -l numbers.map\ntotal 104\n-rw-r--r--\n1 tom tom\n-rw-r--r--\n1 tom tom\n47898 Jul 29 22:06 data\n251 Jul 29 22:06 index\nBoth files are SequenceFiles. The data file contains all of the entries, in order:\n% hadoop fs -text numbers.map/data | head\n1\nOne, two, buckle my shoe\n2\nThree, four, shut the door\n3\nFive, six, pick up sticks\nFile-Based Data Structures | 1114\n5\n6\n7\n8\n9\n10\nSeven, eight, lay them straight\nNine, ten, a big fat hen\nOne, two, buckle my shoe\nThree, four, shut the door\nFive, six, pick up sticks\nSeven, eight, lay them straight\nNine, ten, a big fat hen\nThe index file contains a fraction of the keys, and contains a mapping from the key to\nthat key’s offset in the data file:\n% hadoop fs -text numbers.map/index\n1\n128\n129\n6079\n257\n12054\n385\n18030\n513\n24002\n641\n29976\n769\n35947\n897\n41922\nAs we can see from the output, by default only every 128th key is included in the index,\nalthough you can change this value either by setting the io.map.index.interval\nproperty or by calling the setIndexInterval() method on the MapFile.Writer instance.\nA reason to increase the index interval would be to decrease the amount of memory\nthat the MapFile needs to store the index. Conversely, you might decrease the interval\nto improve the time for random (since fewer records need to be skipped on average) at\nthe expense of memory usage.\nSince the index is only a partial index of keys, MapFile is not able to provide methods\nto enumerate, or even count, all the keys it contains. The only way to perform these\noperations is to read the whole file.\nReading a MapFile\nIterating through the entries in order in a MapFile is similar to the procedure for a\nSequenceFile: you create a MapFile.Reader, then call the next() method until it returns\nfalse, signifying that no entry was read because the end of the file was reached:\npublic boolean next(WritableComparable key, Writable val) throws IOException\nA random access lookup can be performed by calling the get() method:\npublic Writable get(WritableComparable key, Writable val) throws IOException\nThe return value is used to determine if an entry was found in the MapFile; if it’s null,\nthen no value exists for the given key. If key was found, then the value for that key is\nread into val, as well as being returned from the method call.\nIt might be helpful to understand how this is implemented. Here is a snippet of code\nthat retrieves an entry for the MapFile we created in the previous section:\n112 | Chapter 4: Hadoop I/OText value = new Text();\nreader.get(new IntWritable(496), value);\nassertThat(value.toString(), is(""One, two, buckle my shoe""));\nFor this operation, the MapFile.Reader reads the index file into memory (this is cached\nso that subsequent random access calls will use the same in-memory index). The reader\nthen performs a binary search on the in-memory index to find the key in the index that\nis less than or equal to the search key, 496. In this example, the index key found is 385,\nwith value 18030, which is the offset in the data file. Next the reader seeks to this offset\nin the data file and reads entries until the key is greater than or equal to the search key,\n496. In this case, a match is found and the value is read from the data file. Overall, a\nlookup takes a single disk seek and a scan through up to 128 entries on disk. For a\nrandom-access read, this is actually very efficient.\nThe getClosest() method is like get() except it returns the “closest” match to the\nspecified key, rather than returning null on no match. More precisely, if the MapFile\ncontains the specified key then that is the entry returned; otherwise, the key in the\nMapFile that is immediately after (or before, according to a boolean argument) the\nspecified key is returned.\nA very large MapFile’s index can take up a lot of memory. Rather than reindex to change\nthe index interval, it is possible to load only a fraction of the index keys into memory\nwhen reading the MapFile by setting the io.map.index.skip property. This property is\nnormally 0, which means no index keys are skipped; a value of 1 means skip one key\nfor every key in the index (so every other key ends up in the index), 2 means skip two\nkeys for every key in the index (so one third of the keys end up in the index), and so\non. Larger skip values save memory but at the expense of lookup time, since more\nentries have to be scanned on disk, on average.\nConverting a SequenceFile to a MapFile\nOne way of looking at a MapFile is as an indexed and sorted SequenceFile. So it’s quite\nnatural to want to be able to convert a SequenceFile into a MapFile. We covered how\nto sort a SequenceFile in “Sorting and merging SequenceFiles” on page 108, so here we\nlook at how to create an index for a SequenceFile. The program in Example 4-13 hinges\naround the static utility method fix() on MapFile, which recreates the index for a\nMapFile.\nExample 4-13. Recreating the index for a MapFile\npublic class MapFileFixer {\npublic static void main(String[] args) throws Exception {\nString mapUri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(mapUri), conf);\nPath map = new Path(mapUri);\nFile-Based Data Structures | 113Path mapData = new Path(map, MapFile.DATA_FILE_NAME);\n// Get key and value types from data sequence file\nSequenceFile.Reader reader = new SequenceFile.Reader(fs, mapData, conf);\nClass keyClass = reader.getKeyClass();\nClass valueClass = reader.getValueClass();\nreader.close();\n}\n}\n// Create the map file index file\nlong entries = MapFile.fix(fs, map, keyClass, valueClass, false, conf);\nSystem.out.printf(""Created MapFile %s with %d entries\\n"", map, entries);\nThe fix() method is usually used for recreating corrupted indexes, but since it creates\na new index from scratch, it’s exactly what we need here. The recipe is as follows:\n1. Sort the sequence file numbers.seq into a new directory called number.map that will\nbecome the MapFile. (If the sequence file is already sorted, then you can skip this\nstep. Instead, copy it to a file number.map/data, then go to step 3.)\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort -r 1 \\\n-inFormat org.apache.hadoop.mapred.SequenceFileInputFormat \\\n-outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat \\\n-outKey org.apache.hadoop.io.IntWritable \\\n-outValue org.apache.hadoop.io.Text \\\nnumbers.seq numbers.map\n2. Rename the MapReduce output to be the data file:\n% hadoop fs -mv numbers.map/part-00000 numbers.map/data\n3. Create the index file:\n% hadoop MapFileFixer numbers.map\nCreated MapFile numbers.map with 100 entries\nThe MapFile numbers.map now exists and can be used.\n114 | Chapter 4: Hadoop I/OCHAPTER 5\nDeveloping a MapReduce Application\nIn Chapter 2, we introduced the MapReduce model. In this chapter, we look at the\npractical aspects of developing a MapReduce application in Hadoop.\nWriting a program in MapReduce has a certain flow to it. You start by writing your\nmap and reduce functions, ideally with unit tests to make sure they do what you expect.\nThen you write a driver program to run a job, which can run from your IDE using a\nsmall subset of the data to check that it is working. If it fails, then you can use your\nIDE’s debugger to find the source of the problem. With this information, you can\nexpand your unit tests to cover this case, and improve your mapper or reducer as ap-\npropriate to handle such input correctly.\nWhen the program runs as expected against the small dataset, you are ready to unleash\nit on a cluster. Running against the full dataset is likely to expose some more issues,\nwhich you can fix as before, by expanding your tests and mapper or reducer to handle\nthe new cases. Debugging failing programs in the cluster is a challenge, but Hadoop\nprovides some tools to help, such as an IsolationRunner, which allows you to run a\ntask over the same input on which it failed, with a debugger attached, if necessary.\nAfter the program is working, you may wish to do some tuning, first by running through\nsome standard checks for making MapReduce programs faster and then by doing task\nprofiling. Profiling distributed programs is not trivial, but Hadoop has hooks to aid the\nprocess.\nBefore we start writing a MapReduce program, we need to set up and configure the\ndevelopment environment. And to do that, we need to learn a bit about how Hadoop\ndoes configuration.\n115The Configuration API\nComponents in Hadoop are configured using Hadoop’s own configuration API. An\ninstance of the Configuration class (found in the org.apache.hadoop.conf package)\nrepresents a collection of configuration properties and their values. Each property is\nnamed by a String, and the type of a value may be one of several types, including Java\nprimitives such as boolean, int, long, float, and other useful types such as String, Class,\njava.io.File, and collections of Strings.\nConfigurations read their properties from resources—XML files with a simple structure\nfor defining name-value pairs. See Example 5-1.\nExample 5-1. A simple configuration file, configuration-1.xml\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>color</name>\n<value>yellow</value>\n<description>Color</description>\n</property>\n<property>\n<name>size</name>\n<value>10</value>\n<description>Size</description>\n</property>\n<property>\n<name>weight</name>\n<value>heavy</value>\n<final>true</final>\n<description>Weight</description>\n</property>\n<property>\n<name>size-weight</name>\n<value>${size},${weight}</value>\n<description>Size and weight</description>\n</property>\n</configuration>\nAssuming this configuration file is in a file called configuration-1.xml, we can access its\nproperties using a piece of code like this:\nConfiguration conf = new Configuration();\nconf.addResource(""configuration-1.xml"");\nassertThat(conf.get(""color""), is(""yellow""));\nassertThat(conf.getInt(""size"", 0), is(10));\nassertThat(conf.get(""breadth"", ""wide""), is(""wide""));\nThere are a couple of things to note: type information is not stored in the XML file;\ninstead, properties can be interpreted as a given type when they are read. Also, the\n116 | Chapter 5: Developing a MapReduce Applicationget() methods allow you to specify a default value, which is used if the property is not\ndefined in the XML file, as in the case of breadth here.\nCombining Resources\nThings get interesting when more than one resource is used to define a configuration.\nThis is used in Hadoop to separate out the default properties for the system, defined\ninternally in a file called core-default.xml, from the site-specific overrides, in core-\nsite.xml. The file in Example 5-2 defines the size and weight properties.\nExample 5-2. A second configuration file, configuration-2.xml\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>size</name>\n<value>12</value>\n</property>\n<property>\n<name>weight</name>\n<value>light</value>\n</property>\n</configuration>\nResources are added to a Configuration in order:\nConfiguration conf = new Configuration();\nconf.addResource(""configuration-1.xml"");\nconf.addResource(""configuration-2.xml"");\nProperties defined in resources that are added later override the earlier definitions. So\nthe size property takes its value from the second configuration file, configuration-2.xml:\nassertThat(conf.getInt(""size"", 0), is(12));\nHowever, properties that are marked as final cannot be overridden in later definitions.\nThe weight property is final in the first configuration file, so the attempt to override it\nin the second fails and it takes the value from the first:\nassertThat(conf.get(""weight""), is(""heavy""));\nAttempting to override final properties usually indicates a configuration error, so this\nresults in a warning message being logged to aid diagnosis. Administrators mark prop-\nerties as final in the daemon’s site files that they don’t want users to change in their\nclient-side configuration files, or job submission parameters.\nVariable Expansion\nConfiguration properties can be defined in terms of other properties, or system prop-\nerties. For example, the property size-weight in the first configuration file is defined\nThe Configuration API | 117as ${size},${weight}, and these properties are expanded using the values found in the\nconfiguration:\nassertThat(conf.get(""size-weight""), is(""12,heavy""));\nSystem properties take priority over properties defined in resource files:\nSystem.setProperty(""size"", ""14"");\nassertThat(conf.get(""size-weight""), is(""14,heavy""));\nThis feature is useful for overriding properties on the command line by using\n-Dproperty=value JVM arguments.\nNote that while configuration properties can be defined in terms of system properties,\nunless system properties are redefined using configuration properties, they are not ac-\ncessible through the configuration API. Hence:\nSystem.setProperty(""length"", ""2"");\nassertThat(conf.get(""length""), is((String) null));\nConfiguring the Development Environment\nThe first step is to download the version of Hadoop that you plan to use and unpack\nit on your development machine (this is described in Appendix A). Then, in your fa-\nvorite IDE, create a new project and add all the JAR files from the top level of the\nunpacked distribution and from the lib directory to the classpath. You will then be able\nto compile Java Hadoop programs, and run them in local (standalone) mode within\nthe IDE.\nFor Eclipse users, there is a plug-in available for browsing HDFS and\nlaunching MapReduce programs. Instructions are available on the Ha-\ndoop wiki at http://wiki.apache.org/hadoop/EclipsePlugIn.\nManaging Configuration\nWhen developing Hadoop applications, it is common to switch between running the\napplication locally and running it on a cluster. In fact, you may have several clusters\nyou work with, or you may have a local “pseudo-distributed” cluster that you like to\ntest on (a pseudo-distributed cluster is one whose daemons all run on the local machine;\nsetting up this mode is covered in Appendix A, too).\nOne way to accommodate these variations is to have Hadoop configuration files con-\ntaining the connection settings for each cluster you run against, and specify which one\nyou are using when you run Hadoop applications or tools. As a matter of best practice,\nit’s recommended to keep these files outside Hadoop’s installation directory tree, as\nthis makes it easy to switch between Hadoop versions without duplicating or losing\nsettings.\n118 | Chapter 5: Developing a MapReduce ApplicationFor the purposes of this book, we assume the existence of a directory called conf that\ncontains three configuration files: hadoop-local.xml, hadoop-localhost.xml, and\nhadoop-cluster.xml (these are available in the example code for this book). Note that\nthere is nothing special about the names of these files—they are just convenient ways\nto package up some configuration settings. (Compare this to Table A-1 in Appen-\ndix A, which sets out the equivalent server-side configurations.)\nThe hadoop-local.xml file contains the default Hadoop configuration for the default\nfilesystem and the jobtracker:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>file:///</value>\n</property>\n<property>\n<name>mapred.job.tracker</name>\n<value>local</value>\n</property>\n</configuration>\nThe settings in hadoop-localhost.xml point to a namenode and a jobtracker both run-\nning on localhost:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://localhost/</value>\n</property>\n<property>\n<name>mapred.job.tracker</name>\n<value>localhost:8021</value>\n</property>\n</configuration>\nFinally, hadoop-cluster.xml contains details of the cluster’s namenode and jobtracker\naddresses. In practice, you would name the file after the name of the cluster, rather\nthan “cluster” as we have here:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://namenode/</value>\n</property>\nConfiguring the Development Environment | 119<property>\n<name>mapred.job.tracker</name>\n<value>jobtracker:8021</value>\n</property>\n</configuration>\nYou can add other configuration properties to these files as needed. For example, if you\nwanted to set your Hadoop username for a particular cluster, you could do it in the\nappropriate file.\nSetting User Identity\nThe user identity that Hadoop uses for permissions in HDFS is determined by running\nthe whoami command on the client system. Similarly, the group names are derived from\nthe output of running groups.\nIf, however, your Hadoop user identity is different from the name of your user account\non your client machine, then you can explicitly set your Hadoop username and group\nnames by setting the hadoop.job.ugi property. The username and group names are\nspecified as a comma-separated list of strings; e.g., preston,director,inventor would\nset the username to preston and the group names to director and inventor.\nYou can set the user identity that the HDFS web interface runs as by setting\ndfs.web.ugi using the same syntax. By default it is webuser,webgroup, which is not a\nsuper user, so system files are not accessible through the web interface.\nNotice that there is no authentication with this system, a limitation that a future version\nof Hadoop will remedy.\nWith this setup, it is easy to use any configuration with the -conf command-line switch.\nFor example, the following command shows a directory listing on the HDFS server\nrunning in pseudo-distributed mode on localhost:\n% hadoop fs -conf conf/hadoop-localhost.xml -ls .\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\n0 2009-04-08 10:32 /user/tom/input\ndrwxr-xr-x\n- tom supergroup\n0 2009-04-08 13:09 /user/tom/output\nIf you omit the -conf option, then you pick up the Hadoop configuration in the conf\nsubdirectory under $HADOOP_INSTALL. Depending on how you set this up, this may be\nfor a standalone setup or a pseudo-distributed cluster.\nTools that come with Hadoop support the -conf option, but it’s also straightforward\nto make your programs (such as programs that run MapReduce jobs) support it, too,\nusing the Tool interface.\n120 | Chapter 5: Developing a MapReduce ApplicationGenericOptionsParser, Tool, and ToolRunner\nHadoop comes with a few helper classes for making it easier to run jobs from the\ncommand line. GenericOptionsParser is a class that interprets common Hadoop\ncommand-line options and sets them on a Configuration object for your application to\nuse as desired. You don’t usually use GenericOptionsParser directly, as it’s more con-\nvenient to implement the Tool interface and run your application with the ToolRun\nner, which uses GenericOptionsParser internally:\npublic interface Tool extends Configurable {\nint run(String [] args) throws Exception;\n}\nExample 5-3 shows a very simple implementation of Tool, for printing the keys and\nvalues of all the properties in the Tool’s Configuration object.\nExample 5-3. An example Tool implementation for printing the properties in a Configuration\npublic class ConfigurationPrinter extends Configured implements Tool {\nstatic {\nConfiguration.addDefaultResource(""hdfs-default.xml"");\nConfiguration.addDefaultResource(""hdfs-site.xml"");\nConfiguration.addDefaultResource(""mapred-default.xml"");\nConfiguration.addDefaultResource(""mapred-site.xml"");\n}\n@Override\npublic int run(String[] args) throws Exception {\nConfiguration conf = getConf();\nfor (Entry<String, String> entry: conf) {\nSystem.out.printf(""%s=%s\\n"", entry.getKey(), entry.getValue());\n}\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new ConfigurationPrinter(), args);\nSystem.exit(exitCode);\n}\nWe make ConfigurationPrinter a subclass of Configured, which is an implementation\nof the Configurable interface. All implementations of Tool need to implement\nConfigurable (since Tool extends it), and subclassing Configured is often the easiest way\nto achieve this. The run() method obtains the Configuration using Configurable’s\ngetConf() method, and then iterates over it, printing each property to standard output.\nThe static block makes sure that the HDFS and MapReduce configurations are picked\nup in addition to the core ones (which Configuration knows about already).\nConfigurationPrinter’s main() method does not invoke its own run() method directly.\nInstead, we call ToolRunner’s static run() method, which takes care of creating a\nConfiguring the Development Environment | 121Configuration object for the Tool, before calling its run() method. ToolRunner also uses\na GenericOptionsParser to pick up any standard options specified on the command\nline, and set them on the Configuration instance. We can see the effect of picking up\nthe properties specified in conf/hadoop-localhost.xml by running the following\ncommand:\n% hadoop ConfigurationPrinter -conf conf/hadoop-localhost.xml \\\n| grep mapred.job.tracker=\nmapred.job.tracker=localhost:8021\nWhich Properties Can I Set?\nConfigurationPrinter is a useful tool for telling you what a property is set to in your\nenvironment.\nYou can also see the default settings for all the public properties in Hadoop by looking\nin the docs directory of your Hadoop installation for HTML files called\ncore-default.html, hdfs-default.html, and mapred-default.html. Each property has a de-\nscription which explains what it is for and what values it can be set to.\nBe aware that some properties have no effect when set in the client configuration. For\nexample, if in your job submission you set mapred.tasktracker.map.tasks.maximum with\nthe expectation that it would change the number of task slots for the tasktrackers run-\nning your job then you would be disappointed, since this property only is only honored\nif set in the tasktracker’s mapred-site.html file. In general, you can tell the component\nwhere a property should be set by its name, so the fact that mapred.task\ntracker.map.tasks.maximum starts with mapred.tasktracker gives you a clue that it can\nbe set only for the tasktracker daemon. This is not a hard and fast rule, however, so in\nsome cases you may need to resort to trial and error, or even reading the source.\nWe discuss many of Hadoop’s most important configuration properties throughout\nthis book. You can find a configuration property reference on the book’s website at\nhttp://www.hadoopbook.com.\nGenericOptionsParser also allows you to set individual properties. For example:\n% hadoop ConfigurationPrinter -D color=yellow | grep color\ncolor=yellow\nThe -D option is used to set the configuration property with key color to the value\nyellow. Options specified with -D take priority over properties from the configuration\nfiles. This is very useful: you can put defaults into configuration files, and then override\nthem with the -D option as needed. A common example of this is setting the number\nof reducers for a MapReduce job via -D mapred.reduce.tasks=n. This will override the\nnumber of reducers set on the cluster, or if set in any client-side configuration files.\nThe other options that GenericOptionsParser and ToolRunner support are listed in Ta-\nble 5-1. You can find more on Hadoop’s configuration API in “The Configuration\nAPI” on page 116.\n122 | Chapter 5: Developing a MapReduce ApplicationDo not confuse setting Hadoop properties using the -D\nproperty=value option to GenericOptionsParser (and ToolRunner) with\nsetting JVM system properties using the -Dproperty=value option to the\njava command. The syntax for JVM system properties does not allow\nany whitespace between the D and the property name, whereas\nGenericOptionsParser requires them to be separated by whitespace.\nJVM system properties are retrieved from the java.lang.System class,\nwhereas Hadoop properties are accessible only from a Configuration\nobject. So, the following command will print nothing, since the\nSystem class is not used by ConfigurationPrinter:\n% hadoop -Dcolor=yellow ConfigurationPrinter | grep color\nIf you want to be able to set configuration through system properties,\nthen you need to mirror the system properties of interest in the config-\nuration file. See “Variable Expansion” on page 117 for further\ndiscussion.\nTable 5-1. GenericOptionsParser and ToolRunner options\nOption Description\n-D property=value Sets the given Hadoop configuration property to the given value. Overrides any default\n                 or site properties in the configuration, and any properties set via the -conf option.\n-conf filename ... Adds the given files to the list of resources in the configuration. This is a convenient way\n                  to set site properties, or to set a number of properties at once.\n-fs uri Sets the default filesystem to the given URI. Shortcut for -D fs.default.name=uri\n-jt host:port Sets the jobtracker to the given host and port. Shortcut for -D\nmapred.job.tracker=host:port\n-files file1,file2,... Copies the specified files from the local filesystem (or any filesystem if a scheme is\n                      specified) to the shared filesystem used by the jobtracker (usually HDFS) and makes\n                     them available to MapReduce programs in the task’s working directory. (See “Distributed\n                        Cache” on page 239 for more on the distributed cache mechanism for copying files to\n                         tasktracker machines.)\n-archives Copies the specified archives from the local filesystem (or any filesystem if a scheme is\narchive1,archive2,... specified) to the shared filesystem used by the jobtracker (usually HDFS), unarchives\n                     them, and makes them available to MapReduce programs in the task’s working\n                      directory.\n-libjars jar1,jar2,... Copies the specified JAR files from the local filesystem (or any filesystem if a scheme is\n                      specified) to the shared filesystem used by the jobtracker (usually HDFS), and adds them\n                     to the MapReduce task’s classpath. This option is a useful way of shipping JAR files that\n                      a job is dependent on.\nWriting a Unit Test\nThe map and reduce functions in MapReduce are easy to test in isolation, which is a\nconsequence of their functional style. For known inputs, they produce known outputs.\nWriting a Unit Test | 123However, since outputs are written to an OutputCollector, rather than simply being\nreturned from the method call, the OutputCollector needs to be replaced with a mock\nso that its outputs can be verified. There are several Java mock object frameworks that\ncan help build mocks; here we use Mockito, which is noted for its clean syntax, although\nany mock framework should work just as well.*\nAll of the tests described here can be run from within an IDE.\nMapper\nThe test for the mapper is shown in Example 5-4.\nExample 5-4. Unit test for MaxTemperatureMapper\nimport static org.mockito.Matchers.anyObject;\nimport static org.mockito.Mockito.*;\nimport\nimport\nimport\nimport\njava.io.IOException;\norg.apache.hadoop.io.*;\norg.apache.hadoop.mapred.OutputCollector;\norg.junit.*;\npublic class MaxTemperatureMapperTest {\n@Test\npublic void processesValidRecord() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0043011990999991950051518004+68750+023550FM-12+0382"" +\n// Year ^^^^\n""99999V0203201N00261220001CN9999999N9-00111+99999999999"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nmapper.map(null, value, output, null);\n}\n}\nverify(output).collect(new Text(""1950""), new IntWritable(-11));\nThe test is very simple: it passes a weather record as input to the mapper, then checks\nthe output is the year and temperature reading. The input key and Reporter are both\nignored by the mapper, so we can pass in anything, including null as we do here. To\ncreate a mock OutputCollector we call Mockito’s mock() method (a static import),\npassing the class of the type we want to mock. Then we invoke the mapper’s map()\nmethod, which executes the code being tested. Finally, we verify that the mock object\nwas called with the correct method and arguments, using Mockito’s verify() method\n(again, statically imported). Here we verify that OutputCollector’s collect() method\n* See also the MRUnit contrib module, which aims to make unit testing MapReduce programs easier.\n124 | Chapter 5: Developing a MapReduce Applicationwas called with a Text object representing the year (1950) and an IntWritable repre-\nsenting the temperature (−1.1°C).\nProceeding in a test-driven fashion, we create a Mapper implementation that passes the\ntest (see Example 5-5). Since we will be evolving the classes in this chapter, each is put\nin a different package indicating its version for ease of exposition. For example, v1.Max\nTemperatureMapper is version 1 of MaxTemperatureMapper. In reality, of course, you would\nevolve classes without repackaging them.\nExample 5-5. First version of a Mapper that passes MaxTemperatureMapperTest\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature = Integer.parseInt(line.substring(87, 92));\noutput.collect(new Text(year), new IntWritable(airTemperature));\nThis is a very simple implementation, which pulls the year and temperature fields from\nthe line and emits them in the OutputCollector. Let’s add a test for missing values,\nwhich in the raw data are represented by a temperature of +9999:\n@Test\npublic void ignoresMissingTemperatureRecord() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0043011990999991950051518004+68750+023550FM-12+0382"" +\n// Year ^^^^\n""99999V0203201N00261220001CN9999999N9+99991+99999999999"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nmapper.map(null, value, output, null);\n}\nText outputKey = anyObject();\nIntWritable outputValue = anyObject();\nverify(output, never()).collect(outputKey, outputValue);\nSince records with missing temperatures should be filtered out, this test uses Mockito\nto verify that the collect method on the OutputCollector is never called for any Text key\nor IntWritable value.\nThe existing test fails with a NumberFormatException, as parseInt() cannot parse integers\nwith a leading plus sign, so we fix up the implementation (version 2) to handle missing\nvalues:\nWriting a Unit Test | 125public void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nString temp = line.substring(87, 92);\nif (!missing(temp)) {\nint airTemperature = Integer.parseInt(temp);\noutput.collect(new Text(year), new IntWritable(airTemperature));\n}\nprivate boolean missing(String temp) {\nreturn temp.equals(""+9999"");\n}\nWith the test passing, we move on to writing the reducer.\nReducer\nThe reducer has to find the maximum value for a given key. Here’s a simple test for\nthis feature:\n@Test\npublic void returnsMaximumIntegerInValues() throws IOException {\nMaxTemperatureReducer reducer = new MaxTemperatureReducer();\nText key = new Text(""1950"");\nIterator<IntWritable> values = Arrays.asList(\nnew IntWritable(10), new IntWritable(5)).iterator();\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nreducer.reduce(key, values, output, null);\n}\nverify(output).collect(key, new IntWritable(10));\nWe construct an iterator over some IntWritable values and then verify that\nMaxTemperatureReducer picks the largest. The code in Example 5-6 is for an implemen-\ntation of MaxTemperatureReducer that passes the test. Notice that we haven’t tested the\ncase of an empty values iterator, but arguably we don’t need to, since MapReduce\nwould never call the reducer in this case, as every key produced by a mapper has a value.\nExample 5-6. Reducer for maximum temperature example\npublic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nint maxValue = Integer.MIN_VALUE;\n126 | Chapter 5: Developing a MapReduce Application}\n}\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(key, new IntWritable(maxValue));\nRunning Locally on Test Data\nNow that we’ve got the mapper and reducer working on controlled inputs, the next\nstep is to write a job driver and run it on some test data on a development machine.\nRunning a Job in a Local Job Runner\nUsing the Tool interface introduced earlier in the chapter, it’s easy to write a driver to\nrun our MapReduce job for finding the maximum temperature by year (see\nMaxTemperatureDriver in Example 5-7).\nExample 5-7. Application to find the maximum temperature\npublic class MaxTemperatureDriver extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.printf(""Usage: %s [generic options] <input> <output>\\n"",\ngetClass().getSimpleName());\nToolRunner.printGenericCommandUsage(System.err);\nreturn -1;\n}\nJobConf conf = new JobConf(getConf(), getClass());\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureDriver(), args);\nSystem.exit(exitCode);\nRunning Locally on Test Data | 127}\n}\nMaxTemperatureDriver implements the Tool interface, so we get the benefit of being able\nto set the options that GenericOptionsParser supports. The run() method constructs\nand configures a JobConf object, before launching a job described by the JobConf.\nAmong the possible job configuration parameters, we set the input and output file\npaths, the mapper, reducer and combiner classes, and the output types (the input types\nare determined by the input format, which defaults to TextInputFormat and has Long\nWritable keys and Text values). It’s also a good idea to set a name for the job so that\nyou can pick it out in the job list during execution and after it has completed. By default,\nthe name is the name of the JAR file, which is normally not particularly descriptive.\nNow we can run this application against some local files. Hadoop comes with a local\njob runner, a cut-down version of the MapReduce execution engine for running Map-\nReduce jobs in a single JVM. It’s designed for testing, and is very convenient for use in\nan IDE, since you can run it in a debugger to step through the code in your mapper and\nreducer.\nThe local job runner is only designed for simple testing of MapReduce\nprograms, so inevitably it differs from the full MapReduce implemen-\ntation. The biggest difference is that it can’t run more than one reducer.\n(It can support the zero reducer case, too.) This is normally not a prob-\nlem, as most applications can work with one reducer, although on a\ncluster you would choose a larger number to take advantage of paral-\nlelism. The thing to watch out for is that even if you set the number of\nreducers to a value over one, the local runner will silently ignore the\nsetting and use a single reducer.\nThe local job runner also has no support for the DistributedCache fea-\nture (described in “Distributed Cache” on page 239).\nNeither of these limitations is inherent in the local job runner, and future\nversions of Hadoop may relax these restrictions.\nThe local job runner is enabled by a configuration setting. Normally\nmapred.job.tracker is a host:port pair to specify the address of the jobtracker, but when\nit has the special value of local, the job is run in-process without accessing an external\njobtracker.\nFrom the command line, we can run the driver by typing:\n% hadoop v2.MaxTemperatureDriver -conf conf/hadoop-local.xml \\\ninput/ncdc/micro max-temp\nEquivalently, we could use the -fs and -jt options provided by GenericOptionsParser:\n% hadoop v2.MaxTemperatureDriver -fs file:/// -jt local input/ncdc/micro max-temp\n128 | Chapter 5: Developing a MapReduce ApplicationThis command executes MaxTemperatureDriver using input from the local input/ncdc/\nmicro directory, producing output in the local max-temp directory. Note that although\nwe’ve set -fs so we use the local filesystem (file:///), the local job runner will actually\nwork fine against any filesystem, including HDFS (and it can be handy to do this if you\nhave a few files that are on HDFS).\nWhen we run the program it fails and prints the following exception:\njava.lang.NumberFormatException: For input string: ""+0000""\nFixing the mapper\nThis exception shows that the map method still can’t parse positive temperatures. (If\nthe stack trace hadn’t given us enough information to diagnose the fault, we could run\nthe test in a local debugger, since it runs in a single JVM.) Earlier, we made it handle\nthe special case of missing temperature, +9999, but not the general case of any positive\ntemperature. With more logic going into the mapper, it makes sense to factor out a\nparser class to encapsulate the parsing logic; see Example 5-8 (now on version 3).\nExample 5-8. A class for parsing weather records in NCDC format\npublic class NcdcRecordParser {\nprivate static final int MISSING_TEMPERATURE = 9999;\nprivate String year;\nprivate int airTemperature;\nprivate String quality;\npublic void parse(String record) {\nyear = record.substring(15, 19);\nString airTemperatureString;\n// Remove leading plus sign as parseInt doesn't like them\nif (record.charAt(87) == '+') {\nairTemperatureString = record.substring(88, 92);\n} else {\nairTemperatureString = record.substring(87, 92);\n}\nairTemperature = Integer.parseInt(airTemperatureString);\nquality = record.substring(92, 93);\n}\npublic void parse(Text record) {\nparse(record.toString());\n}\npublic boolean isValidTemperature() {\nreturn airTemperature != MISSING_TEMPERATURE && quality.matches(""[01459]"");\n}\npublic String getYear() {\nreturn year;\n}\nRunning Locally on Test Data | 129}\npublic int getAirTemperature() {\nreturn airTemperature;\n}\nThe resulting mapper is much simpler (see Example 5-9). It just calls the parser’s\nparse() method, which parses the fields of interest from a line of input, checks whether\na valid temperature was found using the isValidTemperature() query method, and if it\nwas retrieves the year and the temperature using the getter methods on the parser.\nNotice that we also check the quality status field as well as missing temperatures in\nisValidTemperature() to filter out poor temperature readings.\nAnother benefit of creating a parser class is that it makes it easy to write related mappers\nfor similar jobs without duplicating code. It also gives us the opportunity to write unit\ntests directly against the parser, for more targeted testing.\nExample 5-9. A Mapper that uses a utility class to parse records\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new Text(parser.getYear()),\nnew IntWritable(parser.getAirTemperature()));\n}\nWith these changes, the test passes.\nTesting the Driver\nApart from the flexible configuration options offered by making your application im-\nplement Tool, you also make it more testable because it allows you to inject an arbitrary\nConfiguration. You can take advantage of this to write a test that uses a local job runner\nto run a job against known input data, which checks that the output is as expected.\nThere are two approaches to doing this. The first is to use the local job runner and run\nthe job against a test file on the local filesystem. The code in Example 5-10 gives an\nidea of how to do this.\n130 | Chapter 5: Developing a MapReduce ApplicationExample 5-10. A test for MaxTemperatureDriver that uses a local, in-process job runner\n@Test\npublic void test() throws Exception {\nJobConf conf = new JobConf();\nconf.set(""fs.default.name"", ""file:///"");\nconf.set(""mapred.job.tracker"", ""local"");\nPath input = new Path(""input/ncdc/micro"");\nPath output = new Path(""output"");\nFileSystem fs = FileSystem.getLocal(conf);\nfs.delete(output, true); // delete old output\nMaxTemperatureDriver driver = new MaxTemperatureDriver();\ndriver.setConf(conf);\nint exitCode = driver.run(new String[] {\ninput.toString(), output.toString() });\nassertThat(exitCode, is(0));\n}\ncheckOutput(conf, output);\nThe test explicitly sets fs.default.name and mapred.job.tracker so it uses the local\nfilesystem and the local job runner. It then runs the MaxTemperatureDriver via its Tool\ninterface against a small amount of known data. At the end of the test, the checkOut\nput() method is called to compare the actual output with the expected output, line by\nline.\nThe second way of testing the driver is to run it using a “mini-” cluster. Hadoop has a\npair of testing classes, called MiniDFSCluster and MiniMRCluster, which provide a pro-\ngrammatic way of creating in-process clusters. Unlike the local job runner, these allow\ntesting against the full HDFS and MapReduce machinery. Bear in mind too that task-\ntrackers in a mini-cluster launch separate JVMs to run tasks in, which can make de-\nbugging more difficult.\nMini-clusters are used extensively in Hadoop’s own automated test suite, but they can\nbe used for testing user code too. Hadoop’s ClusterMapReduceTestCase abstract class\nprovides a useful base for writing such a test, handles the details of starting and stopping\nthe in-process HDFS and MapReduce clusters in its setUp() and tearDown() methods,\nand generating a suitable JobConf object that is configured to work with them. Sub-\nclasses need populate only data in HDFS (perhaps by copying from a local file), run a\nMapReduce job, then confirm the output is as expected. Refer to the MaxTemperature\nDriverMiniTest class in the example code that comes with this book for the listing.\nTests like this serve as regression tests, and are a useful repository of input edge cases\nand their expected results. As you encounter more test cases, you can simply add them\nto the input file and update the file of expected output accordingly.\nRunning Locally on Test Data | 131Running on a Cluster\nNow that we are happy with the program running on a small test dataset, we are ready\nto try it on the full dataset on a Hadoop cluster. Chapter 9 covers how to set up a fully\ndistributed cluster, although you can also work through this section on a pseudo-\ndistributed cluster.\nPackaging\nWe don’t need to make any modifications to the program to run on a cluster rather\nthan on a single machine, but we do need to package the program as a JAR file to send\nto the cluster. This is conveniently achieved using Ant, using a task such as this (you\ncan find the complete build file in the example code):\n<jar destfile=""job.jar"" basedir=""${classes.dir}""/>\nIf you have a single job per JAR, then you can specify the main class to run in the JAR\nfile’s manifest. If the main class is not in the manifest, then it must be specified on the\ncommand line (as you will see shortly). Also, any dependent JAR files should be pack-\naged in a lib subdirectory in the JAR file. (This is analogous to a Java Web application\narchive, or WAR file, except in that case the JAR files go in a WEB-INF/lib subdirectory\nin the WAR file.)\nLaunching a Job\nTo launch the job, we need to run the driver, specifying the cluster that we want to run\nthe job on with the -conf option (we could equally have used the -fs and -jt options):\n% hadoop jar job.jar v3.MaxTemperatureDriver -conf conf/hadoop-cluster.xml \\\ninput/ncdc/all max-temp\nThe runJob() method on JobClient launches the job and polls for progress, writing a\nline summarizing the map and reduce’s progress whenever either changes. Here’s the\noutput (some lines have been removed for clarity):\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n...\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n08:15:52 INFO mapred.FileInputFormat: Total input paths to process : 101\n08:15:53 INFO mapred.JobClient: Running job: job_200904110811_0002\n08:15:54 INFO mapred.JobClient: map 0% reduce 0%\n08:16:06 INFO mapred.JobClient: map 28% reduce 0%\n08:16:07 INFO mapred.JobClient: map 30% reduce 0%\n08:21:36 INFO mapred.JobClient: map 100% reduce 100%\n08:21:38 INFO mapred.JobClient: Job complete: job_200904110811_0002\n08:21:38 INFO mapred.JobClient: Counters: 19\n08:21:38 INFO mapred.JobClient: Job Counters\n08:21:38 INFO mapred.JobClient:\n08:21:38 INFO Launched reduce tasks=32\n08:21:38 INFO mapred.JobClient:\n08:21:38 INFO Rack-local map tasks=82\n08:21:38 INFO mapred.JobClient:\n             Launched map tasks=127\n            mapred.JobClient:\n           Data-local map tasks=45\n          mapred.JobClient: FileSystemCounters\n132 | Chapter 5: Developing a MapReduce Application09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nFILE_BYTES_READ=12667214\nHDFS_BYTES_READ=33485841275\nFILE_BYTES_WRITTEN=989397\nHDFS_BYTES_WRITTEN=904\nMap-Reduce Framework\nReduce input groups=100\nCombine output records=4489\nMap input records=1209901509\nReduce shuffle bytes=19140\nReduce output records=100\nSpilled Records=9481\nMap output bytes=10282306995\nMap input bytes=274600205558\nCombine input records=1142482941\nMap output records=1142478555\nReduce input records=103\nThe output includes more useful information. Before the job starts, its ID is printed:\nthis is needed whenever you want to refer to the job, in logfiles for example, or when\ninterrogating it via the hadoop job command. When the job is complete, its statistics\n(known as counters) are printed out. These are very useful for confirming that the job\ndid what you expected. For example, for this job we can see that around 275 GB of\ninput data was analyzed (“Map input bytes”), read from around 34 GB of compressed\nfiles on HDFS (“HDFS_BYTES_READ”). The input was broken into 101 gzipped files\nof reasonable size, so there was no problem with not being able to split them.\nJob, Task, and Task Attempt IDs\nThe format of a job ID is composed of the time that the jobtracker (not the job) started,\nand an incrementing counter maintained by the jobtracker to uniquely identify the job\nto that instance of the jobtracker. So the job with this ID:\njob_200904110811_0002\nis the second (0002, job IDs are 1-based) job run by the jobtracker which started at\n08:11 on April 11, 2009. The counter is formatted with leading zeros to make job IDs\nsort nicely—in directory listings, for example. However, when the counter reaches\n10000 it is not reset, resulting in longer job IDs (which don’t sort so well).\nTasks belong to a job, and their IDs are formed by replacing the job prefix of a job ID\nwith a task prefix, and adding a suffix to identify the task within the job. For example,\ntask_200904110811_0002_m_000003\nis the fourth (000003, task IDs are 0-based) map (m) task of the job with ID\njob_200904110811_0002. The task IDs are created for a job when it is initialized, so they\ndo not necessarily dictate the order that the tasks will be executed in.\nTasks may be executed more than once, due to failure (see “Task Fail-\nure” on page 159) or speculative execution (see “Speculative Execu-\ntion” on page 169), so to identify different instances of a task execution, task attempts\nare given unique IDs on the jobtracker. For example,\nattempt_200904110811_0002_m_000003_0\nRunning on a Cluster | 133is\nthe first (0, attempt IDs are 0-based) attempt at running task\ntask_200904110811_0002_m_000003. Task attempts are allocated during the job run as\nneeded, so their ordering represents the order that they were created for tasktrackers\nto run.\nThe final count in the task attempt ID is incremented by one thousand if the job is\nrestarted after the jobtracker is restarted and recovers its running jobs.\nThe MapReduce Web UI\nHadoop comes with a web UI for viewing information about your jobs. It is useful for\nfollowing a job’s progress while it is running, as well as finding job statistics and logs\nafter the job has completed. You can find the UI at http://jobtracker-host:50030/.\nThe jobtracker page\nA screenshot of the home page is shown in Figure 5-1. The first section of the page gives\ndetails of the Hadoop installation, such as the version number and when it was com-\npiled, and the current state of the jobtracker (in this case, running), and when it was\nstarted.\nNext is a summary of the cluster, which has measures of cluster capacity and utilization.\nThis shows the number of maps and reduces currently running on the cluster, the total\nnumber of job submissions, the number of tasktracker nodes currently available, and\nthe cluster’s capacity: in terms of the number of map and reduce slots available across\nthe cluster (“Map Task Capacity” and “Reduce Task Capacity”), and the number of\navailable slots per node, on average. The number of tasktrackers that have been black-\nlisted by the jobtracker is listed as well (blacklisting is discussed in “Tasktracker Fail-\nure” on page 161).\nBelow the summary there is a section about the job scheduler that is running (here the\ndefault). You can click through to see job queues.\nFurther down we see sections for running, (successfully) completed, and failed jobs.\nEach of these sections has a table of jobs, with a row per job that shows the job’s ID,\nowner, name (as set using JobConf’s setJobName() method, which sets the\nmapred.job.name property) and progress information.\nFinally, at the foot of the page, there are links to the jobtracker’s logs, and the job-\ntracker’s history: information on all the jobs that the jobtracker has run. The main\ndisplay displays only 100 jobs (configurable via the mapred.jobtracker.completeuser\njobs.maximum property), before consigning them to the history page. Note also that the\njob history is persistent, so you can find jobs here from previous runs of the jobtracker.\n134 | Chapter 5: Developing a MapReduce ApplicationFigure 5-1. Screenshot of the jobtracker page\nJob History\nJob history refers to the events and configuration for a completed job. It is retained\nwhether the job was successful or not. Job history is used to support job recovery after\na jobtracker restart (see the mapred.jobtracker.restart.recover property), as well as\nproviding interesting information for the user running a job.\nJob history files are stored on the local filesystem of the jobtracker in a history subdir-\nectory of the logs directory. It is possible to set the location to an arbitrary Hadoop\nfilesystem via the hadoop.job.history.location property. The jobtracker’s history files\nare kept for 30 days before being deleted by the system.\nA second copy is also stored for the user, in the _logs/history subdirectory of the job’s\noutput\ndirectory.\nThis\nlocation\nmay\nbe\noverridden\nby\nsetting\nhadoop.job.history.user.location. By setting it to the special value none, no user job\nRunning on a Cluster | 135history is saved, although job history is still saved centrally. A user’s job history files\nare never deleted by the system.\nThe history log includes job, task and attempt events, all of which are stored in a plain-\ntext file. The history for a particular job may be viewed through the web UI, or via the\ncommand line, using hadoop job -history (which you point at the job’s output\ndirectory).\nThe job page\nClicking on a job ID brings you to a page for the job, illustrated in Figure 5-2. At the\ntop of the page is a summary of the job, with basic information such as job owner and\nname, and how long the job has been running for. The job file is the consolidated\nconfiguration file for the job, containing all the properties and their values that were in\neffect during the job run. If you are unsure of what a particular property was set to, you\ncan click through to inspect the file.\nWhile the job is running, you can monitor its progress on this page, which periodically\nupdates itself. Below the summary is a table that shows the map progress and the reduce\nprogress. “Num Tasks” shows the total number of map and reduce tasks for this job\n(a row for each). The other columns then show the state of these tasks: “Pending”\n(waiting to run), “Running,” “Complete” (successfully run), “Killed” (tasks that have\nfailed—this column would be more accurately labeled “Failed”). The final column\nshows the total number of failed and killed task attempts for all the map or reduce tasks\nfor the job (task attempts may be marked as killed if they are a speculative execution\nduplicate, if the tasktracker they are running on dies, or if they are killed by a user). See\n“Task Failure” on page 159 for background on task failure.\nFarther down the page, you can find completion graphs for each task that show their\nprogress graphically. The reduce completion graph is divided into the three phases of\nthe reduce task: copy (when the map outputs are being transferred to the reduce’s\ntasktracker), sort (when the reduce inputs are being merged), and reduce (when the\nreduce function is being run to produce the final output). The phases are described in\nmore detail in “Shuffle and Sort” on page 163.\nIn the middle of the page is a table of job counters. These are dynamically updated\nduring the job run, and provide another useful window into the job’s progress and\ngeneral health. There is more information about what these counters mean in “Built-\nin Counters” on page 211.\nRetrieving the Results\nOnce the job is finished, there are various ways to retrieve the results. Each reducer\nproduces one output file, so there are 30 part files named part-00000 to part-00029 in\nthe max-temp directory.\n136 | Chapter 5: Developing a MapReduce ApplicationFigure 5-2. Screenshot of the job page\nRunning on a Cluster | 137A good way to think of these “part” files is as parts of the max-temp\n“file.”\nIf the output is large (which it isn’t in this case), then it is important to\nhave multiple parts so that more than one reducer can work in parallel.\nUsually, if a file is in this partitioned form, it can still be used easily\nenough: as the input to another MapReduce job, for example. In some\ncases, you can exploit the structure of multiple partitions; to do a map-\nside join, for example, (“Map-Side Joins” on page 233) or a MapFile\nlookup (“An application: Partitioned MapFile lookups” on page 221).\nThis job produces a very small amount of output, so it is convenient to copy it from\nHDFS to our development machine. The -getmerge option to the hadoop fs command\nis useful here, as it gets all the files in the directory specified in the source pattern and\nmerges them into a single file on the local filesystem:\n% hadoop fs -getmerge max-temp max-temp-local\n% sort max-temp-local | tail\n1991\n607\n1992\n605\n1993\n567\n1994\n568\n1995\n567\n1996\n561\n1997\n565\n1998\n568\n1999\n568\n2000\n558\nWe sorted the output, as the reduce output partitions are unordered (owing to the hash\npartition function). Doing a bit of postprocessing of data from MapReduce is very\ncommon, as is feeding it into analysis tools, such as R, a spreadsheet, or even a relational\ndatabase.\nAnother way of retrieving the output if it is small is to use the -cat option to print the\noutput files to the console:\n% hadoop fs -cat max-temp/*\nOn closer inspection, we see that some of the results don’t look plausible. For instance,\nthe maximum temperature for 1951 (not shown here) is 590°C! How do we find out\nwhat’s causing this? Is it corrupt input data or a bug in the program?\nDebugging a Job\nThe time-honored way of debugging programs is via print statements, and this is cer-\ntainly possible in Hadoop. However, there are complications to consider: with pro-\ngrams running on tens, hundreds, or thousands of nodes, how do we find and examine\nthe output of the debug statements, which may be scattered across these nodes? For\nthis particular case, where we are looking for (what we think is) an unusual case, we\n138 | Chapter 5: Developing a MapReduce Applicationcan use a debug statement to log to standard error, in conjunction with a message to\nupdate the task’s status message to prompt us to look in the error log. The web UI\nmakes this easy, as you will see.\nWe also create a custom counter, to count the total number of records with implausible\ntemperatures in the whole dataset. This gives us valuable information about how to\ndeal with the condition—if it turns out to be a common occurrence, then we might\nneed to learn more about the condition and how to extract the temperature in these\ncases, rather than simply dropping the record. In fact, when trying to debug a job, you\nshould always ask yourself if you can use a counter to get the information you need to\nfind out what’s happening. Even if you need to use logging or a status message, it may\nbe useful to use a counter to gauge the extent of the problem. (There is more on counters\nin “Counters” on page 211.)\nIf the amount of log data you produce in the course of debugging is large, then you’ve\ngot a couple of options. The first is to write the information to the map’s output, rather\nthan to standard error, for analysis and aggregation by the reduce. This approach usu-\nally necessitates structural changes to your program, so start with the other techniques\nfirst. Alternatively, you can write a program (in MapReduce of course) to analyze the\nlogs produced by your job. There are tools to make this easier, such as Chukwa, a\nHadoop subproject.\nWe add our debugging to the mapper (version 4), as opposed to the reducer, as we\nwant to find out what the source data causing the anomalous output looks like:\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nOVER_100\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\nif (airTemperature > 1000) {\nSystem.err.println(""Temperature over 100 degrees for input: "" + value);\nreporter.setStatus(""Detected possibly corrupt record: see logs."");\nreporter.incrCounter(Temperature.OVER_100, 1);\n}\noutput.collect(new Text(parser.getYear()), new IntWritable(airTemperature));\n}\nRunning on a Cluster | 139If the temperature is over 100°C (represented by 1000, since temperatures are in tenths\nof a degree), we print a line to standard error with the suspect line, as well as updating\nthe map’s status message using the setStatus() method on Reporter directing us to\nlook in the log. We also increment a counter, which in Java is represented by a field of\nan enum type. In this program, we have defined a single field OVER_100 as a way to count\nthe number of records with a temperature of over 100°C.\nWith this modification, we recompile the code, recreate the JAR file, then rerun the\njob, and while it’s running go to the tasks page.\nThe tasks page\nThe job page has a number of links for look at the tasks in a job in more detail. For\nexample, by clicking on the “map” link, you are brought to a page which lists infor-\nmation for all of the map tasks on one page. You can also see just the completed tasks.\nThe screenshot in Figure 5-3 shows a portion of this page for the job run with our\ndebugging statements. Each row in the table is a task, and it provides such information\nas the start and end times for each task, any errors reported back from the tasktracker,\nand a link to view the counters for an individual task.\nFigure 5-3. Screenshot of the tasks page\nRelevant to debugging is the Status column, which shows a task’s latest status message.\nBefore a task starts, it shows its status as “initializing,” then once it starts reading re-\ncords it shows the split information for the split it is reading as a filename with a byte\noffset and length. You can see the status we set for debugging for task\ntask_200811201130_0054_m_000000, so let’s click through to the logs page to find the\nassociated debug message. (Notice too that there is an extra counter for this task, since\nour user counter has a nonzero count for this task.)\n140 | Chapter 5: Developing a MapReduce ApplicationThe task details page\nFrom the tasks page, you can click on any task to get more information about it. The\ntask details page, shown in Figure 5-4, shows each task attempt. In this case, there was\none task attempt, which completed successfully. The table provides further useful data,\nsuch as the node the task attempt ran on, and links to task logfiles and counters.\nFigure 5-4. Screenshot of the task details page\nThe “Actions” column contains links for killing a task attempt. By default, this is dis-\nabled, making the web UI a read-only interface. Set webinterface.private.actions to\ntrue to enable the actions links.\nBy setting webinterface.private.actions to true, you also allow anyone\nwith access to the HDFS web interface to delete files. The dfs.web.ugi\nproperty determines the user that the HDFS web UI runs as, thus con-\ntrolling which files may be viewed and deleted.\nFor map tasks, there is also a section showing which nodes the input split was located\non.\nBy following one of the links to the logfiles for the successful task attempt (you can see\nthe last 4 KB or 8 KB of each logfile, or the entire file), we can find the suspect input\nrecord that we logged (the line is wrapped and truncated to fit on the page):\nTemperature over 100 degrees for input:\n0335999999433181957042302005+37950+139117SAO +0004RJSN V020113590031500703569999994\n33201957010100005+35317+139650SAO +000899999V02002359002650076249N004000599+0067...\nThis record seems to be in a different format to the others. For one thing, there are\nspaces in the line, which are not described in the specification.\nRunning on a Cluster | 141When the job has finished, we can look at the value of the counter we defined to see\nhow many records over 100°C there are in the whole dataset. Counters are accessible\nvia the web UI, or the command line:\n% hadoop job -counter job_200904110811_0003 'v4.MaxTemperatureMapper$Temperature' \\\nOVER_100\n3\nThe -counter option takes the job ID, counter group name (which is the fully qualified\nclassname here), and the counter name (the enum name). There are only three mal-\nformed records in the entire dataset of over a billion records. Throwing out bad records\nis standard for many big data problems, although we need to be careful in this case,\nsince we are looking for an extreme value—the maximum temperature rather than an\naggregate measure. Still, throwing away three records is probably not going to change\nthe result.\nHadoop User Logs\nHadoop produces logs in various places, for various audiences. These are summarized\nin Table 5-2. Many of these files can be analyzed in aggregate using Chukwa, a Hadoop\nsubproject.\nAs you have seen in this section, MapReduce task logs are accessible through the web\nUI, which is the most convenient way to view them. You can also find the logfiles on\nthe local filesystem of the tasktracker that ran the task attempt, in a directory named\nby the task attempt. If task JVM reuse is enabled (“Task JVM Reuse” on page 170),\nthen each logfile accumulates the logs for the entire JVM run, so multiple task attempts\nwill be found in each logfile. The web UI hides this by showing only the portion that\nis relevant for the task attempt being viewed.\nIt is straightforward to write to these logfiles. Anything written to standard output, or\nstandard error, is directed to the relevant logfile. (Of course, in Streaming, standard\noutput is used for the map or reduce output, so it will not show up in the standard\noutput log.)\nIn Java, you can write to the task’s syslog file if you wish by using the Apache Commons\nLogging API. The actual logging is done by log4j in this case: the relevant log4j appender\nis called TLA (Task Log Appender) in the log4j.properties file in Hadoop’s configuration\ndirectory.\nThere are some controls for managing retention and size of task logs. By default, logs\nare deleted after a minimum of 24 hours (set using the mapred.userlog.retain.hours\nproperty). You can also set a cap on the maximum size of each logfile using the\nmapred.userlog.limit.kb property, which is 0 by default, meaning there is no cap.\n142 | Chapter 5: Developing a MapReduce ApplicationTable 5-2. Hadoop logs\nLogs Primary audience Description Further information\nSystem Administrators Each Hadoop daemon produces a logfile (using log4j) and “System log-\ndaemon another file that combines standard out and error. Written in files” on page 256.\nlogs the directory defined by the HADOOP_LOG_DIR environment \n     variable. \nHDFS audit Administrators A log of all HDFS requests, turned off by default. Written to “Audit Log-\nlogs the namenode’s log, although this is configurable. ging” on page 280.\nMapReduce Users A log of the events (such as task completion) that occur in the “Job His-\njob history course of running a job. Saved centrally on the jobtracker, and tory” on page 135.\nlogs in the job’s output directory in a _logs/history subdirectory. \nMapReduce Users Each tasktracker child process produces a logfile using log4j See next section.\ntask logs (called syslog), a file for data sent to standard out (stdout), and \n          a file for standard error (stderr). Written in the userlogs sub- \n          directory of the directory defined by the \n          HADOOP_LOG_DIR environment variable. \nHandling malformed data\nCapturing input data that causes a problem is valuable, as we can use it in a test to\ncheck that the mapper does the right thing:\n@Test\npublic void parsesMalformedTemperature() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0335999999433181957042302005+37950+139117SAO +0004"" +\n// Year ^^^^\n""RJSN V02011359003150070356999999433201957010100005+353"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nReporter reporter = mock(Reporter.class);\nmapper.map(null, value, output, reporter);\n}\nText outputKey = anyObject();\nIntWritable outputValue = anyObject();\nverify(output, never()).collect(outputKey, outputValue);\nverify(reporter).incrCounter(MaxTemperatureMapper.Temperature.MALFORMED, 1);\nThe record that was causing the problem is of a different format to the other lines we’ve\nseen. Example 5-11 shows a modified program (version 5) using a parser that ignores\neach line with a temperature field that does not have a leading sign (plus or minus).\nWe’ve also introduced a counter to measure the number of records that we are ignoring\nfor this reason.\nRunning on a Cluster | 143Example 5-11. Mapper for maximum temperature example\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nMALFORMED\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\noutput.collect(new Text(parser.getYear()), new IntWritable(airTemperature));\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n}\nUsing a Remote Debugger\nWhen a task fails and there is not enough information logged to diagnose the error,\nyou may want to resort to running a debugger for that task. This is hard to arrange\nwhen running the job on a cluster, as you don’t know which node is going to process\nwhich part of the input, so you can’t set up your debugger ahead of the failure. Instead,\nyou run the job with a property set that instructs Hadoop to keep all the intermediate\ndata generated during the job run. This data can then be used to rerun the failing task\nin isolation with a debugger attached. Note that the task is run in situ, on the same\nnode that it failed on, which increases the chances of the error being reproducible.†\nFirst, set the configuration property keep.failed.task.files to true, so that when tasks\nfail, the tasktracker keeps enough information to allow the task to be rerun over the\nsame input data. Then run the job again and note which node the task fails on, and the\ntask attempt ID (it begins with the string attempt_) using the web UI.\nNext we need to run a special task runner called IsolationRunner with the retained files\nas input. Log into the node that the task failed on and look for the directory for that\ntask attempt. It will be under one of the local MapReduce directories, as set by the\nmapred.local.dir property (covered in more detail in “Important Hadoop Daemon\nProperties” on page 258). If this property is a comma-separated list of directories (to\n† This feature is currently broken in Hadoop 0.20.0. Track the fix under https://issues.apache.org/jira/browse/\nHADOOP-4041.\n144 | Chapter 5: Developing a MapReduce Applicationspread load across the physical disks on a machine), then you may need to look in all\nof the directories before you find the directory for that particular task attempt. The task\nattempt directory is in the following location:\nmapred.local.dir/taskTracker/jobcache/job-ID/task-attempt-ID\nThis directory contains various files and directories, including job.xml, which contains\nall of the job configuration properties in effect during the task attempt, and which\nIsolationRunner uses to create a JobConf instance. For map tasks, this directory also\ncontains a file containing a serialized representation of the input split, so the same input\ndata can be fetched for the task. For reduce tasks, a copy of the map output, which\nforms the reduce input is stored in a directory named output.\nThere is also a directory called work, which is the working directory for the task attempt.\nWe change into this directory to run the IsolationRunner. We need to set some options\nto allow the remote debugger to connect:‡\n% export HADOOP_OPTS=""-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,\\\naddress=8000""\nThe suspend=y option tells the JVM to wait until the debugger has attached before\nrunning code. The IsolationRunner is launched with the following command:\n% hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml\nNext, set breakpoints, attach your remote debugger (all the major Java IDEs support\nremote debugging—consult the documentation for instructions), and the task will be\nrun under your control. You can rerun the task any number of times like this. With any\nluck, you’ll be able to find and fix the error.\nDuring the process, you can use other, standard, Java debugging techniques, such as\nkill -QUIT pid or jstack to get thread dumps.\nMore generally, it’s worth knowing that this technique isn’t only useful for failing tasks.\nYou can keep the intermediate files for successful tasks too, which may be handy if you\nwant to examine a task that isn’t failing. In this case, set the property\nkeep.task.files.pattern to a regular expression that matches the IDs of the tasks you\nwant to keep.\nTuning a Job\nAfter a job is working the question many developers ask is, “Can I make it run faster?”\nThere are a few Hadoop-specific “usual suspects” that are worth checking to see if they\nare responsible for a performance problem. You should run through the checklist in\nTable 5-3 before you start trying to profile or optimize at the task level.\n‡ You can find details about debugging options on the Java Platform Debugger Architecture web page.\nTuning a Job | 145Table 5-3. Tuning checklist\nArea Best practice Further information\nNumber of map- How long are you mappers running for? If they are only running for a few seconds “Small files and Com-\npers on average, then you should see if there’s a way to have fewer mappers and bineFileInputFor-\n       make them all run longer, a minute or so, as a rule of thumb. The extent to mat” on page 190\n       which this is possible depends on the input format you are using. \nNumber of reducers For maximum performance, the number of reducers should be slightly less than “Choosing the Num-\n                   the number of reduce slots in the cluster. This allows the reducers to finish in ber of Reduc-\n                   one wave, and fully utilizes the cluster during the reduce phase. ers” on page 181\nCombiners Can your job take advantage of a combiner to reduce the amount of data in “Combiner Func-\n          passing through the shuffle? tions” on page 29\nIntermediate Job execution time can almost always benefit from enabling map output “Compressing map\ncompression compression. output” on page 85\nCustom If you are using your own custom Writable objects, or custom comparators, “Implementing a\nserialization then make sure you have implemented RawComparator. RawComparator for\n                                                                speed” on page 99\nShuffle tweaks The MapReduce shuffle exposes around a dozen tuning parameters for memory “Configuration Tun-\n               management, which may help you eke out the last bit of performance. ing” on page 166\nProfiling Tasks\nLike debugging, profiling a job running on a distributed system like MapReduce\npresents some challenges. Hadoop allows you to profile a fraction of the tasks in a job,\nand, as each task completes, pulls down the profile information to your machine for\nlater analysis with standard profiling tools.\nOf course, it’s possible, and somewhat easier, to profile a job running in the local job\nrunner. And provided you can run with enough input data to exercise the map and\nreduce tasks, this can be a valuable way of improving the performance of your mappers\nand reducers. There are a couple of caveats, however. The local job runner is a very\ndifferent environment from a cluster, and the data flow patterns are very different.\nOptimizing the CPU performance of your code may be pointless if your MapReduce\njob is I/O-bound (as many jobs are). To be sure that any tuning is effective, you should\ncompare the new execution time with the old running on a real cluster. Even this is\neasier said than done, since job execution times can vary due to resource contention\nwith other jobs and the decisions the scheduler makes to do with task placement. To\nget a good idea of job execution time under these circumstances, perform a series of\nruns (with and without the change) and check whether any improvement is statistically\nsignificant.\nIt’s unfortunately true that some problems (such as excessive memory use) can be re-\nproduced only on the cluster, and in these cases the ability to profile in situ is\nindispensable.\n146 | Chapter 5: Developing a MapReduce ApplicationThe HPROF profiler\nThere are a number of configuration properties to control profiling, which are also\nexposed via convenience methods on JobConf. The following modification to\nMaxTemperatureDriver (version 6) will enable remote HPROF profiling. HPROF is a\nprofiling tool that comes with the JDK that, although basic, can give valuable infor-\nmation about a program’s CPU and heap usage.§\nconf.setProfileEnabled(true);\nconf.setProfileParams(""-agentlib:hprof=cpu=samples,heap=sites,depth=6,"" +\n""force=n,thread=y,verbose=n,file=%s"");\nconf.setProfileTaskRange(true, ""0-2"");\nThe first line enables profiling, which by default is turned off. (This is equivalent to\nsetting the configuration property mapred.task.profile to true).\nNext we set the profile parameters, which are the extra command-line arguments to\npass to the task’s JVM. (When profiling is enabled, a new JVM is allocated for each\ntask, even if JVM reuse is turned on; see “Task JVM Reuse” on page 170.) The default\nparameters specify the HPROF profiler; here we set an extra HPROF option, depth=6,\nto give more stack trace depth than the HPROF default. The setProfileParams()\nmethod on JobConf is equivalent to setting the mapred.task.profile.params.\nFinally, we specify which tasks we want to profile. We normally only want profile\ninformation from a few tasks, so we use the setProfileTaskRange() method to specify\nthe range of task IDs that we want profile information for. We’ve set it to 0-2 (which\nis actually the default), which means tasks with IDs 0, 1, and 2 are profiled. The first\nargument to the setProfileTaskRange() method dictates whether the range is for map\nor reduce tasks: true is for maps, false is for reduces. A set of ranges is permitted, using\na notation that allows open ranges. For example, 0-1,4,6- would specify all tasks except\nthose with IDs 2, 3, and 5. The tasks to profile can also be controlled using the\nmapred.task.profile.maps property for map tasks, and mapred.task.profile.reduces\nfor reduce tasks.\nWhen we run a job with the modified driver, the profile output turns up at the end of\nthe job in the directory we launched the job from. Since we are only profiling a few\ntasks, we can run the job on a subset of the dataset.\nHere’s a snippet of one of the mapper’s profile files, which shows the CPU sampling\ninformation:\nCPU SAMPLES BEGIN (total =\nrank\nself accum\ncount\n1 3.49% 3.49%\n35\n2 3.39% 6.89%\n34\n3 3.19% 10.08%\n32\n1002) Sat Apr 11 11:17:52 2009\ntrace method\n307969 java.lang.Object.<init>\n307954 java.lang.Object.<init>\n307945 java.util.regex.Matcher.<init>\n§ HPROF uses byte code insertion to profile your code, so you do not need to recompile your application with\nspecial options to use it. For more information on HPROF, see “HPROF: A Heap/CPU Profiling Tool in J2SE\n5.0,” by Kelly O’Hair at http://java.sun.com/developer/technicalArticles/Programming/HPROF.html.\nTuning a Job | 1474\n5\n3.19% 13.27%\n3.19% 16.47%\n32 307963 java.lang.Object.<init>\n32 307973 java.lang.Object.<init>\nCross referencing the trace number 307973 gives us the stacktrace from the same file:\nTRACE 307973: (thread=200001)\njava.lang.Object.<init>(Object.java:20)\norg.apache.hadoop.io.IntWritable.<init>(IntWritable.java:29)\nv5.MaxTemperatureMapper.map(MaxTemperatureMapper.java:30)\nv5.MaxTemperatureMapper.map(MaxTemperatureMapper.java:14)\norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\norg.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:356)\nSo it looks like the mapper is spending 3% of its time constructing IntWritable objects.\nThis observation suggests that it might be worth reusing the Writable instances being\noutput (version 7):\nExample 5-12. Reusing the Text and IntWritable output objects\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nMALFORMED\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\nprivate Text year = new Text();\nprivate IntWritable temp = new IntWritable();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nyear.set(parser.getYear());\ntemp.set(parser.getAirTemperature());\noutput.collect(year, temp);\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n}\nHowever, we know if this is significant only if we can measure an improvement when\nrunning the job over the whole dataset. Running each variant five times on an otherwise\nquiet 11-node cluster showed no statistically significant difference in job execution\ntime. Of course, this result holds only for this particular combination of code, data,\nand hardware, so you should perform similar benchmarks to see whether such a change\nis significant for your setup.\n148 | Chapter 5: Developing a MapReduce ApplicationOther profilers\nAt the time of this writing, the mechanism for retrieving profile output is HPROF-\nspecific. Until this is fixed, it should be possible to use Hadoop’s profiling settings to\ntrigger profiling using any profiler (see the documentation for the particular profiler),\nalthough it may be necessary to manually retrieve the profiler’s output from tasktrack-\ners for analysis.\nIf the profiler is not installed on all the tasktracker machines, consider using the Dis-\ntributed Cache (“Distributed Cache” on page 239) for making the profiler binary\navailable on the required machines.\nMapReduce Workflows\nSo far in this chapter, you have seen the mechanics of writing a program using Map-\nReduce. We haven’t yet considered how to turn a data processing problem into the\nMapReduce model.\nThe data processing you have seen so far in this book is to solve a fairly simple problem\n(finding the maximum recorded temperature for given years). When the processing\ngets more complex, this complexity is generally manifested by having more MapReduce\njobs, rather than having more complex map and reduce functions. In other words, as\na rule of thumb, think about adding more jobs, rather than adding complexity to jobs.\nFor more complex problems, it is worth considering a higher-level language than Map-\nReduce, such as Pig, Hive, or Cascading. One immediate benefit is that it frees you up\nfrom having to do the translation into MapReduce jobs, allowing you to concentrate\non the analysis you are performing.\nDecomposing a Problem into MapReduce Jobs\nLet’s look at an example of a more complex problem that we want to translate into a\nMapReduce workflow.\nImagine that we want to find the mean maximum recorded temperature for every day\nof the year and every weather station. In concrete terms, to calculate the mean maxi-\nmum daily temperature recorded by station 029070-99999, say, on January 1, we take\nthe mean of the maximum daily temperatures for this station for January 1, 1901;\nJanuary 1, 1902; and so on up to January 1, 2000.\nHow can we compute this using MapReduce? The computation decomposes most\nnaturally into two stages:\n1. Compute the maximum daily temperature for every station-date pair.\nThe MapReduce program in this case is a variant of the maximum temperature\nprogram, except that the keys in this case are a composite station-date pair, rather\nthan just the year.\nMapReduce Workflows | 1492. Compute the mean of the maximum daily temperatures for every station-day-\nmonth key.\nThe mapper takes the output from the previous job (station-date, maximum tem-\nperature) records, and projects it into (station-day-month, maximum temperature)\nrecords by dropping the year component. The reduce function then takes the mean\nof the maximum temperatures, for each station-day-month key.\nThe output from first stage looks like this for the station we are interested in. (The\nmean_max_daily_temp.sh script in the examples provides an implementation in\nHadoop Streaming.)\n029070-99999\n029070-99999\n...\n19010101\n19020101\n0\n-94\nThe first two fields form the key, and the final column is the maximum temperature\nfrom all the readings for the given station and date. The second stage averages these\ndaily maxima over years to yield:\n029070-99999\n0101\n-68\nwhich is interpreted as saying the mean maximum daily temperature on January 1 for\nstation 029070-99999 over the century is −6.8°C.\nIt’s possible to do this computation in one MapReduce stage, but it takes more work\non the part of the programmer.‖\nThe arguments for having more (but simpler) MapReduce stages are that doing so leads\nto more composable and more maintainable mappers and reducers. The case studies\nin Chapter 14 cover a wide range of real-world problems that were solved using Map-\nReduce, and in each case, the data processing task is implemented using two or more\nMapReduce jobs. The details in that chapter are invaluable for getting a better idea of\nhow to decompose a processing problem into a MapReduce workflow.\nIt’s possible to make map and reduce functions even more composable than we have\ndone. A mapper commonly performs input format parsing, projection (selecting the\nrelevant fields), and filtering (removing records that are not of interest). In the mappers\nyou have seen so far, we have implemented all of these functions in a single mapper.\nHowever, there is a case for splitting these into distinct mappers and chaining them\ninto a single mapper using the ChainMapper library class that comes with Hadoop.\nCombined with a ChainReducer, you can run a chain of mappers, followed by a reducer\nand another chain of mappers in a single MapReduce job.\n‖ It’s an interesting exercise to do this. Hint: use “Secondary Sort” on page 227.\n150 | Chapter 5: Developing a MapReduce ApplicationRunning Dependent Jobs\nWhen there is more than one job in a MapReduce workflow, the question arises: how\ndo you manage the jobs so they are executed in order? There are several approaches,\nand the main consideration is whether you have a linear chain of jobs, or a more com-\nplex directed acyclic graph (DAG) of jobs.\nFor a linear chain, the simplest approach is to run each job one after another, waiting\nuntil a job completes successfully before running the next.\nJobClient.runJob(conf1);\nJobClient.runJob(conf2);\nIf a job fails, the runJob() method will throw an IOException, so later jobs in the pipeline\ndon’t get executed. Depending on your application, you might want to catch the ex-\nception and clean up any intermediate data that was produced by any previous jobs.\nFor anything more complex than a linear chain, there are libraries that can help or-\nchestrate your workflow (although they are suited to linear chains, or even one-off jobs,\ntoo). The simplest is in the org.apache.hadoop.mapred.jobcontrol package: the\nJobControl class. An instance of JobControl represents a graph of jobs to be run. You\nadd the job configurations, then tell the JobControl instance the dependencies between\njobs. You run the JobControl in a thread, and it runs the jobs in dependency order. You\ncan poll for progress, and when the jobs have finished, you can query for all the jobs’\nstatuses, and the associated errors for any failures. If a job fails, JobControl won’t run\nits dependencies.\nUnlike JobControl which runs on the client machine submitting the jobs, the Hadoop\nWorkflow Scheduler (HWS)# runs as a server, and a client submits a workflow to the\nscheduler. When the workflow completes the scheduler can make an HTTP callback\nto the client to inform it of the jobs’ statuses. HWS can run different types of jobs in\nthe same workflow: a Pig job followed by a Java MapReduce job, for example.\n# At the time of this writing, the Hadoop Workflow Scheduler is still under development. See https://issues\n.apache.org/jira/browse/HADOOP-5303.\nMapReduce Workflows | 151CHAPTER 6\nHow MapReduce Works\nIn this chapter, we look at how MapReduce in Hadoop works in detail. This knowledge\nprovides a good foundation for writing more advanced MapReduce programs, which\nwe will cover in the following two chapters.\nAnatomy of a MapReduce Job Run\nYou can run a MapReduce job with a single line of code: JobClient.runJob(conf). It’s\nvery short, but it conceals a great deal of processing behind the scenes. This section\nuncovers the steps Hadoop takes to run a job.\nThe whole process is illustrated in Figure 6-1. At the highest level, there are four inde-\npendent entities:\n• The client, which submits the MapReduce job.\n• The jobtracker, which coordinates the job run. The jobtracker is a Java application\nwhose main class is JobTracker.\n• The tasktrackers, which run the tasks that the job has been split into. Tasktrackers\nare Java applications whose main class is TaskTracker.\n• The distributed filesystem (normally HDFS, covered in Chapter 3), which is used\nfor sharing job files between the other entities.\nJob Submission\nThe runJob() method on JobClient is a convenience method that creates a new\nJobClient instance and calls submitJob() on it (step 1 in Figure 6-1). Having submitted\nthe job, runJob() polls the job’s progress once a second, and reports the progress to the\nconsole if it has changed since the last report. When the job is complete, if it was\nsuccessful, the job counters are displayed. Otherwise, the error that caused the job to\nfail is logged to the console.\n153Figure 6-1. How Hadoop runs a MapReduce job\nThe job submission process implemented by JobClient’s submitJob() method does the\nfollowing:\n• Asks the jobtracker for a new job ID (by calling getNewJobId() on JobTracker) (step\n2).\n• Checks the output specification of the job. For example, if the output directory has\nnot been specified or it already exists, the job is not submitted and an error is\nthrown to the MapReduce program.\n• Computes the input splits for the job. If the splits cannot be computed, because\nthe input paths don’t exist, for example, then the job is not submitted and an error\nis thrown to the MapReduce program.\n• Copies the resources needed to run the job, including the job JAR file, the config-\nuration file and the computed input splits, to the jobtracker’s filesystem in a di-\nrectory named after the job ID. The job JAR is copied with a high replication factor\n(controlled by the mapred.submit.replication property, which defaults to 10) so\nthat there are lots of copies across the cluster for the tasktrackers to access when\nthey run tasks for the job (step 3).\n154 | Chapter 6: How MapReduce Works• Tells the jobtracker that the job is ready for execution (by calling submitJob() on\nJobTracker) (step 4).\nJob Initialization\nWhen the JobTracker receives a call to its submitJob() method, it puts it into an internal\nqueue from where the job scheduler will pick it up and initialize it. Initialization involves\ncreating an object to represent the job being run, which encapsulates its tasks, and\nbookkeeping information to keep track of the tasks’ status and progress (step 5).\nTo create the list of tasks to run, the job scheduler first retrieves the input splits com-\nputed by the JobClient from the shared filesystem (step 6). It then creates one map task\nfor each split. The number of reduce tasks to create is determined by the\nmapred.reduce.tasks property in the JobConf, which is set by the setNumReduce\nTasks() method, and the scheduler simply creates this number of reduce tasks to be\nrun. Tasks are given IDs at this point.\nTask Assignment\nTasktrackers run a simple loop that periodically sends heartbeat method calls to the\njobtracker. Heartbeats tell the jobtracker that a tasktracker is alive, but they also double\nas a channel for messages. As a part of the heartbeat, a tasktracker will indicate whether\nit is ready to run a new task, and if it is, the jobtracker will allocate it a task, which it\ncommunicates to the tasktracker using the heartbeat return value (step 7).\nBefore it can choose a task for the tasktracker, the jobtracker must choose a job to select\nthe task from. There are various scheduling algorithms as explained later in this chapter\n(see “Job Scheduling” on page 161), but the default one simply maintains a priority\nlist of jobs. Having chosen a job, the jobtracker now chooses a task for the job.\nTasktrackers have a fixed number of slots for map tasks and for reduce tasks: for ex-\nample, a tasktracker may be able to run two map tasks and two reduce tasks simulta-\nneously. (The precise number depends on the number of cores and the amount of\nmemory on the tasktracker; see “Memory” on page 254.) The default scheduler fills\nempty map task slots before reduce task slots, so if the tasktracker has at least one\nempty map task slot, the jobtracker will select a map task; otherwise, it will select a\nreduce task.\nTo choose a reduce task the jobtracker simply takes the next in its list of yet-to-be-run\nreduce tasks, since there are no data locality considerations. For a map task, however,\nit takes account of the tasktracker’s network location and picks a task whose input split\nis as close as possible to the tasktracker. In the optimal case, the task is data-local, that\nis, running on the same node that the split resides on. Alternatively, the task may be\nrack-local: on the same rack, but not the same node, as the split. Some tasks are neither\ndata-local nor rack-local and retrieve their data from a different rack from the one they\nAnatomy of a MapReduce Job Run | 155are running on. You can tell the proportion of each type of task by looking at a job’s\ncounters (see “Built-in Counters” on page 211).\nTask Execution\nNow the tasktracker has been assigned a task, the next step is for it to run the task.\nFirst, it localizes the job JAR by copying it from the shared filesystem to the tasktracker’s\nfilesystem. It also copies any files needed from the distributed cache by the application\nto the local disk; see “Distributed Cache” on page 239 (step 8). Second, it creates a\nlocal working directory for the task, and un-jars the contents of the JAR into this di-\nrectory. Third, it creates an instance of TaskRunner to run the task.\nTaskRunner launches a new Java Virtual Machine (step 9) to run each task in (step 10),\nso that any bugs in the user-defined map and reduce functions don’t affect the task-\ntracker (by causing it to crash or hang, for example). It is however possible to reuse the\nJVM between tasks; see “Task JVM Reuse” on page 170.\nThe child process communicates with its parent through the umbilical interface. This\nway it informs the parent of the task’s progress every few seconds until the task is\ncomplete.\nStreaming and Pipes\nBoth Streaming and Pipes run special map and reduce tasks for the purpose of launching\nthe user-supplied executable and communicating with it (Figure 6-2).\nIn the case of Streaming, the Streaming task communicates with the process (which\nmay be written in any language) using standard input and output streams. The Pipes\ntask, on the other hand, listens on a socket and passes the C++ process a port number\nin its environment, so that on startup, the C++ process can establish a persistent socket\nconnection back to the parent Java Pipes task.\nIn both cases, during execution of the task, the Java process passes input key-value\npairs to the external process, which runs it through the user-defined map or reduce\nfunction and passes the output key-value pairs back to the Java process. From the\ntasktracker’s point of view, it is as if the tasktracker child process ran the map or reduce\ncode itself.\nProgress and Status Updates\nMapReduce jobs are long-running batch jobs, taking anything from minutes to hours\nto run. Because this is a significant length of time, it’s important for the user to get\nfeedback on how the job is progressing. A job and each of its tasks have a status, which\nincludes such things as the state of the job or task (e.g., running, successfully completed,\nfailed), the progress of maps and reduces, the values of the job’s counters, and a status\n156 | Chapter 6: How MapReduce WorksFigure 6-2. The relationship of the Streaming and Pipes executable to the tasktracker and its child\nmessage or description (which may be set by user code). These statuses change over\nthe course of the job, so how do they get communicated back to the client?\nWhen a task is running, it keeps track of its progress, that is, the proportion of the task\ncompleted. For map tasks, this is the proportion of the input that has been processed.\nFor reduce tasks, it’s a little more complex, but the system can still estimate the pro-\nportion of the reduce input processed. It does this by dividing the total progress into\nthree parts, corresponding to the three phases of the shuffle (see “Shuffle and\nSort” on page 163). For example, if the task has run the reducer on half its input, then\nthe task’s progress is 5⁄6, since it has completed the copy and sort phases (1⁄3 each) and\nis half way through the reduce phase (1⁄6).\nAnatomy of a MapReduce Job Run | 157What Constitutes Progress in MapReduce?\nProgress is not always measurable, but nevertheless it tells Hadoop that a task is doing\nsomething. For example, a task writing output records is making progress, even though\nit cannot be expressed as a percentage of the total number that will be written, since\nthe latter figure may not be known, even by the task producing the output.\nProgress reporting is important, as it means Hadoop will not fail a task that’s making\nprogress. All of the following operations constitute progress:\n• Reading an input record (in a mapper or reducer)\n• Writing an output record (in a mapper or reducer)\n• Setting the status description on a reporter (using Reporter’s setStatus() method)\n• Incrementing a counter (using Reporter’s incrCounter() method)\n• Calling Reporter’s progress() method\nTasks also have a set of counters that count various events as the task runs (we saw an\nexample in “A test run” on page 23), either those built into the framework, such as the\nnumber of map output records written, or ones defined by users.\nIf a task reports progress, it sets a flag to indicate that the status change should be sent\nto the tasktracker. The flag is checked in a separate thread every three seconds, and if\nset it notifies the tasktracker of the current task status. Meanwhile, the tasktracker is\nsending heartbeats to the jobtracker every five seconds (this is a minimum, as the\nheartbeat interval is actually dependent on the size of the cluster: for larger clusters,\nthe interval is longer), and the status of all the tasks being run by the tasktracker is sent\nin the call. Counters are sent less frequently than every five seconds, because they can\nbe relatively high-bandwidth.\nThe jobtracker combines these updates to produce a global view of the status of all the\njobs being run and their constituent tasks. Finally, as mentioned earlier, the\nJobClient receives the latest status by polling the jobtracker every second. Clients can\nalso use JobClient’s getJob() method to obtain a RunningJob instance, which contains\nall of the status information for the job.\nThe method calls are illustrated in Figure 6-3.\nJob Completion\nWhen the jobtracker receives a notification that the last task for a job is complete, it\nchanges the status for the job to “successful.” Then, when the JobClient polls for status,\nit learns that the job has completed successfully, so it prints a message to tell the user,\nand then returns from the runJob() method.\n158 | Chapter 6: How MapReduce WorksFigure 6-3. How status updates are propagated through the MapReduce system\nThe jobtracker also sends a HTTP job notification if it is configured to do so. This can\nbe configured by clients wishing to receive callbacks, via the job.end.notifica\ntion.url property.\nLast, the jobtracker cleans up its working state for the job, and instructs tasktrackers\nto do the same (so intermediate output is deleted, for example).\nFailures\nIn the real world, user code is buggy, processes crash, and machines fail. One of the\nmajor benefits of using Hadoop is its ability to handle such failures and allow your job\nto complete.\nTask Failure\nConsider first the case of the child task failing. The most common way that this happens\nis when user code in the map or reduce task throws a runtime exception. If this happens,\nFailures | 159the child JVM reports the error back to its parent tasktracker, before it exits. The error\nultimately makes it into the user logs. The tasktracker marks the task attempt as\nfailed, freeing up a slot to run another task.\nFor Streaming tasks, if the Streaming process exits with a nonzero exit code, it is marked\nas failed. This behavior is governed by the stream.non.zero.exit.is.failure property\n(the default is true).\nAnother failure mode is the sudden exit of the child JVM—perhaps there is a JVM bug\nthat causes the JVM to exit for a particular set of circumstances exposed by the Map-\nReduce user code. In this case, the tasktracker notices that the process has exited, and\nmarks the attempt as failed.\nHanging tasks are dealt with differently. The tasktracker notices that it hasn’t received\na progress update for a while, and proceeds to mark the task as failed. The child JVM\nprocess will be automatically killed after this period.* The timeout period after which\ntasks are considered failed is normally 10 minutes, and can be configured on a per-job\nbasis (or a cluster basis) by setting the mapred.task.timeout property to a value in\nmilliseconds.\nSetting the timeout to a value of zero disables the timeout, so long-running tasks are\nnever marked as failed. In this case, a hanging task will never free up its slot, and over\ntime there may be cluster slowdown as a result. This approach should therefore be\navoided, and making sure that a task is reporting progress periodically will suffice (see\n“What Constitutes Progress in MapReduce?” on page 158).\nWhen the jobtracker is notified of a task attempt that has failed (by the tasktracker’s\nheartbeat call) it will reschedule execution of the task. The jobtracker will try to avoid\nrescheduling the task on a tasktracker where it has previously failed. Furthermore, if a\ntask fails more than four times, it will not be retried further. This value is configurable:\nthe maximum number of attempts to run a task is controlled by the\nmapred.map.max.attempts property for map tasks, and mapred.reduce.max.attempts for\nreduce tasks. By default, if any task fails more than four times (or whatever the maxi-\nmum number of attempts is configured to), the whole job fails.\nFor some applications it is undesirable to abort the job if a few tasks fail, as it may be\npossible to use the results of the job despite some failures. In this case, the maximum\npercentage of tasks that are allowed to fail without triggering job failure can be set for\nthe job. Map tasks and reduce tasks are controlled independently, using the\nmapred.max.map.failures.percent\nand\nmapred.max.reduce.failures.percent\nproperties.\n* If a Streaming process hangs, the tasktracker does not try to kill it (although the JVM that launched it will\nbe killed), so you should take precautions to monitor for this scenario, and kill orphaned processes by some\nother means.\n160 | Chapter 6: How MapReduce WorksA task attempt may also be killed, which is different from it failing. A task attempt may\nbe killed because it is a speculative duplicate (for more, see “Speculative Execu-\ntion” on page 169), or because the tasktracker it was running on failed, and the job-\ntracker marked all the task attempts running on it as killed. Killed task attempts do not\ncount against the number of attempts to run the task (as set by\nmapred.map.max.attempts and mapred.reduce.max.attempts), since it wasn’t the task’s\nfault that an attempt was killed.\nUsers may also kill or fail task attempts using the web UI or the command line (type\nhadoop job to see the options). Jobs may also be killed by the same mechanisms.\nTasktracker Failure\nFailure of a tasktracker is another failure mode. If a tasktracker fails by crashing, or\nrunning very slowly, it will stop sending heartbeats to the jobtracker (or send them very\ninfrequently). The jobtracker will notice a tasktracker that has stopped sending heart-\nbeats (if it hasn’t received one for 10 minutes, configured via the mapred.task\ntracker.expiry.interval property, in milliseconds) and remove it from its pool of\ntasktrackers to schedule tasks on. The jobtracker arranges for map tasks that were run\nand completed successfully on that tasktracker to be rerun if they belong to incomplete\njobs, since their intermediate output residing on the failed tasktracker’s local filesystem\nmay not be accessible to the reduce task. Any tasks in progress are also rescheduled.\nA tasktracker can also be blacklisted by the jobtracker, even if the tasktracker has not\nfailed. A tasktracker is blacklisted if the number of tasks that have failed on it is sig-\nnificantly higher than the average task failure rate on the cluster. Blacklisted tasktrack-\ners can be restarted to remove them from the jobtracker’s blacklist.\nJobtracker Failure\nFailure of the jobtracker is the most serious failure mode. Currently, Hadoop has no\nmechanism for dealing with failure of the jobtracker—it is a single point of failure—\nso in this case the job fails. However, this failure mode has a low chance of occurring\nsince the chance of a particular machine failing is low. It is possible that a future release\nof Hadoop will remove this limitation by running multiple jobtrackers, only one of\nwhich is the primary jobtracker at any time (using ZooKeeper as a coordination mech-\nanism for the jobtrackers to decide who is the primary; see Chapter 13).\nJob Scheduling\nEarly versions of Hadoop had a very simple approach to scheduling users’ jobs: they\nran in order of submission, using a FIFO scheduler. Typically each job would use the\nwhole cluster, so jobs had to wait their turn. Although a shared cluster offers great\npotential for offering large resources to many users, the problem of sharing resources\nJob Scheduling | 161fairly between users requires a better scheduler. Production jobs need to complete in a\ntimely manner, while allowing users who are making smaller ad hoc queries to get\nresults back in a reasonable time.\nLater on, the ability to set a job’s priority was added, via the mapred.job.priority\nproperty or the setJobPriority() method on JobClient (both of which take one of the\nvalues VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW). When the job scheduler is choosing the\nnext job to run, it selects one with the highest priority. However, with the FIFO\nscheduler, priorities do not support preemption, so a high-priority job can still be\nblocked by a long-running low priority job that started before the high-priority job was\nscheduled.\nMapReduce in Hadoop now comes with a choice of schedulers. The default is the\noriginal FIFO queue-based scheduler, and there also a multi-user scheduler called the\nFair Scheduler.\nThe Fair Scheduler\nThe Fair Scheduler aims to give every user a fair share of the cluster capacity over time.\nIf a single job is running, it gets all of the cluster. As more jobs are submitted, free task\nslots are given to the jobs in such a way as to give each user a fair share of the cluster.\nA short job belonging to one user will complete in a reasonable time even while another\nuser’s long job is running, and the long job will still make progress.\nJobs are placed in pools, and by default, each user gets their own pool. A user who\nsubmits more jobs than a second user will not get any more cluster resources than the\nsecond, on average. It is also possible to define custom pools with guaranteed minimum\ncapacities defined in terms of the number of map and reduce slots, and to set weightings\nfor each pool.\nThe Fair Scheduler supports preemption, so if a pool has not received its fair share for\na certain period of time, then the scheduler will kill tasks in pools running over capacity\nin order to give the slots to the pool running under capacity.\nThe Fair Scheduler is a “contrib” module. To enable it, place its JAR file on Hadoop’s\nclasspath, by copying it from Hadoop’s contrib/fairscheduler directory to the lib direc-\ntory. Then set the mapred.jobtracker.taskScheduler property to:\norg.apache.hadoop.mapred.FairScheduler\nThe Fair Scheduler will work without further configuration, but to take full advantage\nof its features and how to configure it (including its web interface), refer to README\nin the src/contrib/fairscheduler directory of the distribution.\n162 | Chapter 6: How MapReduce WorksShuffle and Sort\nMapReduce makes the guarantee that the input to every reducer is sorted by key. The\nprocess by which the system performs the sort—and transfers the map outputs to the\nreducers as inputs—is known as the shuffle.† In this section, we look at how the shuffle\nworks, as a basic understanding would be helpful, should you need to optimize a Map-\nReduce program. The shuffle is an area of the codebase where refinements and\nimprovements are continually being made, so the following description necessarily\nconceals many details (and may change over time). In many ways, the shuffle is the\nheart of MapReduce, and is where the “magic” happens.\nThe Map Side\nWhen the map function starts producing output, it is not simply written to disk. The\nprocess is more involved, and takes advantage of buffering writes in memory and doing\nsome presorting for efficiency reasons. Figure 6-4 shows what happens.\nFigure 6-4. Shuffle and sort in MapReduce\nEach map task has a circular memory buffer that it writes the output to. The buffer is\n100 MB by default, a size which can be tuned by changing the io.sort.mb property.\nWhen the contents of the buffer reaches a certain threshold size (io.sort.spill.per\ncent, default 0.80, or 80%) a background thread will start to spill the contents to disk.\nMap outputs will continue to be written to the buffer while the spill takes place, but if\nthe buffer fills up during this time, the map will block until the spill is complete.\n† The term shuffle is actually imprecise, since in some contexts it refers to only the part of the process where\nmap outputs are fetched by reduce tasks. In this section, we take it to mean the whole process from the point\nwhere a map produces output to where a reduce consumes input.\nShuffle and Sort | 163Spills are written in round-robin fashion to the directories specified by the\nmapred.local.dir property, in a job-specific subdirectory.\nBefore it writes to disk, the thread first divides the data into partitions corresponding\nto the reducers that they will ultimately be sent to. Within each partition, the back-\nground thread performs an in-memory sort by key, and if there is a combiner function,\nit is run on the output of the sort.\nEach time the memory buffer reaches the spill threshold, a new spill file is created, so\nafter the map task has written its last output record there could be several spill files.\nBefore the task is finished, the spill files are merged into a single partitioned and sorted\noutput file. The configuration property io.sort.factor controls the maximum number\nof streams to merge at once; the default is 10.\nIf a combiner function has been specified, and the number of spills is at least three (the\nvalue of the min.num.spills.for.combine property), then the combiner is run before the\noutput file is written. Recall that combiners may be run repeatedly over the input with-\nout affecting the final result. The point is that running combiners makes for a more\ncompact map output, so there is less data to write to local disk and to transfer to the\nreducer.\nIt is often a good idea to compress the map output as it is written to disk, since doing\nso makes it faster to write to disk, saves disk space, and reduces the amount of data to\ntransfer to the reducer. By default the output is not compressed, but it is easy to enable\nby setting mapred.compress.map.output to true. The compression library to use is speci-\nfied by mapred.map.output.compression.codec; see “Compression” on page 77 for more\non compression formats.\nThe output file’s partitions are made available to the reducers over HTTP. The number\nof worker threads used to serve the file partitions is controlled by the task\ntracker.http.threads property—this setting is per tasktracker, not per map task slot.\nThe default of 40 may need increasing for large clusters running large jobs.\nThe Reduce Side\nLet’s turn now to the reduce part of the process. The map output file is sitting on the\nlocal disk of the tasktracker that ran the map task (note that although map outputs\nalways get written to the local disk of the map tasktracker, reduce outputs may not be),\nbut now it is needed by the tasktracker that is about to run the reduce task for the\npartition. Furthermore, the reduce task needs the map output for its particular partition\nfrom several map tasks across the cluster. The map tasks may finish at different times,\nso the reduce task starts copying their outputs as soon as each completes. This is known\nas the copy phase of the reduce task. The reduce task has a small number of copier\nthreads so that it can fetch map outputs in parallel. The default is five threads, but this\nnumber can be changed by setting the mapred.reduce.parallel.copies property.\n164 | Chapter 6: How MapReduce WorksHow do reducers know which tasktrackers to fetch map output from?\nAs map tasks complete successfully, they notify their parent tasktracker\nof the status update, which in turn notifies the jobtracker. These noti-\nfications are transmitted over the heartbeat communication mechanism\ndescribed earlier. Therefore, for a given job, the jobtracker knows the\nmapping between map outputs and tasktrackers. A thread in the reducer\nperiodically asks the jobtracker for map output locations until it has\nretrieved them all.\nTasktrackers do not delete map outputs from disk as soon as the first\nreducer has retrieved them, as the reducer may fail. Instead, they wait\nuntil they are told to delete them by the jobtracker, which is after the\njob has completed.\nThe map outputs are copied to the reduce tasktracker’s memory if they are small enough\n(the buffer’s size is controlled by mapred.job.shuffle.input.buffer.percent, which\nspecifies the proportion of the heap to use for this purpose); otherwise, they are copied\nto disk. When the in-memory buffer reaches a threshold size (controlled by\nmapred.job.shuffle.merge.percent), or reaches a threshold number of map outputs\n(mapred.inmem.merge.threshold), it is merged and spilled to disk.\nAs the copies accumulate on disk, a background thread merges them into larger, sorted\nfiles. This saves some time merging later on. Note that any map outputs that were\ncompressed (by the map task) have to be decompressed in memory in order to perform\na merge on them.\nWhen all the map outputs have been copied, the reduce task moves into the sort\nphase (which should properly be called the merge phase, as the sorting was carried out\non the map side), which merges the map outputs, maintaining their sort ordering. This\nis done in rounds. For example, if there were 50 map outputs, and the merge factor was\n10 (the default, controlled by the io.sort.factor property, just like in the map’s merge),\nthen there would be 5 rounds. Each round would merge 10 files into one, so at the end\nthere would be five intermediate files.\nRather than have a final round that merges these five files into a single sorted file, the\nmerge saves a trip to disk by directly feeding the reduce function in what is the last\nphase: the reduce phase. This final merge can come from a mixture of in-memory and\non-disk segments.\nShuffle and Sort | 165The number of files merged in each round is actually more subtle than\nthis example suggests. The goal is to merge the minimum number of\nfiles to get to the merge factor for the final round. So if there were 40\nfiles, the merge would not merge 10 files in each of the four rounds to\nget 4 files. Instead, the first round would merge only 4 files, and the\nsubsequent three rounds would merge the full 10 files. The 4 merged\nfiles, and the 6 (as yet unmerged) files make a total of 10 files for the\nfinal round.\nNote that this does not change the number of rounds, it’s just an opti-\nmization to minimize the amount of data that is written to disk, since\nthe final round always merges directly into the reduce.\nDuring the reduce phase the reduce function is invoked for each key in the sorted\noutput. The output of this phase is written directly to the output filesystem, typically\nHDFS. In the case of HDFS, since the tasktracker node is also running a datanode, the\nfirst block replica will be written to the local disk.\nConfiguration Tuning\nWe are now in a better position to understand how to tune the shuffle to improve\nMapReduce performance. The relevant settings, which can be used on a per-job basis\n(except where noted), are summarized in Tables 6-1 and 6-2, along with the defaults,\nwhich are good for general-purpose jobs.\nThe general principle is to give the shuffle as much memory as possible. However, there\nis a trade-off, in that you need to make sure that your map and reduce functions get\nenough memory to operate. This is why it is best to write your map and reduce functions\nto use as little memory as possible—certainly they should not use an unbounded\namount of memory (by avoiding accumulating values in a map, for example).\nThe amount of memory given to the JVMs in which the map and reduce tasks run is\nset by the mapred.child.java.opts property. You should try to make this as large as\npossible for the amount of memory on you task nodes; the discussion in “Mem-\nory” on page 254 goes through the constraints to consider.\nOn the map side, the best performance can be obtained by avoiding multiple spills to\ndisk; one is optimal. If you can estimate the size of your map outputs, then you can set\nthe io.sort.* properties appropriately to minimize the number of spills. In particular,\nyou should increase io.sort.mb if you can. There is a MapReduce counter (“Spilled\nrecords”; see “Counters” on page 211) that counts the total number of records that\nwere spilled to disk over the course of a job, which can be useful for tuning. Note that\nthe counter includes both map and reduce side spills.\nOn the reduce side, the best performance is obtained when the intermediate data can\nreside entirely in memory. By default, this does not happen, since for the general case\nall the memory is reserved for the reduce function. But if your reduce function has light\n166 | Chapter 6: How MapReduce Worksmemory requirements, then setting mapred.inmem.merge.threshold to 0 and\nmapred.job.reduce.input.buffer.percent to 1.0 (or a lower value; see Table 6-2) may\nbring a performance boost.\nMore generally, Hadoop’s uses a buffer size of 4 KB by default, which is low, so you\nshould increase this across the cluster (by setting io.file.buffer.size, see also “Other\nHadoop Properties” on page 264).\nIn April 2008, Hadoop won the general-purpose terabyte sort benchmark (described\nin “TeraByte Sort on Apache Hadoop” on page 461), and one of the optimizations\nused was this one of keeping the intermediate data in memory on the reduce side.\nTable 6-1. Map-side tuning properties\nProperty name Type Default value Description\nio.sort.mb int 100 The size, in megabytes, of the mem-\n                  ory buffer to use while sorting map\n                 output.\nio.sort.record.percent float 0.05 The proportion of io.sort.mb re-\n                                 served for storing record boundaries\n                                of the map outputs. The remaining\n                               space is used for the map output re-\n                              cords themselves.\nio.sort.spill.percent float 0.80 The threshold usage proportion for\n                                both the map output memory buffer\n                               and the record boundaries index to\n                              start the process of spilling to disk.\nio.sort.factor int 10 The maximum number of streams to\n                     merge at once when sorting files.\n                    This property is also used in the re-\n                   duce. It’s fairly common to increase\n                    this to 100.\nmin.num.spills.for. int 3 The minimum number of spill files\ncombine needed for the combiner to run (if a\n       combiner is specified).\nmapred.compress.map. boolean false Compress map outputs.\noutput \nmapred.map.output. Class name org.apache.hadoop.io. The compression codec to use for\ncompression.codec compress.DefaultCodec map outputs.\ntask int 40 The number of worker threads per\ntracker.http.threads tasktracker for serving the map out-\n                    puts to reducers. This is a cluster-\n                   wide setting and cannot be set by\n                  individual jobs.\nShuffle and Sort | 167Table 6-2. Reduce-side tuning properties\nProperty name Type Default value Description\nmapred.reduce.parallel. int 5 The number of threads used to copy map outputs\ncopies to the reducer.\nmapred.reduce.copy.backoff int 300 The maximum amount of time, in seconds, to spend\n                                  retrieving one map output for a reducer before de-\n                                 claring it as failed. The reducer may repeatedly re-\n                                attempt a transfer within this time if it fails (using\n                               exponential backoff).\nio.sort.factor int 10 The maximum number of streams to merge at once\n                     when sorting files. This property is also used in the\n                    map.\nmapred.job.shuffle.input. float 0.70 The proportion of total heap size to be allocated to\nbuffer.percent the map outputs buffer during the copy phase of the\n              shuffle.\nmapred.job.shuffle.merge. float 0.66 The threshold usage proportion for the map outputs\npercent buffer (defined by mapred.job.shuf\n       fle.input.buffer.percent) for starting\n      the process of merging the outputs and spilling to\n     disk.\nmapred.inmem.merge.threshold int 1000 The threshold number of map outputs for starting\n                                     the process of merging the outputs and spilling to\n                                    disk. A value of 0 or less means there is no threshold,\n                                   and the spill behavior is governed solely by\n                                  mapred.job.shuffle.merge.percent.\nmapred.job.reduce.input. float 0.0 The proportion of total heap size to be used for re-\nbuffer.percent taining map outputs in memory during the reduce.\n              For the reduce phase to begin, the size of map out-\n             puts in memory must be no more than this size. By\n            default, all map outputs are merged to disk before\n           the reduce begins, to give the reduces as much\n          memory as possible. However, if your reducers re-\n         quire less memory, this value may be increased to\n        minimize the number of trips to disk.\nTask Execution\nWe saw how the MapReduce system executes tasks in the context of the overall job at\nthe beginning of the chapter in “Anatomy of a MapReduce Job Run” on page 153. In\nthis section, we’ll look at some more controls that MapReduce users have over task\nexecution.\n168 | Chapter 6: How MapReduce WorksSpeculative Execution\nThe MapReduce model is to break jobs into tasks and run the tasks in parallel to make\nthe overall job execution time smaller than it would otherwise be if the tasks ran se-\nquentially. This makes job execution time sensitive to slow-running tasks, as it takes\nonly one slow task to make the whole job take significantly longer than it would have\ndone otherwise. When a job consists of hundreds or thousands of tasks, the possibility\nof a few straggling tasks is very real.\nTasks may be slow for various reasons, including hardware degradation, or software\nmis-configuration, but the causes may be hard to detect since the tasks still complete\nsuccessfully, albeit after a longer time than expected. Hadoop doesn’t try to diagnose\nand fix slow-running tasks; instead, it tries to detect when a task is running slower than\nexpected and launches another, equivalent, task as a backup. This is termed speculative\nexecution of tasks.\nIt’s important to understand that speculative execution does not work by launching\ntwo duplicate tasks at about the same time so they can race each other. This would be\nwasteful of cluster resources. Rather, a speculative task is launched only after all the\ntasks for a job have been launched, and then only for tasks that have been running for\nsome time (at least a minute), and have failed to make as much progress, on average,\nas the other tasks from the job. When a task completes successfully, any duplicate tasks\nthat are running are killed since they are no longer needed. So if the original task com-\npletes before the speculative task then the speculative task is killed; on the other hand,\nif the speculative task finishes first, then the original is killed.\nSpeculative execution is an optimization, not a feature to make jobs run more reliably.\nIf there are bugs that sometimes cause a task to hang or slow down, then relying on\nspeculative execution to avoid these problems is unwise, and won’t work reliably, since\nthe same bugs are likely to affect the speculative task. You should fix the bug so that\nthe task doesn’t hang or slow down.\nSpeculative execution is turned on by default. It can be enabled or disabled independ-\nently for map tasks and reduce tasks, on a cluster-wide basis, or on a per-job basis. The\nrelevant properties are shown in Table 6-3.\nTable 6-3. Speculative execution properties\nProperty name Type Default value Description\nmapred.map.tasks.speculative.execution boolean true Whether extra instances of map\n                                                   tasks may be launched if a task is\n                                                  making slow progress.\nmapred.reduce.tasks.speculative. boolean true Whether extra instances of re-\nexecution duce tasks may be launched if a\n         task is making slow progress.\nTask Execution | 169Why would you ever want to turn off speculative execution? The goal of speculative\nexecution is reducing job execution time, but this comes at the cost of cluster efficiency.\nOn a busy cluster speculative execution can reduce overall throughput, since redundant\ntasks are being executed in an attempt to bring down the execution time for a single\njob. For this reason, some cluster administrators prefer to turn it off on the cluster, and\nhave users explicitly turn it on for individual jobs. This was especially relevant for older\nversions of Hadoop, when speculative execution could be overly aggressive in sched-\nuling speculative tasks.\nTask JVM Reuse\nHadoop runs tasks in their own Java Virtual Machine to isolate them from other run-\nning tasks. The overhead of starting a new JVM for each task can take around a second,\nwhich for jobs that run for a minute or so is insignificant. However, jobs that have a\nlarge number of very short-lived tasks (these are usually map tasks) or that have lengthy\ninitialization, can see performance gains when the JVM is reused for subsequent tasks.\nWith task JVM reuse enabled, tasks do not run concurrently in a single JVM. The JVM\nruns tasks sequentially. Tasktrackers can however run more than one task at a time,\nbut this is always done in separate JVMs. The properties for controlling the tasktrackers\nnumber of map task slots and reduce task slots are discussed in “Memory”\non page 254.\nThe property for controlling task JVM reuse is mapred.job.reuse.jvm.num.tasks: it\nspecifies the maximum number of tasks to run for a given job for each JVM launched,\nthe default is 1 (see Table 6-4). Tasks from different jobs are always run in separate\nJVMs. If the property is set to –1, there is no limit to the number of tasks from the same\njob that may share a JVM. The method setNumTasksToExecutePerJvm() on JobConf can\nalso be used to configure this property.\nTable 6-4. Task JVM Reuse properties\nProperty name Type Default value Description\nmapred.job.reuse.jvm.num.tasks int 1 The maximum number of tasks to run for a given\n                                    job for each JVM on a tasktracker. A value of –1\n                                     indicates no limit: the same JVM may be used for\n                                    all tasks for a job.\nTasks that are CPU-bound may also benefit from task JVM reuse by taking advantage\nof runtime optimizations applied by the HotSpot JVM. After running for a while, the\nHotSpot JVM builds up enough information to detect performance-critical sections in\nthe code, and dynamically translates the Java byte codes of these hot spots into native\nmachine code. This works well for long-running processes, but JVMs that run for sec-\nonds or a few minutes may not gain the full benefit of HotSpot. In these cases, it is\nworth enabling task JVM reuse.\n170 | Chapter 6: How MapReduce WorksAnother place where a shared JVM is useful is for sharing state between the tasks of a\njob. By storing reference data in a static field, tasks get rapid access to the shared data.\nSkipping Bad Records\nLarge datasets are messy. They often have corrupt records. They often have records\nthat are in a different format. They often have missing fields. In an ideal world, your\ncode would cope gracefully with all of these conditions. In practice, it is often expedient\nto ignore the offending records. Depending on the analysis being performed, if only a\nsmall percentage of records are affected, then skipping them may not significantly affect\nthe result. However, if a task trips up when it encounters a bad record—by throwing\na runtime exception—then the task fails. Failing tasks are retried (since the failure may\nbe due to hardware failure or some other reason outside the task’s control), but if a\ntask fails four times, then the whole job is marked as failed (see “Task Fail-\nure” on page 159). If it is the data that is causing the task to throw an exception,\nrerunning the task won’t help, since it will fail in exactly the same way each time.\nIf you are using TextInputFormat (“TextInputFormat” on page 196),\nthen you can set a maximum expected line length to safeguard against\ncorrupted files. Corruption in a file can manifest itself as a very long line,\nwhich can cause out of memory errors and then task failure. By setting\nmapred.linerecordreader.maxlength to a value in bytes that fits in mem-\nory (and is comfortably greater than the length of lines in your input\ndata), the record reader will skip the (long) corrupt lines without the\ntask failing.\nThe best way to handle corrupt records is in your mapper or reducer code. You can\ndetect the bad record and ignore it, or you can abort the job by throwing an exception.\nYou can also count the total number of bad records in the job using counters to see\nhow widespread the problem is.\nIn rare cases, though, you can’t handle the problem because there is a bug in a third-\nparty library that you can’t work around in your mapper or reducer. In these cases, you\ncan use Hadoop’s optional skipping mode for automatically skipping bad records.\nWhen skipping mode is enabled, tasks report the records being processed back to the\ntasktracker. When the task fails, the tasktracker retries the task, skipping the records\nthat caused the failure. Because of the extra network traffic and bookkeeping to main-\ntain the failed record ranges, skipping mode is turned on for a task only after it has\nfailed twice.\nThus for a task consistently failing on a bad record, the tasktracker runs the following\ntask attempts with these outcomes:\nTask Execution | 1711.\n2.\n3.\n4.\nTask fails.\nTask fails.\nSkipping mode is enabled. Task fails but failed record is stored by the tasktracker.\nSkipping mode is still enabled. Task succeeds by skipping the bad record that failed\nin the previous attempt.\nSkipping mode is off by default; you enable it independently for map and reduce tasks\nusing the SkipBadRecords class. It’s important to note that skipping mode can detect\nonly one bad record per task attempt, so this mechanism is appropriate only for de-\ntecting occasional bad records (a few per task, say). You may need to increase the\nmaximum number of task attempts (via mapred.map.max.attempts and\nmapred.reduce.max.attempts) to give skipping mode enough attempts to detect and skip\nall the bad records in an input split.\nBad records that have been detected by Hadoop are saved as sequence files in the job’s\noutput directory under the _logs/skip subdirectory. These can be inspected for diag-\nnostic purposes after the job has completed (using hadoop fs -text, for example).\nThe Task Execution Environment\nHadoop provides information to a map or reduce task about the environment in which\nit is running. For example, a map task can discover the name of the file it is processing\n(see “File information in the mapper” on page 192), and a map or reduce task can find\nout the attempt number of the task. The properties in Table 6-5 can be accessed from\nthe job’s configuration, obtained by providing an implementation of the configure()\nmethod for Mapper or Reducer, where the configuration is passed in as an argument.\nTable 6-5. Task environment properties\nProperty name Type Description Example\nmapred.job.id String The job ID. (See “Job, job_200811201130_0004\n                       Task, and Task Attempt \n                       IDs” on page 133 for a \n                         description of the \n                         format.) \nmapred.tip.id String The task ID. task_200811201130_0004_m_000003\nmapred.task.id String The task attempt ID. attempt_200811201130_0004_m_000003_0\n                      (Not the task ID.) \nmapred.task. int The ID of the task within 3\npartition the job. \nmapred.task.is.map boolean Whether this task is a true\n                           map task. \n172 | Chapter 6: How MapReduce WorksStreaming environment variables\nHadoop sets job configuration parameters as environment variables for Streaming pro-\ngrams. However, it replaces nonalphanumeric characters with underscores to make\nsure they are valid names. The following Python expression illustrates how you can\nretrieve the value of the mapred.job.id property from within a Python Streaming script:\nos.environ[""mapred_job_id""]\nYou can also set environment variables for the Streaming processes launched by Map-\nReduce by supplying the -cmdenv option to the Streaming launcher program (once for\neach variable you wish to set). For example, the following sets the MAGIC_PARAMETER\nenvironment variable:\n-cmdenv MAGIC_PARAMETER=abracadabra\nTask side-effect files\nThe usual way of writing output from map and reduce tasks is by using the OutputCol\nlector to collect key-value pairs. Some applications need more flexibility than a single\nkey-value pair model, so these applications write output files directly from the map or\nreduce task to a distributed filesystem, like HDFS. (There are other ways to produce\nmultiple outputs, too, as described in “Multiple Outputs” on page 203.)\nCare needs to be taken to ensure that multiple instances of the same task don’t try to\nwrite to the same file. There are two problems to avoid: if a task failed and was retried,\nthen the old partial output would still be present when the second task ran, and it would\nhave to delete the old file first. Second, with speculative execution enabled, two in-\nstances of the same task could try to write to the same file simultaneously.\nHadoop solves this problem for the regular outputs from a task by writing outputs to\na temporary directory that is specific to that task attempt. The directory is ${mapred.out\nput.dir}/_temporary/${mapred.task.id}. On successful completion of the task, the\ncontents of the directory are copied to the job’s output directory (${mapred.out\nput.dir}). Thus if the task fails and is retried, the first attempt’s partial output will just\nbe cleaned up. A task and another speculative instance of the same task will get separate\nworking directories, and only the first to finish will have the content of its working\ndirectory promoted to the output directory—the other will be discarded.\nThe way that a task’s output is committed on completion is implemen-\nted by an OutputCommitter, which is associated with the OutputFormat.\nThe OutputCommitter for FileOutputFormat is a FileOutputCommitter,\nwhich implements the commit protocol described earlier. The getOut\nputCommitter() method on OutputFormat may be overridden to return a\ncustom OutputCommitter, in case you want to implement the commit\nprocess in a different way.\nTask Execution | 173Hadoop provides a mechanism for application writers to use this feature, too. A task\nmay find its working directory by retrieving the value of the mapred.work.output.dir\nproperty from its configuration file. Alternatively, a MapReduce program using the Java\nAPI may call the getWorkOutputPath() static method on FileOutputFormat to get the\nPath object representing the working directory. The framework creates the working\ndirectory before executing the task, so you don’t need to create it.\nTo take a simple example, imagine a program for converting image files from one format\nto another. One way to do this is to have a map-only job, where each map is given a\nset of images to convert (perhaps using NLineInputFormat; see “NLineInputFor-\nmat” on page 198). If a map task writes the converted images into its working directory,\nthen they will be promoted to the output directory when the task successfully finishes.\n174 | Chapter 6: How MapReduce WorksCHAPTER 7\nMapReduce Types and Formats\nMapReduce has a simple model of data processing: inputs and outputs for the map and\nreduce functions are key-value pairs. This chapter looks at the MapReduce model in\ndetail, and, in particular, how data in various formats, from simple text to structured\nbinary objects, can be used with this model.\nMapReduce Types\nThe map and reduce functions in Hadoop MapReduce have the following general form:\nmap: (K1, V1) → list(K2, V2)\nreduce: (K2, list(V2)) → list(K3, V3)\nIn general, the map input key and value types (K1 and V1) are different from the map\noutput types ( K2 and V2). However, the reduce input must have the same types as the\nmap output, although the reduce output types may be different again (K3 and V3). The\nJava interfaces mirror this form:\npublic interface Mapper<K1, V1, K2, V2> extends JobConfigurable, Closeable {\n}\nvoid map(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter)\nthrows IOException;\npublic interface Reducer<K2, V2, K3, V3> extends JobConfigurable, Closeable {\n}\nvoid reduce(K2 key, Iterator<V2> values,\nOutputCollector<K3, V3> output, Reporter reporter) throws IOException;\nRecall that the OutputCollector is purely for emitting key-value pairs (and is hence\nparameterized with their types), while the Reporter is for updating counters and status.\n(In the new MapReduce API in release 0.20.0 and later, these two functions are com-\nbined in a single context object.)\nIf a combine function is used then it is the same form as the reduce function (and is an\nimplementation of Reducer), except its output types are the intermediate key and value\ntypes (K2 and V2), so they can feed the reduce function:\n175map: (K1, V1) → list(K2, V2)\ncombine: (K2, list(V2)) → list(K2, V2)\nreduce: (K2, list(V2)) → list(K3, V3)\nOften the combine and reduce functions are the same, in which case, K3 is the same as\nK2, and V3 is the same as V2.\nThe partition function operates on the intermediate key and value types (K2 and V2),\nand returns the partition index. In practice, the partition is determined solely by the\nkey (the value is ignored):\npartition: (K2, V2) → integer\nOr in Java:\npublic interface Partitioner<K2, V2> extends JobConfigurable {\n}\nint getPartition(K2 key, V2 value, int numPartitions);\nSo much for the theory, how does this help configure MapReduce jobs? Table 7-1\nsummarizes the configuration options. It is divided into the properties that determine\nthe types, and those that have to be compatible with the configured types.\n176 | Chapter 7: MapReduce Types and FormatsInput types are set by the input format. So, for instance, a TextInputFormat generates\nkeys of type LongWritable and values of type Text. The other types are set explicitly by\ncalling the methods on the JobConf. If not set explicitly, the intermediate types default\nto the (final) output types, which default to LongWritable and Text. So if K2 and K3 are\nthe same, you don’t need to call setMapOutputKeyClass(), since it falls back to the type\nMapReduce Types | 177\nsetOutputKeyComparatorClass() setOutputValueGroupingComparator() setReducerClass() setOutputFormat()\nOutput typesset by calling setOutputKeyClass(). Similarly, if V2 and V3 are the same, you only need\nto use setOutputValueClass().\nIt may seem strange that these methods for setting the intermediate and final output\ntypes exist at all. After all, why can’t the types be determined from a combination of\nthe mapper and the reducer? The answer is that it’s to do with a limitation in Java\ngenerics: type erasure means that the type information isn’t always present at runtime,\nso Hadoop has to be given it explicitly. This also means that it’s possible to configure\na MapReduce job with incompatible types, because the configuration isn’t checked at\ncompile time. The settings that have to be compatible with the MapReduce types are\nlisted in the lower part of Table 7-1. Type conflicts are detected at runtime during job\nexecution, and for this reason, it is wise to run a test job using a small amount of data\nto flush out and fix any type incompatibilities.\nThe Default MapReduce Job\nWhat happens when you run MapReduce without setting a mapper or a reducer? Let’s\ntry it by running this minimal MapReduce program:\npublic class MinimalMapReduce extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.printf(""Usage: %s [generic options] <input> <output>\\n"",\ngetClass().getSimpleName());\nToolRunner.printGenericCommandUsage(System.err);\nreturn -1;\n}\n}\n}\nJobConf conf = new JobConf(getConf(), getClass());\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MinimalMapReduce(), args);\nSystem.exit(exitCode);\n}\nThe only configuration that we set is an input path and an output path. We run it over\na subset of our weather data with the following:\n% hadoop MinimalMapReduce ""input/ncdc/all/190{1,2}.gz"" output\nWe do get some output: one file named part-00000 in the output directory. Here’s what\nthe first few lines look like (truncated to fit the page):\n178 | Chapter 7: MapReduce Types and Formats0→0029029070999991901010106004+64333+023450FM-12+000599999V0202701N01591...\n0→0035029070999991902010106004+64333+023450FM-12+000599999V0201401N01181...\n135→0029029070999991901010113004+64333+023450FM-12+000599999V0202901N00821...\n141→0035029070999991902010113004+64333+023450FM-12+000599999V0201401N01181...\n270→0029029070999991901010120004+64333+023450FM-12+000599999V0209991C00001...\n282→0035029070999991902010120004+64333+023450FM-12+000599999V0201401N01391...\nEach line is an integer followed by a tab character, followed by the original weather\ndata record. Admittedly, it’s not a very useful program, but understanding how it pro-\nduces its output does provide some insight into the defaults that Hadoop uses when\nrunning MapReduce jobs. Example 7-1 shows a program that has exactly the same\neffect as MinimalMapReduce, but explicitly sets the job settings to their defaults.\nExample 7-1. A minimal MapReduce driver, with the defaults explicitly set\npublic class MinimalMapReduceWithDefaults extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(TextInputFormat.class);\nconf.setNumMapTasks(1);\nconf.setMapperClass(IdentityMapper.class);\nconf.setMapRunnerClass(MapRunner.class);\nconf.setMapOutputKeyClass(LongWritable.class);\nconf.setMapOutputValueClass(Text.class);\nconf.setPartitionerClass(HashPartitioner.class);\nconf.setNumReduceTasks(1);\nconf.setReducerClass(IdentityReducer.class);\nconf.setOutputKeyClass(LongWritable.class);\nconf.setOutputValueClass(Text.class);\nconf.setOutputFormat(TextOutputFormat.class);\nJobClient.runJob(conf);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MinimalMapReduceWithDefaults(), args);\nSystem.exit(exitCode);\n}\nWe’ve simplified the first few lines of the run() method, by extracting the logic for\nprinting usage and setting the input and output paths into a helper method. Almost all\nMapReduce Types | 179MapReduce drivers take these two arguments (input and output), so reducing the\nboilerplate code here is a good thing. Here are the relevant methods in the Job\nBuilder class for reference:\npublic static JobConf parseInputAndOutput(Tool tool, Configuration conf,\nString[] args) {\n}\nif (args.length != 2) {\nprintUsage(tool, ""<input> <output>"");\nreturn null;\n}\nJobConf jobConf = new JobConf(conf, tool.getClass());\nFileInputFormat.addInputPath(jobConf, new Path(args[0]));\nFileOutputFormat.setOutputPath(jobConf, new Path(args[1]));\nreturn jobConf;\npublic static void printUsage(Tool tool, String extraArgsUsage) {\nSystem.err.printf(""Usage: %s [genericOptions] %s\\n\\n"",\ntool.getClass().getSimpleName(), extraArgsUsage);\nGenericOptionsParser.printGenericCommandUsage(System.err);\n}\nGoing back to MinimalMapReduceWithDefaults in Example 7-1, although there are many\nother default job settings, the ones highlighted are those most central to running a job.\nLet’s go through them in turn.\nThe default input format is TextInputFormat, which produces keys of type LongWrita\nble (the offset of the beginning of the line in the file) and values of type Text (the line\nof text). This explains where the integers in the final output come from: they are the\nline offsets.\nDespite appearances, the setNumMapTasks() call does not necessarily set the number of\nmap tasks to one, in fact. It is a hint, and the actual number of map tasks depends on\nthe size of the input, and the file’s block size (if the file is in HDFS). This is discussed\nfurther in “FileInputFormat input splits” on page 188.\nThe default mapper is IdentityMapper, which writes the input key and value unchanged\nto the output:\npublic class IdentityMapper<K, V>\nextends MapReduceBase implements Mapper<K, V, K, V> {\n}\npublic void map(K key, V val,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\noutput.collect(key, val);\n}\nIdentityMapper is a generic type, which allows it to work with any key or value types,\nwith the restriction that the map input and output keys are of the same type, and the\n180 | Chapter 7: MapReduce Types and Formatsmap input and output values are of the same type. In this case, the map output key is\nLongWritable and the map output value is Text.\nMap tasks are run by MapRunner, the default implementation of MapRunnable that calls\nthe Mapper’s map() method sequentially with each record.\nThe default partitioner is HashPartitioner, which hashes a record’s key to determine\nwhich partition the record belongs in. Each partition is processed by a reduce task, so\nthe number of partitions is equal to the number of reduce tasks for the job:\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\npublic void configure(JobConf job) {}\npublic int getPartition(K2 key, V2 value,\nint numPartitions) {\nreturn (key.hashCode() & Integer.MAX_VALUE) % numPartitions;\n}\n}\nThe key’s hash code is turned into a nonnegative integer by bitwise ANDing it with the\nlargest integer value. It is then reduced modulo the number of partitions to find the\nindex of the partition that the record belongs in.\nBy default, there is a single reducer, and therefore a single partition, so the action of\nthe partitioner is irrelevant in this case since everything goes into one partition. How-\never, it is important to understand the behavior of HashPartitioner when you have\nmore than one reduce task. Assuming the key’s hash function is a good one, the records\nwill be evenly allocated across reduce tasks, with all records sharing the same key being\nprocessed by the same reduce task.\nChoosing the Number of Reducers\nThe single reducer default is something of a gotcha for new users to Hadoop.Almost\nall real-world jobs should set this to a larger number; otherwise, the job will be very\nslow since all the intermediate data flows through a single reduce task. (Note that when\nrunning under the local job runner, only zero or one reducers are supported.)\nThe optimal number of reducers is related to the total number of available reducer slots\nin your cluster. The total number of slots is given by the product of the number of nodes\nin the cluster and the value of the mapred.tasktracker.reduce.tasks.maximum property\n(see “Environment Settings” on page 254).\nOne common setting is to have slightly fewer reducers than total slots, which gives one\nwave of reduce tasks (and tolerates a few failures, without extending job execution\ntime). If your reduce tasks are very big, then it makes sense to have a larger number of\nreducers (resulting in two waves, for example) so that the tasks are more fine-grained,\nand failure doesn’t affect job execution time significantly.\nMapReduce Types | 181The default reducer is IdentityReducer, again a generic type, which simply writes all\nits input to its output:\npublic class IdentityReducer<K, V>\nextends MapReduceBase implements Reducer<K, V, K, V> {\n}\npublic void reduce(K key, Iterator<V> values,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\nwhile (values.hasNext()) {\noutput.collect(key, values.next());\n}\n}\nFor this job the output key is LongWritable, and the output value is Text. In fact, all the\nkeys for this MapReduce program are LongWritable, and all the values are Text, since\nthese are the input keys and values, and the map and reduce functions are both identity\nfunctions which by definition preserve type. Most MapReduce programs, however,\ndon’t use the same key or value types throughout, so you need to configure the job to\ndeclare the types you are using, as described in the previous section.\nRecords are sorted by the MapReduce system before being presented to the reducer.\nIn this case the keys are sorted numerically, which has the effect of interleaving the lines\nfrom the input files into one combined output file.\nThe default output format is TextOutputFormat, which writes out records, one per line,\nby converting keys and values to strings and separating them with a tab character. This\nis why the output is tab-separated: it is a feature of TextOutputFormat.\nThe default Streaming job\nIn Streaming, the default job is similar, but not identical, to the Java equivalent. The\nminimal form is:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-mapper /bin/cat\nNotice that you have to supply a mapper: the default identity mapper will not work.\nThe reason has to do with the default input format, TextInputFormat, which generates\nLongWritable keys and Text values. However, Streaming output keys and values (in-\ncluding the map keys and values) are always both of type Text.* The identity mapper\ncannot change LongWritable keys to Text keys, so it fails.\nWhen we specify a non-Java mapper, and the input format is TextInputFormat, Stream-\ning does something special. It doesn’t pass the key to the mapper process, it just passes\n* Except when used in binary mode, from version 0.21.0 onward, via the -io rawbytes or -io typedbytes\noptions. Text mode (-io text) is the default.\n182 | Chapter 7: MapReduce Types and Formatsthe value. This is actually very useful, since the key is just the line offset in the file, and\nthe value is the line, which is all most applications are interested in. The overall effect\nof this job is to perform a sort of the input.\nWith more of the defaults spelled out, the command looks like this:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-inputformat org.apache.hadoop.mapred.TextInputFormat \\\n-mapper /bin/cat \\\n-partitioner org.apache.hadoop.mapred.lib.HashPartitioner \\\n-numReduceTasks 1 \\\n-reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n-outputformat org.apache.hadoop.mapred.TextOutputFormat\nThe mapper and reducer arguments take a command or a Java class. A combiner may\noptionally be specified, using the -combiner argument.\nKeys and values in Streaming\nA Streaming application can control the separator that is used when a key-value pair is\nturned into a series of bytes and sent to the map or reduce process over standard input.\nThe default is a tab character, but it is useful to be able to change it in the case that the\nkeys or values themselves contain tab characters.\nSimilarly, when the map or reduce writes out key-value pairs, they may be separated\nby a configurable separator. Furthermore, the key from the output can be composed\nof more than the first field: it can be made up of the first n fields (defined by\nstream.num.map.output.key.fields or stream.num.reduce.output.key.fields), with\nthe value being the remaining fields. For example, if the output from a Streaming proc-\ness was a,b,c (and the separator is a comma), and n is two, then the key would be\nparsed as a,b and the value as c.\nSeparators may be configured independently for maps and reduces. The properties are\nlisted in Table 7-2 and shown in a diagram of the data flow path in Figure 7-1.\nThese settings do not have any bearing on the input and output formats. For example,\nif stream.reduce.output.field.separator were set to be a colon, say, and the reduce\nstream process wrote the line a:b to standard out, then the Streaming reducer would\nknow to extract the key as a and the value as b. With the standard TextOutputFormat,\nthis record would be written to the output file with a tab separating a and b. You can\nchange the separator that TextOutputFormat uses by setting mapred.textoutputfor\nmat.separator.\nMapReduce Types | 183Figure 7-1. Where separators are used in a Streaming MapReduce job\nTable 7-2. Streaming separator properties\nProperty name Type Default value Description\nstream.map.input.field. String \\t The separator to use when passing the input key\nseparator and value strings to the stream map process as a\n         stream of bytes.\nstream.map.output.field. String \\t The separator to use when splitting the output from\nseparator the stream map process into key and value strings\n         for the map output.\nstream.num.map. int 1 The number of fields separated by\noutput.key.fields \nstream.reduce.input.field. String \\t The separator to use when passing the input key\nseparator and value strings to the stream reduce process as a\n         stream of bytes.\nstream.reduce.output.field. String \\t The separator to use when splitting the output from\nseparator the stream reduce process into key and value strings\n         for the final reduce output.\nstream.num.reduce. int 1 The number of fields separated by\noutput.key.fields \nstream.map.output.field.separator\nto treat as the map output key.\nstream.reduce.output.field.separa\ntor to treat as the reduce output key.\nInput Formats\nHadoop can process many different types of data formats, from flat text files to data-\nbases. In this section, we explore the different formats available.\n184 | Chapter 7: MapReduce Types and FormatsInput Splits and Records\nAs you saw in Chapter 2, an input split is a chunk of the input that is processed by a\nsingle map. Each map processes a single split. Each split is divided into records, and\nthe map processes each record—a key-value pair—in turn. Splits and records are log-\nical: there is nothing that requires them to be tied to files, for example, although in their\nmost common incarnations, they are. In a database context, a split might correspond\nto a range of rows from a table, and a record to a row in that range (this is precisely\nwhat DBInputFormat does, an input format for reading data from a relational database).\nInput splits are represented by the Java interface, InputSplit (which, like all of the\nclasses mentioned in this section, is in the org.apache.hadoop.mapred package†):\npublic interface InputSplit extends Writable {\nlong getLength() throws IOException;\nString[] getLocations() throws IOException;\n}\nAn InputSplit has a length in bytes, and a set of storage locations, which are just host-\nname strings. Notice that a split doesn’t contain the input data; it is just a reference to\nthe data. The storage locations are used by the MapReduce system to place map tasks\nas close to the split’s data as possible, and the size is used to order the splits so that the\nlargest get processed first, in an attempt to minimize the job runtime (this is an instance\nof a greedy approximation algorithm).\nAs a MapReduce application writer, you don’t need to deal with InputSplits directly,\nas they are created by an InputFormat. An InputFormat is responsible for creating the\ninput splits, and dividing them into records. Before we see some concrete examples of\nInputFormat, let’s briefly examine how it is used in MapReduce. Here’s the interface:\npublic interface InputFormat<K, V> {\nInputSplit[] getSplits(JobConf job, int numSplits) throws IOException;\n}\nRecordReader<K, V> getRecordReader(InputSplit split,\nJobConf job,\nReporter reporter) throws IOException;\nThe JobClient calls the getSplits() method, passing the desired number of map tasks\nas the numSplits argument. This number is treated as a hint, as InputFormat imple-\nmentations are free to return a different number of splits to the number specified in\nnumSplits. Having calculated the splits, the client sends them to the jobtracker, which\nuses their storage locations to schedule map tasks to process them on the tasktrackers.\n† But see the new MapReduce classes in org.apache.hadoop.mapreduce, described in “The new Java MapReduce\nAPI” on page 25.\nInput Formats | 185On a tasktracker, the map task passes the split to the getRecordReader() method on\nInputFormat to obtain a RecordReader for that split. A RecordReader is little more than\nan iterator over records, and the map task uses one to generate record key-value pairs,\nwhich it passes to the map function. A code snippet (based on the code in MapRunner)\nillustrates the idea:\nK key = reader.createKey();\nV value = reader.createValue();\nwhile (reader.next(key, value)) {\nmapper.map(key, value, output, reporter);\n}\nHere the RecordReader’s next() method is called repeatedly to populate the key and\nvalue objects for the mapper. When the reader gets to the end of the stream, the\nnext() method returns false, and the map task completes.\nThis code snippet makes it clear that the same key and value objects are\nused on each invocation of the map() method—only their contents are\nchanged (by the reader’s next() method). This can be a surprise to users,\nwho might expect keys and values to be immutable. This causes prob-\nlems when a reference to a key or value object is retained outside the\nmap() method, as its value can change without warning. If you need to\ndo this, make a copy of the object you want to hold on to. For example,\nfor a Text object, you can use its copy constructor: new Text(value).\nThe situation is similar with reducers. In this case the value objects in\nthe reducer’s iterator are reused, so you need to copy any that you need\nto retain between calls to the iterator (see Example 8-14).\nFinally, note that MapRunner is only one way of running mappers. MultithreadedMapRun\nner is another implementation of the MapRunnable interface that runs mappers concur-\nrently in a configurable number of threads (set by mapred.map.multithreadedrun\nner.threads). For most data processing tasks, it confers no advantage over MapRunner.\nHowever, for mappers that spend a long time processing each record, because they\ncontact external servers, for example, it allows multiple mappers to run in one JVM\nwith little contention. See “Fetcher: A multi-threaded MapRunner in ac-\ntion” on page 435 for an example of an application that uses MultithreadedMapRunner.\nFileInputFormat\nFileInputFormat is the base class for all implementations of InputFormat that use files\nas their data source (see Figure 7-2). It provides two things: a place to define which files\nare included as the input to a job, and an implementation for generating splits for the\ninput files. The job of dividing splits into records is performed by subclasses.\n186 | Chapter 7: MapReduce Types and FormatsFigure 7-2. InputFormat class hierarchy\nFileInputFormat input paths\nThe input to a job is specified as a collection of paths, which offers great flexibility in\nconstraining the input to a job. FileInputFormat offers four static convenience methods\nfor setting a JobConf’s input paths:\npublic\npublic\npublic\npublic\nstatic\nstatic\nstatic\nstatic\nvoid\nvoid\nvoid\nvoid\naddInputPath(JobConf conf, Path path)\naddInputPaths(JobConf conf, String commaSeparatedPaths)\nsetInputPaths(JobConf conf, Path... inputPaths)\nsetInputPaths(JobConf conf, String commaSeparatedPaths)\nThe addInputPath() and addInputPaths() methods add a path or paths to the list of\ninputs. You can call these methods repeatedly to build the list of paths. The setInput\nPaths() methods set the entire list of paths in one go (replacing any paths set on the\nJobConf in previous calls).\nA path may represent a file, a directory, or, by using a glob, a collection of files and\ndirectories. A path representing a directory includes all the files in the directory as input\nto the job. See “File patterns” on page 60 for more on using globs.\nInput Formats | 187The contents of a directory specified as an input path are not processed\nrecursively. In fact, the directory should only contain files: if the direc-\ntory contains a subdirectory, it will be interpreted as a file, which will\ncause an error. The way to handle this case is to use a file glob or a filter\nto select only the files in the directory.\nThe add and set methods allow files to be specified by inclusion only. To exclude certain\nfiles from the input, you can set a filter using the setInputPathFilter() method on\nFileInputFormat:\npublic static void setInputPathFilter(JobConf conf,\nClass<? extends PathFilter> filter)\nFilters are discussed in more detail in “PathFilter” on page 61.\nEven if you don’t set a filter, FileInputFormat uses a default filter that excludes hidden\nfiles (those whose names begin with a dot or an underscore). If you set a filter by calling\nsetInputPathFilter(), it acts in addition to the default filter. In other words, only non-\nhidden files that are accepted by your filter get through.\nPaths and filters can be set through configuration properties too (Table 7-3), which can\nbe handy for Streaming and Pipes. Setting paths is done with the -input option for both\nStreaming and Pipes interfaces, so setting paths directly is not usually needed.\nTable 7-3. Input path and filter properties\nProperty name Type Default value Description\nmapred.input.dir comma-separated none The input files for a job. Paths that contain com-\n                 paths mas should have those commas escaped by a\n                      backslash character. For example, the glob\n                     {a,b} would be escaped as {a\\,b}.\nmapred.input.path PathFilter none The filter to apply to the input files for a job.\nFilter.class \nclassname\nFileInputFormat input splits\nGiven a set of files, how does FileInputFormat turn them into splits? FileInputFormat\nsplits only large files. Here “large” means larger than an HDFS block. The split size is\nnormally the size of an HDFS block, which is appropriate for most applications; how-\never, it is possible to control this value by setting various Hadoop properties, as shown\nin Table 7-4.\n188 | Chapter 7: MapReduce Types and FormatsTable 7-4. Properties for controlling split size\nProperty name Type Default value Description\nmapred.min.split.size int 1 The smallest valid size in\n                           bytes for a file split.\nmapred.max.split.sizea long Long.MAX_VALUE, that is The largest valid size in\n                                                   bytes for a file split.\ndfs.block.size long 64 MB, that is 67108864 The size of a block in HDFS\n                                           in bytes.\n9223372036854775807\na This property is not present in the old MapReduce API (with the exception of CombineFileInputFormat).\nInstead, it is calculated indirectly as the size of the total input for the job, divided by the guide number of\nmap tasks specified by mapred.map.tasks (or the setNumMapTasks() method on JobConf). Because\nmapred.map.tasks defaults to 1, this makes the maximum split size the size of the input.\nThe minimum split size is usually 1 byte, although some formats have a lower bound\non the split size. (For example, sequence files insert sync entries every so often in the\nstream, so the minimum split size has to be large enough to ensure that every split has\na sync point to allow the reader to resynchronize with a record boundary.)\nApplications may impose a minimum split size: by setting this to a value larger than\nthe block size, they can force splits to be larger than a block. There is no good reason\nfor doing this when using HDFS, since doing so will increase the number of blocks that\nare not local to a map task.\nThe maximum split size defaults to the maximum value that can be represented by a\nJava long type. It has an effect only when it is less than the block size, forcing splits to\nbe smaller than a block.\nThe split size is calculated by the formula (see the computeSplitSize() method in\nFileInputFormat):\nmax(minimumSize, min(maximumSize, blockSize))\nby default:\nminimumSize < blockSize < maximumSize\nso the split size is blockSize. Various settings for these parameters and how they affect\nthe final split size are illustrated in Table 7-5.\nInput Formats | 189Table 7-5. Examples of how to control the split size\nMinimum split size Maximum split size Block size Split size Comment\n1 (default) Long.MAX_VALUE 64 MB (default) 64 MB By default split size is the same as the\n                                                default block size.\nLong.MAX_VALUE 128 MB 128 MB The most natural way to increase the\n                            split size is to have larger blocks in\n                           HDFS, by setting dfs.block.size,\n                          or on a per-file basis at file construction\n                         time.\nLong.MAX_VALUE 64 MB (default) 128 MB Making the minimum split size greater\n                                     than the block size increases the split\n                                    size, but at the cost of locality.\n32 MB 64 MB (default) 32 MB Making the maximum split size less\n                           than the block size decreases the split\n                          size.\n(default)\n1 (default)\n(default)\n128 MB\n(default)\n1 (default)\nSmall files and CombineFileInputFormat\nHadoop works better with a small number of large files than a large number of small\nfiles. One reason for this is that FileInputFormat generates splits in such a way that each\nsplit is all or part of a single file. If the file is very small (“small” means significantly\nsmaller than an HDFS block) and there are a lot of them, then each map task will process\nvery little input, and there will be a lot of them (one per file), each of which imposes\nextra bookkeeping overhead. Compare a 1 GB file broken into sixteen 64 MB blocks,\nand 10,000 or so 100 KB files. The 10,000 files use one map each, and the job time can\nbe tens or hundreds of times slower than the equivalent one with a single input file and\n16 map tasks.\nThe situation is alleviated somewhat by CombineFileInputFormat, which was designed\nto work well with small files. Where FileInputFormat creates a split per file, CombineFi\nleInputFormat packs many files into each split so that each mapper has more to process.\nCrucially, CombineFileInputFormat takes node and rack locality into account when de-\nciding which blocks to place in the same split, so it does not compromise the speed at\nwhich it can process the input in a typical MapReduce job.\nOf course, if possible, it is still a good idea to avoid the many small files case since\nMapReduce works best when it can operate at the transfer rate of the disks in the cluster,\nand processing many small files increases the number of seeks that are needed to run\na job. Also, storing large numbers of small files in HDFS is wasteful of the namenode’s\nmemory. One technique for avoiding the many small files case is to merge small files\ninto larger files by using a SequenceFile: the keys can act as filenames (or a constant\nsuch as NullWritable, if not needed) and the values as file contents. See Example 7-4.\nBut if you already have a large number of small files in HDFS, then CombineFileInput\nFormat is worth trying.\n190 | Chapter 7: MapReduce Types and FormatsCombineFileInputFormat isn’t just good for small files—it can bring ben-\nefits when processing large files too. Essentially, CombineFileInputFor\nmat decouples the amount of data that a mapper consumes from the\nblock size of the files in HDFS.\nIf your mappers can process each block in a matter of seconds, then you\ncould use CombineFileInputFormat with the maximum split size set to a\nsmall multiple of the number of blocks (by setting the\nmapred.max.split.size property in bytes) so that each mapper processes\nmore than one block. In return, the overall processing time falls, since\nproportionally fewer mappers run, which reduces the overhead in task\nbookkeeping and startup time associated with a large number of short-\nlived mappers.\nSince CombineFileInputFormat is an abstract class without any concrete classes (unlike\nFileInputFormat), you need to do a bit more work to use it. (Hopefully, common im-\nplementations will be added to the library over time.) For example, to have the\nCombineFileInputFormat equivalent of TextInputFormat, you would create a concrete\nsubclass of CombineFileInputFormat and implement the getRecordReader() method.\nPreventing splitting\nSome applications don’t want files to be split so that a single mapper can process each\ninput file in its entirety. For example, a simple way to check if all the records in a file\nare sorted is to go through the records in order, checking whether each record is not\nless than the preceding one. Implemented as a map task, this algorithm will work only\nif one map processes the whole file.‡\nThere are a couple of ways to ensure that an existing file is not split. The first (quick\nand dirty) way is to increase the minimum split size to be larger than the largest file in\nyour system. Setting it to its maximum value, Long.MAX_VALUE, has this effect. The sec-\nond is to subclass the concrete subclass of FileInputFormat that you want to use, to\noverride the isSplitable() method§ to return false. For example, here’s a nonsplit-\ntable TextInputFormat:\nimport org.apache.hadoop.fs.*;\nimport org.apache.hadoop.mapred.TextInputFormat;\npublic class NonSplittableTextInputFormat extends TextInputFormat {\n@Override\nprotected boolean isSplitable(FileSystem fs, Path file) {\nreturn false;\n}\n}\n‡ This is how the mapper in SortValidator.RecordStatsChecker is implemented.\n§ In the method name isSplitable(), “splitable” has a single “t.” It is usually spelled “splittable,” which is the\nspelling I have used in this book.\nInput Formats | 191File information in the mapper\nA mapper processing a file input split can find information about the split by reading\nsome special properties from its job configuration object, which may be obtained by\nimplementing configure() in your Mapper implementation to get access to the\nJobConf object. Table 7-6 lists the properties available. These are in addition to the ones\navailable to all mappers and reducers, listed in “The Task Execution Environ-\nment” on page 172.\nTable 7-6. File split properties\nProperty name Type Description\nmap.input.file String The path of the input file being processed\nmap.input.start long The byte offset of the start of the split\nmap.input.length long The length of the split in bytes\nIn the next section, you shall see how to use this when we need to access the split’s\nfilename.\nProcessing a whole file as a record\nA related requirement that sometimes crops up is for mappers to have access to the full\ncontents of a file. Not splitting the file gets you part of the way there, but you also need\nto have a RecordReader that delivers the file contents as the value of the record. The\nlisting for WholeFileInputFormat in Example 7-2 shows a way of doing this.\nExample 7-2. An InputFormat for reading a whole file as a record\npublic class WholeFileInputFormat\nextends FileInputFormat<NullWritable, BytesWritable> {\n@Override\nprotected boolean isSplitable(FileSystem fs, Path filename) {\nreturn false;\n}\n@Override\npublic RecordReader<NullWritable, BytesWritable> getRecordReader(\nInputSplit split, JobConf job, Reporter reporter) throws IOException {\n}\n}\nreturn new WholeFileRecordReader((FileSplit) split, job);\nWholeFileInputFormat defines a format where the keys are not used, represented by\nNullWritable, and the values are the file contents, represented by BytesWritable in-\nstances. It defines two methods. First, the format is careful to specify that input files\nshould never be split, by overriding isSplitable() to return false. Second, we\n192 | Chapter 7: MapReduce Types and Formatsimplement getRecordReader() to return a custom implementation of RecordReader,\nwhich appears in Example 7-3.\nExample 7-3. The RecordReader used by WholeFileInputFormat for reading a whole file as a record\nclass WholeFileRecordReader implements RecordReader<NullWritable, BytesWritable> {\nprivate FileSplit fileSplit;\nprivate Configuration conf;\nprivate boolean processed = false;\npublic WholeFileRecordReader(FileSplit fileSplit, Configuration conf)\nthrows IOException {\nthis.fileSplit = fileSplit;\nthis.conf = conf;\n}\n@Override\npublic NullWritable createKey() {\nreturn NullWritable.get();\n}\n@Override\npublic BytesWritable createValue() {\nreturn new BytesWritable();\n}\n@Override\npublic long getPos() throws IOException {\nreturn processed ? fileSplit.getLength() : 0;\n}\n@Override\npublic float getProgress() throws IOException {\nreturn processed ? 1.0f : 0.0f;\n}\n@Override\npublic boolean next(NullWritable key, BytesWritable value) throws IOException {\nif (!processed) {\nbyte[] contents = new byte[(int) fileSplit.getLength()];\nPath file = fileSplit.getPath();\nFileSystem fs = file.getFileSystem(conf);\nFSDataInputStream in = null;\ntry {\nin = fs.open(file);\nIOUtils.readFully(in, contents, 0, contents.length);\nvalue.set(contents, 0, contents.length);\n} finally {\nIOUtils.closeStream(in);\n}\nprocessed = true;\nreturn true;\n}\nreturn false;\nInput Formats | 193}\n}\n@Override\npublic void close() throws IOException {\n// do nothing\n}\nWholeFileRecordReader is responsible for taking a FileSplit and converting it into a\nsingle record, with a null key and a value containing the bytes of the file. Because there\nis only a single record, WholeFileRecordReader has either processed it or not, so it main-\ntains a boolean called processed. If, when the next() method is called, the file has not\nbeen processed, then we open the file, create a byte array whose length is the length of\nthe file, and use the Hadoop IOUtils class to slurp the file into the byte array. Then we\nset the array on the BytesWritable instance that was passed into the next() method,\nand return true to signal that a record has been read.\nThe other methods are straightforward bookkeeping methods for creating the correct\nkey and value types, getting the position and progress of the reader, and a close()\nmethod, which is invoked by the MapReduce framework when the reader is done with.\nTo demonstrate how WholeFileInputFormat can be used, consider a MapReduce job for\npackaging small files into sequence files, where the key is the original filename, and the\nvalue is the content of the file. The listing is in Example 7-4.\nExample 7-4. A MapReduce program for packaging a collection of small files as a single SequenceFile\npublic class SmallFilesToSequenceFileConverter extends Configured\nimplements Tool {\nstatic class SequenceFileMapper extends MapReduceBase\nimplements Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\nprivate JobConf conf;\n@Override\npublic void configure(JobConf conf) {\nthis.conf = conf;\n}\n@Override\npublic void map(NullWritable key, BytesWritable value,\nOutputCollector<Text, BytesWritable> output, Reporter reporter)\nthrows IOException {\n}\nString filename = conf.get(""map.input.file"");\noutput.collect(new Text(filename), value);\n}\n@Override\npublic int run(String[] args) throws IOException {\n194 | Chapter 7: MapReduce Types and FormatsJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(WholeFileInputFormat.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(BytesWritable.class);\nconf.setMapperClass(SequenceFileMapper.class);\nconf.setReducerClass(IdentityReducer.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(), args);\nSystem.exit(exitCode);\n}\nSince the input format is a WholeFileInputFormat, the mapper has to find only the\nfilename for the input file split. It does this by retrieving the map.input.file property\nfrom the JobConf, which is set to the split’s filename by the MapReduce framework,\nbut only for splits that are FileSplit instances (this includes most subclasses of FileIn\nputFormat). The reducer is the IdentityReducer, and the output format is a SequenceFi\nleOutputFormat.\nHere’s a run on a few small files. We’ve chosen to use two reducers, so we get two\noutput sequence files:\n% hadoop jar job.jar SmallFilesToSequenceFileConverter \\\n-conf conf/hadoop-localhost.xml -D mapred.reduce.tasks=2 input/smallfiles output\nTwo part files are created, each of which is a sequence file, which we can inspect with\nthe -text option to the filesystem shell:\n% hadoop fs -conf conf/hadoop-localhost.xml -text output/part-00000\nhdfs://localhost/user/tom/input/smallfiles/a\n61 61 61 61 61 61 61\nhdfs://localhost/user/tom/input/smallfiles/c\n63 63 63 63 63 63 63\nhdfs://localhost/user/tom/input/smallfiles/e\n% hadoop fs -conf conf/hadoop-localhost.xml -text output/part-00001\nhdfs://localhost/user/tom/input/smallfiles/b\n62 62 62 62 62 62 62\nhdfs://localhost/user/tom/input/smallfiles/d\n64 64 64 64 64 64 64\nhdfs://localhost/user/tom/input/smallfiles/f\n66 66 66 66 66 66 66\n61 61 61\n63 63 63\n62 62 62\n64 64 64\n66 66 66\nThe input files were named a, b, c, d, e, and f, and each contained 10 characters of the\ncorresponding letter (so, for example, a contained 10 “a” characters), except e, which\nwas empty. We can see this in the textual rendering of the sequence files, which prints\nthe filename followed by the hex representation of the file.\nInput Formats | 195There’s at least one way we could improve this program. As mentioned earlier, having\none mapper per file is inefficient, so subclassing CombineFileInputFormat instead of\nFileInputFormat is the better approach (there’s code for this in the accompanying ex-\nample code). Also, for a related technique of packing files into a Hadoop Archive, rather\nthan a sequence file, see the section “Hadoop Archives” on page 71.\nText Input\nHadoop excels at processing unstructured text. In this section, we discuss the different\nInputFormats that Hadoop provides to process text.\nTextInputFormat\nTextInputFormat is the default InputFormat. Each record is a line of input. The key, a\nLongWritable, is the byte offset within the file of the beginning of the line. The value is\nthe contents of the line, excluding any line terminators (newline, carriage return), and\nis packaged as a Text object. So a file containing the following text:\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nis divided into one split of four records. The records are interpreted as the following\nkey-value pairs:\n(0, On the top of the Crumpetty Tree)\n(33, The Quangle Wangle sat,)\n(57, But his face you could not see,)\n(89, On account of his Beaver Hat.)\nClearly, the keys are not line numbers. This would be impossible to implement in gen-\neral, in that a file is broken into splits, at byte, not line, boundaries. Splits are processed\nindependently. Line numbers are really a sequential notion: you have to keep a count\nof lines as you consume them, so knowing the line number within a split would be\npossible, but not within the file.\nHowever, the offset within the file of each line is known by each split independently of\nthe other splits, since each split knows the size of the preceding splits and just adds this\non to the offsets within the split to produce a global file offset. The offset is usually\nsufficient for applications that need a unique identifier for each line. Combined with\nthe file’s name, it is unique within the filesystem. Of course, if all the lines are a fixed\nwidth, then calculating the line number is simply a matter of dividing the offset by the\nwidth.\n196 | Chapter 7: MapReduce Types and FormatsThe Relationship Between Input Splits and HDFS Blocks\nThe logical records that FileInputFormats define do not usually fit neatly into HDFS\nblocks. For example, a TextInputFormat’s logical records are lines, which will cross\nHDFS boundaries more often than not. This has no bearing on the functioning of your\nprogram—lines are not missed or broken, for example—but it’s worth knowing about,\nas it does mean that data-local maps (that is, maps that are running on the same host\nas their input data) will perform some remote reads. The slight overhead this causes is\nnot normally significant.\nFigure 7-3 shows an example. A single file is broken into lines, and the line boundaries\ndo not correspond with the HDFS block boundaries. Splits honor logical record boun-\ndaries, in this case lines, so we see that the first split contains line 5, even though it\nspans the first and second block. The second split starts at line 6.\nFigure 7-3. Logical records and HDFS blocks for TextInputFormat\nKeyValueTextInputFormat\nTextInputFormat’s keys, being simply the offset within the file, are not normally very\nuseful. It is common for each line in a file to be a key-value pair, separated by a delimiter\nsuch as a tab character. For example, this is the output produced by TextOutputFor\nmat, Hadoop’s default OutputFormat. To interpret such files correctly, KeyValueTextIn\nputFormat is appropriate.\nYou can specify the separator via the key.value.separator.in.input.line property. It\nis a tab character by default. Consider the following input file, where → represents a\n(horizontal) tab character:\nline1→On the top of the Crumpetty Tree\nline2→The Quangle Wangle sat,\nline3→But his face you could not see,\nline4→On account of his Beaver Hat.\nLike in the TextInputFormat case, the input is in a single split comprising four records,\nalthough this time the keys are the Text sequences before the tab in each line:\n(line1,\n(line2,\n(line3,\n(line4,\nOn the top of the Crumpetty Tree)\nThe Quangle Wangle sat,)\nBut his face you could not see,)\nOn account of his Beaver Hat.)\nInput Formats | 197NLineInputFormat\nWith TextInputFormat and KeyValueTextInputFormat, each mapper receives a variable\nnumber of lines of input. The number depends on the size of the split and the length\nof the lines. If you want your mappers to receive a fixed number of lines of input, then\nNLineInputFormat is the InputFormat to use. Like TextInputFormat, the keys are the byte\noffsets within the file and the values are the lines themselves.\nN refers to the number of lines of input that each mapper receives. With N set to one\n(the default), each mapper receives exactly one line of input. The\nmapred.line.input.format.linespermap property controls the value of N. By way of\nexample, consider these four lines again:\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nIf, for example, N is two, then each split contains two lines. One mapper will receive\nthe first two key-value pairs:\n(0, On the top of the Crumpetty Tree)\n(33, The Quangle Wangle sat,)\nAnd another mapper will receive the second two key-value pairs:\n(57, But his face you could not see,)\n(89, On account of his Beaver Hat.)\nThe keys and values are the same as TextInputFormat produces. What is different is the\nway the splits are constructed.\nUsually having a map task for a small number of lines of input is inefficient (due to the\noverhead in task setup), but there are applications that take a small amount of input\ndata and run an extensive (that is, CPU-intensive) computation for it, then emit their\noutput. Simulations are a good example. By creating an input file that specifies input\nparameters, one per line, you can perform a parameter sweep: run a set of simulations\nin parallel to find how a model varies as the parameter changes.\nIf you have long-running simulations, you may fall afoul of task time-\nouts. When a task doesn’t report progress for more than 10 minutes,\nthen the tasktracker assumes it has failed and aborts the process (see\n“Task Failure” on page 159).\nThe best way to guard against this is to report progress periodically, by\nwriting a status message, or incrementing a counter, for example. See\n“What Constitutes Progress in MapReduce?” on page 158.\nAnother example is using Hadoop to bootstrap data loading from multiple data sour-\nces, such as databases. You create a “seed” input file which lists the data sources, one\nper line. Then each mapper is allocated a single data source, and it loads the data from\n198 | Chapter 7: MapReduce Types and Formatsthat source into HDFS. The job doesn’t need the reduce phase, so the number of re-\nducers should be set to zero (by calling setNumReduceTasks() on Job). Furthermore,\nMapReduce jobs can be run to process the data loaded into HDFS. See Appendix C for\nan example.\nXML\nMost XML parsers operate on whole XML documents, so if a large XML document is\nmade up of multiple input splits, then it is a challenge to parse these individually. Of\ncourse, you can process the entire XML document in one mapper (if it is not too large)\nusing the technique in “Processing a whole file as a record” on page 192.\nLarge XML documents that are composed of a series of “records” (XML document\nfragments) can be broken into these records using simple string or regular-expression\nmatching to find start and end tags of records. This alleviates the problem when the\ndocument is split by the framework, since the next start tag of a record is easy to find\nby simply scanning from the start of the split, just like TextInputFormat finds newline\nboundaries.\nHadoop comes with a class for this purpose called StreamXmlRecordReader (which is in\nthe org.apache.hadoop.streaming package, although it can be used outside of Stream-\ning). You can use it by setting your input format to StreamInputFormat, and setting the\nstream.recordreader.class property to org.apache.hadoop.streaming.StreamXmlRecor\ndReader. The reader is configured by setting job configuration properties to tell it the\npatterns for the start and end tags (see the class documentation for details).‖\nTo take an example, Wikipedia provides dumps of its content in XML form, which are\nappropriate for processing in parallel using MapReduce using this approach. The data\nis contained in one large XML wrapper document, which contains a series of elements,\nsuch as page elements that contain a page’s content and associated metadata. Using\nStreamXmlRecordReader, the page elements can be interpreted as records for processing\nby a mapper.\nBinary Input\nHadoop MapReduce is not just restricted to processing textual data—it has support\nfor binary formats, too.\nSequenceFileInputFormat\nHadoop’s sequence file format stores sequences of binary key-value pairs. They are well\nsuited as a format for MapReduce data since they are splittable (they have sync points\nso that readers can synchronize with record boundaries from an arbitrary point in the\nfile, such as the start of a split), they support compression as a part of the format, and\n‖ See https://issues.apache.org/jira/browse/HADOOP-2439 for an improved XML input format.\nInput Formats | 199they can store arbitrary types using a variety of serialization frameworks. (These topics\nare covered in “SequenceFile” on page 103.)\nTo use data from sequence files as the input to MapReduce, you use SequenceFileIn\nputFormat. The keys and values are determined by the sequence file, and you need to\nmake sure that your map input types correspond. For example, if your sequence file\nhas IntWritable keys and Text values, like the one created in Chapter 4, then the map\nsignature would be Mapper<IntWritable, Text, K, V>, where K and V are the types of\nthe map’s output keys and values.\nAlthough its name doesn’t give it away, SequenceFileInputFormat can\nread MapFiles as well as sequence files. If it finds a directory where it\nwas expecting a sequence file, SequenceFileInputFormat assumes that it\nis reading a MapFile and uses its data file. This is why there is no\nMapFileInputFormat class.\nSequenceFileAsTextInputFormat\nSequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that converts\nthe sequence file’s keys and values to Text objects. The conversion is performed by\ncalling toString() on the keys and values. This format makes sequence files suitable\ninput for Streaming.\nSequenceFileAsBinaryInputFormat\nSequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves\nthe sequence file’s keys and values as opaque binary objects. They are encapsulated as\nBytesWritable objects, and the application is free to interpret the underlying byte array\nas it pleases. Combined with SequenceFile.Reader’s appendRaw() method, this provides\na way to use any binary data types with MapReduce (packaged as a sequence file),\nalthough plugging into Hadoop’s serialization mechanism is normally a cleaner alter-\nnative (see “Serialization Frameworks” on page 101).\nMultiple Inputs\nAlthough the input to a MapReduce job may consist of multiple input files (constructed\nby a combination of file globs, filters and plain paths), all of the input is interpreted by\na single InputFormat and a single Mapper. What often happens, however, is that over\ntime, the data format evolves, so you have to write your mapper to cope with all of your\nlegacy formats. Or, you have data sources that provide the same type of data but in\ndifferent formats. This arises in the case of performing joins of different datasets; see\n“Reduce-Side Joins” on page 235. For instance, one might be tab-separated plain text,\nthe other a binary sequence file. Even if they are in the same format, they may have\ndifferent representations, and therefore need to be parsed differently.\n200 | Chapter 7: MapReduce Types and FormatsThese cases are handled elegantly by using the MultipleInputs class, which allows you\nto specify the InputFormat and Mapper to use on a per-path basis. For example, if we\nhad weather data from the U.K. Met Office# that we wanted to combine with the\nNCDC data for our maximum temperature analysis, then we might set up the input as\nfollows:\nMultipleInputs.addInputPath(conf, ncdcInputPath,\nTextInputFormat.class, MaxTemperatureMapper.class)\nMultipleInputs.addInputPath(conf, metOfficeInputPath,\nTextInputFormat.class, MetOfficeMaxTemperatureMapper.class);\nThis code replaces the usual calls to FileInputFormat.addInputPath() and conf.setMap\nperClass(). Both Met Office and NCDC data is text-based, so we use TextInputFor\nmat for each. But the line format of the two data sources is different, so we use two\ndifferent mappers. The MaxTemperatureMapper reads NCDC input data and extracts the\nyear and temperature fields. The MetOfficeMaxTemperatureMapper reads Met Office in-\nput data and extracts the year and temperature fields. The important thing is that the\nmap outputs have the same types, since the reducers (which are all of the same type)\nsee the aggregated map outputs and are not aware of the different mappers used to\nproduce them.\nThe MultipleInputs class has an overloaded version of addInputPath() that doesn’t take\na mapper:\npublic static void addInputPath(JobConf conf, Path path,\nClass<? extends InputFormat> inputFormatClass)\nThis is useful when you only have one mapper (set using the JobConf’s setMapper\nClass() method), but multiple input formats.\nDatabase Input (and Output)\nDBInputFormat is an input format for reading data from a relational database, using\nJDBC. Because it doesn’t have any sharding capabilities, you need to be careful not to\noverwhelm the database you are reading from by running too many mappers. For this\nreason, it is best used for loading relatively small datasets, perhaps for joining with\nlarger datasets from HDFS, using MultipleInputs. The corresponding output format is\nDBOutputFormat, which is useful for dumping job outputs (of modest size) into a\ndatabase.*\n# Met Office data is generally available only to the research and academic community. However, there is a\nsmall amount of monthly weather station data available at http://www.metoffice.gov.uk/climate/uk/\nstationdata/.\n* Instructions for how to use these formats are provided in “Database Access with Hadoop,” http://www\n.cloudera.com/blog/2009/03/06/database-access-with-hadoop/, by Aaron Kimball.\nInput Formats | 201HBase’s TableInputFormat is designed to allow a MapReduce program to operate on\ndata stored in an HBase table. TableOutputFormat is for writing MapReduce outputs\ninto an HBase table.\nOutput Formats\nHadoop has output data formats that correspond to the input formats covered in the\nprevious section. The OutputFormat class hierarchy appears in Figure 7-4.\nFigure 7-4. OutputFormat class hierarchy\nText Output\nThe default output format, TextOutputFormat, writes records as lines of text. Its keys\nand values may be of any type, since TextOutputFormat turns them to strings by calling\ntoString() on them. Each key-value pair is separated by a tab character, although that\nmay be changed using the mapred.textoutputformat.separator property. The counter-\npart to TextOuputFormat for reading in this case is KeyValueTextInputFormat, since it\nbreaks lines into key-value pairs based on a configurable separator (see “KeyValue-\nTextInputFormat” on page 197).\nYou can suppress the key or the value (or both, making this output format equivalent\nto NullOutputFormat, which emits nothing) from the output using a NullWritable type.\n202 | Chapter 7: MapReduce Types and FormatsThis also causes no separator to be written, which makes the output suitable for reading\nin using TextInputFormat.\nBinary Output\nSequenceFileOutputFormat\nAs the name indicates, SequenceFileOutputFormat writes sequence files for its output.\nThis is a good choice of output if it forms the input to a further MapReduce job, since\nit is compact, and is readily compressed. Compression is controlled via the static\nmethods on SequenceFileOutputFormat, as described in “Using Compression in Map-\nReduce” on page 84. For an example of how to use SequenceFileOutputFormat, see\n“Sorting” on page 218.\nSequenceFileAsBinaryOutputFormat\nSequenceFileAsBinaryOutputFormat is the counterpart to SequenceFileAsBinaryInput\nFormat, and it writes keys and values in raw binary format into a SequenceFile container.\nMapFileOutputFormat\nMapFileOutputFormat writes MapFiles as output. The keys in a MapFile must be added\nin order, so you need to ensure that your reducers emit keys in sorted order.\nThe reduce input keys are guaranteed to be sorted, but the output keys\nare under the control of the reduce function, and there is nothing in the\ngeneral MapReduce contract that states that the reduce output keys have\nto be ordered in any way. The extra constraint of sorted reduce output\nkeys is just needed for MapFileOutputFormat.\nMultiple Outputs\nFileOutputFormat and its subclasses generate a set of files in the output directory. There\nis one file per reducer and files are named by the partition number: part-00000,\npart-00001, etc. There is sometimes a need to have more control over the naming of\nthe files, or to produce multiple files per reducer. MapReduce comes with two libraries\nto help you do this: MultipleOutputFormat and MultipleOutputs.\nAn example: Partitioning data\nConsider the problem of partitioning the weather dataset by weather station. We would\nlike to run a job whose output is a file per station, with each file containing all the\nrecords for that station.\nOne way of doing this is to have a reducer for each weather station. To arrange this,\nwe need to do two things. First, write a partitioner that puts records from the same\nOutput Formats | 203weather station into the same partition. Second, set the number of reducers on the job\nto be the number of weather stations. The partitioner would look like this:\npublic class StationPartitioner implements Partitioner<LongWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n@Override\npublic int getPartition(LongWritable key, Text value, int numPartitions) {\nparser.parse(value);\nreturn getPartition(parser.getStationId());\n}\nprivate int getPartition(String stationId) {\n...\n}\n}\n@Override\npublic void configure(JobConf conf) { }\nThe getPartition(String) method, whose implementation is not shown, turns the\nstation ID into a partition index. To do this, it needs a list of all the station IDs and\nthen just returns the index of the station ID in the list.\nThere are two drawbacks to this approach. The first is that since the number of parti-\ntions needs to be known before the job is run, so does the number of weather stations.\nAlthough the NCDC provides metadata about its stations, there is no guarantee that\nthe IDs encountered in the data match those in the metadata. A station that appears in\nthe metadata but not in the data wastes a reducer slot. Worse, a station that appears\nin the data but not in the metadata doesn’t get a reducer slot—it has to be thrown away.\nOne way of mitigating this problem would be to write a job to extract the unique station\nIDs, but it’s a shame that we need an extra job to do this.\nThe second drawback is more subtle. It is generally a bad idea to allow the number of\npartitions to be rigidly fixed by the application, since it can lead to small or uneven-\nsized partitions. Having many reducers doing a small amount of work isn’t an efficient\nway of organizing a job: it’s much better to get reducers to do more work and have\nfewer of them, as the overhead in running a task is then reduced. Uneven-sized parti-\ntions can be difficult to avoid too. Different weather stations will have gathered a widely\nvarying amount of data: compare a station that opened one year ago to one that has\nbeen gathering data for one century. If a few reduce tasks take significantly longer than\nthe others, they will dominate the job execution time and cause it to be longer than it\nneeds to be.\n204 | Chapter 7: MapReduce Types and FormatsThere are two special cases when it does make sense to allow the ap-\nplication to set the number of partitions (or equivalently, the number\nof reducers):\nZero reducers\nThis is a vacuous case: there are no partitions, as the application\nneeds to run only map tasks.\nOne reducer\nIt can be convenient to run small jobs to combine the output of\nprevious jobs into a single file. This should only be attempted when\nthe amount of data is small enough to be processed comfortably\nby one reducer.\nIt is much better to let the cluster drive the number of partitions for a job; the idea being\nthat the more cluster reduce slots are available the faster the job can complete. This is\nwhy the default HashPartitioner works so well, as it works with any number of parti-\ntions and ensures each partition has a good mix of keys leading to more even-sized\npartitions.\nIf we go back to using HashPartitioner, each partition will contain multiple stations,\nso to create a file per station, we need to arrange for each reducer to write multiple files,\nwhich is where MultipleOutputFormat comes in.\nMultipleOutputFormat\nMultipleOutputFormat allows you to write data to multiple files whose names are de-\nrived from the output keys and values. MultipleOutputFormat is an abstract class with\ntwo concrete subclasses, MultipleTextOutputFormat and MultipleSequenceFileOutput\nFormat, which are the multiple file equivalents of TextOutputFormat and SequenceFi\nleOutputFormat. MultipleOutputFormat provides a few protected methods that sub-\nclasses can override to control the output filename. In Example 7-5, we create a subclass\nof MultipleTextOutputFormat to override the generateFileNameForKeyValue() method\nto return the station ID, which we extracted from the record value.\nExample 7-5. Partitioning whole dataset into files named by the station ID using\nMultipleOutputFormat\npublic class PartitionByStationUsingMultipleOutputFormat extends Configured\nimplements Tool {\nstatic class StationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\nOutput Formats | 205}\n}\nparser.parse(value);\noutput.collect(new Text(parser.getStationId()), value);\nstatic class StationReducer extends MapReduceBase\nimplements Reducer<Text, Text, NullWritable, Text> {\n}\n@Override\npublic void reduce(Text key, Iterator<Text> values,\nOutputCollector<NullWritable, Text> output, Reporter reporter)\nthrows IOException {\nwhile (values.hasNext()) {\noutput.collect(NullWritable.get(), values.next());\n}\n}\nstatic class StationNameMultipleTextOutputFormat\nextends MultipleTextOutputFormat<NullWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n}\nprotected String generateFileNameForKeyValue(NullWritable key, Text value,\nString name) {\nparser.parse(value);\nreturn parser.getStationId();\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(StationMapper.class);\nconf.setMapOutputKeyClass(Text.class);\nconf.setReducerClass(StationReducer.class);\nconf.setOutputKeyClass(NullWritable.class);\nconf.setOutputFormat(StationNameMultipleTextOutputFormat.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew PartitionByStationUsingMultipleOutputFormat(), args);\nSystem.exit(exitCode);\n}\n206 | Chapter 7: MapReduce Types and FormatsStationMapper pulls the station ID from the record and uses it as the key. This causes\nrecords from the same station to go into the same partition. StationReducer replaces\nthe key with a NullWritable so that when the final output is written using StationName\nMultipleTextOutputFormat (which like TextOutputFormat drops NullWritable keys), it\nconsists solely of weather records (and not the station ID key).\nThe overall effect is to place all the records for one station in a file named by the station\nID. Here is a few lines of output after running the program over a subset of the total\ndataset:\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\n2887145\n1395129\n2054455\n1422448\n1419378\n1384421\n1480077\n1400448\n307141\n1433994\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n10:34\n10:33\n10:33\n10:34\n10:34\n10:33\n10:33\n10:33\n10:34\n10:33\n/output/010010-99999\n/output/010050-99999\n/output/010100-99999\n/output/010280-99999\n/output/010550-99999\n/output/010980-99999\n/output/011060-99999\n/output/012030-99999\n/output/012350-99999\n/output/012620-99999\nThe filename returned by generateFileNameForKeyValue() is actually a path that is in-\nterpreted relative to the output directory. It’s possible to create subdirectories of arbi-\ntrary depth. For example, the following modification partitions the data by station and\nyear so that each year’s data is contained in a directory named by the station ID:\nprotected String generateFileNameForKeyValue(NullWritable key, Text value,\nString name) {\nparser.parse(value);\nreturn parser.getStationId() + ""/"" + parser.getYear();\n}\nMultipleOutputFormat has more features that are not discussed here, such as the ability\nto copy the input directory structure and file naming for a map-only job. Please consult\nthe Java documentation for details.\nMultipleOutputs\nThere’s a second library in Hadoop for generating multiple outputs, provided by the\nMultipleOutputs class. Unlike MultipleOutputFormat, MultipleOutputs can emit differ-\nent types for each output. On the other hand, there is less control over the naming of\noutputs. The program in Example 7-6 shows how to use MultipleOutputs to partition\nthe dataset by station.\nExample 7-6. Partitioning whole dataset into files named by the station ID using MultipleOutputs\npublic class PartitionByStationUsingMultipleOutputs extends Configured\nimplements Tool {\nstatic class StationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, Text> {\nOutput Formats | 207private NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\noutput.collect(new Text(parser.getStationId()), value);\nstatic class MultipleOutputsReducer extends MapReduceBase\nimplements Reducer<Text, Text, NullWritable, Text> {\nprivate MultipleOutputs multipleOutputs;\n@Override\npublic void configure(JobConf conf) {\nmultipleOutputs = new MultipleOutputs(conf);\n}\npublic void reduce(Text key, Iterator<Text> values,\nOutputCollector<NullWritable, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nOutputCollector collector = multipleOutputs.getCollector(""station"",\nkey.toString().replace(""-"", """"), reporter);\nwhile (values.hasNext()) {\ncollector.collect(NullWritable.get(), values.next());\n}\n@Override\npublic void close() throws IOException {\nmultipleOutputs.close();\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(StationMapper.class);\nconf.setMapOutputKeyClass(Text.class);\nconf.setReducerClass(MultipleOutputsReducer.class);\nconf.setOutputKeyClass(NullWritable.class);\nconf.setOutputFormat(NullOutputFormat.class); // suppress empty part file\nMultipleOutputs.addMultiNamedOutput(conf, ""station"", TextOutputFormat.class,\nNullWritable.class, Text.class);\nJobClient.runJob(conf);\n208 | Chapter 7: MapReduce Types and Formats}\nreturn 0;\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new PartitionByStationUsingMultipleOutputs(),\nargs);\nSystem.exit(exitCode);\n}\nThe MultipleOutputs class is used to generate additional outputs to the usual output.\nOutputs are given names, and may be written to a single file (called single named out-\nput), or to multiple files (called multi named output). In this case, we want multiple files,\none for each station, so we use a multi named output, which we initialize in the driver\nby calling the addMultiNamedOutput() method of MultipleOutputs to specify the name\nof the output (here ""station""), the output format, and the output types. In addition,\nwe set the regular output format to be NullOutputFormat in order to suppress the usual\noutput.\nIn the reducer, where we generate the output, we construct an instance of MultipleOut\nputs in the configure() method, and assign it to an instance variable. We use the\nMultipleOutputs instance in the reduce() method to retrieve an OutputCollector for the\nmulti named output. The getCollector() method takes the name of the output\n(""station"" again) as well as a string identifying the part within the multi named output.\nHere we use the station identifier, with the “-” separator in the key removed, since only\nalphanumeric characters are allowed by MultipleOutputs.\nThe overall effect is to produce output files with the naming scheme station_<station\nidentifier>-r-<part_number>. The r appears in the name because the output is pro-\nduced by the reducer, and the part number is appended to be sure that there are no\ncollisions resulting from different partitions (reducers) writing output for the same\nstation. Since we partition by station, it cannot happen in this case (but it can in the\ngeneral case).\nIn one run, the first few output files were named as follows (other columns from the\ndirectory listing have been dropped):\n/output/station_01001099999-r-00027\n/output/station_01005099999-r-00013\n/output/station_01010099999-r-00015\n/output/station_01028099999-r-00014\n/output/station_01055099999-r-00000\n/output/station_01098099999-r-00011\n/output/station_01106099999-r-00025\n/output/station_01203099999-r-00029\n/output/station_01235099999-r-00018\n/output/station_01262099999-r-00004\nOutput Formats | 209What’s the Difference Between MultipleOutputFormat and\nMultipleOutputs?\nIt’s unfortunate (although not necessarily unusual in an open source project) to have\ntwo libraries that do almost the same thing, since it is confusing for users. To help you\nchoose which to use, here is a brief comparison.\nFeature MultipleOutputFormat MultipleOutputs\nComplete control over names of files and directories Yes No\nDifferent key and value types for different outputs No Yes\nUse from map and reduce in the same job No Yes\nMultiple outputs per record No Yes\nUse with any OutputFormat No, need to subclass Yes\nSo in summary, MultipleOutputs is more fully featured, but MultipleOutputFormat has\nmore control over the output directory structure and file naming.\nLazy Output\nFileOutputFormat subclasses will create output (part-nnnnn) files, even if they are empty.\nSome applications prefer that empty files not be created, which is where LazyOutput\nFormat helps.† It is a wrapper output format that ensures that the output file is created\nonly when the first record is emitted for a given partition. To use it, call its setOutput\nFormatClass() method with the JobConf and the underlying output format.\nStreaming and Pipes support a -lazyOutput option to enable LazyOutputFormat.\nDatabase Output\nThe output formats for writing to relational databases and to HBase are mentioned in\n“Database Input (and Output)” on page 201.\n† LazyOutputFormat is available from release 0.21.0 of Hadoop.\n210 | Chapter 7: MapReduce Types and FormatsCHAPTER 8\nMapReduce Features\nThis chapter looks at some of the more advanced features of MapReduce, including\ncounters and sorting and joining datasets.\nCounters\nThere are often things you would like to know about the data you are analyzing but\nwhich are peripheral to the analysis you are performing. For example, if you were\ncounting invalid records, and discovered that the proportion of invalid records in the\nwhole dataset was very high, you might be prompted to check why so many records\nwere being marked as invalid—perhaps there is a bug in the part of the program that\ndetects invalid records? Or if the data were of poor quality and genuinely did have very\nmany invalid records, after discovering this, you might decide to increase the size of\nthe dataset so that the number of good records was large enough for meaningful\nanalysis.\nCounters are a useful channel for gathering statistics about the job: for quality control,\nor for application level-statistics. They are also useful for problem diagnosis. If you are\ntempted to put a log message into your map or reduce task, then it is often better to\nsee whether you can use a counter instead to record that a particular condition occurred.\nIn addition to counter values being much easier to retrieve than log output for large\ndistributed jobs, you get a record of the number of times that condition occurred, which\nis more work to obtain from a set of logfiles.\nBuilt-in Counters\nHadoop maintains some built-in counters for every job (Table 8-1), which report var-\nious metrics for your job. For example, there are counters for the number of bytes and\nrecords processed, which allows you to confirm that the expected amount of input was\nconsumed and the expected amount of output was produced.\n211Table 8-1. Built-in counters\nGroup Counter Description\nMap-Reduce Map input records The number of input records consumed by all the maps in the job. Incremented\nFramework every time a record is read from a RecordReader and passed to the map’s\n           map() method by the framework.\nMap skipped records The number of input records skipped by all the maps in the job. See “Skipping\n                     Bad Records” on page 171.\nMap input bytes The number of bytes of uncompressed input consumed by all the maps in the\n               job. Incremented every time a record is read from a RecordReader and passed\n              to the map’s map() method by the framework.\nMap output records The number of map output records produced by all the maps in the job. Incre-\n                  mented every time the collect() method is called on a map’s OutputCol\n                   lector.\nMap output bytes The number of bytes of uncompressed output produced by all the maps in the\n                job. Incremented every time the collect() method is called on a map’s\n                 OutputCollector.\nCombine input records The number of input records consumed by all the combiners (if any) in the job.\n                     Incremented every time a value is read from the combiner’s iterator over values.\n                      Note that this count is the number of values consumed by the combiner, not the\n                     number of distinct key groups (which would not be a useful metric, since there\n                    is not necessarily one group per key for a combiner; see “Combiner Func-\n                     tions” on page 29, and also “Shuffle and Sort” on page 163).\nCombine output records The number of output records produced by all the combiners (if any) in the job.\n                      Incremented every time the collect() method is called on a combiner’s\n                       OutputCollector.\nReduce input groups The number of distinct key groups consumed by all the reducers in the job.\n                   Incremented every time the reducer’s reduce() method is called by the\n                    framework.\nReduce input records The number of input records consumed by all the reducers in the job. Incremented\n                    every time a value is read from the reducer’s iterator over values. If reducers\n                     consume all of their inputs this count should be the same as the count for Map\n                    output records.\nReduce output records The number of reduce output records produced by all the maps in the job.\n                     Incremented every time the collect() method is called on a reducer’s\n                      OutputCollector.\nReduce skipped groups The number of distinct key groups skipped by all the reducers in the job. See\n                     “Skipping Bad Records” on page 171.\nReduce skipped The number of input records skipped by all the reducers in the job.\nßrecords \nSpilled records The number of records spilled to disk in all map and reduce tasks in the job.\nFilesystem bytes read The number of bytes read by each filesystem by map and reduce tasks. There is\n                     a counter for each filesystem: Filesystem may be Local, HDFS, S3, KFS, etc.\nFilesystem bytes written The number of bytes written by each filesystem by map and reduce tasks.\nFile Systems\n212 | Chapter 8: MapReduce FeaturesGroup Counter Description\nJob Counters Launched map tasks The number of map tasks that were launched. Includes tasks that were started\n                               speculatively.\nLaunched reduce tasks The number of reduce tasks that were launched. Includes tasks that were started\n                     speculatively.\nFailed map tasks The number of map tasks that failed. See “Task Failure” on page 159 for potential\n                    causes.\nFailed reduce tasks The number of reduce tasks that failed.\nData-local map tasks The number of map tasks that ran on the same node as their input data.\nRack-local map tasks The number of map tasks that ran on a node in the same rack as their input data.\nOther local map tasks The number of map tasks that ran on a node in a different rack to their input\n                     data. Inter-rack bandwidth is scarce, and Hadoop tries to place map tasks close\n                    to their input data, so this count should be low.\nCounters are maintained by the task with which they are associated, and periodically\nsent to the tasktracker and then to the jobtracker, so they can be globally aggregated.\n(This is described in “Progress and Status Updates” on page 156.) The built-in Job\nCounters are actually maintained by the jobtracker, so they don’t need to be sent across\nthe network, unlike all other counters, including user-defined ones.\nA task’s counters are sent in full every time, rather than sending the counts since the\nlast transmission, since this guards against errors due to lost messages. Furthermore,\nduring a job run, counters may go down if a task fails. Counter values are definitive\nonly once a job has successfully completed.\nUser-Defined Java Counters\nMapReduce allows user code to define a set of counters, which are then incremented\nas desired in the mapper or reducer. Counters are defined by a Java enum, which serves\nto group related counters. A job may define an arbitrary number of enums, each with\nan arbitrary number of fields. The name of the enum is the group name, and the enum’s\nfields are the counter names. Counters are global: the MapReduce framework aggre-\ngates them across all maps and reduces to produce a grand total at the end of the job.\nWe created some counters in Chapter 5 for counting malformed records in the weather\ndataset. The program in Example 8-1 extends that example to count the number of\nmissing records and the distribution of temperature quality codes.\nCounters | 213Example 8-1. Application to run the maximum temperature job, including counting missing and\nmalformed fields and quality codes\npublic class MaxTemperatureWithCounters extends Configured implements Tool {\nenum Temperature {\nMISSING,\nMALFORMED\n}\nstatic class MaxTemperatureMapperWithCounters extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\noutput.collect(new Text(parser.getYear()),\nnew IntWritable(airTemperature));\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n} else if (parser.isMissingTemperature()) {\nreporter.incrCounter(Temperature.MISSING, 1);\n}\n// dynamic counter\nreporter.incrCounter(""TemperatureQuality"", parser.getQuality(), 1);\n}\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(MaxTemperatureMapperWithCounters.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\nJobClient.runJob(conf);\nreturn 0;\n214 | Chapter 8: MapReduce Features}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureWithCounters(), args);\nSystem.exit(exitCode);\n}\nThe best way to see what this program does is run it over the complete dataset:\n% hadoop jar job.jar MaxTemperatureWithCounters input/ncdc/all output-counters\nWhen the job has successfully completed, it prints out the counters at the end (this is\ndone by JobClient’s runJob() method). Here are the ones we are interested in:\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nTemperatureQuality\n2=1246032\n1=973422173\n0=1\n6=40066\n5=158291879\n4=10764500\n9=66136858\nAir Temperature Records\nMalformed=3\nMissing=66136856\nDynamic counters\nThe code makes use of a dynamic counter—one that isn’t defined by a Java enum. Since\na Java enum’s fields are defined at compile time, you can’t create new counters on the\nfly using enums. Here we want to count the distribution of temperature quality codes,\nand though the format specification defines the values that it can take, it is more con-\nvenient to use a dynamic counter to emit the values that it actually takes. The method\nwe use on the Reporter object takes a group and counter name using String names:\npublic void incrCounter(String group, String counter, long amount)\nThe two ways of creating and accessing counters—using enums and using Strings—\nare actually equivalent since Hadoop turns enums into Strings to send counters over\nRPC. Enums are slightly easier to work with, provide type safety, and are suitable for\nmost jobs. For the odd occasion when you need to create counters dynamically, you\ncan use the String interface.\nReadable counter names\nBy default, a counter’s name is the enum’s fully qualified Java classname. These names\nare not very readable when they appear on the web UI, or in the console, so Hadoop\nprovides a way to change the display names using resource bundles. We’ve done this\nhere, so we see “Air Temperature Records” instead of “Temperature$MISSING.” For\ndynamic counters, the group and counter names are used for the display names, so this\nis not normally an issue.\nCounters | 215The recipe to provide readable names is as follows. Create a properties file named after\nthe enum, using an underscore as a separator for nested classes. The properties file\nshould be in the same directory as the top-level class containing the enum. The file is\nnamed MaxTemperatureWithCounters_Temperature.properties for the counters in Ex-\nample 8-1.\nThe properties file should contain a single property named CounterGroupName, whose\nvalue is the display name for the whole group. Then each field in the enum should have\na corresponding property defined for it, whose name is the name of the field suffixed\nwith .name, and whose value is the display name for the counter. Here are the contents\nof MaxTemperatureWithCounters_Temperature.properties:\nCounterGroupName=Air Temperature Records\nMISSING.name=Missing\nMALFORMED.name=Malformed\nHadoop uses the standard Java localization mechanisms to load the correct properties\nfor the locale you are running in, so, for example, you can create a Chinese version of\nthe properties in a file named MaxTemperatureWithCounters_Tempera\nture_zh_CN.properties and they will be used when running in the zh_CN locale. Refer\nto the documentation for java.util.PropertyResourceBundle for more information.\nRetrieving counters\nIn addition to being available via the web UI and the command line (using hadoop job\n-counter), you can retrieve counter values using the Java API. You can do this while\nthe job is running, although it is more usual to get counters at the end of a job run,\nwhen they are stable. Example 8-2 shows a program that calculates the proportion of\nrecords that have missing temperature fields.\nExample 8-2. Application to calculate the proportion of records with missing temperature fields\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.mapred.*;\nimport org.apache.hadoop.util.*;\npublic class MissingTemperatureFields extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 1) {\nJobBuilder.printUsage(this, ""<job ID>"");\nreturn -1;\n}\nJobClient jobClient = new JobClient(new JobConf(getConf()));\nString jobID = args[0];\nRunningJob job = jobClient.getJob(JobID.forName(jobID));\nif (job == null) {\nSystem.err.printf(""No job with ID %s found.\\n"", jobID);\nreturn -1;\n}\n216 | Chapter 8: MapReduce Featuresif (!job.isComplete()) {\nSystem.err.printf(""Job %s is not complete.\\n"", jobID);\nreturn -1;\n}\nCounters counters = job.getCounters();\nlong missing = counters.getCounter(\nMaxTemperatureWithCounters.Temperature.MISSING);\nlong total = counters.findCounter(""org.apache.hadoop.mapred.Task$Counter"",\n""MAP_INPUT_RECORDS"").getCounter();\nSystem.out.printf(""Records with missing temperature fields: %.2f%%\\n"",\n100.0 * missing / total);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MissingTemperatureFields(), args);\nSystem.exit(exitCode);\n}\nFirst we retrieve a RunningJob object from a JobClient, by calling the getJob() method\nwith the job ID. We check whether there is actually a job with the given ID. There may\nnot be, either because the ID was incorrectly specified or because the jobtracker no\nlonger has a reference to the job (only the last 100 jobs are kept in memory, and all are\ncleared out if the jobtracker is restarted).\nAfter confirming that the job has completed, we call the RunningJob’s getCounters()\nmethod, which returns a Counters object, encapsulating all the counters for a job. The\nCounters class provides various methods for finding the names and values of counters.\nWe use the getCounter() method, which takes an enum to find the number of records\nthat had a missing temperature field.\nThere are also findCounter() methods, all of which return a Counter object. We use\nthis form to retrieve the built-in counter for map input records. To do this, we refer to\nthe counter by its group name—the fully qualified Java classname for the enum—and\ncounter name (both strings).*\nFinally, we print the proportion of records that had a missing temperature field. Here’s\nwhat we get for the whole weather dataset:\n% hadoop jar job.jar MissingTemperatureFields job_200904200610_0003\nRecords with missing temperature fields: 5.47%\n* The built-in counter’s enums are not currently a part of the public API, so this is the only way to retrieve\nthem. https://issues.apache.org/jira/browse/HADOOP-4043 will remedy this deficiency.\nCounters | 217User-Defined Streaming Counters\nA Streaming MapReduce program can increment counters by sending a specially for-\nmatted line to the standard error stream, which is co-opted as a control channel in this\ncase. The line must have the following format:\nreporter:counter:group,counter,amount\nThis snippet in Python shows how to increment the “Missing” counter in the “Tem-\nperature” group by one:\nsys.stderr.write(""reporter:counter:Temperature,Missing,1\\n"")\nIn a similar way, a status message may be sent with a line formatted like this:\nreporter:status:message\nSorting\nThe ability to sort data is at the heart of MapReduce. Even if your application isn’t\nconcerned with sorting per se, it may be able to use the sorting stage that MapReduce\nprovides to organize its data. In this section, we will examine different ways of sorting\ndatasets, and how you can control the sort order in MapReduce.\nPreparation\nWe are going to sort the weather dataset by temperature. Storing temperatures as\nText objects doesn’t work for sorting purposes, since signed integers don’t sort lexico-\ngraphically.† Instead, we are going to store the data using sequence files whose IntWrit\nable keys represent the temperature (and sort correctly), and whose Text values are the\nlines of data.\nThe MapReduce job in Example 8-3 is a map-only job that also filters the input to\nremove records that don’t have a valid temperature reading. Each map creates a single\nblock-compressed sequence file as output. It is invoked with the following command:\n% hadoop jar job.jar SortDataPreprocessor input/ncdc/all input/ncdc/all-seq\nExample 8-3. A MapReduce program for transforming the weather data into SequenceFile format\npublic class SortDataPreprocessor extends Configured implements Tool {\nstatic class CleanerMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, IntWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n† One commonly used workaround for this problem—particularly in text-based Streaming applications—is\nto add an offset to eliminate all negative numbers, and left pad with zeros, so all numbers are the same number\nof characters. However, see “Streaming” on page 231 for another approach.\n218 | Chapter 8: MapReduce Featurespublic void map(LongWritable key, Text value,\nOutputCollector<IntWritable, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new IntWritable(parser.getAirTemperature()), value);\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(CleanerMapper.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputValueClass(Text.class);\nconf.setNumReduceTasks(0);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\nJobClient.runJob(conf);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortDataPreprocessor(), args);\nSystem.exit(exitCode);\n}\nPartial Sort\nIn “The Default MapReduce Job” on page 178, we saw that, by default, MapReduce\nwill sort input records by their keys. Example 8-4 is a variation for sorting sequence\nfiles with IntWritable keys.\nExample 8-4. A MapReduce program for sorting a SequenceFile with IntWritable keys using the\ndefault HashPartitioner\npublic class SortByTemperatureUsingHashPartitioner extends Configured\nimplements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\nSorting | 219}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortByTemperatureUsingHashPartitioner(),\nargs);\nSystem.exit(exitCode);\n}\nControlling Sort Order\nThe sort order for keys is controlled by a RawComparator, which is found as follows:\n1. If the property mapred.output.key.comparator.class is set, an instance of that class\nis used. (The setOutputKeyComparatorClass() method on JobConf is a convenient\nway to set this property.)\n2. Otherwise, keys must be a subclass of WritableComparable and the registered com-\nparator for the key class is used.\n3. If there is no registered comparator, then a RawComparator is used that deserializes\nthe byte streams being compared into objects, and delegates to the WritableCom\nparable’s compareTo() method.\nThese rules reinforce why it’s important to register optimized versions of RawCompara\ntors for your own custom Writable classes (which is covered in “Implementing a Raw-\nComparator for speed” on page 99), and also that it’s straightforward to override the\nsort order by setting your own comparator (we do this in “Secondary\nSort” on page 227).\nSuppose we run this program using 30 reducers:‡\n% hadoop jar job.jar SortByTemperatureUsingHashPartitioner \\\n-D mapred.reduce.tasks=30 input/ncdc/all-seq output-hashsort\nThis command produces 30 output files, each of which is sorted. However, there is no\neasy way to combine the files (by concatenation, for example, in the case of plain-text\n‡ See “Sorting and merging SequenceFiles” on page 108 for how to do the same thing using the sort program\nexample that comes with Hadoop.\n220 | Chapter 8: MapReduce Featuresfiles) to produce a globally sorted file. For many applications, this doesn’t matter. For\nexample, having a partially sorted set of files is fine if you want to do lookups.\nAn application: Partitioned MapFile lookups\nTo perform lookups by key, for instance, having multiple files works well. If we change\nthe output format to be a MapFileOutputFormat, as shown in Example 8-5, then the\noutput is 30 map files, which we can perform lookups against.\nExample 8-5. A MapReduce program for sorting a SequenceFile and producing MapFiles as output\npublic class SortByTemperatureToMapFile extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(MapFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortByTemperatureToMapFile(), args);\nSystem.exit(exitCode);\n}\nMapFileOutputFormat provides a pair of convenience static methods for performing\nlookups against MapReduce output; their use is shown in Example 8-6.\nExample 8-6. Retrieve the first entry with a given key from a collection of MapFiles\npublic class LookupRecordByTemperature extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nJobBuilder.printUsage(this, ""<path> <key>"");\nreturn -1;\n}\nPath path = new Path(args[0]);\nIntWritable key = new IntWritable(Integer.parseInt(args[1]));\nFileSystem fs = path.getFileSystem(getConf());\nSorting | 221}\n}\nReader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf());\nPartitioner<IntWritable, Text> partitioner =\nnew HashPartitioner<IntWritable, Text>();\nText val = new Text();\nWritable entry =\nMapFileOutputFormat.getEntry(readers, partitioner, key, val);\nif (entry == null) {\nSystem.err.println(""Key not found: "" + key);\nreturn -1;\n}\nNcdcRecordParser parser = new NcdcRecordParser();\nparser.parse(val.toString());\nSystem.out.printf(""%s\\t%s\\n"", parser.getStationId(), parser.getYear());\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new LookupRecordByTemperature(), args);\nSystem.exit(exitCode);\n}\nThe getReaders() method opens a MapFile.Reader for each of the output files created\nby the MapReduce job. The getEntry() method then uses the partitioner to choose the\nreader for the key, and finds the value for that key by calling Reader’s get() method. If\ngetEntry() returns null, it means no matching key was found. Otherwise, it returns\nthe value, which we translate into a station ID and year.\nTo see this in action, let’s find the first entry for a temperature of –10°C (remember\nthat temperatures are stored as integers representing tenths of a degree, which is why\nwe ask for a temperature of –100):\n% hadoop jar job.jar LookupRecordByTemperature output-hashmapsort -100\n357460-99999\n1956\nWe can also use the readers directly, in order to get all the records for a given key. The\narray of readers that is returned is ordered by partition, so that the reader for a given\nkey may be found using the same partitioner that was used in the MapReduce job:\nReader reader = readers[partitioner.getPartition(key, val, readers.length)];\nThen once we have the reader, we get the first key using MapFile’s get() method, then\nrepeatedly call next() to retrieve the next key and value, until the key changes. A pro-\ngram to do this is shown in Example 8-7.\nExample 8-7. Retrieve all entries with a given key from a collection of MapFiles\npublic class LookupRecordsByTemperature extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nJobBuilder.printUsage(this, ""<path> <key>"");\n222 | Chapter 8: MapReduce Featuresreturn -1;\n}\nPath path = new Path(args[0]);\nIntWritable key = new IntWritable(Integer.parseInt(args[1]));\nFileSystem fs = path.getFileSystem(getConf());\nReader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf());\nPartitioner<IntWritable, Text> partitioner =\nnew HashPartitioner<IntWritable, Text>();\nText val = new Text();\n}\n}\nReader reader = readers[partitioner.getPartition(key, val, readers.length)];\nWritable entry = reader.get(key, val);\nif (entry == null) {\nSystem.err.println(""Key not found: "" + key);\nreturn -1;\n}\nNcdcRecordParser parser = new NcdcRecordParser();\nIntWritable nextKey = new IntWritable();\ndo {\nparser.parse(val.toString());\nSystem.out.printf(""%s\\t%s\\n"", parser.getStationId(), parser.getYear());\n} while(reader.next(nextKey, val) && key.equals(nextKey));\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new LookupRecordsByTemperature(), args);\nSystem.exit(exitCode);\n}\nAnd here is a sample run to retrieve all readings of –10°C and count them:\n% hadoop jar job.jar LookupRecordsByTemperature output-hashmapsort -100 \\\n2> /dev/null | wc -l\n1489272\nTotal Sort\nHow can you produce a globally sorted file using Hadoop? The naive answer is to use\na single partition.§ But this is incredibly inefficient for large files, since one machine\nhas to process all of the output, so you are throwing away the benefits of the parallel\narchitecture that MapReduce provides.\nInstead it is possible to produce a set of sorted files that, if concatenated, would form\na globally sorted file. The secret to doing this is to use a partitioner that respects the\ntotal order of the output. For example, if we had four partitions, we could put keys for\ntemperatures less than –10°C in the first partition, those between –10°C and 0°C in the\nsecond, those between 0°C and 10°C in the third, and those over 10°C in the fourth.\n§ A better answer is to use Pig, which can sort with a single command. See “Sorting Data” on page 338.\nSorting | 223Although this approach works, you have to choose your partition sizes carefully to\nensure that they are fairly even so that job times aren’t dominated by a single reducer.\nFor the partitioning scheme just described, the relative sizes of the partitions are as\nfollows:\nTemperature range < –10°C [–10°C, 0°C) [0°C, 10°C) >= 10°C\nProportion of records 11% 13% 17% 59%\nThese partitions are not very even. To construct more even partitions, we need to have\na better understanding of the temperature distribution for the whole dataset. It’s fairly\neasy to write a MapReduce job to count the number of records that fall into a collection\nof temperature buckets. For example, Figure 8-1 shows the distribution for buckets of\nsize 1°C, where each point on the plot corresponds to one bucket.\nFigure 8-1. Temperature distribution for the weather dataset\nWhile we could use this information to construct a very even set of partitions, the fact\nthat we needed to run a job that used the entire dataset to construct them is not ideal.\nIt’s possible to get a fairly even set of partitions, by sampling the key space. The idea\nbehind sampling is that you look at a small subset of the keys to approximate the key\ndistribution, which is then used to construct partitions. Luckily, we don’t have to write\nthe code to do this ourselves, as Hadoop comes with a selection of samplers.\n224 | Chapter 8: MapReduce FeaturesThe InputSampler class defines a nested Sampler interface whose implementations re-\nturn a sample of keys given an InputFormat and JobConf:\npublic interface Sampler<K,V> {\nK[] getSample(InputFormat<K,V> inf, JobConf job) throws IOException;\n}\nThis interface is not usually called directly by clients. Instead, the writePartition\nFile() static method on InputSampler is used, which creates a sequence file to store the\nkeys that define the partitions:\npublic static <K,V> void writePartitionFile(JobConf job,\nSampler<K,V> sampler) throws IOException\nThe sequence file is used by TotalOrderPartitioner to create partitions for the sort job.\nExample 8-8 puts it all together.\nExample 8-8. A MapReduce program for sorting a SequenceFile with IntWritable keys using the\nTotalOrderPartitioner to globally sort the data\npublic class SortByTemperatureUsingTotalOrderPartitioner extends Configured\nimplements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\nconf.setPartitionerClass(TotalOrderPartitioner.class);\nInputSampler.Sampler<IntWritable, Text> sampler =\nnew InputSampler.RandomSampler<IntWritable, Text>(0.1, 10000, 10);\nPath input = FileInputFormat.getInputPaths(conf)[0];\ninput = input.makeQualified(input.getFileSystem(conf));\nPath partitionFile = new Path(input, ""_partitions"");\nTotalOrderPartitioner.setPartitionFile(conf, partitionFile);\nInputSampler.writePartitionFile(conf, sampler);\n// Add to DistributedCache\nURI partitionUri = new URI(partitionFile.toString() + ""#_partitions"");\nDistributedCache.addCacheFile(partitionUri, conf);\nDistributedCache.createSymlink(conf);\nSorting | 225}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew SortByTemperatureUsingTotalOrderPartitioner(), args);\nSystem.exit(exitCode);\n}\nWe use a RandomSampler, which chooses keys with a uniform probability—here, 0.1.\nThere are also parameters for the maximum number of samples to take, and the max-\nimum number of splits to sample (here, 10,000 and 10, respectively; these settings are\nthe defaults when InputSampler is run as an application), and the sampler stops when\nthe first of these limits is met. Samplers run on the client, making it important to limit\nthe number of splits that are downloaded, so the sampler runs quickly. In practice, the\ntime taken to run the sampler is a small fraction of the overall job time.\nThe partition file that InputSampler writes is called _partitions, which we have set to be\nin the input directory (it will not be picked up as an input file since it starts with an\nunderscore). To share the partition file with the tasks running on the cluster, we add\nit to the distributed cache (see “Distributed Cache” on page 239).\nOn one run, the sampler chose –5.6°C, 13.9°C, and 22.0°C as partition boundaries (for\nfour partitions), which translates into more even partition sizes that the earlier choice\nof partitions:\nTemperature range < –5.6°C [–5.6°C, 13.9°C) [13.9°C, 22.0°C) >= 22.0°C\nProportion of records 29% 24% 23% 24%\nYour input data determines the best sampler for you to use. For example, SplitSam\npler, which samples only the first n records in a split, is not so good for sorted data‖\nbecause it doesn’t select keys from throughout the split.\nOn the other hand, IntervalSampler chooses keys at regular intervals through the split,\nand makes a better choice for sorted data. RandomSampler is a good general-purpose\nsampler. If none of these suits your application (and remember that the point of sam-\npling is to produce partitions that are approximately equal in size), you can write your\nown implementation of the Sampler interface.\nOne of the nice properties of InputSampler and TotalOrderPartitioner is that you are\nfree to choose the number of partitions. This choice is normally driven by the number\nof reducer slots in you cluster (choose a number slightly fewer than the total, to allow\n‖ In some applications, it’s common for some of the input to already be sorted, or at least partially sorted. For\nexample, the weather dataset is ordered by time, which may introduce certain biases, making the\nRandomSampler a safe choice.\n226 | Chapter 8: MapReduce Featuresfor failures). However, TotalOrderPartitioner will work only if the partition\nboundaries are distinct: one problem with choosing a high number is that you may get\ncollisions if you have a small key space.\nHere’s how we run it:\n% hadoop jar job.jar SortByTemperatureUsingTotalOrderPartitioner \\\n-D mapred.reduce.tasks=30 input/ncdc/all-seq output-totalsort\nThe program produces 30 output partitions, each of which is internally sorted; in ad-\ndition, for these partitions, all the keys in partition i are less than the keys in partition\ni + 1.\nSecondary Sort\nThe MapReduce framework sorts the records by key before they reach the reducers.\nFor any particular key, however, the values are not sorted. The order that the values\nappear is not even stable from one run to the next, since they come from different map\ntasks, which may finish at different times from run to run. Generally speaking, most\nMapReduce programs are written so as not to depend on the order that the values\nappear to the reduce function. However, it is possible to impose an order on the values\nby sorting and grouping the keys in a particular way.\nTo illustrate the idea, consider the MapReduce program for calculating the maximum\ntemperature for each year. If we arranged for the values (temperatures) to be sorted in\ndescending order, we wouldn’t have to iterate through them to find the maximum—\nwe could take the first for each year and ignore the rest. (This approach isn’t the most\nefficient way to solve this particular problem, but it illustrates how secondary sort works\nin general.)\nTo achieve this, we change our keys to be composite: a combination of year and\ntemperature. We want the sort order for keys to be by year (ascending) and then by\ntemperature (descending):\n1900\n1900\n1900\n...\n1901\n1901\n35°C\n34°C\n34°C\n36°C\n35°C\nIf all we did was change the key, then this wouldn’t help since now records for the same\nyear would not (in general) go to the same reducer since they have different keys. For\nexample, (1900, 35°C) and (1900, 34°C) could go to different reducers. By setting a\npartitioner to partition by the year part of the key, we can guarantee that records for\nthe same year go to the same reducer. This still isn’t enough to achieve our goal, how-\never. A partitioner ensures only that one reducer receives all the records for a year; it\ndoesn’t change the fact that the reducer groups by key within the partition:\nSorting | 227The final piece of the puzzle is the setting to control the grouping. If we group values\nin the reducer by the year part of the key, then we will see all the records for the same\nyear in one reduce group. And since they are sorted by temperature in descending order,\nthe first is the maximum temperature:\nTo summarize, there is a recipe here to get the effect of sorting by value:\n• Make the key a composite of the natural key and the natural value.\n• The key comparator should order by the composite key, that is, the natural key\nand natural value.\n• The partitioner and grouping comparator for the composite key should consider\nonly the natural key for partitioning and grouping.\nJava code\nPutting this all together results in the code in Example 8-9. This program uses the plain-\ntext input again.\nExample 8-9. Application to find the maximum temperature by sorting temperatures in the key\npublic class MaxTemperatureUsingSecondarySort\nextends Configured implements Tool {\nstatic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, IntPair, NullWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<IntPair, NullWritable> output, Reporter reporter)\nthrows IOException {\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new IntPair(parser.getYearInt(),\n228 | Chapter 8: MapReduce Features}\n}\n}\n+ parser.getAirTemperature()), NullWritable.get());\nstatic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<IntPair, NullWritable, IntPair, NullWritable> {\npublic void reduce(IntPair key, Iterator<NullWritable> values,\nOutputCollector<IntPair, NullWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\noutput.collect(key, NullWritable.get());\npublic static class FirstPartitioner\nimplements Partitioner<IntPair, NullWritable> {\n@Override\npublic void configure(JobConf job) {}\n}\n@Override\npublic int getPartition(IntPair key, NullWritable value, int numPartitions) {\nreturn Math.abs(key.getFirst() * 127) % numPartitions;\n}\npublic static class KeyComparator extends WritableComparator {\nprotected KeyComparator() {\nsuper(IntPair.class, true);\n}\n@Override\npublic int compare(WritableComparable w1, WritableComparable w2) {\nIntPair ip1 = (IntPair) w1;\nIntPair ip2 = (IntPair) w2;\nint cmp = IntPair.compare(ip1.getFirst(), ip2.getFirst());\nif (cmp != 0) {\nreturn cmp;\n}\nreturn -IntPair.compare(ip1.getSecond(), ip2.getSecond()); //reverse\n}\n}\npublic static class GroupComparator extends WritableComparator {\nprotected GroupComparator() {\nsuper(IntPair.class, true);\n}\n@Override\npublic int compare(WritableComparable w1, WritableComparable w2) {\nIntPair ip1 = (IntPair) w1;\nIntPair ip2 = (IntPair) w2;\nreturn IntPair.compare(ip1.getFirst(), ip2.getFirst());\n}\n}\nSorting | 229@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setPartitionerClass(FirstPartitioner.class);\nconf.setOutputKeyComparatorClass(KeyComparator.class);\nconf.setOutputValueGroupingComparator(GroupComparator.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(IntPair.class);\nconf.setOutputValueClass(NullWritable.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureUsingSecondarySort(), args);\nSystem.exit(exitCode);\n}\nIn the mapper, we create a key representing the year and temperature, using an IntPair\nWritable implementation. (IntPair is like the TextPair class we developed in “Imple-\nmenting a Custom Writable” on page 96.) We don’t need to carry any information in\nthe value, since we can get the first (maximum) temperature in the reducer from the\nkey, so we use a NullWritable. The reducer emits the first key, which due to the sec-\nondary sorting, is an IntPair for the year and its maximum temperature. IntPair’s\ntoString() method creates a tab-separated string, so the output is a set of tab-separated\nyear-temperature pairs.\nMany applications need to access all the sorted values, not just the first\nvalue as we have provided here. To do this, you need to populate the\nvalue fields since in the reducer you can retrieve only the first key. This\nnecessitates some unavoidable duplication of information between key\nand value.\n230 | Chapter 8: MapReduce FeaturesWe set the partitioner to partition by the first field of the key (the year), using a custom\npartitioner. To sort keys by year (ascending) and temperature (descending), we use a\ncustom key comparator that extracts the fields and performs the appropriate compar-\nisons. Similarly, to group keys by year, we set a custom comparator, using setOutput\nValueGroupingComparator(), to extract the first field of the key for comparison.#\nRunning this program gives the maximum temperatures for each year:\n% hadoop jar job.jar MaxTemperatureUsingSecondarySort input/ncdc/all output-secondarysort\n% hadoop fs -cat output-secondarysort/part-* | sort | head\n1901\n317\n1902\n244\n1903\n289\n1904\n256\n1905\n283\n1906\n294\n1907\n283\n1908\n289\n1909\n278\n1910\n294\nStreaming\nTo do a secondary sort in Streaming, we can take advantage of a couple of library classes\nthat Hadoop provides. Here’s the driver that we can use to do a secondary sort:\nhadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-D stream.num.map.output.key.fields=2 \\\n-D mapred.text.key.partitioner.options=-k1,1 \\\n-D mapred.output.key.comparator.class=\\\norg.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n-D mapred.text.key.comparator.options=""-k1n -k2nr"" \\\n-input input/ncdc/all \\\n-output output_secondarysort_streaming \\\n-mapper src/main/ch08/python/secondary_sort_map.py \\\n-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n-reducer src/main/ch08/python/secondary_sort_reduce.py \\\n-file src/main/ch08/python/secondary_sort_map.py \\\n-file src/main/ch08/python/secondary_sort_reduce.py\nOur map function (Example 8-10) emits records with year and temperature fields. We\nwant to treat the combination of both of these fields as the key, so we set\nstream.num.map.output.key.fields to 2. This means that values will be empty, just like\nin the Java case.\n# For simplicity, these custom comparators as shown are not optimized; see “Implementing a RawComparator\nfor speed” on page 99 for the steps we would need to take to make them faster.\nSorting | 231Example 8-10. Map function for secondary sort in Python\n#!/usr/bin/env python\nimport re\nimport sys\nfor line in sys.stdin:\nval = line.strip()\n(year, temp, q) = (val[15:19], int(val[87:92]), val[92:93])\nif temp == 9999:\nsys.stderr.write(""reporter:counter:Temperature,Missing,1\\n"")\nelif re.match(""[01459]"", q):\nprint ""%s\\t%s"" % (year, temp)\nHowever, we don’t want to partition by the entire key, so we use the KeyFieldBased\nPartitioner partitioner, which allows us to partition by a part of the key. The specifi-\ncation mapred.text.key.partitioner.options configures the partitioner. The value\n-k1,1 instructs the partitioner to use only the first field of the key, where fields are\nassumed to be separated by a string defined by the map.output.key.field.separator\nproperty (a tab character by default).\nNext, we want a comparator that sorts the year field in ascending order and the tem-\nperature field in descending order, so that the reduce function can simply return the\nfirst record in each group. Hadoop provides KeyFieldBasedComparator, which is ideal\nfor this purpose. The comparison order is defined by a specification that is like the one\nused for GNU sort. It is set using the mapred.text.key.comparator.options property.\nThe value -k1n -k2nr used in this example means “sort by the first field in numerical\norder, then by the second field in reverse numerical order.” Like its partitioner cousin,\nKeyFieldBasedPartitioner, it uses the separator defined by the map.out\nput.key.field.separator to split a key into fields.\nIn the Java version, we had to set the grouping comparator; however, in Streaming\ngroups are not demarcated in any way, so in the reduce function we have to detect the\ngroup boundaries ourselves by looking for when the year changes (Example 8-11).\nExample 8-11. Reducer function for secondary sort in Python\n#!/usr/bin/env python\nimport sys\nlast_group = None\nfor line in sys.stdin:\nval = line.strip()\n(year, temp) = val.split(""\\t"")\ngroup = year\nif last_group != group:\nprint val\nlast_group = group\nWhen we run the streaming program, we get the same output as the Java version.\n232 | Chapter 8: MapReduce FeaturesFinally, note that KeyFieldBasedPartitioner and KeyFieldBasedComparator are not con-\nfined to use in Streaming programs—they are applicable to Java MapReduce programs,\ntoo.\nJoins\nMapReduce can perform joins between large datasets, but writing the code to do joins\nfrom scratch is fairly involved. Rather than writing MapReduce programs, you might\nconsider using a higher-level framework such as Pig, Hive, or Cascading, in which join\noperations are a core part of the implementation.\nLet’s briefly consider the problem we are trying to solve. We have two datasets; for\nexample, the weather stations database, and the weather records—and we want to\nreconcile the two. For example, we want to see each station’s history, with the station’s\nmetadata inlined in each output row. This is illustrated in Figure 8-2.\nHow we implement the join depends on how large the datasets are and how they are\npartitioned. If one dataset is large (the weather records) but the other one is small\nenough to be distributed to each node in the cluster (as the station metadata is), then\nthe join can be effected by a MapReduce job that brings the records for each station\ntogether (a partial sort on station ID, for example). The mapper or reducer uses the\nsmaller dataset to look up the station metadata for a station ID, so it can be written out\nwith each record. See “Side Data Distribution” on page 238 for a discussion of this\napproach, where we focus on the mechanics of distributing the data to tasktrackers.\nIf both datasets are too large for either to be copied to each node in the cluster, then\nwe can join them using MapReduce, using either a map-side join or a reduce-side join.\nOne common example of this case is a user database, and a log of some user activity\n(such as access logs). For a popular service, it is not feasible to distribute the user\ndatabase (or the logs) to all the MapReduce nodes.\nMap-Side Joins\nA map-side join works by performing the join before the data reaches the map function.\nFor this to work, though, the inputs to each map must be partitioned and sorted in a\nparticular way. Each input dataset must be divided into the same number of partitions,\nand it must be sorted by the same key (the join key) in each source. All the records for\na particular key must reside in the same partition. This may sound like a strict require-\nment (and it is), but it actually fits the description of the output of a MapReduce job.\nA map-side join can be used to join the outputs of several jobs that had the same number\nof reducers, the same keys, and output files that are not splittable (by being smaller\nthan an HDFS block, or by virtue of being gzip compressed, for example). In the context\nof the weather example, if we ran a partial sort on the stations file by station ID, and\nanother, identical sort on the records, again by station ID, and with the same number\nJoins | 233Figure 8-2. Inner join of two datasets\nof reducers, then the two outputs would satisfy the conditions for running a map-side\njoin.\nUse a CompositeInputFormat from the org.apache.hadoop.mapred.join package to run\na map-side join. The input sources and join type (inner or outer) for CompositeInput\nFormat are configured through a join expression that is written according to a simple\ngrammar. The package documentation has details and examples.\nThe org.apache.hadoop.examples.Join example is a general-purpose command-line\nprogram for running a map-side join, since it allows you to run a MapReduce job for\nany specified mapper and reducer, over multiple inputs that are joined with a given join\noperation.\n234 | Chapter 8: MapReduce FeaturesReduce-Side Joins\nA reduce-side join is more general than a map-side join, in that the input datasets don’t\nhave to be structured in any particular way, but it is less efficient as both datasets have\nto go through the MapReduce shuffle. The basic idea is that the mapper tags each record\nwith its source, and uses the join key as the map output key so that the records with\nthe same key are brought together in the reducer. We use several ingredients to make\nthis work in practice:\nMultiple inputs\nThe input sources for the datasets have different formats, in general, so it is very\nconvenient to use the MultipleInputs class (see “Multiple Inputs” on page 200) to\nseparate the logic for parsing and tagging each source.\nSecondary sort\nAs described, the reducer will see the records from both sources that have same\nkey, but they are not guaranteed to be in any particular order. However, to perform\nthe join, it is important to have the data from one source before another. For the\nweather data join, the station record must be the first of the values seen for each\nkey, so the reducer can fill in the weather records with the station name and emit\nthem straightaway. Of course, it would be possible to receive the records in any\norder if we buffered them in memory, but this should be avoided, since the number\nof records in any group may be very large and exceed the amount of memory avail-\nable to the reducer.*\nWe saw in “Secondary Sort” on page 227 how to impose an order on the values\nfor each key that the reducers see, so we use this technique here.\nTo tag each record, we use TextPair from Chapter 4 for the keys, to store the station\nID, and the tag. The only requirement for the tag values is that they sort in such a way\nthat the station records come before the weather records. This can be achieved by\ntagging station records as 0 and weather records as 1. The mapper classes to do this are\nshown in Examples 8-12 and 8-13.\nExample 8-12. Mapper for tagging station records for a reduce-side join\npublic class JoinStationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, TextPair, Text> {\nprivate NcdcStationMetadataParser parser = new NcdcStationMetadataParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<TextPair, Text> output, Reporter reporter)\nthrows IOException {\nif (parser.parse(value)) {\noutput.collect(new TextPair(parser.getStationId(), ""0""),\n* The data_join package in the contrib directory implements reduce-side joins by buffering records in memory,\nso it suffers from this limitation.\nJoins | 235}\n}\n}\nnew Text(parser.getStationName()));\nExample 8-13. Mapper for tagging weather records for a reduce-side join\npublic class JoinRecordMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, TextPair, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<TextPair, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\noutput.collect(new TextPair(parser.getStationId(), ""1""), value);\nThe reducer knows that it will receive the station record first, so it extracts its name\nfrom the value and writes it out as a part of every output record (Example 8-14).\nExample 8-14. Reducer for joining tagged station records with tagged weather records\npublic class JoinReducer extends MapReduceBase implements\nReducer<TextPair, Text, Text, Text> {\npublic void reduce(TextPair key, Iterator<Text> values,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nText stationName = new Text(values.next());\nwhile (values.hasNext()) {\nText record = values.next();\nText outValue = new Text(stationName.toString() + ""\\t"" + record.toString());\noutput.collect(key.getFirst(), outValue);\n}\nThe code assumes that every station ID in the weather records has exactly one matching\nrecord in the station dataset. If this were not the case, we would need to generalize the\ncode to put the tag into the value objects, by using another TextPair. The reduce()\nmethod would then be able to tell which entries were station names, and detect (and\nhandle) missing or duplicate entries, before processing the weather records.\n236 | Chapter 8: MapReduce FeaturesBecause objects in the reducer’s values iterator are re-used (for efficiency\npurposes), it is vital that the code makes a copy of the first Text object\nfrom the values iterator:\nText stationName = new Text(values.next());\nIf the copy is not made, then the stationName reference will refer to the\nvalue just read when it is turned into a string, which is a bug.\nTying the job together is the driver class, shown in Example 8-15. The essential point\nis that we partition and group on the first part of the key, the station ID, which we do\nwith a custom Partitioner (KeyPartitioner), and a custom comparator, FirstCompara\ntor (from TextPair).\nExample 8-15. Application to join weather records with station names\npublic class JoinRecordWithStationName extends Configured implements Tool {\npublic static class KeyPartitioner implements Partitioner<TextPair, Text> {\n@Override\npublic void configure(JobConf job) {}\n}\n@Override\npublic int getPartition(TextPair key, Text value, int numPartitions) {\nreturn (key.getFirst().hashCode() & Integer.MAX_VALUE) % numPartitions;\n}\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 3) {\nJobBuilder.printUsage(this, ""<ncdc input> <station input> <output>"");\nreturn -1;\n}\nJobConf conf = new JobConf(getConf(), getClass());\nconf.setJobName(""Join record with station name"");\nPath ncdcInputPath = new Path(args[0]);\nPath stationInputPath = new Path(args[1]);\nPath outputPath = new Path(args[2]);\nMultipleInputs.addInputPath(conf, ncdcInputPath,\nTextInputFormat.class, JoinRecordMapper.class);\nMultipleInputs.addInputPath(conf, stationInputPath,\nTextInputFormat.class, JoinStationMapper.class);\nFileOutputFormat.setOutputPath(conf, outputPath);\nconf.setPartitionerClass(KeyPartitioner.class);\nconf.setOutputValueGroupingComparator(TextPair.FirstComparator.class);\nconf.setMapOutputKeyClass(TextPair.class);\nconf.setReducerClass(JoinReducer.class);\nJoins | 237conf.setOutputKeyClass(Text.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new JoinRecordWithStationName(), args);\nSystem.exit(exitCode);\n}\nRunning the program on the sample data yields the following output:\n011990-99999\n011990-99999\n011990-99999\n012650-99999\n012650-99999\nSIHCCAJAVRI\nSIHCCAJAVRI\nSIHCCAJAVRI\nTYNSET-HANSMOEN\nTYNSET-HANSMOEN\n0067011990999991950051507004+68750...\n0043011990999991950051512004+68750...\n0043011990999991950051518004+68750...\n0043012650999991949032412004+62300...\n0043012650999991949032418004+62300...\nSide Data Distribution\nSide data can be defined as extra read-only data needed by a job to process the main\ndataset. The challenge is to make side data available to all the map or reduce tasks\n(which are spread across the cluster) in a convenient and efficient fashion.\nIn addition to the distribution mechanisms described in this section, it is possible to\ncache side-data in memory in a static field, so that tasks of the same job that run in\nsuccession on the same tasktracker can share the data. “Task JVM Re-\nuse” on page 170 describes how to enable this feature. If you take this approach, be\naware of the amount of memory that you are using, as it might affect the memory needed\nby the shuffle (see “Shuffle and Sort” on page 163).\nUsing the Job Configuration\nYou can set arbitrary key-value pairs in the job configuration using the various setter\nmethods on JobConf (inherited from Configuration). This is very useful if you need to\npass a small piece of metadata to your tasks. To retrieve the values in the task, override\nthe configure() method in the Mapper or Reducer and use a getter method on the\nJobConf object passed in.\nUsually a primitive type is sufficient to encode your metadata, but for arbitrary objects\nyou can either handle the serialization yourself (if you have an existing mechanism for\nturning objects to strings and back), or you can use Hadoop’s Stringifier class.\nDefaultStringifier uses Hadoop’s serialization framework to serialize objects (see\n“Serialization” on page 86).\n238 | Chapter 8: MapReduce FeaturesYou shouldn’t use this mechanism for transferring more than a few kilobytes of data\nbecause it can put pressure on the memory usage in the Hadoop daemons, particularly\nin a system running hundreds of jobs. The job configuration is read by the jobtracker,\nthe tasktracker, and the child JVM, and each time the configuration is read, all of its\nentries are read into memory, even if they are not used. User properties are not read on\nthe jobtracker or the tasktracker, so they just waste time and memory.\nDistributed Cache\nRather than serializing side data in the job configuration, it is preferable to distribute\ndatasets using Hadoop’s distributed cache mechanism. This provides a service for\ncopying files and archives to the task nodes in time for the tasks to use them when they\nrun. To save network bandwidth, files are normally copied to any particular node once\nper job.\nUsage\nFor tools that use GenericOptionsParser (this includes many of the programs in this\nbook—see “GenericOptionsParser, Tool, and ToolRunner” on page 121), you can\nspecify the files to be distributed as a comma-separated list of URIs as the argument to\nthe -files option. Files can be on the local filesystem, on HDFS, or on another Hadoop\nreadable filesystem (such as S3). If no scheme is supplied, then the files are assumed to\nbe local. (This is true even if the default filesystem is not the local filesystem.)\nYou can also copy archive files (JAR files, ZIP files, tar files, and gzipped tar files) to\nyour tasks, using the -archives option; these are unarchived on the task node. The\n-libjars option will add JAR files to the classpath of the mapper and reducer tasks.\nThis is useful if you haven’t bundled library JAR files in your job JAR file.\nStreaming doesn’t use the distributed cache for copying the streaming\nscripts across the cluster. You specify a file to be copied using the\n-file option (note the singular), which should be repeated for each file\nto be copied. Furthermore, files specified using the -file option must\nbe file paths only, not URIs, so they must be accessible from the local\nfilesystem of the client launching the Streaming job.\nStreaming also accepts the -files and -archives options for copying\nfiles into the distributed cache for use by your Streaming scripts.\nLet’s see how to use the distributed cache to share a metadata file for station names.\nThe command we will run is:\n% hadoop jar job.jar MaxTemperatureByStationNameUsingDistributedCacheFile \\\n-files input/ncdc/metadata/stations-fixed-width.txt input/ncdc/all output\nThis command will copy the local file stations-fixed-width.txt (no scheme is supplied,\nso the path is automatically interpreted as a local file) to the task nodes, so we can use\nSide Data Distribution | 239it to look up station names. The listing for MaxTemperatureByStationNameUsingDistri\nbutedCacheFile appears in Example 8-16.\nExample 8-16. Application to find the maximum temperature by station, showing station names from\na lookup table passed as a distributed cache file\npublic class MaxTemperatureByStationNameUsingDistributedCacheFile\nextends Configured implements Tool {\nstatic class StationTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new Text(parser.getStationId()),\nnew IntWritable(parser.getAirTemperature()));\n}\nstatic class MaxTemperatureReducerWithStationLookup extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\nprivate NcdcStationMetadata metadata;\n@Override\npublic void configure(JobConf conf) {\nmetadata = new NcdcStationMetadata();\ntry {\nmetadata.initialize(new File(""stations-fixed-width.txt""));\n} catch (IOException e) {\nthrow new RuntimeException(e);\n}\n}\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nString stationName = metadata.getStationName(key.toString());\n}\n}\nint maxValue = Integer.MIN_VALUE;\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(new Text(stationName), new IntWritable(maxValue));\n240 | Chapter 8: MapReduce Features@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(StationTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducerWithStationLookup.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew MaxTemperatureByStationNameUsingDistributedCacheFile(), args);\nSystem.exit(exitCode);\n}\nThe program finds the maximum temperature by weather station, so the mapper\n(StationTemperatureMapper) simply emits (station ID, temperature) pairs. For the\ncombiner we reuse MaxTemperatureReducer (from Chapters 2 and 5) to pick the\nmaximum temperature for any given group of map outputs on the map side. The re-\nducer (MaxTemperatureReducerWithStationLookup) is different from the combiner, since\nin addition to finding the maximum temperature, it uses the cache file to look up the\nstation name.\nWe use the reducer’s configure() method to retrieve the cache file using its original\nname, relative to the working directory of the task.\nYou can use the distributed cache for copying files that do not fit in\nmemory. MapFiles are very useful in this regard, since they serve as an\non-disk lookup format (see “MapFile” on page 110). Because MapFiles\nare a collection of files with a defined directory structure, you should\nput them into an archive format (JAR, ZIP, tar, or gzipped tar) and add\nthem to the cache using the -archives option.\nHere’s a snippet of the output, showing some maximum temperatures for a few weather\nstations:\nPEATS RIDGE WARATAH\nSTRATHALBYN RACECOU\nSHEOAKS AWS\nWANGARATTA AERO\n372\n410\n399\n409\nSide Data Distribution | 241MOOGARA\nMACKAY AERO\n334\n331\nHow it works\nWhen you launch a job, Hadoop copies the files specified by the -files and\n-archives options to the jobtracker’s filesystem (normally HDFS). Then, before a task\nis run, the tasktracker copies the files from the jobtracker’s filesystem to a local disk—\nthe cache—so the task can access the files. From the task’s point of view, the files are\njust there (and it doesn’t care that they came from HDFS).\nThe tasktracker also maintains a reference count for the number of tasks using each\nfile in the cache. After the task has run, the file’s reference count is decreased by one,\nand when it reaches zero it is eligible for deletion. Files are deleted to make room for a\nnew file when the cache exceeds a certain size—10 GB by default. The cache size may\nbe changed by setting the configuration property local.cache.size, which is measured\nin bytes.\nAlthough this design doesn’t guarantee that subsequent tasks from the same job run-\nning on the same tasktracker will find the file in the cache, it is very likely that they will,\nsince tasks from a job are usually scheduled to run at around the same time, so there\nisn’t the opportunity for enough other jobs to run and cause the original task’s file to\nbe deleted from the cache.\nFiles are localized under the ${mapred.local.dir}/taskTracker/archive directory on\nthe tasktrackers. Applications don’t have to know this, however, since the files are\nsymbolically linked from the task’s working directory.\nThe DistributedCache API\nMost applications don’t need to use the DistributedCache API because they can use the\ndistributed cache indirectly via GenericOptionsParser. GenericOptionsParser makes it\nmuch more convenient to use the distributed cache: for example, it copies local files\ninto HDFS and then the JobClient informs the DistributedCache of their locations in\nHDFS using the addCacheFile() and addCacheArchive() methods. The JobClient also\ngets DistributedCache to create symbolic links when the files are localized, by adding\nfragment identifiers to the files’ URIs. For example, the file specified by the URI hdfs://\nnamenode/foo/bar#myfile is symlinked as myfile in the task’s working directory.\nOn the task node, it is most convenient to access the localized file directly; however,\nsometimes you may need to get a list of all the available cache files. JobConf has two\nmethods for this purpose: getLocalCacheFiles() and getLocalCacheArchives(), which\nboth return an array of Path objects pointing to local files.\n242 | Chapter 8: MapReduce FeaturesMapReduce Library Classes\nHadoop comes with a library of mappers and reducers for commonly used functions.\nThey are listed with brief descriptions in Table 8-2. For further information on how to\nuse them, please consult their Java documentation.\nTable 8-2. MapReduce library classes\nClasses Description\nChainMapper, ChainReducer Run a chain of mappers in a single mapper, and a reducer followed by a chain of mappers\n                         in a single reducer. (Symbolically: M+RM*, where M is a mapper and R is a reducer.) This\n                        can substantially reduce the amount of disk I/O incurred compared to running multiple\n                       MapReduce jobs.\nFieldSelectionMapReduce A mapper and a reducer that can select fields (like the Unix cut command) from the\n                       input keys and values and emit them as output keys and values.\nIntSumReducer, Reducers that sum integer values to produce a total for every key.\nLongSumReducer \nInverseMapper A mapper that swaps keys and values.\nTokenCounterMapper A mapper that tokenizes the input value into words (using Java’s StringToken\n                    izer) and emits each word along with a count of one.\nRegexMapper A mapper that finds matches of a regular expression in the input value, and emits the\n           matches along with a count of one.\nMapReduce Library Classes | 243CHAPTER 9\nSetting Up a Hadoop Cluster\nThis chapter explains how to set up Hadoop to run on a cluster of machines. Running\nHDFS and MapReduce on a single machine is great for learning about these systems,\nbut to do useful work they need to run on multiple nodes.\nThere are a few options when it comes to getting a Hadoop cluster, from building your\nown to running on rented hardware, or using an offering that provides Hadoop as a\nservice in the cloud. This chapter and the next give you enough information to set up\nand operate your own cluster, but even if you are using a Hadoop service in which a\nlot of the routine maintenance is done for you, these chapters still offer valuable infor-\nmation about how Hadoop works from an operations point of view.\nCluster Specification\nHadoop is designed to run on commodity hardware. That means that you are not tied\nto expensive, proprietary offerings from a single vendor; rather, you can choose stand-\nardized, commonly available hardware from any of a large range of vendors to build\nyour cluster.\n“Commodity” does not mean “low-end.” Low-end machines often have cheap com-\nponents, which have higher failure rates than more expensive (but still commodity-\nclass) machines. When you are operating tens, hundreds, or thousands of machines,\ncheap components turn out to be a false economy, as the higher failure rate incurs a\ngreater maintenance cost. On the other hand, large database class machines are not\nrecommended either, since they don’t score well on the price/performance curve. And\neven though you would need fewer of them to build a cluster of comparable perform-\nance to one built of mid-range commodity hardware, when one did fail it would have\na bigger impact on the cluster, since a larger proportion of the cluster hardware would\nbe unavailable.\nHardware specifications rapidly become obsolete, but for the sake of illustration, a\ntypical choice of machine for running a Hadoop datanode and tasktracker in late 2008\nwould have the following specifications:\n245Processor\n2 quad-core Intel Xeon 2.0GHz CPUs\nMemory\n8 GB ECC RAM*\nStorage\n41 TB SATA disks\nNetwork\nGigabit Ethernet\nWhile the hardware specification for your cluster will assuredly be different, Hadoop\nis designed to use multiple cores and disks, so it will be able to take full advantage of\nmore powerful hardware.\nWhy Not Use RAID?\nHDFS clusters do not benefit from using RAID (Redundant Array of Independent\nDisks) for datanode storage (although RAID is used for the namenode’s disks, to protect\nagainst corruption of its metadata). The redundancy that RAID provides is not needed,\nsince HDFS handles it by replication between nodes.\nFurthermore, RAID striping (RAID 0) which is commonly used to increase perform-\nance, turns out to be slower than the JBOD (Just a Bunch Of Disks) configuration used\nby HDFS, which round-robins HDFS blocks between all disks. The reason for this is\nthat RAID 0 read and write operations are limited by the speed of the slowest disk in\nthe RAID array. In JBOD, disk operations are independent, so the average speed of\noperations is greater than that of the slowest disk. Disk performance often shows con-\nsiderable variation in practice, even for disks of the same model. In some benchmarking\ncarried out on a Yahoo! cluster (http://markmail.org/message/xmzc45zi25htr7ry),\nJBOD performed 10% faster than RAID 0 in one test (Gridmix), and 30% better in\nanother (HDFS write throughput).\nFinally, if a disk fails in a JBOD configuration, HDFS can continue to operate without\nthe failed disk, whereas with RAID, failure of a single disk causes the whole array (and\nhence the node) to become unavailable.\nThe bulk of Hadoop is written in Java, and can therefore run on any platform with a\nJVM, although there are enough parts that harbor Unix assumptions (the control\nscripts, for example) to make it unwise to run on a non-Unix platform in production.\nIn fact, Windows operating systems are not supported production platforms (although\nthey can be used with Cygwin as a development platform; see Appendix A).\nHow large should your cluster be? There isn’t an exact answer to this question, but the\nbeauty of Hadoop is that you can start with a small cluster (say, 10 nodes) and grow it\n* ECC memory is strongly recommended, as several Hadoop users have reported seeing many checksum errors\nwhen using non-ECC memory on Hadoop clusters.\n246 | Chapter 9: Setting Up a Hadoop Clusteras your storage and computational needs grow. In many ways, a better question is this:\nhow fast does my cluster need to grow? You can get a good feel for this by considering\nstorage capacity.\nFor example, if your data grows by 1 TB a week, and you have three-way HDFS repli-\ncation, then you need an additional 3 TB of raw storage per week. Allow some room\nfor intermediate files and logfiles (around 30%, say), and this works out at about one\nmachine (2008 vintage) per week, on average. In practice, you wouldn’t buy a new\nmachine each week and add it to the cluster. The value of doing a back-of-the-envelope\ncalculation like this is that it gives you a feel for how big your cluster should be: in this\nexample, a cluster that holds two years of data needs 100 machines.\nFor a small cluster (on the order of 10 nodes), it is usually acceptable to run the name-\nnode and the jobtracker on a single master machine (as long as at least one copy of the\nnamenode’s metadata is stored on a remote filesystem). As the cluster and the number\nof files stored in HDFS grow, the namenode needs more memory, so the namenode\nand jobtracker should be moved onto separate machines.\nThe secondary namenode can be run on the same machine as the namenode, but again\nfor reasons of memory usage (the secondary has the same memory requirements as the\nprimary), it is best to run it on a separate piece of hardware, especially for larger clusters.\n(This topic is discussed in more detail in “Master node scenarios” on page 254.) Ma-\nchines running the namenodes should typically run on 64-bit hardware to avoid the 3\nGB limit on Java heap size in 32-bit architectures.†\nNetwork Topology\nA common Hadoop cluster architecture consists of a two-level network topology, as\nillustrated in Figure 9-1. Typically there are 30 to 40 servers per rack, with a 1 GB switch\nfor the rack (only three are shown in the diagram), and an uplink to a core switch or\nrouter (which is normally 1 GB or better). The salient point is that the aggregate band-\nwidth between nodes on the same rack is much greater than that between nodes on\ndifferent racks.\nRack awareness\nTo get maximum performance out of Hadoop, it is important to configure Hadoop so\nthat it knows the topology of your network. If your cluster runs on a single rack, then\nthere is nothing more to do, since this is the default. However, for multirack clusters,\nyou need to map nodes to racks. By doing this, Hadoop will prefer within-rack transfers\n(where there is more bandwidth available) to off-rack transfers when placing\n† The traditional advice says other machines in the cluster (jobtracker, datanodes/tasktrackers) should be 32-\nbit to avoid the memory overhead of larger pointers. Sun’s Java 6 update 14 features “compressed ordinary\nobject pointers,” which eliminates much of this overhead, so there’s now no real downside to running on 64-\nbit hardware.\nCluster Specification | 247Figure 9-1. Typical two-level network architecture for a Hadoop cluster\nMapReduce tasks on nodes. HDFS will be able to place replicas more intelligently to\ntrade-off performance and resilience.\nNetwork locations such as nodes and racks are represented in a tree, which reflects the\nnetwork “distance” between locations. The namenode uses the network location when\ndetermining where to place block replicas (see “Network Topology and Ha-\ndoop” on page 64); the jobtracker uses network location to determine where the closest\nreplica is as input for a map task that is scheduled to run on a tasktracker.\nFor the network in Figure 9-1, the rack topology is described by two network locations,\nsay, /switch1/rack1 and /switch1/rack2. Since there is only one top-level switch in this\ncluster, the locations can be simplified to /rack1 and /rack2.\nThe Hadoop configuration must specify a map between node addresses and network\nlocations. The map is described by a Java interface, DNSToSwitchMapping, whose signa-\nture is:\npublic interface DNSToSwitchMapping {\npublic List<String> resolve(List<String> names);\n}\nThe names parameter is a list of IP addresses, and the return value is a list of corre-\nsponding network location strings. The topology.node.switch.mapping.impl configu-\nration property defines an implementation of the DNSToSwitchMapping interface that the\nnamenode and the jobtracker use to resolve worker node network locations.\nFor the network in our example, we would map node1, node2, and node3 to /rack1,\nand node4, node5, and node6 to /rack2.\nMost installations don’t need to implement the interface themselves, however, since\nthe default implementation is ScriptBasedMapping, which runs a user-defined script to\n248 | Chapter 9: Setting Up a Hadoop Clusterdetermine the mapping. The script’s location is controlled by the property\ntopology.script.file.name. The script must accept a variable number of arguments\nthat are the hostnames or IP addresses to be mapped, and it must emit the correspond-\ning network locations to standard output, separated by whitespace. The example code\nincludes a script for this purpose.\nIf no script location is specified, the default behavior is to map all nodes to a single\nnetwork location, called /default-rack.\nCluster Setup and Installation\nYour hardware has arrived. The next steps are to get it racked up and install the software\nneeded to run Hadoop.\nThere are various ways to install and configure Hadoop. This chapter describes how\nto do it from scratch using the Apache Hadoop distribution, and will give you the\nbackground to cover the things you need to think about when setting up Hadoop.\nAlternatively, if you would like to use RPMs or Debian packages for managing your\nHadoop installation, then you might want to start with Cloudera’s Distribution, de-\nscribed in Appendix B.\nTo ease the burden of installing and maintaining the same software on each node, it is\nnormal to use an automated installation method like Red Hat Linux’s Kickstart or\nDebian’s Fully Automatic Installation. These tools allow you to automate the operating\nsystem installation by recording the answers to questions that are asked during the\ninstallation process (such as the disk partition layout), as well as which packages to\ninstall. Crucially, they also provide hooks to run scripts at the end of the process, which\nare invaluable for doing final system tweaks and customization that is not covered by\nthe standard installer.\nThe following sections describe the customizations that are needed to run Hadoop.\nThese should all be added to the installation script.\nInstalling Java\nJava 6 or later is required to run Hadoop. The latest stable Sun JDK is the preferred\noption, although Java distributions from other vendors may work too. The following\ncommand confirms that Java was installed correctly:\n% java -version\njava version ""1.6.0_12""\nJava(TM) SE Runtime Environment (build 1.6.0_12-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 11.2-b01, mixed mode)\nCluster Setup and Installation | 249Creating a Hadoop User\nIt’s good practice to create a dedicated Hadoop user account to separate the Hadoop\ninstallation from other services running on the same machine.\nSome cluster administrators choose to make this user’s home directory an NFS-\nmounted drive, to aid with SSH key distribution (see the following discussion). The\nNFS server is typically outside the Hadoop cluster. If you use NFS, it is worth consid-\nering autofs, which allows you to mount the NFS filesystem on demand, when the\nsystem accesses it. Autofs provides some protection against the NFS server failing, and\nalso allows you to use replicated filesystems for failover. There are other NFS gotchas\nto watch out for, such as synchronizing UIDs and GIDs. For help setting up NFS on\nLinux, refer to the HOWTO at http://nfs.sourceforge.net/nfs-howto/index.html.\nInstalling Hadoop\nDownload Hadoop from the Apache Hadoop releases page (http://hadoop.apache.org/\ncore/releases.html), and unpack the contents of the distribution in a sensible location,\nsuch as /usr/local (/opt is another standard choice). Note that Hadoop is not installed\nin the hadoop user’s home directory, as that may be an NFS-mounted directory.\n% cd /usr/local\n% sudo tar xzf hadoop-x.y.z.tar.gz\nWe also need to change the owner of the Hadoop files to be the hadoop user and group:\n% sudo chown -R hadoop:hadoop hadoop-x.y.z\nSome administrators like to install HDFS and MapReduce in separate\nlocations on the same system. At the time of this writing, only HDFS\nand MapReduce from the same Hadoop release are compatible with one\nanother; however, in future releases, the compatibility requirements will\nbe loosened. When this happens, having independent installations\nmakes sense, as it gives more upgrade options (for more, see “Up-\ngrades” on page 296). For example, it is convenient to be able to up-\ngrade MapReduce—perhaps to patch a bug—while leaving HDFS\nrunning.\nNote that separate installations of HDFS and MapReduce can still share\nconfiguration by using the --config option (when starting daemons) to\nrefer to a common configuration directory. They can also log to the same\ndirectory, as the logfiles they produce are named in such a way as to\navoid clashes.\nTesting the Installation\nOnce you’ve created the installation file, you are ready to test it by installing it on the\nmachines in your cluster. This will probably take a few iterations as you discover kinks\n250 | Chapter 9: Setting Up a Hadoop Clusterin the install. When it’s working, you can proceed to configure Hadoop and give it a\ntest run. This process is documented in the following sections.\nSSH Configuration\nThe Hadoop control scripts rely on SSH to perform cluster-wide operations. For ex-\nample, there is a script for stopping and starting all the daemons in the cluster. Note\nthat the control scripts are optional—cluster-wide operations can be performed by\nother mechanisms, too (such as a distributed shell).\nTo work seamlessly, SSH needs to be set up to allow password-less login for the\nhadoop user from machines in the cluster. The simplest way to achieve this is to generate\na public/private key pair, and it will be shared across the cluster using NFS.\nFirst, generate an RSA key pair by typing the following in the hadoop user account:\n% ssh-keygen -t rsa -f ~/.ssh/id_rsa\nEven though we want password-less logins, keys without passphrases are not consid-\nered good practice (it’s OK to have an empty passphrase when running a local pseudo-\ndistributed cluster, as described in Appendix A), so we specify a passphrase when\nprompted for one. We shall use ssh-agent to avoid the need to enter a password for\neach connection.\nThe private key is in the file specified by the -f option, ~/.ssh/id_rsa, and the public key\nis stored in a file with the same name with .pub appended, ~/.ssh/id_rsa.pub.\nNext we need to make sure that the public key is in the ~/.ssh/authorized_keys file on\nall the machines in the cluster that we want to connect to. If the hadoop user’s home\ndirectory is an NFS filesystem, as described earlier, then the keys can be shared across\nthe cluster by typing:\n% cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nIf the home directory is not shared using NFS, then the public keys will need to be\nshared by some other means.\nTest that you can SSH from the master to a worker machine by making sure ssh-\nagent is running,‡ and then run ssh-add to store your passphrase. You should be able\nto ssh to a worker without entering the passphrase again.\nHadoop Configuration\nThere are a handful of files for controlling the configuration of a Hadoop installation;\nthe most important ones are listed in Table 9-1.\n‡ See its man page for instructions on how to start ssh-agent.\nHadoop Configuration | 251Table 9-1. Hadoop configuration files\nFilename Format Description\nhadoop-env.sh Bash script Environment variables that are used in the scripts to run Hadoop.\ncore-site.xml Hadoop configura- Configuration settings for Hadoop Core, such as I/O settings that are common to\n              tion XML HDFS and MapReduce.\nhdfs-site.xml Hadoop configura- Configuration settings for HDFS daemons: the namenode, the secondary name-\n              tion XML node, and the datanodes.\nmapred-site.xml Hadoop configura- Configuration settings for MapReduce daemons: the jobtracker, and the\n                tion XML tasktrackers.\nmasters Plain text A list of machines (one per line) that each run a secondary namenode.\nslaves Plain text A list of machines (one per line) that each run a datanode and a tasktracker.\nhadoop-met Java Properties Properties for controlling how metrics are published in Hadoop (see “Met-\nrics.properties rics” on page 286).\nlog4j.properties Java Properties Properties for system logfiles, the namenode audit log, and the task log for the\n                                tasktracker child process (“Hadoop User Logs” on page 142).\nThese files are all found in the conf directory of the Hadoop distribution. The config-\nuration directory can be relocated to another part of the filesystem (outside the Hadoop\ninstallation, which makes upgrades marginally easier) as long as daemons are started\nwith the --config option specifying the location of this directory on the local filesystem.\nConfiguration Management\nHadoop does not have a single, global location for configuration information. Instead,\neach Hadoop node in the cluster has its own set of configuration files, and it is up to\nadministrators to ensure that they are kept in sync across the system. Hadoop provides\na rudimentary facility for synchronizing configuration using rsync (see upcoming dis-\ncussion), alternatively there are parallel shell tools that can help do this, like dsh or pdsh.\nHadoop is designed so that it is possible to have a single set of configuration files that\nare used for all master and worker machines. The great advantage of this is simplicity,\nboth conceptually (since there is only one configuration to deal with) and operationally\n(as the Hadoop scripts are sufficient to manage a single configuration setup).\nFor some clusters, the one-size-fits-all configuration model breaks down. For example,\nif you expand the cluster with new machines that have a different hardware specifica-\ntion to the existing ones, then you need a different configuration for the new machines\nto take advantage of their extra resources.\nIn these cases, you need to have the concept of a class of machine, and maintain a\nseparate configuration for each class. Hadoop doesn’t provide tools to do this, but there\nare several excellent tools for doing precisely this type of configuration management,\nsuch as Puppet, cfengine, and bcfg2.\n252 | Chapter 9: Setting Up a Hadoop ClusterFor a cluster of any size, it can be a challenge to keep all of the machines in sync: consider\nwhat happens if the machine is unavailable when you push out an update—who en-\nsures it gets the update when it becomes available? This is a big problem and can lead\nto divergent installations, so even if you use the Hadoop control scripts for managing\nHadoop, it may be a good idea to use configuration management tools for maintaining\nthe cluster. These tools are also excellent for doing regular maintenance, such as patch-\ning security holes and updating system packages.\nControl scripts\nHadoop comes with scripts for running commands, and starting and stopping daemons\nacross the whole cluster. To use these scripts (which can be found in the bin directory),\nyou need to tell Hadoop which machines are in the cluster. There are two files for this\npurpose, called masters and slaves, each of which contains a list of the machine host-\nnames or IP addresses, one per line. The masters file is actually a misleading name, in\nthat it determines which machine or machines should run a secondary namenode. The\nslaves file lists the machines that the datanodes and tasktrackers should run on. Both\nmasters and slaves files reside in the configuration directory, although the slaves file\nmay be placed elsewhere (and given another name) by changing the HADOOP_SLAVES\nsetting in hadoop-env.sh. Also, these files do not need to be distributed to worker nodes,\nsince they are used only by the control scripts running on the namenode or jobtracker.\nYou don’t need to specify which machine (or machines) the namenode and jobtracker\nruns on in the masters file, as this is determined by the machine the scripts are run on.\n(In fact, specifying these in the masters file would cause a secondary namenode to run\nthere, which isn’t always what you want.) For example, the start-dfs.sh script, which\nstarts all the HDFS daemons in the cluster, runs the namenode on the machine the\nscript is run on. In slightly more detail, it:\n1. Starts a namenode on the local machine (the machine that the script is run on)\n2. Starts a datanode on each machine listed in the slaves file\n3. Starts a secondary namenode on each machine listed in the masters file\nThere is a similar script called start-mapred.sh, which starts all the MapReduce dae-\nmons in the cluster. More specifically, it:\n1. Starts a jobtracker on the local machine\n2. Starts a tasktracker on each machine listed in the slaves file\nNote that masters is not used by the MapReduce control scripts.\nAlso provided are stop-dfs.sh and stop-mapred.sh scripts to stop the daemons started\nby the corresponding start script.\nThese scripts start and stop Hadoop daemons using the hadoop-daemon.sh script. If\nyou use the aforementioned scripts, you shouldn’t call hadoop-daemon.sh directly. But\nif you need to control Hadoop daemons from another system or from your own scripts,\nHadoop Configuration | 253then the hadoop-daemon.sh script is a good integration point. Likewise, hadoop-dae\nmons.sh (with an “s”) is handy for starting the same daemon on a set of hosts.\nMaster node scenarios\nDepending on the size of the cluster, there are various configurations for running the\nmaster daemons: the namenode, secondary namenode, and jobtracker. On a small\ncluster (a few tens of nodes), it is convenient to put them on a single machine; however,\nas the cluster gets larger, there are good reasons to separate them.\nThe namenode has high memory requirements, as it holds file and block metadata for\nthe entire namespace in memory. The secondary namenode, while idle most of the time,\nhas a comparable memory footprint to the primary when it creates a checkpoint. (This\nis explained in detail in “The filesystem image and edit log” on page 274.) For filesys-\ntems with a large number of files, there may not be enough physical memory on one\nmachine to run both the primary and secondary namenode.\nThe secondary namenode keeps a copy of the latest checkpoint of the filesystem met-\nadata that it creates. Keeping this (stale) backup on a different node to the namenode\nallows recovery in the event of loss (or corruption) of all the namenode’s metadata files.\n(This is discussed further in Chapter 10.)\nOn a busy cluster running lots of MapReduce jobs, the jobtracker uses considerable\nmemory and CPU resources, so it should run on a dedicated node.\nWhether the master daemons run on one or more nodes, the following instructions\napply:\n• Run the HDFS control scripts from the namenode machine. The masters file should\ncontain the address of the secondary namenode.\n• Run the MapReduce control scripts from the jobtracker machine.\nWhen the namenode and jobtracker are on separate nodes, their slaves files need to be\nkept in sync, since each node in the cluster should run a datanode and a tasktracker.\nEnvironment Settings\nIn this section, we consider how to set the variables in hadoop-env.sh.\nMemory\nBy default, Hadoop allocates 1000 MB (1 GB) of memory to each daemon it runs. This\nis controlled by the HADOOP_HEAPSIZE setting in hadoop-env.sh. In addition, the task\ntracker launches separate child JVMs to run map and reduce tasks in, so we need to\nfactor these into the total memory footprint of a worker machine.\nThe maximum number of map tasks that will be run on a tasktracker at one time is\ncontrolled by the mapred.tasktracker.map.tasks.maximum property, which defaults to\n254 | Chapter 9: Setting Up a Hadoop Clustertwo tasks. There is a corresponding property for reduce tasks, mapred.task\ntracker.reduce.tasks.maximum, which also defaults to two tasks. The memory given to\neach of these child JVMs can be changed by setting the mapred.child.java.opts prop-\nerty. The default setting is -Xmx200m, which gives each task 200 MB of memory. (Inci-\ndentally, you can provide extra JVM options here, too. For example, you might enable\nverbose GC logging to debug GC.) The default configuration therefore uses 2800 MB\nof memory for a worker machine (see Table 9-2).\nTable 9-2. Worker node memory calculation\nJVM Default memory used (MB) Memory used for 8 processors, 400 MB per child (MB)\nDatanode 1000 1000\nTasktracker 1000 1000\nTasktracker child map task 2 × 200 7 × 400\nTasktracker child reduce task 2 × 200 7 × 400\nTotal 2800 7600\nThe number of tasks that can be run simultaneously on a tasktracker is governed by\nthe number of processors available on the machine. Because MapReduce jobs are nor-\nmally I/O-bound, it makes sense to have more tasks than processors to get better uti-\nlization. The amount of oversubscription depends on the CPU utilization of jobs you\nrun, but a good rule of thumb is to have a factor of between one and two more tasks\n(counting both map and reduce tasks) than processors.\nFor example, if you had 8 processors and you wanted to run 2 processes on each pro-\ncessor, then you could set mapred.tasktracker.map.tasks.maximum and mapred.task\ntracker.map.tasks.maximum to both be 7 (not 8, since the datanode and the tasktracker\neach take one slot). If you also increased the memory available to each child task to 400\nMB, then the total memory usage would be 7,600 MB (see Table 9-2).\nWhether this Java memory allocation will fit into 8 GB of physical memory depends\non the other processes that are running on the machine. If you are running Streaming\nor Pipes programs, this allocation will probably be inappropriate (and the memory\nallocated to the child should be dialled down), since it doesn’t allow enough memory\nfor users’ (Streaming or Pipes) processes to run. The thing to avoid is processes being\nswapped out, as this it leads to severe performance degradation. The precise memory\nsettings are necessarily very cluster-dependent, and can be optimized over time with\nexperience gained from monitoring the memory usage across the cluster. Tools like\nGanglia (“GangliaContext” on page 288) are good for gathering this information.\nHadoop also provides settings to control how much memory is used for MapReduce\noperations. These can be set on a per-job basis, and are covered in the section on\n“Shuffle and Sort” on page 163.\nHadoop Configuration | 255For the master node, each of the namenode, secondary namenode, and jobtracker dae-\nmons uses 1,000 MB by default, a total of 3,000 MB.\nA namenode can eat up memory, since a reference to every block of every\nfile is maintained in memory. For example, 1,000 MB is enough for a\nfew million files. You can increase the namenode’s memory without\nchanging the memory allocated to other Hadoop daemons by setting\nHADOOP_NAMENODE_OPTS in hadoop-env.sh to include a JVM option for set-\nting the memory size. HADOOP_NAMENODE_OPTS allows you to pass extra\noptions to the namenode’s JVM. So, for example, if using a Sun JVM,\n-Xmx2000m would specify that 2000 MB of memory should be allocated\nto the namenode.\nIf you change the namenode’s memory allocation, don’t forget to do the\nsame for the secondary namenode (using the HADOOP_SECONDARYNAME\nNODE_OPTS variable), since its memory requirements are comparable to\nthe primary namenode’s. You will probably also want to run the sec-\nondary namenode on a different machine, in this case.\nThere are corresponding environment variables for the other Hadoop\ndaemons, so you can customize their memory allocations, if desired. See\nhadoop-env.sh for details.\nJava\nThe location of the Java implementation to use is determined by the JAVA_HOME setting\nin hadoop-env.sh, or from the JAVA_HOME shell environment variable, if not set in hadoop-\nenv.sh. It’s a good idea to set the value in hadoop-env.sh, so that it is clearly defined in\none place, and to ensure that the whole cluster is using the same version of Java.\nSystem logfiles\nSystem logfiles produced by Hadoop are stored in $HADOOP_INSTALL/logs by default.\nThis can be changed using the HADOOP_LOG_DIR setting in hadoop-env.sh. It’s a good idea\nto change this so that logfiles are kept out of the directory that Hadoop is installed in,\nsince this keeps logfiles in one place even after the installation directory changes after\nan upgrade. A common choice is /var/log/hadoop, set by including the following line in\nhadoop-env.sh:\nexport HADOOP_LOG_DIR=/var/log/hadoop\nThe log directory will be created if it doesn’t already exist (if not, confirm that the\nHadoop user has permission to create it). Each Hadoop daemon running on a machine\nproduces two logfiles. The first is the log output written via log4j. This file, which ends\nin .log, should be the first port of call when diagnosing problems, since most application\nlog messages are written here. The standard Hadoop log4j configuration uses a Daily\nRolling File Appender to rotate logfiles. Old logfiles are never deleted, so you should\n256 | Chapter 9: Setting Up a Hadoop Clusterarrange for them to be periodically deleted or archived, so as to not run out of disk\nspace on the local node.\nThe second logfile is the combined standard output and standard error log. This logfile,\nwhich ends in .out, usually contains little or no output, since Hadoop uses log4j for\nlogging. It is only rotated when the daemon is restarted, and only the last five logs are\nretained. Old logfiles are suffixed with a number between 1 and 5, with 5 being the\noldest file.\nLogfile names (of both types) are a combination of the name of the user running the\ndaemon, the daemon name, and the machine hostname. For example, hadoop-tom-\ndatanode-sturges.local.log.2008-07-04 is the name of a logfile after it has been rotated.\nThis naming structure makes it possible to archive logs from all machines in the cluster\nin a single directory, if needed, since the filenames are unique.\nThe username in the logfile name is actually the default for the HADOOP_IDENT_STRING\nsetting in hadoop-env.sh. If you wish to give the Hadoop instance a different identity\nfor the purposes of naming the logfiles, change HADOOP_IDENT_STRING to be the identifier\nyou want.\nSSH settings\nThe control scripts allow you to run commands on (remote) worker nodes from the\nmaster node using SSH. It can be useful to customize the SSH settings, for various\nreasons. For example, you may want to reduce the connection timeout (using the\nConnectTimeout option) so the control scripts don’t hang around waiting to see whether\na dead node is going to respond. Obviously, this can be taken too far. If the timeout is\ntoo low, then busy nodes will be skipped, which is bad.\nAnother useful SSH setting is StrictHostKeyChecking, which can be set to no to auto-\nmatically add new host keys to the known hosts files. The default, ask, is to prompt\nthe user to confirm they have verified the key fingerprint, which is not a suitable setting\nin a large cluster environment.§\nTo pass extra options to SSH, define the HADOOP_SSH_OPTS environment variable in\nhadoop-env.sh. See the ssh and ssh_config manual pages for more SSH settings.\nThe Hadoop control scripts can distribute configuration files to all nodes of the cluster\nusing rsync. This is not enabled by default, but by defining the HADOOP_MASTER setting\nin hadoop-env.sh, worker daemons will rsync the tree rooted at HADOOP_MASTER to the\nlocal node’s HADOOP_INSTALL whenever the daemon starts up.\nWhat if you have two masters—a namenode and a jobtracker on separate machines?\nYou can pick one as the source, and the other can rsync from it, along with all the\n§ For more discussion on the security implications of SSH Host Keys, consult the article “SSH Host Key\nProtection” by Brian Hatch at http://www.securityfocus.com/infocus/1806.\nHadoop Configuration | 257workers. In fact, you could use any machine, even one outside the Hadoop cluster, to\nrsync from.\nBecause HADOOP_MASTER is unset by default, there is a bootstrapping problem: how do\nwe make sure hadoop-env.sh with HADOOP_MASTER set is present on worker nodes? For\nsmall clusters, it is easy to write a small script to copy hadoop-env.sh from the master\nto all of the worker nodes. For larger clusters, tools like dsh can do the copies in parallel.\nAlternatively, a suitable hadoop-env.sh can be created as a part of the automated in-\nstallation script (such as Kickstart).\nWhen starting a large cluster with rsyncing enabled, the worker nodes can overwhelm\nthe master node with rsync requests since the workers start at around the same time.\nTo avoid this, set the HADOOP_SLAVE_SLEEP setting to a small number of seconds, such\nas 0.1, for one-tenth of a second. When running commands on all nodes of the cluster,\nthe master will sleep for this period between invoking the command on each worker\nmachine in turn.\nImportant Hadoop Daemon Properties\nHadoop has a bewildering number of configuration properties. In this section, we ad-\ndress the ones that you need to define (or at least understand why the default is ap-\npropriate) for any real-world working cluster. These properties are set in the Hadoop\nsite files: core-site.xml, hdfs-site.xml, and mapred-site.xml. Example 9-1 shows a typical\nexample set of files. Notice that most are marked as final, in order to prevent them from\nbeing overridden by job configurations. You can learn more about how to write Ha-\ndoop’s configuration files in “The Configuration API” on page 116.\nExample 9-1. A typical set of site configuration files\n<?xml version=""1.0""?>\n<!-- core-site.xml -->\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://namenode/</value>\n<final>true</final>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- hdfs-site.xml -->\n<configuration>\n<property>\n<name>dfs.name.dir</name>\n<value>/disk1/hdfs/name,/remote/hdfs/name</value>\n<final>true</final>\n</property>\n<property>\n<name>dfs.data.dir</name>\n258 | Chapter 9: Setting Up a Hadoop Cluster<value>/disk1/hdfs/data,/disk2/hdfs/data</value>\n<final>true</final>\n</property>\n<property>\n<name>fs.checkpoint.dir</name>\n<value>/disk1/hdfs/namesecondary,/disk2/hdfs/namesecondary</value>\n<final>true</final>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- mapred-site.xml -->\n<configuration>\n<property>\n<name>mapred.job.tracker</name>\n<value>jobtracker:8021</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.local.dir</name>\n<value>/disk1/mapred/local,/disk2/mapred/local</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.system.dir</name>\n<value>/tmp/hadoop/mapred/system</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.tasktracker.map.tasks.maximum</name>\n<value>7</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.tasktracker.reduce.tasks.maximum</name>\n<value>7</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.child.java.opts</name>\n<value>-Xmx400m</value>\n<!-- Not marked as final so jobs can include JVM debugging options -->\n</property>\n</configuration>\nHDFS\nTo run HDFS, you need to designate one machine as a namenode. In this case, the\nproperty fs.default.name is a HDFS filesystem URI, whose host is the namenode’s\nHadoop Configuration | 259hostname or IP address, and port is the port that the namenode will listen on for RPCs.\nIf no port is specified, the default of 8020 is used.\nThe masters file that is used by the control scripts is not used by the\nHDFS (or MapReduce) daemons to determine hostnames. In fact, be-\ncause the masters file is only used by the scripts, you can ignore it if you\ndon’t use them.\nThe fs.default.name property also doubles as specifying the default filesystem. The\ndefault filesystem is used to resolve relative paths, which are handy to use since they\nsave typing (and avoid hardcoding knowledge of a particular namenode’s address). For\nexample, with the default filesystem defined in Example 9-1, the relative URI /a/b is\nresolved to hdfs://namenode/a/b.\nIf you are running HDFS, the fact that fs.default.name is used to specify\nboth the HDFS namenode and the default filesystem means HDFS has\nto be the default filesystem in the server configuration. Bear in mind,\nhowever, that it is possible to specify a different filesystem as the default\nin the client configuration, for convenience.\nFor example, if you use both HDFS and S3 filesystems, then you have\na choice of specifying either as the default in the client configuration,\nwhich allows you to refer to the default with a relative URI, and the\nother with an absolute URI.\nThere are a few other configuration properties you should set for HDFS: those that set\nthe storage directories for the namenode and for datanodes. The property\ndfs.name.dir specifies a list of directories where the namenode stores persistent file-\nsystem metadata (the edit log, and the filesystem image). A copy of each of the metadata\nfiles is stored in each directory for redundancy. It’s common to configure\ndfs.name.dir so that the namenode metadata is written to one or two local disks, and\na remote disk, such as a NFS-mounted directory. Such a setup guards against failure\nof a local disk, and failure of the entire namenode, since in both cases the files can be\nrecovered and used to start a new namenode. (The secondary namenode takes only\nperiodic checkpoints of the namenode, so it does not provide an up-to-date backup of\nthe namenode.)\nYou should also set the dfs.data.dir property, which specifies a list of directories for\na datanode to store its blocks. Unlike the namenode, which uses multiple directories\nfor redundancy, a datanode round-robins writes between its storage directories, so for\nperformance you should specify a storage directory for each local disk. Read perform-\nance also benefits from having multiple disks for storage, because blocks will be spread\nacross them, and concurrent reads for distinct blocks will be correspondingly spread\nacross disks.\n260 | Chapter 9: Setting Up a Hadoop ClusterFor maximum performance, you should mount storage disks with the\nnoatime option. This setting means that last accessed time information\nis not written on file reads, which gives significant performance gains.\nFinally, you should configure where the secondary namenode stores its checkpoints of\nthe filesystem. The fs.checkpoint.dir property specifies a list of directories where the\ncheckpoints are kept. Like the storage directories for the namenode, which keep re-\ndundant copies of the namenode metadata, the checkpointed filesystem image is stored\nin each checkpoint directory for redundancy.\nTable 9-3 summarizes the important configuration properties for HDFS.\nTable 9-3. Important HDFS daemon properties\nProperty name Type Default value Description\nfs.default.name URI file:/// The default filesystem. The URI defines\n                            the hostname and port that the job-\n                           tracker’s RPC server runs on. The default\n                            port is 8020. This property should be set\n                           in core-site.xml.\ndfs.name.dir comma-separated di- ${hadoop.tmp.dir}/ The list of directories where the name-\n             rectory names dfs/name node stores its persistent metadata.\n                                   The namenode stores a copy of the\n                                  metadata in each directory in the list.\ndfs.data.dir comma-separated di- ${hadoop.tmp.dir}/ A list of directories where the datanode\n             rectory names dfs/data stores blocks.\nfs.check comma-separated di- ${hadoop.tmp.dir}/ A list of directories where the\npoint.dir rectory names dfs/namesecondary secondary namenode stores check-\n                                         points. It stores a copy of the checkpoint\n                                        in each directory in the list.\nNote that the storage directories for HDFS are under Hadoop’s tempo-\nrary directory by default (the hadoop.tmp.dir property, whose default\nis /tmp/hadoop-${user.name}). Therefore it is critical that these proper-\nties are set so that data is not lost by the system clearing out temporary\ndirectories.\nMapReduce\nTo run MapReduce, you need to designate one machine as a jobtracker, which on small\nclusters may be the same machine as the namenode. To do this, set the\nmapred.job.tracker property to the hostname or IP address and port that the jobtracker\nwill listen on. Note that this property is not a URI, but a host-port pair, separated by\na colon. The port number 8021 is a common choice.\nHadoop Configuration | 261During a MapReduce job, intermediate data and working files are written to temporary\nlocal files. Since this data includes the potentially very large output of map tasks, you\nneed to ensure that the mapred.local.dir property, which controls the location of local\ntemporary storage, is configured to use disk partitions that are large enough. The\nmapred.local.dir property takes a comma-separated list of directory names, and you\nshould use all available local disks to spread disk I/O. Typically, you will use the same\ndisks and partitions (but different directories) for MapReduce temporary data as you\nuse for datanode block storage, as governed by the dfs.data.dir property, discussed\nearlier.\nMapReduce uses a distributed filesystem to share files (such as the job JAR file) with\nthe tasktrackers that run the MapReduce tasks. The mapred.system.dir property is used\nto specify a directory where these files can be stored. This directory is resolved relative\nto the default filesystem (configured in fs.default.name), which is usually HDFS.\nFinally, you should set the mapred.tasktracker.map.tasks.maximum and mapred.task\ntracker.reduce.tasks.maximum properties to reflect the number of available cores on\nthe tasktracker machines and mapred.child.java.opts to reflect the amount of memory\navailable for the tasktracker child JVMs. See the discussion in “Memory”\non page 254.\nTable 9-4 summarizes the important configuration properties for HDFS.\nTable 9-4. Important MapReduce daemon properties\nProperty name Type Default value Description\nmapred.job.tracker hostname and port local The hostname and port that the\n                                          jobtracker’s RPC server runs on. If\n                                           set to the default value of\n                                          local, then the jobtracker is run\n                                         in-process on demand when you\n                                        run a MapReduce job (you don’t\n                                         need to start the MapReduce dae-\n                                        mons in this case).\nmapred.local.dir comma-separated di- ${hadoop.tmp.dir} A list of directories where the\n                 rectory names /mapred/local MapReduce stores intermediate\n                                            data for jobs. The data is cleared\n                                           out when the job ends.\nmapred.system.dir URI ${hadoop.tmp.dir} The directory relative to\n                      /mapred/system fs.default.name where\n                                    shared files are stored, during a\n                                   job run.\nmapred.task int 2 The number of map tasks that\ntracker.map.tasks. may be run on a tasktracker at any\nmaximum one time.\n262 | Chapter 9: Setting Up a Hadoop ClusterProperty name Type Default value Description\nmapred.task int 2 The number of reduce tasks that\ntracker.reduce.tasks. may be run on a tasktracker at any\nmaximum one time.\nmapred.child.java.opts String -Xmx200m The JVM options used to launch\n                                      the tasktracker child process that\n                                     runs map and reduce tasks. This\n                                    property can be set on a per-job\n                                   basis, which can be useful for set-\n                                  ting JVM properties for debug-\n                                 ging, for example.\nHadoop Daemon Addresses and Ports\nHadoop daemons generally run both an RPC server (Table 9-5) for communication\nbetween daemons, and a HTTP server to provide web pages for human consumption\n(Table 9-6). Each server is configured by setting the network address and port number\nto listen on. By specifying the network address as 0.0.0.0, Hadoop will bind to all\naddresses on the machine. Alternatively, you can specify a single address to bind to. A\nport number of 0 instructs the server to start on a free port: this is generally discouraged,\nsince it is incompatible with setting cluster-wide firewall policies.\nTable 9-5. RPC server properties\nProperty name Default value Description\nfs.default.name file:/// When set to an HDFS URI, this property determines\n                        the namenode’s RPC server address and port. The\n                         default port is 8020 if not specified.\ndfs.datanode.ipc.address 0.0.0.0:50020 The datanode’s RPC server address and port.\nmapred.job.tracker local When set to a hostname and port, this property\n                        specifies the jobtracker’s RPC server address and\n                         port. A commonly used port is 8021.\nmapred.task.tracker.report.address 127.0.0.1:0 The tasktracker’s RPC server address and port. This\n                                                is used by the tasktracker’s child JVM to commu-\n                                                 nicate with the tasktracker. Using any free port is\n                                                acceptable in this case, as the server only binds to\n                                               the loopback address. You should change this set-\n                                              ting only if the machine has no loopback\n                                             address.\nIn addition to an RPC server, datanodes run a TCP/IP server for block transfers. The\nserver address and port is set by the dfs.datanode.address property, and has a default\nvalue of 0.0.0.0:50010.\nHadoop Configuration | 263Table 9-6. HTTP server properties\nProperty name Default value Description\nmapred.job.tracker.http.address 0.0.0.0:50030 The jobtracker’s HTTP server address and port.\nmapred.task.tracker.http.address 0.0.0.0:50060 The tasktracker’s HTTP server address and port.\ndfs.http.address 0.0.0.0:50070 The namenode’s HTTP server address and port.\ndfs.datanode.http.address 0.0.0.0:50075 The datanode’s HTTP server address and port.\ndfs.secondary.http.address 0.0.0.0:50090 The secondary namenode’s HTTP server address and\n                                          port.\nThere are also settings for controlling which network interfaces the datanodes and\ntasktrackers report as their IP addresses (for HTTP and RPC servers). The relevant\nproperties are dfs.datanode.dns.interface and mapred.tasktracker.dns.interface,\nboth of which are set to default, which will use the default network interface. You can\nset this explicitly to report the address of a particular interface; eth0, for example.\nOther Hadoop Properties\nThis section discusses some other properties that you might consider setting.\nCluster membership\nTo aid the addition and removal of nodes in the future, you can specify a list of au-\nthorized machines that may join the cluster as datanodes or tasktrackers. The list is\nspecified using the dfs.hosts (for datanodes) and mapred.hosts (for tasktrackers) prop-\nerties, as well as the corresponding dfs.hosts.exclude and mapred.hosts.exclude files\nused for decommissioning. See “Commissioning and Decommissioning No-\ndes” on page 293 for further discussion.\nService-level authorization\nYou can define ACLs to control which users and groups have permission to connect to\neach Hadoop service. Services are defined at the protocol level, so there are ones for\nMapReduce job submission, namenode communication, and so on. By default, au-\nthorization is turned off (see the hadoop.security.authorization property) so all users\nmay access all services. See the hadoop-policy.xml configuration file for more\ninformation.\nBuffer size\nHadoop uses a buffer size of 4 KB (4096 bytes) for its I/O operations. This is a conser-\nvative setting, and with modern hardware and operating systems, you will likely see\nperformance benefits by increasing it. 64 KB (65536 bytes) or 128 KB (131072 bytes)\nare common choices. Set this using the io.file.buffer.size property in core-site.xml.\n264 | Chapter 9: Setting Up a Hadoop ClusterHDFS block size\nThe HDFS block size is 64 MB by default, but many clusters use 128 MB (134,217,728\nbytes) or even 256 MB (268,435,456 bytes) to give mappers more data to work on. Set\nthis using the dfs.block.size property in hdfs-site.xml.\nReserved storage space\nBy default, datanodes will try to use all of the space available in their storage directories.\nIf you want to reserve some space on the storage volumes for non-HDFS use, then you\ncan set dfs.datanode.du.reserved to the amount, in bytes, of space to reserve.\nTrash\nHadoop filesystems have a trash facility, in which deleted files are not actually deleted,\nbut rather are moved to a trash folder, where they remain for a minimum period before\nbeing permanently deleted by the system. The minimum period in minutes that a file\nwill remain in the trash is set using the fs.trash.interval configuration property in\ncore-site.xml. By default the trash interval is zero, which disables trash.\nLike in many operating systems, Hadoop’s trash facility is a user-level feature, meaning\nthat only files that are deleted using the filesystem shell are put in the trash. Files deleted\nprogrammatically are deleted immediately. It is possible to use the trash programmat-\nically, however, by constructing a Trash instance, then calling its moveToTrash() method\nwith the Path of the file intended for deletion. The method returns a value indicating\nsuccess; a value of false means either that trash is not enabled or that the file is already\nin the trash.\nWhen trash is enabled, each user has her own trash directory called .Trash in her home\ndirectory. File recovery is simple: you look for the file in a subdirectory of .Trash and\nmove it out of the trash subtree.\nHDFS will automatically delete files in trash folders, but other filesystems will not, so\nyou have to arrange for this to be done periodically. You can expunge the trash, which\nwill delete files that have been in the trash longer than their minimum period, using\nthe filesystem shell:\n% hadoop fs -expunge\nThe Trash class exposes an expunge() method that has the same effect.\nTask memory limits\nOn a shared cluster, it shouldn’t be possible for one user’s errant MapReduce program\nto bring down nodes in the cluster. This can happen if the map or reduce task has a\nmemory leak, for example, because the machine on which the tasktracker is running\nwill run out of memory and may affect the other running processes. To prevent this\nsituation, you can set mapred.child.ulimit, which sets a maximum limit on the virtual\nmemory of the child process launched by the tasktracker. It is set in kilobytes, and\nHadoop Configuration | 265should\nbe\ncomfortably\nlarger\nthan\nthe\nmemory\nof\nthe\nJVM\nset\nby\nmapred.child.java.opts; otherwise, the child JVM might not start.\nAs an alternative, you can use limits.conf to set process limits at the operating system\nlevel.\nJob scheduler\nParticularly in a multiuser MapReduce setting, consider changing the default FIFO job\nscheduler to one of the more fully featured alternatives. See “Job Schedul-\ning” on page 161.\nPost Install\nOnce you have a Hadoop cluster up and running, you need to give users access to it.\nThis involves creating a home directory for each user, and setting ownership permis-\nsions on it:\n% hadoop fs -mkdir /user/username\n% hadoop fs -chown username:username /user/username\nThis is a good time to set space limits on the directory. The following sets a 1 TB limit\non the given user directory:\n% hadoop dfsadmin -setSpaceQuota 1t /user/username\nBenchmarking a Hadoop Cluster\nIs the cluster set up correctly? The best way to answer this question is empirically: run\nsome jobs and confirm that you get the expected results. Benchmarks make good tests,\nas you also get numbers that you can compare with other clusters as a sanity check on\nwhether your new cluster is performing roughly as expected. And you can tune a cluster\nusing benchmark results to squeeze the best performance out of it. This is often done\nwith monitoring systems in place (“Monitoring” on page 285), so you can see how\nresources are being used across the cluster.\nTo get the best results, you should run benchmarks on a cluster that is not being used\nby others. In practice, this is just before it is put into service, and users start relying on\nit. Once users have periodically scheduled jobs on a cluster it is generally impossible\nto find a time when the cluster is not being used (unless you arrange downtime with\nusers), so you should run benchmarks to your satisfaction before this happens.\nExperience has shown that most hardware failures for new systems are hard drive fail-\nures. By running I/O intensive benchmarks—such as the ones described next—you\ncan “burn in” the cluster before it goes live.\n266 | Chapter 9: Setting Up a Hadoop ClusterHadoop Benchmarks\nHadoop comes with several benchmarks that you can run very easily with minimal\nsetup cost. Benchmarks are packaged in the test JAR file, and you can get a list of them,\nwith descriptions, by invoking the JAR file with no arguments:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar\nMost of the benchmarks show usage instructions when invoked with no arguments.\nFor example:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO\nTestFDSIO.0.0.4\nUsage: TestFDSIO -read | -write | -clean [-nrFiles N] [-fileSize MB] [-resFile\nresultFileName] [-bufferSize Bytes]\nBenchmarking HDFS with TestDFSIO\nTestDFSIO tests the I/O performance of HDFS. It does this by using a MapReduce job\nas a convenient way to read or write files in parallel. Each file is read or written in a\nseparate map task, and the output of the map is used for collecting statistics relating\nto the file just processed. The statistics are accumulated in the reduce, to produce a\nsummary.\nThe following command writes 10 files of 1,000 MB each:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -write -nrFiles 10\n-fileSize 1000\nAt the end of the run, the results are written to the console and also recorded in a local\nfile (which is appended to, so you can rerun the benchmark and not lose old results):\n% cat TestDFSIO_results.log\n----- TestDFSIO ----- : write\nDate & time: Sun Apr 12 07:14:09 EDT 2009\nNumber of files: 10\nTotal MBytes processed: 10000\nThroughput mb/sec: 7.796340865378244\nAverage IO rate mb/sec: 7.8862199783325195\nIO rate std deviation: 0.9101254683525547\nTest exec time sec: 163.387\nThe files are written under the /benchmarks/TestDFSIO directory by default (this can\nbe changed by setting the test.build.data system property), in a directory called\nio_data.\nTo run a read benchmark, use the -read argument. Note that these files must already\nexist (having been written by TestDFSIO -write):\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -read -nrFiles 10\n-fileSize 1000\nHere are the results for a real run:\nBenchmarking a Hadoop Cluster | 267----- TestDFSIO ----- :\nDate & time:\nNumber of files:\nTotal MBytes processed:\nThroughput mb/sec:\nAverage IO rate mb/sec:\nIO rate std deviation:\nTest exec time sec:\nread\nSun Apr 12 07:24:28 EDT 2009\n10\n10000\n80.25553361904304\n98.6801528930664\n36.63507598174921\n47.624\nWhen you’ve finished benchmarking, you can delete all the generated files from HDFS\nusing the -clean argument:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -clean\nBenchmarking MapReduce with Sort\nHadoop comes with a MapReduce program that does a partial sort of its input. It is\nvery useful for benchmarking the whole MapReduce system, as the full input dataset\nis transferred through the shuffle. The three steps are: generate some random data,\nperform the sort, then validate the results.\nFirst we generate some random data using RandomWriter. It runs a MapReduce job with\n10 maps per node, and each map generates (approximately) 10 GB of random binary\ndata, with key and values of various sizes. You can change these values if you like by\nsetting the properties test.randomwriter.maps_per_host and test.random\nwrite.bytes_per_map. There are also settings for the size ranges of the keys and values;\nsee RandomWriter for details.\nHere’s how to invoke RandomWriter (found in the example JAR file, not the test one) to\nwrite its output to a directory called random-data:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar randomwriter random-data\nNext we can run the Sort program:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort random-data sorted-data\nThe overall execution time of the sort is the metric we are interested in, but it’s in-\nstructive to watch the job’s progress via the web UI (http://jobtracker-host:50030/),\nwhere you can get a feel for how long each phase of the job takes. Adjusting the pa-\nrameters mentioned in “Tuning a Job” on page 145 is a useful exercise, too.\nAs a final sanity check, we validate the data in sorted-data is, in fact, correctly sorted:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar testmapredsort -sortInput random-data \\\n-sortOutput sorted-data\nThis command runs the SortValidator program, which performs a series of checks on\nthe unsorted and sorted data to check whether the sort is accurate. It reports the out-\ncome to the console at the end of its run:\nSUCCESS! Validated the MapReduce framework's 'sort' successfully.\n268 | Chapter 9: Setting Up a Hadoop ClusterOther benchmarks\nThere are many more Hadoop benchmarks, but the following are widely used:\n• MRBench (invoked with mrbench) runs a small job a number of times. It acts as a good\ncounterpoint to sort, as it checks whether small job runs are responsive.\n• NNBench (invoked with nnbench) is useful for load testing namenode hardware.\n• Gridmix is a suite of benchmarks designed to model a realistic cluster workload,\nby mimicking a variety of data-access patterns seen in practice. See src/benchmarks/\ngridmix2 in the distribution for further details.‖\nUser Jobs\nFor tuning, it is best to include a few jobs that are representative of the jobs that your\nusers run, so your cluster is tuned for these and not just for the standard benchmarks.\nIf this is your first Hadoop cluster and you don’t have any user jobs yet, then Gridmix\nis a good substitute.\nWhen running your own jobs as benchmarks you should select a dataset for your user\njobs that you use each time you run the benchmarks to allow comparisons between\nruns. When you set up a new cluster, or upgrade a cluster, you will be able to use the\nsame dataset to compare the performance with previous runs.\nHadoop in the Cloud\nAlthough many organizations choose to run Hadoop in-house, it is also popular to run\nHadoop in the cloud on rented hardware or as a service. For instance, Cloudera offers\ntools for running Hadoop (see Appendix B) in a public or private cloud.\nIn this section, we look at running Hadoop on Amazon EC2, which is a great way to\ntry out your own Hadoop cluster on a low-commitment, trial basis.\nHadoop on Amazon EC2\nAmazon Elastic Compute Cloud (EC2) is a computing service that allows customers\nto rent computers (instances) on which they can run their own applications. A customer\ncan launch and terminate instances on demand, paying by the hour for active instances.\nHadoop comes with a set of scripts that make is easy to run Hadoop on EC2. The scripts\nallow you to perform such operations as launching or terminating a cluster, or adding\ninstances to an existing cluster.\nRunning Hadoop on EC2 is especially appropriate for certain workflows. For example,\nif you store data on Amazon S3, then you can run a cluster on EC2 and run MapReduce\n‖ In a similar vein, PigMix is a set of benchmarks for Pig available from http://wiki.apache.org/pig/PigMix.\nHadoop in the Cloud | 269jobs that read the S3 data, and write output back to S3, before shutting down the cluster.\nIf you’re working with longer-lived clusters, you might copy S3 data onto HDFS run-\nning on EC2 for more efficient processing, as HDFS can take advantage of data locality,\nbut S3 cannot (since S3 storage is not colocated with EC2 nodes).\nEC2 Network Topology\nThe EC2 APIs offer no control over network topology to clients. At the time of this\nwriting, it is not possible to give hints to direct placement of instances—to request that\nthey are split between two racks, say. Nor is it possible to discover in which rack an\ninstance is located after it has booted, which means that Hadoop considers the whole\ncluster to be situated in one rack (“Rack awareness” on page 247), even though in reality\ninstances are likely to be spread across racks. Hadoop will still work, but not as effi-\nciently as it might otherwise.\nSetup\nBefore you can run Hadoop on EC2, you need to work through Amazon’s Getting\nStarted Guide (linked from the EC2 website http://aws.amazon.com/ec2/), which goes\nthrough setting up an account, installing the EC2 command-line tools, and launching\nan instance.\nNext, get a copy of the Hadoop EC2 launch scripts. If you have already installed Apache\nHadoop on your workstation (Appendix A), you will find them in the src/contrib/ec2\nsubdirectory of the installation. If you haven’t installed Hadoop, then do so now. (Note\nthat the same scripts can be used to launch a cluster running Apache Hadoop, or Clou-\ndera’s Distribution for Hadoop, described in Appendix B.)\nYou need to configure the scripts to set your Amazon Web Service credentials, security\nkey details, the version of Hadoop to run, and the EC2 instance type to use (the extra-\nlarge instances are most suitable for running Hadoop). Do this by editing the file src/\ncontrib/ec2/bin/hadoop-ec2-env.sh: the comments in the file document how to set each\nvariable.\nLaunching a cluster\nWe are now ready to launch a cluster. To launch a cluster named test-hadoop-cluster\nwith one master node (running the namenode and jobtracker) and five worker nodes\n(running the datanodes and tasktackers), type:\n% bin/hadoop-ec2 launch-cluster test-hadoop-cluster 5\nThis will create EC2 security groups for the cluster, if they don’t already exist, and give\nthe master and worker nodes unfettered access to one another. It will also enable SSH\naccess from anywhere. Once the security groups have been set up, the master instance\nwill be launched, then once it has started, the five worker instances will be launched.\nThe reason that the worker nodes are launched separately is so that the master’s\n270 | Chapter 9: Setting Up a Hadoop Clusterhostname can be passed to the worker instances, and allow the datanodes and task-\ntrackers to connect to the master when they start up.\nRunning a MapReduce job\nJobs need to be run from within EC2 (so EC2 hostnames resolve correctly), and to do\nthis we need to transfer the job JAR file to the cluster. The scripts provide a convenient\nway to do this. The following command copies the JAR file to the master node:\n% bin/hadoop-ec2 push test-hadoop-cluster /Users/tom/htdg-examples/job.jar\nThere is also a shortcut for logging into the cluster. This command logs into the master\n(using SSH), but it is also possible to specify the EC2 instance ID instead of the cluster\nname to log into any node in the cluster. (There is also a shortcut for the screen com-\nmand that you may prefer.)\n% bin/hadoop-ec2 login test-hadoop-cluster\nThe cluster’s filesystem is empty, so before we run a job, we need to populate it with\ndata. Doing a parallel copy from S3 (see “Hadoop Filesystems” on page 47 for more on\nthe S3 filesystems in Hadoop) using Hadoop’s distcp tool is an efficient way to transfer\ndata into HDFS:\n# hadoop distcp s3n://hadoopbook/ncdc/all input/ncdc/all\nAfter the data has been copied, we can run a job in the usual way:\n# hadoop jar job.jar MaxTemperatureWithCombiner input/ncdc/all output\nAlternatively, we could have specified the input to be S3, which would have the same\neffect. When running multiple jobs over the same input data, it’s best to copy the data\nto HDFS first to save bandwidth:\n# hadoop jar job.jar MaxTemperatureWithCombiner s3n://hadoopbook/ncdc/all output\nWe can track the progress of the job using the jobtracker’s web UI, found at http://\nmaster_host:50030/.\nTerminating a cluster\nTo shut down the cluster, first log out from the EC2 node and issue the terminate-\ncluster command from your workstation:\n# exit\n% bin/hadoop-ec2 terminate-cluster test-hadoop-cluster\nYou will be asked to confirm that you want to terminate all the instances in the cluster.\nHadoop in the Cloud | 271CHAPTER 10\nAdministering Hadoop\nThe previous chapter was devoted to setting up a Hadoop cluster. In this chapter, we\nlook at the procedures to keep a cluster running smoothly.\nHDFS\nPersistent Data Structures\nAs an administrator, it is invaluable to have a basic understanding of how the compo-\nnents of HDFS—the namenode, the secondary namenode, and the datanodes—\norganize their persistent data on disk. Knowing which files are which can help you\ndiagnose problems, or spot that something is awry.\nNamenode directory structure\nA newly formatted namenode creates the following directory structure:\n${dfs.name.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\nRecall from Chapter 9 that the dfs.name.dir property is a list of directories, with the\nsame contents mirrored in each directory. This mechanism provides resilience, partic-\nularly if one of the directories is an NFS mount, as is recommended.\nThe VERSION file is a Java properties file that contains information about the version\nof HDFS that is running. Here are the contents of a typical file:\n#Tue Mar 10 19:21:36 GMT 2009\nnamespaceID=134368441\ncTime=0\nstorageType=NAME_NODE\nlayoutVersion=-18\n273The layoutVersion is a negative integer that defines the version of HDFS’s persistent\ndata structures. This version number has no relation to the release number of the Ha-\ndoop distribution. Whenever the layout changes the version number is decremented\n(for example, the version after −18 is −19). When this happens, HDFS needs to be\nupgraded, since a newer namenode (or datanode) will not operate if its storage layout\nis an older version. Upgrading HDFS is covered in “Upgrades” on page 296.\nThe namespaceID is a unique identifier for the filesystem, which is created when the\nfilesystem is first formatted. The namenode uses it to identify new datanodes, since\nthey will not know the namespaceID until they have registered with the namenode.\nThe cTime property marks the creation time of the namenode’s storage. For newly for-\nmatted storage, the value is always zero, but it is updated to a timestamp whenever the\nfilesystem is upgraded.\nThe storageType indicates that this storage directory contains data structures for a\nnamenode.\nThe other files in the namenode’s storage directory are edits, fsimage, and fstime. These\nare all binary files, which use Hadoop Writable objects as their serialization format (see\n“Serialization” on page 86). To understand what these files are for, we need to dig into\nthe workings of the namenode a little more.\nThe filesystem image and edit log\nWhen a filesystem client performs a write operation (such as creating or moving a file),\nit is first recorded in the edit log. The namenode also has an in-memory representation\nof the filesystem metadata, which it updates after the edit log has been modified. The\nin-memory metadata is used to serve read requests.\nThe edit log is flushed and synced after every write before a success code is returned to\nthe client. For namenodes that write to multiple directories, the write must be flushed\nand synced to every copy before returning successfully. This ensures that no operation\nis lost due to machine failure.\nThe fsimage file is a persistent checkpoint of the filesystem metadata. However, it is\nnot updated for every filesystem write operation, since writing out the fsimage file,\nwhich can grow to be gigabytes in size, would be very slow. This does not compromise\nresilience, however, because if the namenode fails, then the latest state of its metadata\ncan be reconstructed by loading the fsimage from disk into memory, then applying each\nof the operations in the edit log. In fact, this is precisely what the namenode does when\nit starts up (see “Safe Mode” on page 278).\n274 | Chapter 10: Administering HadoopThe fsimage file contains a serialized form of all the directory and file\ninodes in the filesystem. Each inode is an internal representation of a\nfile or directory’s metadata, and contains such information as the file’s\nreplication level, modification and access times, access permissions,\nblock size, and the blocks a file is made up of. For directories, the mod-\nification time, permissions, and quota metadata is stored.\nThe fsimage file does not record the datanodes on which the blocks are\nstored. Instead the namenode keeps this mapping in memory, which it\nconstructs by asking the datanodes for their block lists when they join\nthe cluster, and periodically afterward to ensure the namenode’s block\nmapping is up-to-date.\nAs described, the edits file would grow without bound. Though this state of affairs\nwould have no impact on the system while the namenode is running, if the namenode\nwere restarted, it would take a long time to apply each of the operations in its (very\nlong) edit log. During this time the filesystem would be offline, which is generally\nundesirable.\nThe solution is to run the secondary namenode, whose purpose is to produce check-\npoints of the primary’s in-memory filesystem metadata.* The checkpointing process\nproceeds as follows (and is shown schematically in Figure 10-1):\n1. The secondary asks the primary to roll its edits file, so new edits go to a new file.\n2. The secondary retrieves fsimage and edits from the primary (using HTTP GET).\n3. The secondary loads fsimage into memory, applies each operation from edits, then\ncreates a new consolidated fsimage file.\n4. The secondary sends the new fsimage back to the primary (using HTTP POST).\n5. The primary replaces the old fsimage with the new one from the secondary, and\nthe old edits file with the new one it started in step 1. It also updates the fstime file\nto record the time that the checkpoint was taken.\nAt the end of the process, the primary has an up-to-date fsimage file, and a shorter\nedits file (it is not necessarily empty, as it may have received some edits while the\ncheckpoint was being taken). It is possible for an administrator to run this process\nmanually while the namenode is in safe mode, using the hadoop dfsadmin\n-saveNamespace command.\n* From Hadoop version 0.21.0 onward (which will be released after this book is due to be published), the\nsecondary namenode will be replaced by a checkpoint node, which has the same functionality. At the same\ntime, a new type of namenode, called a backup node, will be introduced whose purpose is to maintain an up-\nto-date copy of the namenode metadata, and will act as a replacement for storing a copy of the metadata on\nNFS.\nHDFS | 275Figure 10-1. The checkpointing process\nThis procedure makes it clear why the secondary has similar memory requirements to\nthe primary (since it loads the fsimage into memory), which is the reason that the sec-\nondary needs a dedicated machine on large clusters.\nThe schedule for checkpointing is controlled by two configuration parameters. The\nsecondary namenode checkpoints every hour (fs.checkpoint.period in seconds) or\nsooner if the edit log has reached 64 MB (fs.checkpoint.size in bytes), which it checks\nevery five minutes.\nSecondary namenode directory structure\nA useful side effect of the checkpointing process is that the secondary has a checkpoint\nat the end of the process, which can be found in a subdirectory called previous.check\npoint. This can be used as a source for making (stale) backups of the namenode’s\nmetadata:\n276 | Chapter 10: Administering Hadoop${fs.checkpoint.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\n/previous.checkpoint/VERSION\n/edits\n/fsimage\n/fstime\nThe layout of this directory and of the secondary’s current directory is identical to the\nnamenode’s. This is by design, since in the event of total namenode failure (when there\nare no recoverable backups, even from NFS), it allows recovery from a secondary\nnamenode. This can be achieved either by copying the relevant storage directory to a\nnew namenode, or, if the secondary is taking over as the new primary namenode, by\nusing the -importCheckpoint option when starting the namenode daemon. The\n-importCheckpoint option will load the namenode metadata from the latest checkpoint\nin the directory defined by the fs.checkpoint.dir property, but only if there is no\nmetadata in the dfs.name.dir directory, so there is no risk of overwriting precious\nmetadata.\nDatanode directory structure\nUnlike namenodes, datanodes do not need to be explicitly formatted, since they create\ntheir storage directories automatically on startup. Here are the key files and directories:\n${dfs.data.dir}/current/VERSION\n/blk_<id_1>\n/blk_<id_1>.meta\n/blk_<id_2>\n/blk_<id_2>.meta\n/...\n/blk_<id_64>\n/blk_<id_64>.meta\n/subdir0/\n/subdir1/\n/...\n/subdir63/\nA datanode’s VERSION file is very similar to the namenode’s:\n#Tue Mar 10 21:32:31 GMT 2009\nnamespaceID=134368441\nstorageID=DS-547717739-172.16.85.1-50010-1236720751627\ncTime=0\nstorageType=DATA_NODE\nlayoutVersion=-18\nThe namespaceID, cTime, and layoutVersion are all the same as the values in the name-\nnode (in fact, the namespaceID is retrieved from the namenode when the datanode first\nconnects). The storageID is unique to the datanode (it is the same across all storage\ndirectories), and is used by the namenode to uniquely identify the datanode. The\nstorageType identifies this directory as a datanode storage directory.\nHDFS | 277The other files in the datanode’s current storage directory are the files with the blk_\nprefix. There are two types: the HDFS blocks themselves (which just consist of the file’s\nraw bytes) and the metadata for a block (with a .meta suffix). A block file just consists\nof the raw bytes of a portion of the file being stored; the metadata file is made up of a\nheader with version and type information, followed by a series of checksums for sec-\ntions of the block.\nWhen the number of blocks in a directory grows to a certain size, the datanode creates\na new subdirectory in which to place new blocks and their accompanying metadata. It\ncreates a new subdirectory every time the number of blocks in a directory reaches 64\n(set by the dfs.datanode.numblocks configuration property). The effect is to have a tree\nwith high fan-out, so even for systems with a very large number of blocks, the directories\nwill only be a few levels deep. By taking this measure, the datanode ensures that there\nis a manageable number of files per directory, which avoids the problems that most\noperating systems encounter when there are a large number of files (tens or hundreds\nof thousands) in a single directory.\nIf the configuration property dfs.data.dir specifies multiple directories (on different\ndrives), blocks are written to each in a round-robin fashion. Note that blocks are not\nreplicated on each drive on a single datanode: block replication is across distinct\ndatanodes.\nSafe Mode\nWhen the namenode starts, the first thing it does is load its image file (fsimage) into\nmemory and apply the edits from the edit log (edits). Once it has reconstructed a con-\nsistent in-memory image of the filesystem metadata, it creates a new fsimage file (ef-\nfectively doing the checkpoint itself, without recourse to the secondary namenode) and\nan empty edit log. Only at this point does the namenode start listening for RPC and\nHTTP requests. However, the namenode is running in safe mode, which means that it\noffers only a read-only view of the filesystem to clients.\nStrictly speaking, in safe mode only filesystem operations that access the\nfilesystem metadata (like producing a directory listing) are guaranteed\nto work. Reading a file will work only if the blocks are available on the\ncurrent set of datanodes in the cluster; and file modifications (writes,\ndeletes, or renames) will always fail.\nRecall that the locations of blocks in the system are not persisted by the namenode—\nthis information resides with the datanodes, in the form of a list of the blocks it is\nstoring. During normal operation of the system, the namenode has a map of block\nlocations stored in memory. Safe mode is needed to give the datanodes time to check\nin to the namenode with their block lists, so the namenode can be informed of enough\nblock locations to run the filesystem effectively. If the namenode didn’t wait for enough\ndatanodes to check in, then it would start the process of replicating blocks to new\n278 | Chapter 10: Administering Hadoopdatanodes, which would be unnecessary in most cases (since it only needed to wait for\nthe extra datanodes to check in), and would put a great strain on the cluster’s resources.\nIndeed, while in safe mode, the namenode does not issue any block replication or\ndeletion instructions to datanodes.\nSafe mode is exited when the minimal replication condition is reached, plus an extension\ntime of 30 seconds. The minimal replication condition is when 99.9% of the blocks in\nthe whole filesystem meet their minimum replication level (which defaults to one, and\nis set by dfs.replication.min).\nWhen you are starting a newly formatted HDFS cluster, the namenode does not go into\nsafe mode since there are no blocks in the system.\nTable 10-1. Safe mode properties\nProperty name Type Default value Description\ndfs.replication.min int 1 The minimum number of replicas that have to be writ-\n                         ten for a write to be successful.\ndfs.safemode.threshold.pct float 0.999 The proportion of blocks in the system that must meet\n                                      the minimum replication level defined by dfs.rep\n                                     lication.min before the namenode will exit safe\n                                    mode. Setting this value to 0 or less forces the name-\n                                   node not to start in safe mode. Setting this value to\n                                  more than 1 means the namenode never exits safe\n                                 mode.\ndfs.safemode.extension int 30000 The time, in milliseconds, to extend safe mode by after\n                                the minimum replication condition defined by\n                               dfs.safemode.threshold.pct has been satis-\n                              fied. For small clusters (tens of nodes), it can be set to\n                             0.\nEntering and leaving safe mode\nTo see whether the namenode is in safe mode, you can use the dfsadmin command:\n% hadoop dfsadmin -safemode get\nSafe mode is ON\nThe front page of the HDFS web UI provides another indication of whether the name-\nnode is in safe mode.\nSometimes you want to wait for the namenode to exit safe mode before carrying out a\ncommand, particularly in scripts. The wait option achieves this:\nhadoop dfsadmin -safemode wait\n# command to read or write a file\nAn administrator has the ability to make the namenode enter or leave safe mode at any\ntime. It is sometimes necessary to do this when carrying out maintenance on the cluster,\nHDFS | 279or after upgrading a cluster to confirm that data is still readable. To enter safe mode,\nuse the following command:\n% hadoop dfsadmin -safemode enter\nSafe mode is ON\nYou can use this command when the namenode is still in safe mode while starting up\nto ensure that it never leaves safe mode. Another way of making sure that the namenode\nstays in safe mode indefinitely is to set the property dfs.safemode.threshold.pct to a\nvalue over one.\nYou can make the namenode leave safe mode by using:\n% hadoop dfsadmin -safemode leave\nSafe mode is OFF\nAudit Logging\nHDFS has the ability to log all filesystem access requests, a feature that some organi-\nzations require for auditing purposes. Audit logging is implemented using log4j logging\nat the INFO level, and in the default configuration it is disabled, as the log threshold is\nset to WARN in log4j.properties:\nlog4j.logger.org.apache.hadoop.fs.FSNamesystem.audit=WARN\nYou can enable audit logging by replacing WARN with INFO, and the result will be a log\nline written to the namenode’s log for every HDFS event. Here’s an example for a list\nstatus request on /user/tom:\n2009-03-13 07:11:22,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit:\nugi=tom,staff,admin\nip=/127.0.0.1\ncmd=listStatus src=/user/tom dst=null\nperm=null\nIt is a good idea to configure log4j so that the audit log is written to a separate file and\nisn’t mixed up with the namenode’s other log entries. An example of how to do this\ncan be found on the Hadoop wiki at http://wiki.apache.org/hadoop/HowToConfigure.\nTools\ndfsadmin\nThe dfsadmin tool is a multi-purpose tool for finding information about the state of\nHDFS, as well as performing administration operations on HDFS. It is invoked as\nhadoop dfsadmin. Commands that alter HDFS state typically require superuser\nprivileges.\nThe available commands to dfsadmin are described in Table 10-2.\n280 | Chapter 10: Administering HadoopTable 10-2. dfsadmin commands\nCommand Description\n-help Shows help for a given command, or all commands if no command is specified.\n-report Shows filesystem statistics (similar to that shown in the web UI) and information on connected\n       datanodes.\n-metasave Dumps information to a file in Hadoop’s log directory about blocks that are being replicated or\n           deleted, and a list of connected datanodes.\n-safemode Changes or query the state of safe mode. See “Safe Mode” on page 278.\n-saveNamespace Saves the current in-memory filesystem image to a new fsimage file, and resets the edits file. This\n              operation may be performed only in safe mode.\n-refreshNodes Updates the set of datanodes that are permitted to connect to the namenode. See “Commissioning\n               and Decommissioning Nodes” on page 293.\n-upgradeProgress Gets information on the progress of a HDFS upgrade, or force an upgrade to proceed. See\n                “Upgrades” on page 296.\n-finalizeUpgrade Removes the previous version of the datanodes’ and namenode’s storage directories. Used after\n                    an upgrade has been applied and the cluster is running successfully on the new version. See\n                   “Upgrades” on page 296.\n-setQuota Sets directory quotas. Directory quotas set a limit on the number of names (files or directories) in\n         the directory tree. Directory quotas are useful for preventing users from creating large numbers\n        of small files, a measure that helps preserve the namenode’s memory (recall that accounting\n         information for every file, directory, and block in the filesystem is stored in memory).\n-clrQuota Clears specified directory quotas.\n-setSpaceQuota Sets space quotas on directories. Space quotas set a limit on the size of files that may be stored in\n              a directory tree. They are useful for giving users a limited amount of storage.\n-clrSpaceQuota Clears specified space quotas.\n-refreshServiceAcl Refreshes the namenode’s service-level authorization policy file.\nFilesystem check (fsck)\nHadoop provides an fsck utility for checking the health of files in HDFS. The tool looks\nfor blocks that are missing from all datanodes, as well as under- or over-replicated\nblocks. Here is an example of checking the whole filesystem for a small cluster:\n% hadoop fsck /\n......................Status: HEALTHY\nTotal size:\n511799225 B\nTotal dirs:\n10\nTotal files:\n22\nTotal blocks (validated):\n22 (avg. block size 23263601 B)\nMinimally replicated blocks:\n22 (100.0 %)\nOver-replicated blocks:\n0 (0.0 %)\nUnder-replicated blocks:\n0 (0.0 %)\nMis-replicated blocks:\n0 (0.0 %)\nDefault replication factor:\n3\nAverage block replication:\n3.0\nHDFS | 281Corrupt blocks:\nMissing replicas:\nNumber of data-nodes:\nNumber of racks:\n0\n0 (0.0 %)\n4\n1\nThe filesystem under path '/' is HEALTHY\nfsck recursively walks the filesystem namespace, starting at the given path (here the\nfilesystem root), and checks the files it finds. It prints a dot for every file it checks. To\ncheck a file, fsck retrieves the metadata for the file’s blocks, and looks for problems or\ninconsistencies. Note that fsck retrieves all of its information from the namenode; it\ndoes not communicate with any datanodes to actually retrieve any block data.\nMost of the output from fsck is self-explanatory, but here are some of the conditions it\nlooks for:\nOver-replicated blocks\nThese are blocks that exceed their target replication for the file they belong to.\nOver-replication is not normally a problem, and HDFS will automatically delete\nexcess replicas.\nUnder-replicated blocks\nThese are blocks that do not meet their target replication for the file they belong\nto. HDFS will automatically create new replicas of under-replicated blocks until\nthey meet the target replication. You can get information about the blocks being\nreplicated (or waiting to be replicated) using hadoop dfsadmin -metasave.\nMisreplicated blocks\nThese are blocks that do not satisfy the block replica placement policy (see “Replica\nPlacement” on page 67). For example, for a replication level of three in a multirack\ncluster, if all three replicas of a block are on the same rack, then the block is mis-\nreplicated since the replicas should be spread across at least two racks for resilience.\nA misreplicated block is not fixed automatically by HDFS (at the time of this writ-\ning). As a workaround, you can fix the problem manually by increasing the repli-\ncation of the file the block belongs to (using hadoop fs -setrep), waiting until the\nblock gets replicated, then decreasing the replication of the file back to its original\nvalue.\nCorrupt blocks\nThese are blocks whose replicas are all corrupt. Blocks with at least one noncorrupt\nreplica are not reported as corrupt; the namenode will replicate the noncorrupt\nreplica until the target replication is met.\nMissing replicas\nThese are blocks with no replicas anywhere in the cluster.\nCorrupt or missing blocks are the biggest cause for concern, as it means data has been\nlost. By default, fsck leaves files with corrupt or missing blocks, but you can tell it to\nperform one of the following actions on them:\n282 | Chapter 10: Administering Hadoop• Move the affected files to the /lost+found directory in HDFS, using the -move option.\nFiles are broken into chains of contiguous blocks to aid any salvaging efforts you\nmay attempt.\n• Delete the affected files, using the -delete option. Files cannot be recovered after\nbeing deleted.\nFinding the blocks for a file. The fsck tool provides an easy way to find out which blocks are\nin any particular file. For example:\n% hadoop fsck /user/tom/part-00007 -files -blocks -racks\n/user/tom/part-00007 25582428 bytes, 1 block(s): OK\n0. blk_-3724870485760122836_1035 len=25582428 repl=3 [/default-rack/10.251.43.2:50010,\n/default-rack/10.251.27.178:50010, /default-rack/10.251.123.163:50010]\nThis says that the file /user/tom/part-00007 is made up of one block, and shows the\ndatanodes where the blocks are located. The fsck options used are as follows:\n• The -files option shows the line with the filename, size, number of blocks, and\nits health (whether there are any missing blocks).\n• The -blocks option shows information about each block in the file, one line per\nblock.\n• The -racks option displays the rack location and the datanode addresses for each\nblock.\nRunning hadoop fsck without any arguments displays full usage instructions.\nDatanode block scanner\nEvery datanode runs a block scanner, which periodically verifies all the blocks stored\non the datanode. This allows bad blocks to be detected and fixed before they are read\nby clients. The DataBlockScanner maintains a list of blocks to verify, and scans them\none by one for checksum errors. The scanner employs a throttling mechanism to pre-\nserve disk bandwidth on the datanode.\nBlocks are periodically verified every three weeks to guard against disk errors over time\n(this is controlled by the dfs.datanode.scan.period.hours property, which defaults to\n504 hours). Corrupt blocks are reported to the namenode to be fixed.\nYou can get a block verification report for a datanode by visiting the datanode’s web\ninterface at http://datanode:50075/blockScannerReport. Here’s an example of a report,\nwhich should be self-explanatory:\nTotal Blocks\nVerified in last hour\nVerified in last day\nVerified in last week\nVerified in last four weeks\nVerified in SCAN_PERIOD\nNot yet verified\nVerified since restart\n:\n:\n:\n:\n:\n:\n:\n:\n21131\n70\n1767\n7360\n20057\n20057\n1074\n35912\nHDFS | 283Scans since restart\nScan errors since restart\nTransient scan errors\nCurrent scan rate limit KBps\nProgress this period\nTime left in cur period\n:\n:\n:\n:\n:\n:\n6541\n0\n0\n1024\n109%\n53.08%\nBy specifying the listblocks parameter, http://datanode:50075/blockScannerReport?\nlistblocks, the report is preceded by a list of all the blocks on the datanode along with\ntheir latest verification status. Here is a snippet of the block list (lines are split to fit the\npage):\nblk_6035596358209321442\n: status : ok\nnot yet verified\nblk_3065580480714947643\n: status : ok\n2008-07-11 05:48:26,400\nblk_8729669677359108508\n: status : ok\n2008-07-11 05:55:27,345\ntype : none\nscan time : 0\ntype : remote scan time : 1215755306400\ntype : local scan time : 1215755727345\nThe first column is the block ID, followed by some key-value pairs. The status can be\none of failed or ok according to whether the last scan of the block detected a checksum\nerror. The type of scan is local if it was performed by the background thread, remote\nif it was performed by a client or a remote datanode, or none if a scan of this block has\nyet to be made. The last piece of information is the scan time, which is displayed as the\nnumber of milliseconds since midnight 1 January 1970, and also as a more readable\nvalue.\nbalancer\nOver time the distribution of blocks across datanodes can become unbalanced. An\nunbalanced cluster can affect locality for MapReduce, and it puts a greater strain on\nthe highly utilized datanodes, so it’s best avoided.\nThe balancer program is a Hadoop daemon that re-distributes blocks by moving them\nfrom over-utilized datanodes to under-utilized datanodes, while adhering to the block\nreplica placement policy that makes data loss unlikely by placing block replicas on\ndifferent racks (see “Replica Placement” on page 67). It moves blocks until the cluster\nis deemed to be balanced, which means that the utilization of every datanode (ratio of\nused space on the node to total capacity of the node) differs from the utilization of the\ncluster (ratio of used space on the cluster to total capacity of the cluster) by no more\nthan a given threshold percentage. You can start the balancer with:\n% start-balancer.sh\nThe -threshold argument specifies the threshold percentage that defines what it means\nfor the cluster to be balanced. The flag is optional, in which case the threshold is 10%.\nAt any one time, only one balancer may be running on the cluster.\nThe balancer runs until the cluster is balanced, it cannot move any more blocks, or it\nloses contact with the namenode. It produces a logfile in the standard log directory,\n284 | Chapter 10: Administering Hadoopwhere it writes a line for every iteration of redistribution that it carries out. Here is the\noutput from a short run on a small cluster:\nTime Stamp\nIteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved\nMar 18, 2009 5:23:42 PM 0\n0 KB\n219.21 MB\n150.29 MB\nMar 18, 2009 5:27:14 PM 1\n195.24 MB\n22.45 MB\n150.29 MB\nThe cluster is balanced. Exiting...\nBalancing took 6.072933333333333 minutes\nThe balancer is designed to run in the background without unduly taxing the cluster,\nor interfering with other clients using the cluster. It limits the bandwidth that it uses to\ncopy a block from one node to another. The default is a modest 1 MB/s, but this can\nbe changed by setting the dfs.balance.bandwidthPerSec property in hdfs-site.xml, speci-\nfied in bytes.\nMonitoring\nMonitoring is an important part of system administration. In this section, we look at\nthe monitoring facilities in Hadoop, and how they can hook into external monitoring\nsystems.\nThe purpose of monitoring is to detect when the cluster is not providing the expected\nlevel of service. The master daemons are the most important to monitor: the namenodes\n(primary and secondary), and the jobtracker. Failure of datanodes and tasktrackers is\nto be expected, particularly on larger clusters, so you should provide extra capacity so\nthat the cluster can tolerate having a small percentage of dead nodes at any time.\nIn addition to the facilities described next, some administrators run test jobs on a pe-\nriodic basis as a test of the cluster’s health.\nThere is lot of work going on to add more monitoring capabilities to Hadoop, which\nis not covered here. For example, Chukwa† is a data collection and monitoring system\nbuilt on HDFS and MapReduce, and excels at mining log data for finding large-scale\ntrends. Another example is the work to define a service lifecycle for Hadoop daemons,\nwhich includes adding a “ping” method to get a brief summary of a daemon’s health.‡\nLogging\nAll Hadoop daemons produce logfiles that can be very useful for finding out what is\nhappening in the system. “System logfiles” on page 256 explains how to configure these\nfiles.\n† http://hadoop.apache.org/chukwa\n‡ https://issues.apache.org/jira/browse/HADOOP-3628\nMonitoring | 285Setting log levels\nWhen debugging a problem, it is very convenient to be able to change the log level\ntemporarily for a particular component in the system.\nHadoop daemons have a web page for changing the log level for any log4j log name,\nwhich can be found at /logLevel in the daemon’s web UI. By convention, log names in\nHadoop correspond to the classname doing the logging, although there are exceptions\nto this rule, so you should consult the source code to find log names.\nFor example, to enable debug logging for the JobTracker class, we would visit the job-\ntracker’s web UI at http://jobtracker-host:50030/logLevel and set the log name\norg.apache.hadoop.mapred.JobTracker to level DEBUG.\nThe same thing can be achieved from the command line as follows:\n% hadoop daemonlog -setlevel jobtracker-host:50030 \\\norg.apache.hadoop.mapred.JobTracker DEBUG\nLog levels changed in this way are reset when the daemon restarts, which is usually\nwhat you want. However, to make a persistent change to a log level, simply change the\nlog4j.properties file in the configuration directory. In this case, the line to add is:\nlog4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\nGetting stack traces\nHadoop daemons expose a web page (/stacks in the web UI) that produces a thread\ndump for all running threads in the daemon’s JVM. For example, you can get a thread\ndump for a jobtracker from http://jobtracker-host:50030/stacks.\nMetrics\nThe HDFS and MapReduce daemons collect information about events and measure-\nments that are collectively known as metrics. For example, datanodes collect the fol-\nlowing metrics (and many more): the number of bytes written, the number of blocks\nreplicated, and the number of read requests from clients (both local and remote).\nMetrics belong to a context, and Hadoop currently uses “dfs”, “mapred”, “rpc”, and\n“jvm” contexts. Hadoop daemons usually collect metrics under several contexts. For\nexample, datanodes collect metrics for the “dfs”, “rpc”, and “jvm” contexts.\nHow Do Metrics Differ from Counters?\nThe main difference is their scope: metrics are collected by Hadoop daemons, whereas\ncounters (see “Counters” on page 211) are collected for MapReduce tasks, and aggre-\ngated for the whole job. They have different audiences, too: broadly speaking, metrics\nare for administrators, and counters are for MapReduce users.\nThe way they are collected and aggregated is also different. Counters are a MapReduce\nfeature, and the MapReduce system ensures that counter values are propagated from\n286 | Chapter 10: Administering Hadoopthe tasktrackers where they are produced, back to the jobtracker, and finally back to\nthe client running the MapReduce job. (Counters are propagated via RPC heartbeats;\nsee “Progress and Status Updates” on page 156.) Both the tasktrackers and the job-\ntracker perform aggregation.\nThe collection mechanism for metrics is decoupled from the component that receives\nthe updates, and there are various pluggable outputs, including local files, Ganglia, and\nJMX. The daemon collecting the metrics performs aggregation on them before they are\nsent to the output.\nA context defines the unit of publication, you can choose to publish the “dfs” context,\nbut not the “jvm” context, for instance. Metrics are configured in the conf/hadoop-\nmetrics.properties file, and, by default, all contexts are configured so they do not publish\ntheir metrics. This is the contents of the default configuration file (minus the\ncomments):\ndfs.class=org.apache.hadoop.metrics.spi.NullContext\nmapred.class=org.apache.hadoop.metrics.spi.NullContext\njvm.class=org.apache.hadoop.metrics.spi.NullContext\nrpc.class=org.apache.hadoop.metrics.spi.NullContext\nEach line in this file configures a different context, and specifies the class that handles\nthe metrics for that context. The class must be an implementation of the MetricsCon\ntext interface; and, as the name suggests, the NullContext class neither publishes nor\nupdates metrics.§\nThe other implementations of MetricsContext are covered in the following sections.\nFileContext\nFileContext writes metrics to a local file. It exposes two configuration properties:\nfileName, which specifies the absolute name of the file to write to, and period, for the\ntime interval (in seconds) between file updates. Both properties are optional; if not set,\nthe metrics will be written to standard output every five seconds.\nConfiguration properties apply to a context name, and are specified by appending the\nproperty name to the context name (separated by a dot). For example, to dump the\n“jvm” context to a file, we alter its configuration to be the following:\njvm.class=org.apache.hadoop.metrics.file.FileContext\njvm.fileName=/tmp/jvm_metrics.log\nIn the first line, we have changed the “jvm” context to use a FileContext, and in the\nsecond, we have set the “jvm” context’s fileName property to be a temporary file. Here\nare two lines of output from the logfile, split over several lines to fit the page:\n§ The term “context” is (perhaps unfortunately) overloaded here, since it can refer to either a collection of\nmetrics (the “dfs” context, for example) or the class that publishes metrics (the NullContext, for example).\nMonitoring | 287jvm.metrics: hostName=ip-10-250-59-159, processName=NameNode, sessionId=, gcCount=46, ↵\ngcTimeMillis=394, logError=0, logFatal=0, logInfo=59, logWarn=1, ↵\nmemHeapCommittedM=4.9375, memHeapUsedM=2.5322647, memNonHeapCommittedM=18.25, ↵\nmemNonHeapUsedM=11.330269, threadsBlocked=0, threadsNew=0, threadsRunnable=6, ↵\nthreadsTerminated=0, threadsTimedWaiting=8, threadsWaiting=13\njvm.metrics: hostName=ip-10-250-59-159, processName=SecondaryNameNode, sessionId=, ↵\ngcCount=36, gcTimeMillis=261, logError=0, logFatal=0, logInfo=18, logWarn=4, ↵\nmemHeapCommittedM=5.4414062, memHeapUsedM=4.46756, memNonHeapCommittedM=18.25, ↵\nmemNonHeapUsedM=10.624519, threadsBlocked=0, threadsNew=0, threadsRunnable=5, ↵\nthreadsTerminated=0, threadsTimedWaiting=4, threadsWaiting=2\nFileContext can be useful on a local system for debugging purposes, but is unsuitable\non a larger cluster since the output files are spread across the cluster, which makes\nanalyzing them difficult.\nGangliaContext\nGanglia (http://ganglia.info/) is an open source distributed monitoring system for very\nlarge clusters. It is designed to impose very low resource overheads on each node in the\ncluster. Ganglia itself collects metrics, such as CPU and memory usage; by using\nGangliaContext, you can inject Hadoop metrics into Ganglia.\nGangliaContext has one required property, servers, which takes a space- and/or\ncomma-separated list of Ganglia server host-port pairs. Further details on configuring\nthis context can be found on the Hadoop wiki.\nFor a flavor of the kind of information you can get out of Ganglia, see Figure 10-2,\nwhich shows how the number of tasks in the jobtracker’s queue varies over time.\nFigure 10-2. Ganglia plot of number of tasks in the jobtracker queue\nNullContextWithUpdateThread\nBoth FileContext and a GangliaContext push metrics to an external system. However,\nsome monitoring systems—notably JMX—need to pull metrics from Hadoop. Null\nContextWithUpdateThread is designed for this. Like NullContext, it doesn’t publish any\nmetrics, but in addition it runs a timer that periodically updates the metrics stored in\nmemory. This ensures that the metrics are up-to-date when they are fetched by another\nsystem.\n288 | Chapter 10: Administering HadoopAll implementations of MetricsContext except NullContext perform this updating func-\ntion (and they all expose a period property that defaults to five seconds), so you need\nto use NullContextWithUpdateThread only if you are not collecting metrics using another\noutput. If you were using GangliaContext, for example, then it would ensure the metrics\nare updated, so you would be able to use JMX in addition with no further configuration\nof the metrics system. JMX is discussed in more detail shortly.\nCompositeContext\nCompositeContext allows you to output the same set of metrics to multiple contexts,\nsuch as a FileContext and a GangliaContext. The configuration is slightly tricky, and is\nbest shown by an example:\njvm.class=org.apache.hadoop.metrics.spi.CompositeContext\njvm.arity=2\njvm.sub1.class=org.apache.hadoop.metrics.file.FileContext\njvm.fileName=/tmp/jvm_metrics.log\njvm.sub2.class=org.apache.hadoop.metrics.ganglia.GangliaContext\njvm.servers=ip-10-250-59-159.ec2.internal:8649\nThe arity property is used to specify the number of subcontexts; in this case, there are\ntwo. The property names for each subcontext are modified to have a part specifying\nthe subcontext number, hence jvm.sub1.class and jvm.sub2.class.\nJava Management Extensions\nJava Management Extensions (JMX) is a standard Java API for monitoring and man-\naging applications. Hadoop includes several managed beans (MBeans), which expose\nHadoop metrics to JMX-aware applications. There are MBeans that expose the metrics\nin the “dfs” and “rpc” contexts, but none for the “mapred” context (at the time of this\nwriting), or the “jvm” context (as the JVM itself exposes a richer set of JVM metrics).\nThese MBeans are listed in Table 10-3.\nTable 10-3. Hadoop MBeans\nMBean Class Daemons Metrics\nNameNodeActivityM Namenode Namenode activity metrics,\nBean such as the number of create\n    file operations\nFSNamesystemMBean Namenode Namenode status metrics,\n                          such as the number of con-\n                         nected datanodes\nDataNodeActivityM Datanode Datanode activity metrics,\nBean such as number of bytes\n    read\nFSDatasetMBean Datanode Datanode storage metrics,\n                       such as capacity and free\n                      storage space\nMonitoring | 289MBean Class Daemons Metrics\nRpcActivityMBean All daemons that use RPC: namenode, datanode, jobtracker, RPC statistics, such as aver-\n                 tasktracker age processing time\nThe JDK comes with a tool called JConsole for viewing MBeans in a running JVM. It’s\nuseful for browsing Hadoop metrics, as demonstrated in Figure 10-3.\nFigure 10-3. JConsole view of a locally running namenode, showing metrics for the filesystem state\nAlthough you can see Hadoop metrics via JMX using the default metrics\nconfiguration, they will not be updated unless you change the\nMetricsContext implementation to something other than NullContext.\nFor example, NullContextWithUpdateThread is appropriate if JMX is the\nonly way you will be monitoring metrics.\nMany third-party monitoring and alerting systems (such as Nagios or Hyperic) can\nquery MBeans, making JMX the natural way to monitor your Hadoop cluster from an\nexisting monitoring system. You will need to enable remote access to JMX, however,\nand choose a level of security that is appropriate for your cluster. The options here\ninclude password authentication, SSL connections, and SSL client-authentication. See\nthe official Java documentation‖ for an in-depth guide on configuring these options.\n‖ http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n290 | Chapter 10: Administering HadoopAll the options for enabling remote access to JMX involve setting Java system proper-\nties, which we do for Hadoop by editing the conf/hadoop-env.sh file. The following\nconfiguration settings show how to enable password-authenticated remote access to\nJMX on the namenode (with SSL disabled). The process is very similar for other Hadoop\ndaemons:\nexport HADOOP_NAMENODE_OPTS=""-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.password.file=$HADOOP_CONF_DIR/jmxremote.password\n-Dcom.sun.management.jmxremote.port=8004 $HADOOP_NAMENODE_OPTS""\nThe jmxremote.password file lists the usernames and their passwords in plain text; the\nJMX documentation has further details on the format of this file.\nWith this configuration, we can use JConsole to browse MBeans on a remote name-\nnode. Alternatively, we can use one of the many JMX tools to retrieve MBean attribute\nvalues. Here is an example of using the “jmxquery” command-line tool (and Nagios\nplug-in, available from http://code.google.com/p/jmxquery/) to retrieve the number of\nunder-replicated blocks:\n% ./check_jmx -U service:jmx:rmi:///jndi/rmi://namenode-host:8004/jmxrmi -O \\\nhadoop:service=NameNode,name=FSNamesystemState -A UnderReplicatedBlocks \\\n-w 100 -c 1000 -username monitorRole -password secret\nJMX OK - UnderReplicatedBlocks is 0\nThis command establishes a JMX RMI connection to the host namenode-host on port\n8004 and authenticates using the given username and password. It reads the attribute\nUnderReplicatedBlocks of the object named hadoop:service=NameNode,name=FSNamesys\ntemState and prints out its value on the console.# The -w and -c options specify warning\nand critical levels for the value: the appropriate values of these are normally determined\nafter operating a cluster for a while.\nIt’s common to use Ganglia in conjunction with an alerting system like Nagios for\nmonitoring a Hadoop cluster. Ganglia is good for efficiently collecting a large number\nof metrics and graphing them, whereas Nagios and similar systems are good at sending\nalerts when a critical threshold is reached in any of a smaller set of metrics.\n# It’s convenient to use JConsole to find the object names of the MBeans that you want to monitor. Note that\nMBeans for datanode metrics currently contain a random identifier, which makes it difficult to monitor them\nin anything but an ad hoc way. See https://issues.apache.org/jira/browse/HADOOP-4482.\nMonitoring | 291Maintenance\nRoutine Administration Procedures\nMetadata backups\nIf the namenode’s persistent metadata is lost or damaged, the entire filesystem is ren-\ndered unusable, so it is critical that backups are made of these files. You should keep\nmultiple copies of different ages (one hour, one day, one week, and one month, say) to\nprotect against corruption, either in the copies themselves or in the live files running\non the namenode.\nA straightforward way to make backups is to write a script to periodically archive the\nsecondary namenode’s previous.checkpoint subdirectory (under the directory defined\nby the fs.checkpoint.dir property) to an offsite location. The script should additionally\ntest the integrity of the copy. This can be done by starting a local namenode daemon,\nand verifying that it has successfully read the fsimage and edits files into memory (by\nscanning the namenode log for the appropriate success message, for example).*\nData backups\nAlthough HDFS is designed to store data reliably, data loss can occur, just like in any\nstorage system, and thus a backup strategy is essential. With the large data volumes\nthat Hadoop can store, deciding what data to back up, and where to store it is a chal-\nlenge. The key here is to prioritize your data. The highest priority is the data that cannot\nbe regenerated, and which is critical to the business; though data that is straightforward\nto regenerate, or essentially disposable because it is of limited business value, is the\nlowest priority, and you may choose not to make backups of this category of data.\nDo not make the mistake of thinking that HDFS replication is a substi-\ntute for making backups. Bugs in HDFS can cause replicas to be lost. So\ncan hardware failures. Although Hadoop is expressly designed so that\nhardware failure is very unlikely to result in data loss, the possibility can\nnever be completely ruled out, particularly when combined with soft-\nware bugs, or human error.\nWhen it comes to backups, think of HDFS in the same way as you would\nRAID. Although the data will survive the loss of an individual RAID\ndisk, it may not if the RAID controller fails, or is buggy (perhaps over-\nwriting some data), or the entire array is damaged.\n* Hadoop 0.21.0 comes with an Offline Image Viewer, which can be used to check the integrity of the image\nfiles.\n292 | Chapter 10: Administering HadoopIt’s common to have a policy for user directories in HDFS. For example, they may have\nspace quotas, and be backed up nightly. Whatever the policy, make sure your users\nknow what it is, so they know what to expect.\nThe distcp tool is ideal for making backups to other HDFS clusters (preferably running\non a different version of the software, to guard against loss due to bugs in HDFS) or\nother Hadoop filesystems (such as S3 or KFS), since it can copy files in parallel. Alter-\nnatively, you can employ an entirely different storage system for backups, using one of\nthe ways to export data from HDFS described in “Hadoop Filesystems” on page 47.\nFilesystem check (fsck)\nIt is advisable to run HDFS’s fsck tool regularly (for example, daily) on the whole file-\nsystem to proactively look for missing or corrupt blocks. See “Filesystem check\n(fsck)” on page 281.\nFilesystem balancer\nRun the balancer tool (see “balancer” on page 284) regularly to keep the filesystem\ndatanodes evenly balanced.\nCommissioning and Decommissioning Nodes\nAs an administrator of a Hadoop cluster, you will need to add or remove nodes from\ntime to time. For example, to grow the storage available to a cluster, commission new\nnodes. Conversely, sometimes you may wish to shrink a cluster, and to do so, you\ndecommission nodes. It can sometimes be necessary to decommission a node if it is\nmisbehaving, perhaps because it is failing more often than it should or its performance\nis noticeably slow.\nNodes normally run both a datanode and a tasktracker, and both are typically\ncommissioned or decommissioned in tandem.\nCommissioning new nodes\nAlthough commissioning a new node can be as simple as configuring the hdfs-\nsite.xml file to point to the namenode and the mapred-site.xml file to point to the job-\ntracker and starting the datanode and jobtracker daemons, it is generally best to have\na list of authorized nodes.\nIt is a potential security risk to allow any machine to connect to the namenode and act\nas a datanode, since the machine may gain access to data that it is not authorized to\nsee. Furthermore, since such a machine is not a real datanode, it is not under your\ncontrol, and may stop at any time, causing potential data loss. (Imagine what would\nhappen if a number of such nodes were connected, and a block of data was present\nonly on the “alien” nodes?) This scenario is a risk even inside a firewall, through\nMaintenance | 293misconfiguration, so datanodes (and tasktrackers) should be explicitly managed on all\nproduction clusters.\nDatanodes that are permitted to connect to the namenode are specified in a file whose\nname is specified by the dfs.hosts property. The file resides on the namenode’s local\nfilesystem, and it contains a line for each datanode, specified by network address (as\nreported by the datanode—you can see what this is by looking at the namenode’s web\nUI). If you need to specify multiple network addresses for a datanode, put them on one\nline, separated by whitespace.\nSimilarly, tasktrackers that may connect to the jobtracker are specified in a file whose\nname is specified by the mapred.hosts property. In most cases, there is one shared file,\nreferred to as the include file, that both dfs.hosts and mapred.hosts refer to, since nodes\nin the cluster run both datanode and tasktracker daemons.\nThe file (or files) specified by the dfs.hosts and mapred.hosts properties\nis different from the slaves file. The former is used by the namenode and\njobtracker to determine which worker nodes may connect. The slaves\nfile is used by the Hadoop control scripts to perform cluster-wide op-\nerations, such as cluster restarts. It is never used by the Hadoop\ndaemons.\nTo add new nodes to the cluster:\n1. Add the network addresses of the new nodes to the include file.\n2. Update the namenode with the new set of permitted datanodes using this\ncommand:\n% hadoop dfsadmin -refreshNodes\n3. Update the slaves file with the new nodes, so that they are included in future op-\nerations performed by the Hadoop control scripts.\n4. Start the new datanodes.\n5. Restart the MapReduce cluster.†\n6. Check that the new datanodes and tasktrackers appear in the web UI.\nHDFS will not move blocks from old datanodes to new datanodes to balance the cluster.\nTo do this you should run the balancer described in “balancer” on page 284.\nDecommissioning old nodes\nAlthough HDFS is designed to tolerate datanode failures, this does not mean you can\njust terminate datanodes en masse with no ill effect. With a replication level of three,\n† At the time of this writing, there is no command to refresh the set of permitted nodes in the jobtracker.\nConsider setting the mapred.jobtracker.restart.recover property to true to make the jobtracker recover\nrunning jobs after a restart.\n294 | Chapter 10: Administering Hadoopfor example, the chances are very high that you will lose data by simultaneously shutting\ndown three datanodes if they are on different racks. The way to decommission\ndatanodes is to inform the namenode of the nodes that you wish to take out of circu-\nlation, so that it can replicate the blocks to other datanodes before the datanodes are\nshut down.\nWith tasktrackers, Hadoop is more forgiving. If you shut down a tasktracker that is\nrunning tasks, the jobtracker will notice the failure and reschedule the tasks on other\ntasktrackers.\nThe decommissioning process is controlled by an exclude file, which for HDFS is set\nby the dfs.hosts.exclude property, and for MapReduce by the mapred.hosts.exclude\nproperty. It is often the case that these properties refer to the same file. The exclude file\nlists the nodes that are not permitted to connect to the cluster.\nThe rules for whether a tasktracker may connect to the jobtracker are simple: a task-\ntracker may connect only if it appears in the include file and does not appear in the\nexclude file. An unspecified or empty include file is taken to mean that all nodes are in\nthe include file.\nFor HDFS, the rules are slightly different. If a datanode appears in both the include and\nthe exclude file, then it may connect, but only to be decommissioned. Table 10-4 sum-\nmarizes the different combinations for datanodes. As for tasktrackers, an unspecified\nor empty include file means all nodes are included.\nTable 10-4. HDFS include and exclude file precedence\nNode appears in include file Node appears in exclude file Interpretation\nNo No Node may not connect.\nNo Yes Node may not connect.\nYes No Node may connect.\nYes Yes Node may connect and will be decommissioned.\nTo remove nodes from the cluster:\n1. Add the network addresses of the nodes to be decommissioned to the exclude file.\nDo not update the include file at this point.\n2. Restart the MapReduce cluster to stop the tasktrackers on the nodes being\ndecommissioned.\n3. Update the namenode with the new set of permitted datanodes, with this\ncommand:\n% hadoop dfsadmin -refreshNodes\n4. Go to the web UI and check whether the admin state has changed to “Decommis-\nsion In Progress” for the datanodes being decommissioned. They will start copying\ntheir blocks to other datanodes in the cluster.\nMaintenance | 2955. When all the datanodes report their state as “Decommissioned,” then all the blocks\nhave been replicated. Shut down the decommissioned nodes.\n6. Remove the nodes from the include file, and run:\n% hadoop dfsadmin -refreshNodes\n7. Remove the nodes from the slaves file.\nUpgrades\nUpgrading an HDFS and MapReduce cluster requires careful planning. The most im-\nportant consideration is the HDFS upgrade. If the layout version of the filesystem has\nchanged, then the upgrade will automatically migrate the filesystem data and metadata\nto a format that is compatible with the new version. As with any procedure that involves\ndata migration, there is a risk of data loss, so you should be sure that both your data\nand metadata is backed up (see “Routine Administration Procedures” on page 292).\nPart of the planning process should include a trial run on a small test cluster with a\ncopy of data that you can afford to lose. A trial run will allow you to familiarize yourself\nwith the process, customize it to your particular cluster configuration and toolset, and\niron out any snags before running the upgrade procedure on a production cluster. A\ntest cluster also has the benefit of being available to test client upgrades on.\nVersion Compatibility\nAll pre-1.0 Hadoop components have very rigid version compatibility requirements.\nOnly components from the same release are guaranteed to be compatible with each\nother, which means the whole system—from daemons to clients—has to be upgraded\nsimultaneously, in lockstep. This necessitates a period of cluster downtime.\nVersion 1.0 of Hadoop promises to loosen these requirements so that, for example,\nolder clients can talk to newer servers (within the same major release number). In later\nreleases, rolling upgrades may be supported, which would allow cluster daemons to be\nupgraded in phases, so that the cluster would still be available to clients during the\nupgrade.\nUpgrading a cluster when the filesystem layout has not changed is fairly\nstraightforward: install the new versions of HDFS and MapReduce on the cluster (and\non clients at the same time), shut down the old daemons, update configuration files,\nthen start up the new daemons and switch clients to use the new libraries. This process\nis reversible, so rolling back an upgrade is also straightforward.\nAfter every successful upgrade, you should perform a couple of final clean up steps:\n• Remove the old installation and configuration files from the cluster.\n• Fix any deprecation warnings in your code and configuration.\n296 | Chapter 10: Administering HadoopHDFS data and metadata upgrades\nIf you use the procedure just described to upgrade to a new version of HDFS and it\nexpects a different layout version, then the namenode will refuse to run. A message like\nthe following will appear in its log:\nFile system image contains an old layout version -16.\nAn upgrade to version -18 is required.\nPlease restart NameNode with -upgrade option.\nThe most reliable way of finding out whether you need to upgrade the filesystem is by\nperforming a trial on a test cluster.\nAn upgrade of HDFS makes a copy of the previous version’s metadata and data. Doing\nan upgrade does not double the storage requirements of the cluster, as the datanodes\nuse hard links to keep two references (for the current and previous version) to the same\nblock of data. This design makes it straightforward to roll back to the previous version\nof the filesystem, should you need to. You should understand that any changes made\nto the data on the upgraded system will be lost after the rollback completes.\nYou can keep only the previous version of the filesystem: you can’t roll back several\nversions. Therefore, to carry out another upgrade to HDFS data and metadata, you will\nneed to delete the previous version, a process called finalizing the upgrade. Once an\nupgrade is finalized, there is no procedure for rolling back to a previous version.\nIn general, you can skip releases when upgrading (for example, you can upgrade from\nrelease 0.18.3 to 0.20.0 without having to upgrade to a 0.19.x release first), but in some\ncases, you may have to go through intermediate releases. The release notes make it clear\nwhen this is required.\nYou should only attempt to upgrade a healthy filesystem. Before running the upgrade\ndo a full fsck (see “Filesystem check (fsck)” on page 281). As an extra precaution, you\ncan keep a copy of the fsck output that lists all the files and blocks in the system, so\nyou can compare it with the output of running fsck after the upgrade.\nIt’s also worth clearing out temporary files before doing the upgrade, both from the\nMapReduce system directory on HDFS and local temporary files.\nWith these preliminaries out of the way, here is the high-level procedure for upgrading\na cluster when the filesystem layout needs to be migrated:\n1. Make sure that any previous upgrade is finalized before proceeding with another\nupgrade.\n2. Shut down MapReduce and kill any orphaned task processes on the tasktrackers.\n3. Shut down HDFS and backup the namenode directories.\n4. Install new versions of Hadoop HDFS and MapReduce on the cluster and on\nclients.\n5. Start HDFS with the -upgrade option.\nMaintenance | 2976.\n7.\n8.\n9.\nWait until the upgrade is complete.\nPerform some sanity checks on HDFS.\nStart MapReduce.\nRoll back or finalize the upgrade (optional).\nWhile running the upgrade procedure, it is a good idea to remove the Hadoop scripts\nfrom your PATH environment variable. This forces you to be explicit about which version\nof the scripts you are running. It can be convenient to define two environment variables\nfor the new installation directories; in the following instructions, we have defined\nOLD_HADOOP_INSTALL and NEW_HADOOP_INSTALL.\nStart the upgrade. To perform the upgrade, run the following command (this is step 5 in\nthe high-level upgrade procedure):\n% $NEW_HADOOP_INSTALL/bin/start-dfs.sh -upgrade\nThis causes the namenode to upgrade its metadata, placing the previous version in a\nnew directory called previous:\n${dfs.name.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\n/previous/VERSION\n/edits\n/fsimage\n/fstime\nSimilarly, datanodes upgrade their storage directories, preserving the old copy in a\ndirectory called previous.\nWait until the upgrade is complete. The upgrade process is not instantaneous, but you can\ncheck the progress of an upgrade using dfsadmin (upgrade events also appear in the\ndaemons’ logfiles; step 6:\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -upgradeProgress status\nUpgrade for version -18 has been completed.\nUpgrade is not finalized.\nCheck the upgrade. This shows that the upgrade is complete. At this stage, you should run\nsome sanity checks (step 7) on the filesystem (check files and blocks using fsck, basic\nfile operations). You might choose to put HDFS into safe mode while you are running\nsome of these checks (the ones that are read-only) to prevent others from making\nchanges.\nRoll back the upgrade (optional). If you find that the new version is not working correctly,\nyou may choose to roll back to the previous version (step 9). This is only possible if\nyou have not finalized the upgrade.\n298 | Chapter 10: Administering HadoopA rollback reverts the filesystem state to before the upgrade was per-\nformed, so any changes made in the meantime will be lost. In other\nwords, it rolls back to the previous state of the filesystem, rather than\ndowngrading the current state of the filesystem to a former version.\nFirst, shut down the new daemons:\n% $NEW_HADOOP_INSTALL/bin/stop-dfs.sh\nThen start up the old version of HDFS with the -rollback option.\n% $OLD_HADOOP_INSTALL/bin/start-dfs.sh -rollback\nThis command gets the namenode and datanodes to replace their current storage di-\nrectories with their previous copies. The filesystem will be returned to its previous state.\nFinalize the upgrade (optional). When you are happy with the new version of HDFS, you\ncan finalize the upgrade (step 9) to remove the previous storage directories.\nAfter an upgrade has been finalized, there is no way to roll back to the\nprevious version.\nThis step is required before performing another upgrade:\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -finalizeUpgrade\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -upgradeProgress status\nThere are no upgrades in progress.\nHDFS is now fully upgraded to the new version.\nMaintenance | 299CHAPTER 11\nPig\nPig raises the level of abstraction for processing large datasets. With MapReduce, there\nis a map function and there is a reduce function, and working out how to fit your data\nprocessing into this pattern, which often requires multiple MapReduce stages, can be\na challenge. With Pig the data structures are much richer, typically being multivalued\nand nested; and the set of transformations you can apply to the data are much more\npowerful—they include joins, for example, which are not for the faint of heart in\nMapReduce.\nPig is made up of two pieces:\n• The language used to express data flows, called Pig Latin.\n• The execution environment to run Pig Latin programs. There are currently two\nenvironments: local execution in a single JVM and distributed execution on a Ha-\ndoop cluster.\nA Pig Latin program is made up of a series of operations, or transformations, that are\napplied to the input data to produce output. Taken as a whole, the operations describe\na data flow, which the Pig execution environment translates into an executable repre-\nsentation and then runs. Under the covers, Pig turns the transformations into a series\nof MapReduce jobs, but as a programmer you are mostly unaware of this, which allows\nyou to focus on the data rather than the nature of the execution.\nPig is a scripting language for exploring large datasets. One criticism of MapReduce is\nthat the development cycle is very long. Writing the mappers and reducers, compiling\nand packaging the code, submitting the job(s) and retrieving the results is a time-\nconsuming business, and even with Streaming, which removes the compile and package\nstep, the experience is still involved. Pig’s sweet spot is its ability to process terabytes\nof data simply by issuing a half-dozen lines of Pig Latin from the console. Pig is very\nsupportive of a programmer writing a query, since it provides several commands for\nintrospecting the data structures in your program, as it is written. Even more useful, it\ncan perform a sample run on a representative subset of your input data, so you can see\nwhether there are errors in the processing before unleashing it on the full dataset.\n301Pig was designed to be extensible. Virtually all parts of the processing path are cus-\ntomizable: loading, storing, filtering, grouping, ordering, and joining can all be altered\nby user-defined functions (UDFs). These functions operate on Pig’s nested data model,\nso they can integrate very deeply with Pig’s operators. As another benefit, UDFs tend\nto be more reusable than the libraries developed for writing MapReduce programs.\nPig isn’t suitable for all data processing tasks, however. Like MapReduce, it is designed\nfor batch processing of data. If you want to perform a query that touches only a small\namount of data in a large dataset, then Pig will not perform well, since it is set up to\nscan the whole dataset, or at least large portions of it.\nAlso, Pig doesn’t perform as well as programs written in MapReduce. This is not sur-\nprising, as Pig is a general system that compiles queries into MapReduce jobs, so in-\nevitably there will be some overhead in doing this. The good news, however, is that\nthis overhead will come down as the Pig team optimizes the planner—all without you\nhaving to change your Pig queries.\nInstalling and Running Pig\nPig runs as a client-side application. Even if you want to run Pig on a Hadoop cluster,\nthere is nothing extra to install on the cluster: Pig launches jobs and interacts with\nHDFS (or other Hadoop filesystems) from your workstation.\nInstallation is straightforward. Java 6 is a prerequisite (and on Windows, you will need\nCygwin). Download a stable release from http://hadoop.apache.org/pig/releases.html,\nand unpack the tarball in a suitable place on your workstation:\n% tar xzf pig-x.y.z.tar.gz\nIt’s convenient to add Pig’s binary directory to your command-line path. For example:\n% export PIG_INSTALL=/home/tom/pig-x.y.z\n% export PATH=$PATH:$PIG_INSTALL/bin\nYou also need to set the JAVA_HOME environment variable to point to a suitable Java\ninstallation.\nTry typing pig -help to get usage instructions.\nExecution Types\nPig has two execution types or modes: local mode and Hadoop mode.\nLocal mode\nIn local mode, Pig runs in a single JVM and accesses the local filesystem. This mode is\nsuitable only for small datasets, and when trying out Pig. Local mode does not use\nHadoop. In particular, it does not use Hadoop’s local job runner; instead, Pig translates\nqueries into a physical plan that it executes itself.\n302 | Chapter 11: PigThe execution type is set using the -x or -exectype option. To run in local mode, set\nthe option to local:\n% pig -x local\ngrunt>\nThis starts Grunt, the Pig interactive shell, which is discussed in more detail shortly.\nHadoop mode\nIn Hadoop mode, Pig translates queries into MapReduce jobs and runs them on a\nHadoop cluster. The cluster may be a pseudo- or fully distributed cluster. Hadoop mode\n(with a fully distributed cluster) is what you use when you want to run Pig on large\ndatasets.\nTo use Hadoop mode, you need to tell Pig which version of Hadoop you are using and\nwhere your cluster is running. Pig releases will work against only particular versions of\nHadoop. For example, Pig 0.2.0 will run against only a Hadoop 0.17.x or 0.18.x release.\nThe environment variable PIG_HADOOP_VERSION is used to tell Pig the version of Hadoop\nit is connecting to. For example, the following allows Pig to connect to any 0.18.x\nversion of Hadoop:\n% export PIG_HADOOP_VERSION=18\nNext, you need to point Pig at the cluster’s namenode and jobtracker. If you already\nhave a Hadoop site file (or files) that define fs.default.name and mapred.job.tracker,\nyou can simply add Hadoop’s configuration directory to Pig’s classpath:\n% export PIG_CLASSPATH=$HADOOP_INSTALL/conf/\nAlternatively, you can create a pig.properties file in Pig’s conf directory (this directory\nmay need creating, too), which sets these two properties. Here’s an example for a\npseudo-distributed setup:\nfs.default.name=hdfs://localhost/\nmapred.job.tracker=localhost:8021\nOnce you have configured Pig to connect to a Hadoop cluster, you can launch Pig,\nsetting the -x option to mapreduce, or omitting it entirely, as Hadoop mode is the default:\n% pig\n2009-03-29 21:22:20,489 [main] INFO org.apache.pig.backend.hadoop.executionengine.\nHExecutionEngine - Connecting to hadoop file system at: hdfs://localhost/\n2009-03-29 21:22:20,760 [main] INFO org.apache.pig.backend.hadoop.executionengine.\nHExecutionEngine - Connecting to map-reduce job tracker at: localhost:8021\ngrunt>\nAs you can see from the output, Pig reports the filesystem and jobtracker that it has\nconnected to.\nInstalling and Running Pig | 303Running Pig Programs\nThere are three ways of executing Pig programs, all of which work in both local and\nHadoop mode:\nScript\nPig can run a script file that contains Pig commands. For example,\npig script.pig runs the commands in the local file script.pig. Alternatively, for very\nshort scripts, you can use the -e option to run a script specified as a string on the\ncommand line.\nGrunt\nGrunt is an interactive shell for running Pig commands. Grunt is started when no\nfile is specified for Pig to run, and the -e option is not used. It is also possible to\nrun Pig scripts from within Grunt using run and exec.\nEmbedded\nYou can run Pig programs from Java, much like you can use JDBC to run SQL\nprograms from Java. There are more details on the Pig wiki at http://wiki.apache\n.org/pig/EmbeddedPig.\nGrunt\nGrunt has line-editing facilities like those found in GNU Readline (used in the bash\nshell and many other command-line applications). For instance, the Ctrl-E key com-\nbination will move the cursor to the end of the line. Grunt remembers command his-\ntory, too,* and you can recall lines in the history buffer using Ctrl-P or Ctrl-N (for\nprevious and next), or, equivalently, the up or down cursor keys.\nAnother handy feature is Grunt’s completion mechanism, which will try to complete\nPig Latin keywords and functions when you press the Tab key. For example, consider\nthe following incomplete line:\ngrunt> a = foreach b ge\nIf you press the Tab key at this point, ge will expand to generate, a Pig Latin keyword:\ngrunt> a = foreach b generate\nYou can customize the completion tokens by creating a file named autocomplete, and\nplacing it on Pig’s classpath (such as in the conf directory in Pig’s install directory), or\nin the directory you invoked Grunt from. The file should have one token per line, and\ntokens must not contain any whitespace. Matching is case-sensitive. It can be very\nhandy to add commonly used file paths (especially because Pig does not perform file-\nname completion), or the names of any user-defined functions you have created.\n* History is stored in a file called .pig_history in your home directory.\n304 | Chapter 11: PigYou can get a list of commands using the help command. When you’ve finished your\nGrunt session, you can exit with the quit command.\nPig Latin Editors\nPigPen is an Eclipse plug-in that provides an environment for developing Pig programs.\nIt includes a Pig script text editor, an example generator (equivalent to the ILLUS-\nTRATE command), and a button for running the script on a Hadoop cluster. There is\nalso an operator graph window, which shows a script in graph form, for visualizing the\ndata flow. For full installation and usage instructions, please refer to the Pig wiki at\nhttp://wiki.apache.org/pig/PigPen.\nThere are also Pig Latin syntax highlighters for other editors, including Vim and Text-\nMate. Details are available on the Pig wiki.\nAn Example\nLet’s look at a simple example by writing the program to calculate the maximum re-\ncorded temperature by year for the weather dataset in Pig Latin (just like we did using\nMapReduce in Chapter 2). The complete program is only a few lines long:\n-- max_temp.pig: Finds the maximum temperature by year\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nTo explore what’s going on, we’ll use Pig’s Grunt interpreter, which allows us to enter\nlines and interact with the program to understand what it’s doing. Start up Grunt in\nlocal mode, then enter the first line of the Pig script:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year:chararray, temperature:int, quality:int);\nFor simplicity, the program assumes that the input is tab-delimited text, with each line\nhaving just year, temperature, and quality fields. (Pig actually has more flexibility than\nthis with regard to the input formats it accepts, as you’ll see later.) This line describes\nthe input data we want to process. The year:chararray notation describes the field’s\nname and type; a chararray is like a Java string, and an int is like a Java int. The LOAD\noperator takes a URI argument; here we are just using a local file, but we could refer\nto an HDFS URI. The AS clause (which is optional) gives the fields names to make it\nconvenient to refer to them in subsequent statements.\nThe result of the LOAD operator, indeed any operator in Pig Latin, is a relation, which\nis just a set of tuples. A tuple is just like a row of data in a database table, with multiple\nAn Example | 305fields in a particular order. In this example, the LOAD function produces a set of (year,\ntemperature, quality) tuples that are present in the input file. We write a relation with\none tuple per line, where tuples are represented as comma-separated items in\nparentheses:\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\nRelations are given names, or aliases, so they can be referred to. This relation is given\nthe records alias. We can examine the contents of an alias using the DUMP operator:\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nWe can also see the structure of a relation—the relation’s schema—using the\nDESCRIBE operator on the relation’s alias:\ngrunt> DESCRIBE records;\nrecords: {year: chararray,temperature: int,quality: int}\nThis tells us that records has three fields, with aliases year, temperature, and quality,\nwhich are the names we gave them in the AS clause. The fields have the types given to\nthem in the AS clause, too. We shall examine types in Pig in more detail later.\nThe second statement removes records that have a missing temperature (indicated by\na value of 9999), or an unsatisfactory quality reading. For this small dataset, no records\nare filtered out:\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrunt> DUMP filtered_records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nThe third statement uses the GROUP function to group the records relation by the\nyear field. Let’s use DUMP to see what it produces:\ngrunt> grouped_records = GROUP filtered_records BY year;\ngrunt> DUMP grouped_records;\n(1949,{(1949,111,1),(1949,78,1)})\n(1950,{(1950,0,1),(1950,22,1),(1950,-11,1)})\nWe now have two rows, or tuples, one for each year in the input data. The first field in\neach tuple is the field being grouped by (the year), and the second field is a bag of tuples\nfor that year. A bag is just an unordered collection of tuples, which in Pig Latin is\nrepresented using curly braces.\n306 | Chapter 11: PigBy grouping the data in this way, we have created a row per year, so now all that remains\nis to find the maximum temperature for the tuples in each bag. Before we do this, let’s\nunderstand the structure of the grouped_records relation:\ngrunt> DESCRIBE grouped_records;\ngrouped_records: {group: chararray,filtered_records: {year: chararray,\ntemperature: int,quality: int}}\nThis tells us that the grouping field is given the alias group by Pig, and the second field\nis the same structure as the filtered_records relation that was being grouped. With\nthis information, we can try the fourth transformation:\ngrunt> max_temp = FOREACH grouped_records GENERATE group,\n>>\nMAX(filtered_records.temperature);\nFOREACH processes every row to generate a derived set of rows, using a GENERATE\nclause to define the fields in each derived row. In this example, the first field is group,\nwhich is just the year. The second field is a little more complex. The\nfiltered_records.temperature reference is to the temperature field of the fil\ntered_records bag in the grouped_records relation. MAX is a built-in function for calcu-\nlating the maximum value of fields in a bag. In this case, it calculates the maximum\ntemperature for the fields in each filtered_records bag. Let’s check the result:\ngrunt> DUMP max_temp;\n(1949,111)\n(1950,22)\nSo we’ve successfully calculated the maximum temperature for each year.\nGenerating Examples\nIn this example, we’ve used a small sample dataset with just a handful of rows to make\nit easier to follow the data flow and aid debugging. Creating a cut-down dataset is an\nart, as ideally it should be rich enough to cover all the cases to exercise your queries\n(the completeness property), yet be small enough to reason about by the programmer\n(the conciseness property). Using a random sample doesn’t work well in general, since\njoin and filter operations tend to remove all random data, leaving an empty result,\nwhich is not illustrative of the general flow.\nWith the ILLUSTRATE operator, Pig provides a tool for generating a reasonably com-\nplete and concise dataset. Although it can’t generate examples for all queries (it doesn’t\nsupport LIMIT, SPLIT, or nested FOREACH statements, for example), it can generate\nuseful examples for many queries. ILLUSTRATE works only if the relation has a\nschema.\nHere is the output from running ILLUSTRATE (slightly reformatted to fit the page):\nAn Example | 307grunt> ILLUSTRATE max_temp;\n-------------------------------------------------------------------------------\n| records\n| year: bytearray | temperature: bytearray | quality: bytearray |\n-------------------------------------------------------------------------------\n|\n| 1949\n| 9999\n| 1\n|\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------\n| records\n| year: chararray | temperature: int | quality: int |\n-------------------------------------------------------------------\n|\n| 1949\n| 9999\n| 1\n|\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n-------------------------------------------------------------------\n----------------------------------------------------------------------------\n| filtered_records\n| year: chararray | temperature: int | quality: int |\n----------------------------------------------------------------------------\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n----------------------------------------------------------------------------\n------------------------------------------------------------------------------------\n| grouped_records\n| group: chararray | filtered_records: bag({year: chararray, |\ntemperature: int,quality: int}) |\n------------------------------------------------------------------------------------\n|\n| 1949\n| {(1949, 111, 1), (1949, 78, 1)}\n|\n------------------------------------------------------------------------------------\n-------------------------------------------\n| max_temp\n| group: chararray | int\n|\n-------------------------------------------\n|\n| 1949\n| 111\n|\n-------------------------------------------\nNotice that Pig used some of the original data (this is important to keep the generated\ndataset realistic), as well as creating some new data. It noticed the special value 9999\nin the query and created a tuple containing this value to exercise the FILTER statement.\nIn summary, the output of the ILLUSTRATE is easy to follow and can help you un-\nderstand what your query is doing.\nComparison with Databases\nHaving seen Pig in action, it might seem that Pig Latin is similar to SQL. The presence\nof such operators as GROUP BY and DESCRIBE reinforces this impression. However,\nthere are several differences between the two languages, and between Pig and RDBMSs\nin general.\nThe most significant difference is that Pig Latin is a data flow programming language,\nwhereas SQL is a declarative programming language. In other words, a Pig Latin pro-\ngram is a step-by-step set of operations on an input relation, in which each step is a\nsingle transformation. By contrast, SQL statements are a set of constraints that taken\ntogether define the output. In many ways, programming in Pig Latin is like working at\n308 | Chapter 11: Pigthe level of an RDBMS query planner, which figures out how to turn a declarative\nstatement into a system of steps.\nRDBMSs store data in tables, with tightly predefined schemas. Pig is more relaxed about\nthe data that it processes: you can define a schema at runtime, but it’s optional. Es-\nsentially, it will operate on any source of tuples (although the source should support\nbeing read in parallel, by being in multiple files, for example), where a UDF is used to\nread the tuples from their raw representation.† The most common representation is a\ntext file with tab-separated fields, and Pig provides a built-in load function for this\nformat. Unlike with a traditional database, there is no data import process to load the\ndata into the RDBMS. The data is loaded from the filesystem (usually HDFS) as the\nfirst step in the processing.\nPig’s support for complex, nested data structures differentiates it from SQL, which\noperates on flatter data structures. Also, Pig’s ability to use UDFs and streaming op-\nerators that are tightly integrated with the language and Pig’s nested data structures\nmakes Pig Latin more customizable than most SQL dialects.\nThere are several features to support online, low-latency queries that RDBMSs have\nthat are absent in Pig, such as transactions and indexes. As mentioned earlier, Pig does\nnot support random reads or queries in the order of tens of milliseconds. Nor does it\nsupport random writes, to update small portions of data; all writes are bulk, streaming\nwrites, just like MapReduce.\nHive is a subproject of Hadoop that sits between Pig and conventional RDBMSs. Like\nPig, Hive is designed to use HDFS for storage, but otherwise there are some significant\ndifferences. Its query language, Hive QL, is based on SQL, and anyone who is familiar\nwith SQL would have little trouble writing queries in Hive QL. Like RDBMSs, Hive\nmandates that all data be stored in tables, with a schema under its management; how-\never, it can associate a schema with preexisting data in HDFS, so the load step is op-\ntional. Hive does not support low-latency queries, a characteristic it shares with Pig.\nHive is discussed further in “Hadoop and Hive at Facebook” on page 414.\nPig Latin\nThis section gives an informal description of the syntax and semantics of the Pig Latin\nprogramming language.‡ It is not meant to offer a complete reference to the\n† Or as the Pig Philosophy has it, “Pigs eat anything.”\n‡ Not to be confused with Pig Latin the language game. English words are translated into Pig Latin by moving\nthe initial consonant sound to the end of the word and adding an “ay” sound. For example, “pig” becomes\n“ig-pay,” and “Hadoop” becomes “Adoop-hay.”\nPig Latin | 309language,§ but there should be enough here for you to get a good understanding of Pig\nLatin’s constructs.\nStructure\nA Pig Latin program consists of a collection of statements. A statement can be thought\nof as an operation, or a command.‖ For example, a GROUP operation is a type of\nstatement:\ngrouped_records = GROUP records BY year;\nThe command to list the files in a Hadoop filesystem is another example of a statement:\nls /\nStatements are usually terminated with a semicolon, as in the example of the GROUP\nstatement. In fact, this is an example of a statement that must be terminated with a\nsemicolon: it is a syntax error to omit it. The ls command, on the other hand, does not\nhave to be terminated with a semicolon. As a general guideline, statements or com-\nmands for interactive use in Grunt do not need the terminating semicolon. This group\nincludes the interactive Hadoop commands, as well as the diagnostic operators like\nDESCRIBE. It’s never an error to add a terminating semicolon, so if in doubt, it’s sim-\nplest to add one.\nStatements that have to be terminated with a semicolon can be split across multiple\nlines for readability:\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nPig Latin has two forms of comments. Double hyphens are single-line comments.\nEverything from the first hyphen to the end of the line is ignored by the Pig Latin\ninterpreter:\n-- My program\nDUMP A; -- What's in A?\nC-style comments are more flexible since they delimit the beginning and end of the\ncomment block with /* and */ markers. They can span lines or be embedded in a single\nline:\n/*\n* Description of my program spanning\n* multiple lines.\n*/\nA = LOAD 'input/pig/join/A';\nB = LOAD 'input/pig/join/B';\n§ Pig Latin does not have a formal language definition as such, but there is a comprehensive guide to the\nlanguage that can be found linked to from the Pig wiki at http://wiki.apache.org/pig/.\n‖ You sometimes these terms being used interchangeably in documentation on Pig Latin. For example,\n“GROUP command, ” “GROUP operation,” “GROUP statement.”\n310 | Chapter 11: PigC = JOIN A BY $0, /* ignored */ B BY $1;\nDUMP C;\nPig Latin has a list of keywords that have a special meaning in the language and cannot\nbe used as identifiers. These include the operators (LOAD, ILLUSTRATE), commands\n(cat, ls), expressions (matches, FLATTEN), and functions (DIFF, MAX)—all of which\nare covered in the following sections.\nPig Latin has mixed rules on case sensitivity. Operators and commands are not case-\nsensitive (to make interactive use more forgiving); however, aliases and function names\nare case-sensitive.\nStatements\nAs a Pig Latin program is executed, each statement is parsed in turn. If there are syntax\nerrors, or other (semantic) problems such as undefined aliases, the interpreter will halt\nand display an error message. The interpreter builds a logical plan for every relational\noperation, which forms the core of a Pig Latin program. The logical plan for the state-\nment is added to the logical plan for the program so far, then the interpreter moves on\nto the next statement.\nIt’s important to note that no data processing takes place while the logical plan of the\nprogram is being constructed. For example, consider again the Pig Latin program from\nthe first example:\n-- max_temp.pig: Finds the maximum temperature by year\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nWhen the Pig Latin interpreter sees the first line containing the LOAD statement, it\nconfirms that it is syntactically and semantically correct, and adds it to the logical plan,\nbut it does not load the data from the file (or even check whether the file exists). Indeed,\nwhere would it load it? Into memory? Even if it did fit into memory, what would it do\nwith the data? Perhaps not all the input data is needed (since later statements filter it,\nfor example), so it would be pointless to load it. The point is that it makes no sense to\nstart any processing until the whole flow is defined. Similarly, Pig validates the GROUP\nand FOREACH ... GENERATE statements, and adds them to the logical plan without\nexecuting them. The trigger for Pig to start processing is the DUMP statement (a STORE\nstatement also triggers processing). At that point, the logical plan is compiled into a\nphysical plan and executed.\nPig Latin | 311The type of physical plan that Pig prepares depends on the execution environment. For\nlocal execution, Pig will create a physical plan that runs in a single local JVM, whereas\nfor execution on Hadoop, Pig compiles the logical plan into a series of MapReduce jobs.\nYou can see the logical and physical plans created by Pig using the\nEXPLAIN command on a relation (EXPLAIN max_temp; for example).\nIn MapReduce mode, EXPLAIN will also show the MapReduce plan,\nwhich shows how the physical operators are grouped into MapReduce\njobs. This is a good way to find out how many MapReduce jobs Pig will\nrun for your query.\nThe relational operators that can be a part of a logical plan in Pig are summarized in\nTable 11-1. We shall go through each operator in more detail in “Data Processing\nOperators” on page 331.\nTable 11-1. Pig Latin relational operators\nCategory Operator Description\nLoading and storing LOAD Loads data from the filesystem or other storage into a relation\nSTORE Saves a relation to the filesystem or other storage\nDUMP Prints a relation to the console\nFILTER Removes unwanted rows from a relation\nDISTINCT Removes duplicate rows from a relation\nFOREACH ... GENERATE Adds or removes fields from a relation\nSTREAM Transforms a relation using an external program\nJOIN Joins two or more relations\nCOGROUP Groups the data in two or more relations\nGROUP Groups the data in a single relation\nCROSS Creates the cross product of two or more relations\nORDER Sorts a relation by one or more fields\nLIMIT Limits the size of a relation to a maximum number of tuples\nUNION Combines two or more relations into one\nSPLIT Splits a relation into two or more relations\nFiltering\nGrouping and joining\nSorting\nCombining and splitting\nThere are other types of statement that are not added to the logical plan. For example,\nthe diagnostic operators, DESCRIBE, EXPLAIN, and ILLUSTRATE are provided to\nallow the user to interact with the logical plan, for debugging purposes (see Ta-\nble 11-2). DUMP is a sort of diagnostic operator too, since it is used only to allow\ninteractive debugging of small result sets, or in combination with LIMIT to retrieve a\n312 | Chapter 11: Pigfew rows from a larger relation. The STORE statement should be used when the size\nof the output is more than a few lines, as it writes to a file, rather than to the console.\nTable 11-2. Pig Latin diagnostic operators\nOperator Description\nDESCRIBE Prints a relation’s schema\nEXPLAIN Prints the logical and physical plans\nILLUSTRATE Shows a sample execution of the logical plan, using a generated subset of the input\nPig Latin provides two statements, REGISTER and DEFINE, to make it possible to\nincorporate user-defined functions into Pig scripts (see Table 11-3).\nTable 11-3. Pig Latin UDF statements\nStatement Description\nREGISTER Registers a JAR file with the Pig runtime\nDEFINE Creates an alias for a UDF, streaming script, or a command specification\nSince they do not process relations, commands are not added to the logical plan; in-\nstead, they are executed immediately. Pig provides commands to interact with Hadoop\nfilesystems (which are very handy for moving data around before or after processing\nwith Pig) and MapReduce, as well as a few utility commands (described in Table 11-4).\nTable 11-4. Pig Latin commands\nCategory Command Description\nHadoop Filesystem cat Prints the contents of one or more files\ncd Changes the current directory\ncopyFromLocal Copies a local file or directory to a Hadoop filesystem\ncopyToLocal Copies a file or directory on a Hadoop filesystem to the local filesystem\ncp Copies a file or directory to another directory\nls Lists files\nmkdir Creates a new directory\nmv Moves a file or directory to another directory\npwd Prints the path of the current working directory\nrm Deletes a file or directory\nrmf Forcibly deletes a file or directory (does not fail if the file or directory does not exist)\nHadoop MapReduce kill Kills a MapReduce job\nUtility help Shows the available commands and options\nquit Exits the interpreter\nset Sets Pig options\nPig Latin | 313The filesystem commands can operate on files or directories in any Hadoop filesystem,\nand they are very similar to the hadoop fs commands (which is not surprising, as both\nare simple wrappers around the Hadoop FileSystem interface). Precisely which Hadoop\nfilesystem is used is determined by the fs.default.name property in the site file for\nHadoop Core. See “The Command-Line Interface” on page 45 for more details on how\nto configure this file.\nThese commands are mostly self-explanatory, except set, which is used to set options\nthat control Pig’s behavior. The debug option is used to turn debug logging on or off\nfrom within a script (you can also control the log level when launching Pig, using the\n-d or -debug option).\ngrunt> set debug on\nAnother useful option is the job.name option, which gives a Pig job a meaningful name,\nmaking it easier to pick out your Pig MapReduce jobs when running on a shared Ha-\ndoop cluster. If Pig is running a script (rather than being an interactive query from\nGrunt), its job name defaults to a value based on the script name.\nExpressions\nAn expression is something that is evaluated to yield a value. Expressions can be used\nin Pig as a part of a statement containing a relational operator. Pig has a rich variety of\nexpressions, many of which will be familiar from other programming languages. They\nare listed in Table 11-5, with brief descriptions and examples. We shall examples of\nmany of these expressions throughout the chapter.\nTable 11-5. Pig Latin expressions\nCategory Expressions Description Examples\nConstant Literal Constant value (see also literals in Table 11-6) 1.0, 'a'\nField (by posi- $n Field in position n (zero-based) $0\ntion) \nField (by name) f Field named f year\nProjection c.$n, c.f Field in container c (relation, bag, or tuple) records.$0, records.year\n                     by position, by name \nMap lookup m#k Value associated with key k in map m items#'Coat'\nCast (t) f Cast of field f to type t (int) year\nArithmetic x + y, x - y Addition, subtraction $1 + $2, $1 - $2\nx * y, x / y Multiplication, division $1 * $2, $1 / $2\nx % y Modulo, the remainder of x divided by y $1 % $2\n+x, -x Unary positive, negation +1, –1\nx ? y : z Bincond/ternary, y if x evaluates to true, z quality == 0 ? 0 : 1\n          otherwise \nConditional\n314 | Chapter 11: PigCategory Expressions Description Examples\nComparison x == y, x != y Equals, not equals quality == 0, tempera\n                                            ture != 9999\nx > y, x < y Greater than, less than quality > 0, quality < 10\nx >= y, x <= y Greater than or equal to, less than or equal to quality >= 1, quality <= 9\nx matches y Pattern matching with regular expression quality matches\n                                                    '[01459]'\nx is null Is null temperature is null\nx is not null Is not null temperature is not null\nx or y Logical or q == 0 or q == 1\nx and y Logical and q == 0 and r == 0\nBoolean\nnot x Logical negation not q matches '[01459]'\nFunctional fn(f1,f2,...) Invocation of function fn on fields f1, f2, isGood(quality)\n                         etc. \nFlatten FLATTEN(f) Removal of nesting from bags and tuples FLATTEN(group)\nTypes\nSo far you have seen some of the simple types in Pig, such as int and chararray. Here\nwe will discuss Pig’s built-in types in more detail.\nPig has four numeric types: int, long, float, and double, which are identical to their\nJava counterparts. There is also a bytearray type, like Java’s byte array type for repre-\nsenting a blob of binary data; and chararray, which, like java.lang.String, represents\ntextual data in UTF-16 format, although it can be loaded or stored in UTF-8 format.\nPig does not have types corresponding to Java’s boolean,# byte, short, or char primitive\ntypes. These are all easily represented using Pig’s int type, or chararray for char.\nThe numeric, textual, and binary types are simple atomic types. Pig Latin also has three\ncomplex types for representing nested structures: tuple, bag, and map. All of Pig Latin’s\ntypes are listed in Table 11-6.\n# Although there is no boolean type for data, Pig has the concept of an expression evaluating to true or false,\nfor testing conditions (such as in a FILTER statement). However, Pig does not allow a boolean expression to\nbe stored in a field.\nPig Latin | 315Table 11-6. Pig Latin types\nCategory Type Description Literal example\nNumeric int 32-bit signed integer 1\nlong 64-bit signed integer 1L\nfloat 32-bit floating-point number 1.0F\ndouble 64-bit floating-point number 1.0\nText chararray Character array in UTF-16 format 'a'\nBinary bytearray Byte array Not supported\nComplex tuple Sequence of fields of any type (1,'pomegranate')\nbag An unordered collection of tuples, possibly with duplicates {(1,'pomegranate'),(2)}\nmap A set of key-value pairs. Keys must be atoms, values may [1#'pomegranate']\n    be any type \nThe complex types are usually loaded from files or constructed using relational oper-\nators. Be aware, however, that the literal form in Table 11-6 is used when a constant\nvalue is created from within a Pig Latin program. The raw form in a file is usually\ndifferent when using the standard PigStorage loader. For example, the representation\nin a file of the bag in Table 11-6 would be {(1,pomegranate),(2)} (note the lack of\nquotes), and with a suitable schema, this would be loaded as a relation with a single\nfield and row, whose value was the bag.\nMaps are always loaded from files, since there is no relational operator in Pig that\nproduces a map. It’s possible to write a UDF to generate maps, if desired.\nAlthough relations and bags are conceptually the same (an unordered collection of\ntuples), in practice Pig treats them slightly differently. A relation is a top-level construct,\nwhereas a bag has to be contained in a relation. Normally you don’t have to worry\nabout this, but there are a few restrictions that can trip up the uninitiated. For example,\nit’s not possible to create a relation from a bag literal. So the following statement fails:\nA = {(1,2),(3,4)}; -- Error\nThe simplest workaround in this case is to load the data from a file using the LOAD\nstatement.\nAs another example, you can’t treat a relation like a bag and project a field into a new\nrelation ($0 refers to the first field of A, using the positional notation):\nB = A.$0;\nInstead, you have to use a relational operator to turn the relation A into relation B:\nB = FOREACH A GENERATE $0;\nIt’s possible that a future version of Pig Latin will remove these inconsistencies and\ntreat relations and bags in the same way.\n316 | Chapter 11: PigSchemas\nA relation in Pig may have an associated schema, which gives the fields in the relation\nnames and types. We’ve seen how an AS clause in a LOAD statement is used to attach\na schema to a relation:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year:int, temperature:int, quality:int);\ngrunt> DESCRIBE records;\nrecords: {year: int,temperature: int,quality: int}\nThis time we’ve declared the year to be an integer, rather than of type chararray, even\nthough the file it is being loaded from is the same. An integer may be more appropriate\nif we needed to manipulate the year arithmetically (to turn it into a timestamp, for\nexample), whereas the chararray representation might be more appropriate when it’s\nbeing used as a simple identifier. Pig’s flexibility in the degree to which schemas are\ndeclared contrasts with schemas in traditional SQL databases, which are declared be-\nfore the data is loaded into to the system. Pig is designed for analyzing plain input files\nwith no associated type information, so it is quite natural to choose types for fields later\nthan you would with an RDBMS.\nIt’s possible to omit type declarations completely, too:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year, temperature, quality);\ngrunt> DESCRIBE records;\nrecords: {year: bytearray,temperature: bytearray,quality: bytearray}\nIn this case, we have specified only the names of the fields in the schema, year, temper\nature, and quality. The types default to bytearray, the most general type, representing\na binary string.\nYou don’t need to specify types for every field; you can leave some to default to byte\narray, as we have done for year in this declaration:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year, temperature:int, quality:int);\ngrunt> DESCRIBE records;\nrecords: {year: bytearray,temperature: int,quality: int}\nHowever, if you specify a schema in this way, you do need to specify every field. Also\nthere’s no way to specify the type of a field without specifying the name. On the other\nhand, the schema is entirely optional, and can be omitted by not specifying an AS clause:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt';\ngrunt> DESCRIBE records;\nSchema for records unknown.\nFields in a relation with no schema can be referenced only using positional notation:\n$0 refers to the first field in a relation, $1 to the second, and so on. Their types default\nto bytearray:\nPig Latin | 317grunt> projected_records = FOREACH records GENERATE $0, $1, $2;\ngrunt> DUMP projected_records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> DESCRIBE projected_records;\nprojected_records: {bytearray,bytearray,bytearray}\nAlthough it can be convenient not to have to assign types to fields (particularly in the\nfirst stages of writing a query), doing so can improve the clarity and efficiency of Pig\nLatin programs, and is generally recommended.\nDeclaring a schema as a part of the query is flexible, but doesn’t lend\nitself to schema reuse. A set of Pig queries over the same input data will\noften have the same schema repeated in each query. If the query pro-\ncesses a large number of fields, this repetition can become hard to main-\ntain, since Pig (unlike Hive) doesn’t have a way to associate a schema\nwith data outside of a query. One way to solve this problem is to write\nyour own load function, which encapsulates the schema. This is descri-\nbed in more detail in “A Load UDF” on page 327.\nValidation and nulls\nA SQL database will enforce the constraints in a table’s schema at load time: for ex-\nample, trying to load a string into a column that is declared to be a numeric type will\nfail. In Pig, if the value cannot be cast to the type declared in the schema, then it will\nsubstitute a null value. Let’s see how this works if we have the following input for the\nweather data, which has an “e” character in place of an integer:\n1950\n1950\n1950\n1949\n1949\n0\n22\ne\n111\n78\n1\n1\n1\n1\n1\nPig handles the corrupt line by producing a null for the offending value, which is dis-\nplayed as the absence of a value when dumped to screen (and also when saved using\nSTORE):\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'\n>>\nAS (year:chararray, temperature:int, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,,1)\n(1949,111,1)\n(1949,78,1)\n318 | Chapter 11: PigPig produces a warning for the invalid field (not shown here), but does not halt its\nprocessing. For large datasets, it is very common to have corrupt, invalid, or merely\nunexpected data, and it is generally infeasible to incrementally fix every unparsable\nrecord. Instead, we can pull out all of the invalid records in one go, so we can take\naction on them, perhaps by fixing our program (because they indicate we have made a\nmistake), or by filtering them out (because the data is genuinely unusable).\ngrunt> corrupt_records = FILTER records BY temperature is null;\ngrunt> DUMP corrupt_records;\n(1950,,1)\nNote the use of the is null operator, which is analogous to SQL. In practice, we would\ninclude more information from the original record such as an identifier and the value\nthat could not be parsed, to help our analysis of the bad data.\nWe can find the number of corrupt records using the following idiom for counting the\nnumber of rows in a relation:\ngrunt> grouped = GROUP corrupt_records ALL;\ngrunt> all_grouped = FOREACH grouped GENERATE group, COUNT(corrupt_records);\ngrunt> DUMP all_grouped;\n(all,1L)\nAnother useful technique is to use the SPLIT operator to partition the data into “good”\nand “bad” relations, which can then be analyzed separately:\ngrunt> SPLIT records INTO good_records IF temperature is not null,\n>>\nbad_records IF temperature is null;\ngrunt> DUMP good_records;\n(1950,0,1)\n(1950,22,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> DUMP bad_records;\n(1950,,1)\nGoing back to the case in which temperature’s type was left undeclared, the corrupt\ndata cannot be easily detected, since it doesn’t surface as a null:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'\n>>\nAS (year:chararray, temperature, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,e,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrunt> grouped_records = GROUP filtered_records BY year;\ngrunt> max_temp = FOREACH grouped_records GENERATE group,\n>>\nMAX(filtered_records.temperature);\ngrunt> DUMP max_temp;\n(1949,111.0)\n(1950,22.0)\nPig Latin | 319What happens in this case is that the temperature field is interpreted as a bytearray, so\nthe corrupt field is not detected when the input is loaded. When passed to the MAX\nfunction the temperature field is cast to a double, since MAX works only with numeric\ntypes. The corrupt field can not be represented as a double, so it becomes a null, which\nMAX silently ignores. The best approach is generally to declare types for your data on\nloading, and look for missing or corrupt values in the relations themselves before you\ndo your main processing.\nSometimes corrupt data shows up as smaller tuples since fields are simply missing. You\ncan filter these out by using the SIZE function as follows:\ngrunt> A = LOAD 'input/pig/corrupt/missing_fields';\ngrunt> DUMP A;\n(2,Tie)\n(4,Coat)\n(3)\n(1,Scarf)\ngrunt> B = FILTER A BY SIZE(*) > 1;\ngrunt> DUMP B;\n(2,Tie)\n(4,Coat)\n(1,Scarf)\nSchema merging\nIn Pig, you don’t declare the schema for every new relation in the data flow. In most\ncases, Pig can figure out the resulting schema for the output of a relational operation\nby considering the schema of the input relation.\nHow are schemas propagated to new relations? Some relational operators don’t change\nthe schema, so the relation produced by the LIMIT operator (which restricts a relation\nto a maximum number of tuples), for example, has the same schema as the relation it\noperates on. For other operators, the situation is more complicated. UNION, for ex-\nample, combines two or more relations into one, and tries to merge the input relations\nschemas. If the schemas are incompatible, due to different types or number of fields,\nthen the schema of the result of the UNION is unknown.\nYou can find out the schema for any relation in the data flow using the DESCRIBE\noperator. If you want to redefine the schema for a relation, you can use the FORE-\nACH ... GENERATE operator with AS clauses to define the schema for some or all of\nthe fields of the input relation.\nSee “User-Defined Functions” on page 322 for further discussion of schemas.\nFunctions\nFunctions in Pig come in five types:\n320 | Chapter 11: PigEval function\nA function that takes one or more expressions and returns another expression. An\nexample of a built-in eval function is MAX, which returns the maximum value of the\nentries in a bag. Some eval functions are aggregate functions, which means they\noperate on a bag of data to produce a scalar value; MAX is an example of an aggregate\nfunction. Furthermore, many aggregate functions are algebraic, which means that\nthe result of the function may be calculated incrementally. In MapReduce terms,\nalgebraic functions make use of the combiner, and are much more efficient to cal-\nculate (see “Combiner Functions” on page 29). MAX is an algebraic function,\nwhereas a function to calculate the median of a collection of values is an example\nof a function that is not algebraic.\nFilter function\nA special type of eval function that returns a logical boolean result. As the name\nsuggests, filter functions are used in the FILTER operator to remove unwanted\nrows. They can also be used in other relational operators that take boolean con-\nditions, and in general expressions using boolean or conditional expressions. An\nexample of a built-in filter function is IsEmpty, which tests whether a bag or a map\ncontains any items.\nComparison function\nA function that can impose an ordering on a pair of tuples. You can specify the\ncomparison function to use in an ORDER clause to control the sort order.\nLoad function\nA function that specifies how to load data into a relation from external storage.\nStore function\nA function that specifies how to save the contents of a relation to external storage.\nOften load and store functions are implemented by the same type. For example,\nPigStorage, which loads data from delimited text files, can store data in the same\nformat.\nPig has a small collection of built-in functions, which are listed in Table 11-7.\nTable 11-7. Pig built-in functions\nCategory Function Description\nEval AVG Calculates the average (mean) value of entries in a bag.\nCONCAT Concatenates two byte arrays or two character arrays together.\nCOUNT Calculates the number of entries in a bag.\nDIFF Calculates the set difference of two bags. If the two arguments are not bags, then returns\n    a bag containing both if they are equal; otherwise, returns an empty bag.\nMAX Calculates the maximum value of entries in a bag.\nMIN Calculates the minimum value of entries in a bag.\nPig Latin | 321Category\nFunction Description\nSIZE Calculates the size of a type. The size of numeric types is always one, for character arrays\n    it is the number of characters, for byte arrays the number of bytes, and for containers\n   (tuple, bag, map) it is the number of entries.\nSUM Calculates the sum of the values of entries in a bag.\nTOKENIZE Tokenizes a character array into a bag of its constituent words.\nFilter IsEmpty Tests if a bag or map is empty.\nLoad/Store PigStorage Loads or stores relations using a field-delimited text format. Each line is broken into fields\n                     using a configurable field delimiter (defaults to a tab character) to be stored in the tuple’s\n                      fields. It is the default storage when none is specified.\nBinStorage Loads or stores relations from or to binary files. An internal Pig format is used that uses\n          Hadoop Writable objects.\nBinaryStorage Loads or stores relations containing only single-field tuples with a value of type byte\n             array from or to binary files. The bytes of the bytearray values are stored verbatim.\n            Used with Pig streaming.\nTextLoader Loads relations from a plain-text format. Each line corresponds to a tuple whose single\n          field is the line of text.\nPigDump Stores relations by writing the toString() representation of tuples, one per line. Useful\n       for debugging.\nIf the function you need is not available, you can write your own. Before you do that,\nhowever, have a look in the Piggy Bank, a repository of Pig functions shared by the Pig\ncommunity. There are details on the Pig wiki at http://wiki.apache.org/pig/PiggyBank\non how to browse and obtain the Piggy Bank functions. If the Piggy Bank doesn’t have\nwhat you need, you can write your own function (and if it is sufficiently general, you\nmight consider contributing it to the Piggy Bank so that others can benefit from it, too).\nThese are known as user-defined functions, or UDFs.\nUser-Defined Functions\nPig’s designers realized that the ability to plug-in custom code is crucial for all but the\nmost trivial data processing jobs. For this reason, they made it easy to define and use\nuser-defined functions.\nA Filter UDF\nLet’s demonstrate by writing a filter function for filtering out weather records that do\nnot have a temperature quality reading of satisfactory (or better). The idea is to change\nthis line:\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\n322 | Chapter 11: Pigto:\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\nThis achieves two things: it makes the Pig script more concise, and it encapsulates the\nlogic in one place so that it can be easily reused in other scripts. If we were just writing\nan ad hoc query, then we probably wouldn’t bother to write a UDF. It’s when you start\ndoing the same kind of processing over and over again that you see opportunities for\nreusable UDFs.\nUDFs are written in Java, and filter functions are all subclasses of FilterFunc, which\nitself is a subclass of EvalFunc. We’ll look at EvalFunc in more detail later, but for the\nmoment just note that, in essence, EvalFunc looks like the following class:\npublic abstract class EvalFunc<T> {\npublic abstract T exec(Tuple input) throws IOException;\n}\nEvalFunc’s only abstract method, exec(), takes a tuple and returns a single value, the\n(parameterized) type T. The fields in the input tuple consist of the expressions passed\nto the function; in this case, a single integer. For FilterFunc, T is Boolean, so the method\nshould return true only for those tuples that should not be filtered out.\nFor the quality filter, we write a class, IsGoodQuality, that extends FilterFunc and im-\nplements the exec() method. See Example 11-1. The Tuple class is essentially a list of\nobjects with associated types. Here we are concerned only with the first field (since the\nfunction only has a single argument), which we extract by index using the get() method\non Tuple. The field is an integer, so if it’s not null, we cast it and check whether the\nvalue is one that signifies the temperature was a good reading, returning the appropriate\nvalue, true or false.\nExample 11-1. A FilterFunc UDF to remove records with unsatisfactory temperature quality readings\npackage com.hadoopbook.pig;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.pig.FilterFunc;\nimport\nimport\nimport\nimport\norg.apache.pig.backend.executionengine.ExecException;\norg.apache.pig.data.DataType;\norg.apache.pig.data.Tuple;\norg.apache.pig.impl.logicalLayer.FrontendException;\npublic class IsGoodQuality extends FilterFunc {\n@Override\npublic Boolean exec(Tuple tuple) throws IOException {\nif (tuple == null || tuple.size() == 0) {\nreturn false;\n}\nUser-Defined Functions | 323}\ntry {\nObject object = tuple.get(0);\nif (object == null) {\nreturn false;\n}\nint i = (Integer) object;\nreturn i == 0 || i == 1 || i == 4 || i == 5 || i == 9;\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\nTo use the new function, we first compile it, and package it in a JAR file (in the example\ncode that accompanies this book, we can do this by typing ant pig). Then we tell Pig\nabout the JAR file with the REGISTER operator, which is given the local path to the\nfilename (and is not enclosed in quotes):\ngrunt> REGISTER pig.jar;\nFinally, we can invoke the function:\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\ncom.hadoopbook.pig.IsGoodQuality(quality);\nPig resolves function calls by treating the function’s name as a Java classname and\nattempting to load a class of that name. (This, incidentally, is why function names are\ncase-sensitive: because Java classnames are.) When searching for classes, Pig uses a\nclassloader that includes the JAR files that have been registered. When running in dis-\ntributed mode, Pig will ensure that your JAR files get shipped to the cluster.\nFor the UDF in this example, Pig looks for a class with the name com.hadoop\nbook.pig.IsGoodQuality, which it finds in the JAR file we registered.\nResolution of built-in functions proceeds in the same way, except for one difference:\nPig has a set of built-in package names that it searches, so the function call does not\nhave to be a fully qualified name. For example, the function MAX is actually implemented\nby a class MAX in the package org.apache.pig.builtin. This is one of the packages that\nPig looks in, so we can write MAX rather than org.apache.pig.builtin.MAX in our Pig\nprograms.\nWe can’t register our package with Pig, but we can shorten the function name by de-\nfining an alias, using the DEFINE operator:\ngrunt> DEFINE isGood com.hadoopbook.pig.IsGoodQuality();\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND isGood(quality);\nDefining an alias is a good idea if you want to use the function several times in the same\nscript. It’s also necessary if you want to pass arguments to the constructor of the UDF’s\nimplementation class.\n324 | Chapter 11: PigLeveraging types\nThe filter works when the quality field is declared to be of type int, but if the type\ninformation is absent, then the UDF fails! This happens because the field is the default\ntype, bytearray, represented by the DataByteArray class. Because DataByteArray is not\nan Integer, the cast fails.\nThe obvious way to fix this is to convert the field to an integer in the exec() method.\nHowever, there is a better way, which is to tell Pig the types of the fields that the function\nexpects. The getArgToFuncMapping() method on EvalFunc is provided for precisely this\nreason. We can override it to tell Pig that the first field should be an integer:\n@Override\npublic List<FuncSpec> getArgToFuncMapping() throws FrontendException {\nList<FuncSpec> funcSpecs = new ArrayList<FuncSpec>();\nfuncSpecs.add(new FuncSpec(this.getClass().getName(),\nnew Schema(new Schema.FieldSchema(null, DataType.INTEGER))));\n}\nreturn funcSpecs;\nThis method returns a FuncSpec object corresponding to each of the fields of the tuple\nthat are passed to the exec() method. Here there is a single field, and we construct an\nanonymous FieldSchema (the name is passed as null, since Pig ignores the name when\ndoing type conversion). The type is specified using the INTEGER constant on Pig’s Data\nType class.\nWith the amended function, Pig will attempt to convert the argument passed to the\nfunction to an integer. If the field cannot be converted, then a null is passed for the\nfield. The exec() method always returns false if the field is null. For this application\nthis behavior is appropriate, as we want to filter out records whose quality field is\nunintelligible.\nHere’s the final program using the new function:\n-- max_temp_filter_udf.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nAn Eval UDF\nWriting an eval function is a small step up from writing a filter function. Consider a\nUDF (see Example 11-2) for trimming leading and trailing whitespace from\nUser-Defined Functions | 325chararray values, just like the trim() method on java.lang.String. We will use this\nUDF later in the chapter.\nExample 11-2. An EvalFunc UDF to trim leading and trailing whitespace from chararray values\npublic class Trim extends EvalFunc<String> {\n@Override\npublic String exec(Tuple input) throws IOException {\nif (input == null || input.size() == 0) {\nreturn null;\n}\ntry {\nObject object = input.get(0);\nif (object == null) {\nreturn null;\n}\nreturn ((String) object).trim();\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\n@Override\npublic List<FuncSpec> getArgToFuncMapping() throws FrontendException {\nList<FuncSpec> funcList = new ArrayList<FuncSpec>();\nfuncList.add(new FuncSpec(this.getClass().getName(), new Schema(\nnew Schema.FieldSchema(null, DataType.CHARARRAY))));\n}\n}\nreturn funcList;\nThe exec() and getArgToFuncMapping() methods are straightforward; like the ones in\nthe IsGoodQuality UDF.\nWhen you write an eval function, you need to consider what the output’s schema looks\nlike. In the following statement, the schema of B is determined by the function udf:\nB = FOREACH A GENERATE udf($0);\nIf udf creates tuples with scalar fields, then Pig can determine B’s schema through\nreflection. For complex types such as bags, tuples, or maps, Pig needs more help, and\nyou should implement the outputSchema() method to give Pig the information about\nthe output schema.\nThe Trim UDF returns a string, which Pig translates as a chararray, as can be seen from\nthe following session:\ngrunt> DUMP A;\n( pomegranate)\n(banana )\n(apple)\n( lychee )\ngrunt> DESCRIBE A;\n326 | Chapter 11: PigA: {fruit: chararray}\ngrunt> B = FOREACH A GENERATE com.hadoopbook.pig.Trim(fruit);\ngrunt> DUMP B;\n(pomegranate)\n(banana)\n(apple)\n(lychee)\ngrunt> DESCRIBE B;\nB: {chararray}\nA has chararray fields that have leading and trailing spaces. We create B from A by\napplying the Trim function to the first field in A (named fruit). B’s fields are correctly\ninferred to be of type chararray.\nA Load UDF\nWe’ll demonstrate a custom load function that can read plain-text column ranges as\nfields, very much like the Unix cut command. It is used as follows:\ngrunt> records = LOAD 'input/ncdc/micro/sample.txt'\n>>\nUSING com.hadoopbook.pig.CutLoadFunc('16-19,88-92,93-93')\n>>\nAS (year:int, temperature:int, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nThe string passed to CutLoadFunc is the column specification; each comma-separated\nrange defines a field, which is assigned a name and type in the AS clause. Let’s examine\nthe implementation of CutLoadFunc, shown in Example 11-3.\nExample 11-3. A LoadFunc UDF to load tuple fields as column ranges\npublic class CutLoadFunc extends Utf8StorageConverter implements LoadFunc {\nprivate static final Log LOG = LogFactory.getLog(CutLoadFunc.class);\nprivate static final Charset UTF8 = Charset.forName(""UTF-8"");\nprivate static final byte RECORD_DELIMITER = (byte) '\\n';\nprivate\nprivate\nprivate\nprivate\nTupleFactory tupleFactory = TupleFactory.getInstance();\nBufferedPositionedInputStream in;\nlong end = Long.MAX_VALUE;\nList<Range> ranges;\npublic CutLoadFunc(String cutPattern) {\nranges = Range.parse(cutPattern);\n}\n@Override\npublic void bindTo(String fileName, BufferedPositionedInputStream in,\nlong offset, long end) throws IOException {\nUser-Defined Functions | 327this.in = in;\nthis.end = end;\n}\n// Throw away the first (partial) record - it will be picked up by another\n// instance\nif (offset != 0) {\ngetNext();\n}\n@Override\npublic Tuple getNext() throws IOException {\nif (in == null || in.getPosition() > end) {\nreturn null;\n}\n}\nString line;\nwhile ((line = in.readLine(UTF8, RECORD_DELIMITER)) != null) {\nTuple tuple = tupleFactory.newTuple(ranges.size());\nfor (int i = 0; i < ranges.size(); i++) {\ntry {\nRange range = ranges.get(i);\nif (range.getEnd() > line.length()) {\nLOG.warn(String.format(\n""Range end (%s) is longer than line length (%s)"",\nrange.getEnd(), line.length()));\ncontinue;\n}\ntuple.set(i, new DataByteArray(range.getSubstring(line)));\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\nreturn tuple;\n}\nreturn null;\n@Override\npublic void fieldsToRead(Schema schema) {\n// Can't use this information to optimize, so ignore it\n}\n}\n@Override\npublic Schema determineSchema(String fileName, ExecType execType,\nDataStorage storage) throws IOException {\n// Cannot determine schema in general\nreturn null;\n}\nWhen Pig is using the Hadoop execution engine, data loading takes place in the mapper,\nso it is important that the input can be split into portions that are independently handled\nby each mapper (see “Input Splits and Records” on page 185 for background).\n328 | Chapter 11: PigIn Pig, load functions are asked to load a portion of the input, specified as a byte range.\nThe start and end offsets are typically not on record boundaries (since they correspond\nto HDFS blocks), so on initialization, in the bindTo() method, the load function needs\nto find the start of the next record boundary. The Pig runtime then calls getNext()\nrepeatedly, and the load function reads tuples from the stream until it gets past the byte\nrange it was asked to load. At this point, it returns null to signal that there are no more\ntuples to be read.\nCutLoadFunc is constructed with a string that specifies the column ranges to use for each\nfield. The logic for parsing this string and creating a list of internal Range objects that\nencapsulates these ranges is contained in the Range class, and is not shown here (it is\navailable in the example code that accompanies this book).\nIn CutLoadFunc’s bindTo() method, we find the next record boundary. If we are at the\nbeginning of the stream, we know the stream is positioned at the first record boundary.\nOtherwise, we call getNext() to discard the first partial line, since it will be handled in\nfull by another instance of CutLoadFunc—the one whose byte range immediately pro-\nceeds the current one. In some cases, the start offset may fall on a line boundary, but\nit is not possible to detect this case, so we always have to discard the first line. This is\nnot a problem, since we always read lines until the position of the input stream is strictly\npast the end offset. In others words, if the end offset is on a line boundary, then we will\nread one further line. In this way we can be sure that no lines are omitted by the load\nfunction. (The situation is analogous to the one described and illustrated in “TextIn-\nputFormat” on page 196.)\nThe role of the getNext() implementation is to turn lines of the input file into Tuple\nobjects. It does this by means of a TupleFactory, a Pig class for creating Tuple instances.\nThe newTuple() method creates a new tuple with the required number of fields, which\nis just the number of Range classes, and the fields are populated using substrings of the\nline, which are determined by the Range objects.\nWe need to think about what to do if the line is shorter than the range asked for. One\noption is to throw an exception, and stop further processing. This is appropriate if your\napplication cannot tolerate incomplete or corrupt records. In many cases, it is better\nto return a tuple with null fields and let the Pig script handle the incomplete data as it\nsees fit. This is the approach we take here, by exiting the for loop if the range end is\npast the end of the line, we leave the current field, and any subsequent fields in the\ntuple, with their default value of null.\nUsing a schema\nLet’s now consider the type of the fields being loaded. If the user has specified a schema,\nthen the fields need converting to the relevant types. However, this is performed lazily\nby Pig, and so the loader should always construct tuples of type bytearrary, using the\nDataByteArray type. The loader function still has the opportunity to do the conversion,\nhowever, since it must implement a collection of conversion methods for this purpose:\nUser-Defined Functions | 329public interface LoadFunc {\n// Cast methods\npublic Integer bytesToInteger(byte[] b) throws IOException;\npublic Long bytesToLong(byte[] b) throws IOException;\npublic Float bytesToFloat(byte[] b) throws IOException;\npublic Double bytesToDouble(byte[] b) throws IOException;\npublic String bytesToCharArray(byte[] b) throws IOException;\npublic Map<Object, Object> bytesToMap(byte[] b) throws IOException;\npublic Tuple bytesToTuple(byte[] b) throws IOException;\npublic DataBag bytesToBag(byte[] b) throws IOException;\n}\n// Other methods omitted\nCutLoadFunc doesn’t implement these methods itself, since it extends Pig’s Utf8Stora\ngeConverter, which provides standard conversions between UTF-8 encoded data and\nPig data types.\nIn some cases, the load function itself can determine the schema. For example, if we\nwere loading self-describing data like XML or JSON, we could create a schema for Pig\nby looking at the data. Alternatively, the load function may determine the schema in\nanother way, such as an external file, or being passed information in its constructor.\nTo support such cases, the load function can provide an implementation of determine\nSchema() that returns a schema. Note, however, that if a user supplies a schema in the\nAS clause of LOAD, then it takes precedence over the schema returned by the load\nfunction’s determineSchema() method.\nFor CutLoadFunc, we return null in determineSchema(), so there is a schema only if the\nuser supplies one.\nFinally, LoadFunc has a fieldsToRead() method that lets the loader function find out\nwhich columns the query is asking for. This is can be a useful optimization for column-\noriented storage, so the loader only loads the columns that are needed by the query.\nThere is no obvious way for CutLoadFunc to load only a subset of columns, since it reads\nthe whole line for each tuple, so we ignore this information.\nAdvanced loading with Slicer\nFor more control over the data loading process, your load function can implement the\nSlicer interface, in addition to LoadFunc. A Slicer implementation is free to use the\nlocation information passed to LOAD in any way it chooses: it doesn’t have to be a\nHadoop path, but could be interpreted as a database reference, or in some other way.\nFurthermore, it is up to the Slicer to split the input dataset into Slice objects, where\neach Slice is processed by a single MapReduce mapper. A custom Slicer can therefore\nsplit the input in a different way to Pig’s standard slicing behavior, which is to break\nfiles into HDFS block-sized slices. For example, a Slicer for processing large image or\nvideo files might create one file per slice.\n330 | Chapter 11: PigYou can learn more about writing a Slicer on the Pig wiki at http://wiki.apache.org/pig/\nUDFManual.\nData Processing Operators\nLoading and Storing Data\nThroughout this chapter we have seen how to load data from external storage for pro-\ncessing in Pig. Storing results is straightforward, too. Here’s an example of using Pig-\nStorage to store tuples as plain-text values separated by a colon character:\ngrunt> STORE A INTO 'out' USING PigStorage(':');\ngrunt> cat out\nJoe:cherry:2\nAli:apple:3\nJoe:banana:2\nEve:apple:7\nOther built-in storage functions are described in Table 11-7.\nFiltering Data\nOnce you have some data loaded into a relation, the next step is often to filter it to\nremove the data that you are not interested in. By filtering early in the processing pipe-\nline, you minimize the amount of data flowing through the system, which can improve\nefficiency.\nFOREACH .. GENERATE\nWe have already seen how to remove rows from a relation using the FILTER operator\nwith simple expressions and a UDF. The FOREACH .. GENERATE operator is used\nto act on every row in a relation. It can be used to remove fields, or to generate new\nones. In this example, we do both:\ngrunt> DUMP A;\n(Joe,cherry,2)\n(Ali,apple,3)\n(Joe,banana,2)\n(Eve,apple,7)\ngrunt> B = FOREACH A GENERATE $0, $2+1, 'Constant';\ngrunt> DUMP B;\n(Joe,3,Constant)\n(Ali,4,Constant)\n(Joe,3,Constant)\n(Eve,8,Constant)\nHere we have created a new relation B with three fields. Its first field is a projection of\nthe first field ($0) of A. B’s second field is the third field of A ($1) with one added to it.\nData Processing Operators | 331B’s third field is a constant field (every row in B has the same third field) with the\nchararray value Constant.\nThe FOREACH .. GENERATE operator has a nested form for to support more complex\nprocessing. In the following example, we compute various statistics for the weather\ndataset:\n-- year_stats.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nrecords = LOAD 'input/ncdc/all/19{1,2,3,4,5}0*'\nUSING com.hadoopbook.pig.CutLoadFunc('5-10,11-15,16-19,88-92,93-93')\nAS (usaf:chararray, wban:chararray, year:int, temperature:int, quality:int);\ngrouped_records = GROUP records BY year PARALLEL 30;\nyear_stats = FOREACH grouped_records {\nuniq_stations = DISTINCT records.usaf;\ngood_records = FILTER records BY isGood(quality);\nGENERATE FLATTEN(group), COUNT(uniq_stations) AS station_count,\nCOUNT(good_records) AS good_record_count, COUNT(records) AS record_count;\n}\nDUMP year_stats;\nUsing the cut UDF we developed earlier, we load various fields from the input dataset\ninto the records relation. Next we group records by year. Notice the PARALLEL key-\nword for setting the number of reducers to use; this is vital when running on a cluster.\nThen we process each group using a nested FOREACH .. GENERATE operator. The\nfirst nested statement creates a relation for the distinct USAF identifiers for stations\nusing the DISTINCT operator. The second nested statement creates a relation for the\nrecords with “good” readings using the FILTER operator, and a UDF. The final nested\nstatement is a GENERATE statement (a nested FOREACH .. GENERATE must always\nhave a GENERATE statement as the last nested statement) that generates the summary\nfields of interest using the grouped records, as well as the relations created in the nested\nblock.\nRunning it on a few years of data, we get the following:\n(1920,8L,8595L,8595L)\n(1950,1988L,8635452L,8641353L)\n(1930,121L,89245L,89262L)\n(1910,7L,7650L,7650L)\n(1940,732L,1052333L,1052976L)\nThe fields are year, number of unique stations, total number of good readings, and total\nnumber of readings. We can see how the number of weather stations and readings grew\nover time.\n332 | Chapter 11: PigSTREAM\nThe STREAM operator allows you to transform data in a relation using an external\nprogram or script. It is named by analogy with Hadoop Streaming, which provides a\nsimilar capability for MapReduce (see “Hadoop Streaming” on page 32).\nSTREAM can use built-in commands with arguments. Here is an example that uses the\nUnix cut command to extract the second field of each tuple in A. Note that the com-\nmand and its arguments are enclosed in backticks:\ngrunt> C = STREAM A THROUGH `cut -f 2`;\ngrunt> DUMP C;\n(cherry)\n(apple)\n(banana)\n(apple)\nThe STREAM operator uses PigStorage to serialize and deserialize relations to and from\nthe program’s standard input and output streams. Tuples in A are converted to tab-\ndelimited lines that are passed to the script. The output of the script is read one line at\na time and split on tabs to create new tuples for the output relation C. You can provide\na custom serializer and deserializer using the DEFINE command.\nPig streaming is most powerful when you write custom processing scripts. The follow-\ning Python script filters out bad weather records:\n#!/usr/bin/env python\nimport re\nimport string\nimport sys\nfor line in sys.stdin:\n(year, temp, q) = string.strip().split(line)\nif (temp != ""9999"" and re.match(""[01459]"", q)):\nprint ""%s\\t%s"" % (year, temp)\nTo use the script, you need to ship it to the cluster. This is achieved via a DEFINE\nclause, which also creates an alias for the STREAM command. The STREAM statement\ncan then refer to the alias, as the following Pig script shows:\n-- max_temp_filter_stream.pig\nDEFINE is_good_quality `is_good_quality.py`\nSHIP ('src/main/ch11/python/is_good_quality.py');\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = STREAM records THROUGH is_good_quality\nAS (year:chararray, temperature:int);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nData Processing Operators | 333Grouping and Joining Data\nJoining datasets in MapReduce takes some work on the part of the programmer (see\n“Joins” on page 233), whereas Pig has very good built-in support for join operations,\nmaking it much more approachable. Since the large datasets that are suitable for anal-\nysis by Pig (and MapReduce in general), are not normalized, joins are used more in-\nfrequently in Pig than they are in SQL.\nJOIN\nLet’s look at an example of an inner join. Consider the relations A and B:\ngrunt> DUMP A;\n(2,Tie)\n(4,Coat)\n(3,Hat)\n(1,Scarf)\ngrunt> DUMP B;\n(Joe,2)\n(Hank,4)\n(Ali,0)\n(Eve,3)\n(Hank,2)\nWe can join the two relations on the numerical (identity) field in each:\ngrunt> C = JOIN A BY $0, B BY $1;\ngrunt> DUMP C;\n(2,Tie,Joe,2)\n(2,Tie,Hank,2)\n(3,Hat,Eve,3)\n(4,Coat,Hank,4)\nThis is a classic inner join, where each match between the two relations corresponds\nto a row in the result. (It’s actually an equijoin since the join predicate is equality.) The\nresult’s fields are made up of all the fields of all the input relations.\nYou should use the general join operator if all the relations being joined are too large\nto fit in memory. If one of the relations is small enough to fit in memory, there is a\nspecial type of join called a fragment replicate join, which is implemented by distributing\nthe small input to all the mappers, and performing a map-side join using an in-memory\nlookup table against the (fragmented) larger relation. There is a special syntax for telling\nPig to use a fragment replicate join:\ngrunt> C = JOIN A BY $0, B BY $1 USING ""replicated"";\nThe first relation must be the large one, followed by one or more small ones.\n334 | Chapter 11: PigCOGROUP\nJOIN always gives a flat structure: a set of tuples. The COGROUP statement is similar\nto JOIN, but creates a nested set of output tuples. This can be useful if you want to\nexploit the structure in subsequent statements:\ngrunt> D = COGROUP A BY $0, B BY $1;\ngrunt> DUMP D;\n(0,{},{(Ali,0)})\n(1,{(1,Scarf)},{})\n(2,{(2,Tie)},{(Joe,2),(Hank,2)})\n(3,{(3,Hat)},{(Eve,3)})\n(4,{(4,Coat)},{(Hank,4)})\nCOGROUP generates a tuple for each unique grouping key. The first field of each tuple\nis the key, and the remaining fields are bags of tuples from the relations with a matching\nkey. The first bag contains the matching tuples from relation A with the same key.\nSimilarly, the second bag contains the matching tuples from relation B with the same\nkey.\nIf for a particular key a relation has no matching key, then the bag for that relation is\nempty. For example, since no one has bought a scarf (with ID 1), the second bag in the\ntuple for that row is empty. This is an example of an outer join, which is the default\ntype for COGROUP. It can be made explicit using the OUTER keyword; making this\nCOGROUP statement the same as the previous one:\nD = COGROUP A BY $0 OUTER, B BY $1 OUTER;\nYou can suppress rows with empty bags by using the INNER keyword, which gives the\nCOGROUP inner join semantics. The INNER keyword is applied per relation, so the\nfollowing only suppresses rows when relation A has no match (dropping the unknown\nproduct 0 here):\ngrunt> E = COGROUP A BY $0 INNER, B BY $1;\ngrunt> DUMP E;\n(1,{(1,Scarf)},{})\n(2,{(2,Tie)},{(Joe,2),(Hank,2)})\n(3,{(3,Hat)},{(Eve,3)})\n(4,{(4,Coat)},{(Hank,4)})\nWe can flatten this structure to discover who bought each of the items in relation A:\ngrunt> F = FOREACH E GENERATE FLATTEN(A), B.$0;\ngrunt> DUMP F;\n(1,Scarf,{})\n(2,Tie,{(Joe),(Hank)})\n(3,Hat,{(Eve)})\n(4,Coat,{(Hank)})\nUsing a combination of COGROUP, INNER, and FLATTEN (which removes nesting)\nit’s possible to simulate a JOIN:\ngrunt> G = COGROUP A BY $0 INNER, B BY $1 INNER;\ngrunt> H = FOREACH G GENERATE FLATTEN($1), FLATTEN($2);\nData Processing Operators | 335grunt> DUMP H;\n(2,Tie,Joe,2)\n(2,Tie,Hank,2)\n(3,Hat,Eve,3)\n(4,Coat,Hank,4)\nThis gives the same result as JOIN A BY $0, B BY $1.\nIf the join key is composed of several fields, you can specify them all in the BY clauses\nof the JOIN or COGROUP statement. Make sure that the number of fields in each BY\nclause is the same.\nHere’s another example of a join in Pig, in a script for calculating the maximum tem-\nperature of for every station over a time period controlled by the input:\n-- max_temp_station_name.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nstations = LOAD 'input/ncdc/metadata/stations-fixed-width.txt'\nUSING com.hadoopbook.pig.CutLoadFunc('1-6,8-12,14-42')\nAS (usaf:chararray, wban:chararray, name:chararray);\ntrimmed_stations = FOREACH stations GENERATE usaf, wban,\ncom.hadoopbook.pig.Trim(name);\nrecords = LOAD 'input/ncdc/all/191*'\nUSING com.hadoopbook.pig.CutLoadFunc('5-10,11-15,88-92,93-93')\nAS (usaf:chararray, wban:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\ngrouped_records = GROUP filtered_records BY (usaf, wban) PARALLEL 30;\nmax_temp = FOREACH grouped_records GENERATE FLATTEN(group),\nMAX(filtered_records.temperature);\nmax_temp_named = JOIN max_temp BY (usaf, wban), trimmed_stations BY (usaf, wban)\nPARALLEL 30;\nmax_temp_result = FOREACH max_temp_named GENERATE $0, $1, $5, $2;\nSTORE max_temp_result INTO 'max_temp_by_station';\nWe use the cut UDF we developed earlier to load one relation holding the station IDs\n(USAF and WBAN identifiers) and names, and one relation holding all the weather\nrecords, keyed by station ID. We group the filtered weather records by station ID and\naggregate by maximum temperature, before joining with the stations. Finally, we\nproject out the fields we want in the final result: USAF, WBAN, station name, maxi-\nmum temperature.\nHere are a few results for the 1910s:\n228020\n029110\n040650\n99999\n99999\n99999\nSORTAVALA\nVAASA AIRPORT\nGRIMSEY\n322\n300\n378\nThis query could be made more efficient by using a fragment replicate join, as the station\nmetadata is small.\n336 | Chapter 11: PigCROSS\nPig Latin includes the cross product operator (also known as the cartesian product),\nwhich joins every tuple in a relation with every tuple in a second relation (and with\nevery tuple in further relations if supplied). The size of the output is the product of the\nsize of the inputs, potentially making the output very large:\ngrunt> I = CROSS A, B;\ngrunt> DUMP I;\n(2,Tie,Joe,2)\n(2,Tie,Hank,4)\n(2,Tie,Ali,0)\n(2,Tie,Eve,3)\n(2,Tie,Hank,2)\n(4,Coat,Joe,2)\n(4,Coat,Hank,4)\n(4,Coat,Ali,0)\n(4,Coat,Eve,3)\n(4,Coat,Hank,2)\n(3,Hat,Joe,2)\n(3,Hat,Hank,4)\n(3,Hat,Ali,0)\n(3,Hat,Eve,3)\n(3,Hat,Hank,2)\n(1,Scarf,Joe,2)\n(1,Scarf,Hank,4)\n(1,Scarf,Ali,0)\n(1,Scarf,Eve,3)\n(1,Scarf,Hank,2)\nWhen dealing with large datasets, you should try to avoid operations that generate\nintermediate representations that are quadratic (or worse) in size. Computing the cross\nproduct of the whole input dataset is rarely needed, if ever.\nFor example, at first blush one might expect that calculating pairwise document simi-\nlarity in a corpus of documents would require every document pair to be generated\nbefore calculating their similarity. However, if one starts with the insight that most\ndocument pairs have a similarity score of zero (that is, they are unrelated), then we can\nfind a way to a better algorithm.\nIn this case, the key idea is to focus on the entities that we are using to calculate similarity\n(terms in a document, for example), and make them the center of the algorithm. In\npractice, we also remove terms that don’t help discriminate between documents (stop-\nwords) and this reduces the problem space still further. Using this technique to analyze\na set of roughly one million (106) documents generates in the order of one billion\n(109) intermediate pairs,* rather than the one trillion (1012) produced by the naive ap-\nproach (generating the cross product of the input), or the approach with no stopword\nremoval.\n* “Pairwise Document Similarity in Large Collections with MapReduce,” Elsayed, Lin, and Oard (2008, College\nPark, MD: University of Maryland).\nData Processing Operators | 337GROUP\nAlthough COGROUP groups the data in two or more relations, the GROUP statement\ngroups the data in a single relation. GROUP supports grouping by more than equality\nof keys: you can use an expression or user-defined function as the group key. For ex-\nample, consider the following relation A:\ngrunt> DUMP A;\n(Joe,cherry)\n(Ali,apple)\n(Joe,banana)\n(Eve,apple)\nLet’s group by the number of characters in the second field:\ngrunt> B = GROUP A BY SIZE($1);\ngrunt> DUMP B;\n(5L,{(Ali,apple),(Eve,apple)})\n(6L,{(Joe,cherry),(Joe,banana)})\nGROUP creates a relation whose first field is the grouping field, which is given the alias\ngroup. The second field is a bag containing the grouped fields with the same schema as\nthe original relation (in this case, A).\nThere are also two special grouping operations: ALL and ANY. ALL groups all the\ntuples in a relation in a single group, as if the GROUP function was a constant:\ngrunt> C = GROUP A ALL;\ngrunt> DUMP C;\n(all,{(Joe,cherry),(Ali,apple),(Joe,banana),(Eve,apple)})\nNote that there is no BY in this form of the GROUP statement. The ALL grouping is\ncommonly used to count the number of tuples in a relation, as shown in “Validation\nand nulls” on page 318.\nThe ANY keyword is used to group the tuples in a relation randomly, which can be\nuseful for sampling.\nSorting Data\nRelations are unordered in Pig. Consider a relation A:\ngrunt> DUMP A;\n(2,3)\n(1,2)\n(2,4)\nThere is no guarantee which order the rows will be processed in. In particular, when\nretrieving the contents of A using DUMP or STORE, the rows may be written in any\norder. If you want to impose an order on the output you can use the ORDER operator\nto sort a relation by one or more fields. The default sort order compares fields of the\nsame type using the natural ordering, and different types are given an arbitrary, but\ndeterministic, ordering (a tuple is always “less than” a bag, for example). A different\n338 | Chapter 11: Pigordering may be imposed with a USING clause that specifies a UDF that extends Pig’s\nComparisonFunc class.\nThe following example sorts A by the first field in ascending order, and by the second\nfield in descending order:\ngrunt> B = ORDER A BY $0, $1 DESC;\ngrunt> DUMP B;\n(1,2)\n(2,4)\n(2,3)\nAny further processing on a sorted relation does not guarantee to retain its order. For\nexample:\ngrunt> C = FOREACH B GENERATE *;\nEven though relation C has the same contents as relation B, its tuples may be emitted\nin any order by a DUMP or a STORE. It is for this reason that it is usual to perform the\nORDER operation just before retrieving the output.\nThe LIMIT statement is useful for limiting the number of results, as a quick and dirty\nway to get a sample of a relation; prototyping (the ILLUSTRATE command should be\npreferred for generating more representative samples of the data). It can be used im-\nmediately after the ORDER statement to retrieve the first n tuples. Usually LIMIT will\nselect any n tuples from a relation, but when used immediately after an ORDER state-\nment, the order is retained (in an exception to the rule that processing a relation does\nnot retain its order).\ngrunt> D = LIMIT B 2;\ngrunt> DUMP D;\n(1,2)\n(2,4)\nIf the limit is greater than the number of tuples in the relation, all tuples are returned\n(so LIMIT has no effect).\nUsing LIMIT can improve the performance of a query because Pig tries to apply the\nlimit as early as possible in the processing pipeline, to minimize the amount of data\nthat needs to be processed. For this reason, you should always use LIMIT if you are\nnot interested in the entire output.\nCombining and Splitting Data\nSometimes you have several relations that you would like to combine into one. For this,\nthe UNION statement is used. For example:\ngrunt> DUMP A;\n(2,3)\n(1,2)\n(2,4)\ngrunt> DUMP B;\nData Processing Operators | 339(z,x,8)\n(w,y,1)\ngrunt> C = UNION A, B;\ngrunt> DUMP C;\n(2,3)\n(z,x,8)\n(1,2)\n(w,y,1)\n(2,4)\nC is the union of relations A and B, and since relations are unordered, the order of the\ntuples in C is undefined. Also, it’s possible to form the union of two relations with\ndifferent schemas or with different numbers of fields, as we have done here. Pig attempts\nto merge the schemas from the relations that UNION is operating on. In this case, they\nare incompatible, so C has no schema:\ngrunt> DESCRIBE A;\nA: {f0: int,f1: int}\ngrunt> DESCRIBE B;\nB: {f0: chararray,f1: chararray,f2: int}\ngrunt> DESCRIBE C;\nSchema for C unknown.\nIf the output relation has no schema, your script needs to be able to handle tuples that\nvary in the number of fields and/or types.\nThe SPLIT operator is the opposite of UNION; it partitions a relation into two or more\nrelations. See “Validation and nulls” on page 318 for an example of how to use it.\nPig in Practice\nThere are some practical techniques that are worth knowing about when you are de-\nveloping and running Pig programs. This section covers some of them.\nParallelism\nWhen running in Hadoop mode, you need to tell Pig how many reducers you want for\neach job. You do this using a PARALLEL clause for operators that run in the reduce\nphase, which includes all the grouping and joining operators (GROUP, COGROUP,\nJOIN, CROSS), as well as DISTINCT and ORDER. By default the number of reducers\nis one (just like for MapReduce), so it is important to set the degree of parallelism when\nrunning on a large dataset. The following line sets the number of reducers to 30 for the\nGROUP:\ngrouped_records = GROUP records BY year PARALLEL 30;\nA good setting for the number of reduce tasks is slightly fewer than the number of\nreduce slots in the cluster. See “Choosing the Number of Reducers” on page 181 for\nfurther discussion.\n340 | Chapter 11: PigThe number of map tasks is set by the size of the input (with one map per HDFS block),\nand is not affected by the PARALLEL clause.\nParameter Substitution\nIf you have a Pig script that you run on a regular basis, then it’s quite common to want\nto be able to run the same script with different parameters. For example, a script that\nruns daily may use the date to determine which input files it runs over. Pig supports\nparameter substitution, where parameters in the script are substituted with values\nsupplied at runtime. Parameters are denoted by identifiers prefixed with a $ character;\nfor example $input and $output, used in the following script to specify the input and\noutput paths:\n-- max_temp_param.pig\nrecords = LOAD '$input' AS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nSTORE max_temp into '$output';\nParameters can be specified when launching Pig, using the -param option, one for each\nparameter:\n% pig \\\n-param input=/user/tom/input/ncdc/micro-tab/sample.txt \\\n-param output=/tmp/out \\\nsrc/main/ch11/pig/max_temp_param.pig\nYou can also put parameters in a file, and pass them to Pig using the -param_file option.\nFor example, we can achieve the same result as the previous command by placing the\nparameter definitions in a file:\n# Input file\ninput=/user/tom/input/ncdc/micro-tab/sample.txt\n# Output file\noutput=/tmp/out\nThe pig invocation then becomes:\n% pig \\\n-param_file src/main/ch11/pig/max_temp_param.param \\\nsrc/main/ch11/pig/max_temp_param.pig\nYou can specify multiple parameter files using -param_file repeatedly. You can also\nuse a combination of -param and -param_file options, and if any parameter is defined\nin both a parameter file and on the command line, the last value on the command line\ntakes precedence.\nPig in Practice | 341Dynamic parameters\nFor parameters that are supplied using the -param option, it is easy to make the value\ndynamic by running a command or script. Many Unix shells support command sub-\nstitution for a command enclosed in backticks, and we can use this to make the output\ndirectory date-based:\n% pig \\\n-param input=/user/tom/input/ncdc/micro-tab/sample.txt \\\n-param output=/tmp/`date ""+%Y-%m-%d""`/out \\\nsrc/main/ch11/pig/max_temp_param.pig\nPig also supports backticks in parameter files, by executing the enclosed command in\na shell and using the shell output as the substituted value. If the command or scripts\nexits with a nonzero exit status, then the error message is reported and execution halts.\nBacktick support in parameter files is a useful feature; it means that parameters can be\ndefined in the same way if they are defined in a file or on the command line.\nParameter substitution processing\nParameter substitution occurs as a preprocessing step before the script is run. You can\nsee the substitutions that the preprocessor made by executing Pig with the -dryrun\noption. In dry run mode, Pig performs parameter substitution and generates a copy of\nthe original script with substituted values, but does not execute the script. You can\ninspect the generated script and check that the substitutions look sane (because they\nare dynamically generated, for example) before running it in normal mode.\nAt the time of this writing, Grunt does not support parameter substitution.\n342 | Chapter 11: PigCHAPTER 12\nHBase\nJonathan Gray and Michael Stack\nHBasics\nHBase is a distributed column-oriented database built on top of HDFS. HBase is the\nHadoop application to use when you require real-time read/write random-access to\nvery large datasets.\nAlthough there are countless strategies and implementations for database storage and\nretrieval, most solutions—especially those of the relational variety—are not built with\nvery large scale and distribution in mind. Many vendors offer replication and parti-\ntioning solutions to grow the database beyond the confines of a single node but these\nadd-ons are generally an afterthought and are complicated to install and maintain. They\nalso come at some severe compromise to the RDBMS feature set. Joins, complex quer-\nies, triggers, views, and foreign-key constraints become prohibitively expensive to run\non a scaled RDBMS or do not work at all.\nHBase comes at the scaling problem from the opposite direction. It is built from the\nground-up to scale linearly just by adding nodes. HBase is not relational and does not\nsupport SQL, but given the proper problem space it is able to do what an RDBMS\ncannot: ably host very large, sparsely populated tables on clusters made from com-\nmodity hardware.\nThe canonical HBase use case is the webtable, a table of crawled web pages and their\nattributes (such as language and MIME type) keyed by the web page URL. The webtable\nis large with row counts that run into the billions. Batch analytic and parsing\nMapReduce jobs are continuously run against the webtable deriving statistics and add-\ning new columns of MIME type and parsed text content for later indexing by a search\nengine. Concurrently, the table is randomly accessed by crawlers running at various\nrates updating random rows while random web pages are served in real time as users\nclick on a website’s cached-page feature.\n343Backdrop\nThe HBase project was started toward the end of 2006 by Chad Walters and Jim\nKellerman of Powerset. It was modeled after Google’s “Bigtable: A Distributed Storage\nSystem for Structured Data” by Chang et al. (http://labs.google.com/papers/bigtable\n.html), which had just been published. In February 2007, Mike Cafarella made a code\ndrop of a mostly working system that Jim Kellerman then carried forward.\nThe first HBase release was bundled as part of Hadoop 0.15.0. At the start of 2008,\nHBase became a Hadoop subproject at http://hadoop.apache.org/hbase (or http://hbase\n.org). HBase has been in production use at Powerset since late 2007. Other production\nusers of HBase include WorldLingo, Streamy.com, OpenPlaces, and groups at Yahoo!\nand Adobe.\nConcepts\nIn this section, we provide a quick overview of core HBase concepts. At a minimum, a\npassing familiarity will ease the digestion of all that follows.*\nWhirlwind Tour of the Data Model\nApplications store data into labeled tables. Tables are made of rows and columns. Table\ncells—the intersection of row and column coordinates—are versioned. By default, their\nversion is a timestamp auto-assigned by HBase at the time of cell insertion. A cell’s\ncontent is an uninterpreted array of bytes.\nTable row keys are also byte arrays, so theoretically anything can serve as a row key\nfrom strings to binary representations of longs or even serialized data structures. Table\nrows are sorted by row key, the table’s primary key. By default, the sort is byte-ordered.\nAll table accesses are via the table primary key.†\nRow columns are grouped into column families. All column family members have a\ncommon prefix, so, for example, the columns temperature:air and tempera-\nture:dew_point are both members of the temperature column family, whereas\nstation:identifier belongs to the station family.‡ The column family prefix must be com-\nposed of printable characters. The qualifying tail can be made of any arbitrary bytes.\n* For more detail than is provided here, see the HBase Architecture page http://wiki.apache.org/hadoop/Hbase/\nHbaseArchitecture on the HBase wiki.\n† Though there are facilities in HBase to support secondary indices, these are ancillary rather than core and\nare likely to be moved to their own project.\n‡ In HBase, by convention, the colon character (:) delimits the column family from the column family\nqualifier. It is hardcoded.\n344 | Chapter 12: HBaseA table’s column families must be specified up front as part of the table schema defi-\nnition, but new column family members can be added on demand. For example, a new\ncolumn station:address can be offered by a client as part of an update, and its value\npersisted, as long as the column family station is already in existence on the targeted\ntable.\nPhysically, all column family members are stored together on the filesystem. So, though\nearlier we described HBase as a column-oriented store, it would be more accurate if it\nwere described as a column-family-oriented store. Because tunings and storage speci-\nfications are done at the column family level, it is advised that all column family mem-\nbers have the same general access pattern and size characteristics.\nIn synopsis, HBase tables are like those in an RDBMS, only cells are versioned, rows\nare sorted, and columns can be added on the fly by the client as long as the column\nfamily they belong to preexists.\nRegions\nTables are automatically partitioned horizontally by HBase into regions. Each region\ncomprises a subset of a table’s rows. A region is defined by its first row, inclusive, and\nlast row, exclusive, plus a randomly generated region identifier. Initially a table com-\nprises a single region but as the size of the region grows, after it crosses a configurable\nsize threshold, it splits at a row boundary into two new regions of approximately equal\nsize. Until this first split happens, all loading will be against the single server hosting\nthe original region. As the table grows, the number of its regions grows. Regions are\nthe units that get distributed over an HBase cluster. In this way, a table that is too big\nfor any one server can be carried by a cluster of servers with each node hosting a subset\nof the table’s total regions. This is also the means by which the loading on a table gets\ndistributed. At any one time, the online set of sorted regions comprises the table’s total\ncontent.\nLocking\nRow updates are atomic, no matter how many row columns constitute the row-level\ntransaction. This keeps the locking model simple.\nImplementation\nJust as HDFS and MapReduce are built of clients, slaves and a coordinating master—\nnamenode and datanodes in HDFS and jobtracker and tasktrackers in MapReduce—so\nis HBase characterized with an HBase master node orchestrating a cluster of one or\nmore regionserver slaves (see Figure 12-1). The HBase master is responsible for boot-\nstrapping a virgin install, for assigning regions to registered regionservers, and for re-\ncovering regionserver failures. The master node is lightly loaded. The regionservers\ncarry zero or more regions and field client read/write requests. They also manage region\nsplits informing the HBase master about the new daughter regions for it to manage the\nConcepts | 345offlining of parent region and assignment of the replacement daughters. HBase depends\non ZooKeeper (Chapter 13) and by default it manages a ZooKeeper instance as the\nauthority on cluster state.§\nRegionserver slave nodes are listed in the HBase conf/regionservers file as you would\nlist datanodes and tasktrackers in the Hadoop conf/slaves file. Start and stop scripts are\nlike those in Hadoop using the same SSH-based running of remote commands mech-\nanism. Cluster site-specific configuration is made in the HBase conf/hbase-site.xml and\nconf/hbase-env.sh files, which have the same format as that of their equivalents up in\nthe Hadoop parent project (see Chapter 9).\nWhere there is commonality to be found, HBase directly uses or sub-\nclasses the parent Hadoop implementation, whether a service or type.\nWhen this is not possible, HBase will follow the Hadoop model where\nit can. For example, HBase uses the Hadoop Configuration system so\nconfiguration files have the same format. What this means for you, the\nuser, is that you can leverage any Hadoop familiarity in your exploration\nof HBase. HBase deviates from this rule only when adding its\nspecializations.\nHBase persists data via the Hadoop filesystem API. Since there are multiple implemen-\ntations of the filesystem interface—one for the local filesystem, one for the KFS file-\nsystem, Amazon’s S3, and HDFS (the Hadoop Distributed Filesystem)—HBase can\npersist to any of these implementations. Most experience though has been had using\nHDFS, though by default, unless told otherwise, HBase writes the local filesystem. The\nlocal filesystem is fine for experimenting with your initial HBase install, but thereafter,\nusually the first configuration made in an HBase cluster involves pointing HBase at the\nHDFS cluster to use.\nHBase in operation\nHBase, internally, keeps special catalog tables named -ROOT- and .META. within which\nit maintains the current list, state, recent history, and location of all regions afloat on\nthe cluster. The -ROOT- table holds the list of .META. table regions. The .META. table\nholds the list of all user-space regions. Entries in these tables are keyed using the region’s\nstart row. Row keys, as noted previously, are sorted so finding the region that hosts a\nparticular row is a matter of a lookup to find the first entry whose key is greater than\nor equal to that of the requested row key. As regions transition—are split, disabled/\nenabled, deleted, redeployed by the region load balancer, or redeployed due to a\nregionserver crash—the catalog tables are updated so the state of all regions on the\ncluster is kept current.\n§ HBase can be configured to use an existing ZooKeeper cluster instead.\n346 | Chapter 12: HBaseFigure 12-1. HBase cluster members\nFresh clients connect to the ZooKeeper cluster first to learn the location of -ROOT-.\nClients consult -ROOT- to elicit the location of the .META. region whose scope covers\nthat of the requested row. The client then does a lookup against the found .META. region\nto figure the hosting user-space region and its location. Thereafter the client interacts\ndirectly with the hosting regionserver.\nTo save on having to make three round-trips per row operation, clients cache all they\nlearn traversing -ROOT- and .META. caching locations as well as user-space region start\nand stop rows so they can figure hosting regions themselves without having to go back\nto the .META. table. Clients continue to use the cached entry as they work until there is\na fault. When this happens—the region has moved—the client consults the .META.\nagain to learn the new location. If, in turn, the consulted .META. region has moved, then\n-ROOT- is reconsulted.\nWrites arriving at a regionserver are first appended to a commit log and then are added\nto an in-memory cache. When this cache fills, its content is flushed to the filesystem.\nThe commit log is hosted on HDFS, so it remains available through a regionserver crash.\nWhen the master notices that a regionserver is no longer reachable, it splits the dead\nregionserver’s commit log by region. On reassignment, regions that were on the dead\nregionserver, before they open for business, pick up their just-split file of not yet per-\nsisted edits and replay them to bring themselves up-to-date with the state they had just\nbefore the failure.\nConcepts | 347Reading, the region’s memcache is consulted first. If sufficient versions are found to\nsatisfy the query, we return. Otherwise, flush files are consulted in order, from newest\nto oldest until sufficient versions are found or until we run out of flush files to consult.\nA background process compacts flush files once their number has broached a threshold,\nrewriting many files as one, because the fewer files a read consults, the more performant\nit will be. On compaction, versions beyond the configured maximum, deletes and ex-\npired cells are cleaned out. A separate process running in the regionserver monitors\nflush file sizes splitting the region when they grow in excess of the configured maximum.\nInstallation\nDownload a stable release from the HBase Release page and unpack it somewhere on\nyour filesystem. For example:\n% tar xzf hbase-x.y.z.tar.gz\nAs with Hadoop, you first need to tell HBase where Java is located on your system. If\nyou have the JAVA_HOME environment variable set to point to a suitable Java installation,\nthen that will be used, and you don’t have to configure anything further. Otherwise,\nyou can set the Java installation that HBase uses by editing HBase’s conf/hbase-\nenv.sh, and specifying the JAVA_HOME variable (see Appendix A for some examples) to\npoint to version 1.6.0 of Java.\nHBase, just like Hadoop, requires Java 6.\nFor convenience, add the HBase binary directory to your command-line path. For\nexample:\n% export HBASE_INSTALL=/home/hbase/hbase-x.y.z\n% export PATH=$PATH:$HBASE_INSTALL/bin\nTo get the list of HBase options, type:\n% hbase\nUsage: hbase <command>\nwhere <command> is one of:\nshell\nrun the HBase shell\nmaster\nrun an HBase HMaster node\nregionserver\nrun an HBase HRegionServer node\nrest\nrun an HBase REST server\nthrift\nrun an HBase Thrift server\nzookeeper\nrun a Zookeeper server\nmigrate\nupgrade an hbase.rootdir\nor\nCLASSNAME\nrun the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n348 | Chapter 12: HBaseTest Drive\nTo start a temporary instance of HBase that uses the /tmp directory on the local file-\nsystem for persistence, type\n% start-hbase.sh\nThis will launch two daemons: a standalone HBase instance that persists to the local\nfilesystem—by default, HBase will write to /tmp/hbase-${USERID}—and a single in-\nstance of ZooKeeper to host cluster state.‖\nTo administer your HBase instance, launch the HBase shell by typing:\n% hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nVersion: 0.19.1, r751874, Thu Mar 12 22:54:22 PDT 2009\nhbase(main):001:0>\nThis will bring up a JRuby IRB interpreter that has had some HBase-specific commands\nadded to it. Type help and then RETURN to see the list of shell commands that are\navailable. The list is long. Each listed command includes an example of how its used.\nCommands use Ruby formatting, specifying lists and dictionaries. See the end of the\nhelp screen for a quick tutorial.\nNow let us create a simple table, add some data, and then clean up.\nTo create a table, you must name your table and define its schema. A table’s schema\ncomprises table attributes and the list of table column families. Column families them-\nselves have attributes that you in turn set at schema definition time. Examples of column\nfamily attributes include whether the family content should be compressed on the fil-\nesystem and how many versions of a cell to keep. Schemas can be later edited by off-\nlining the table using the shell disable command, making the necessary alterations\nusing alter, then putting the table back on line with enable.\nTo create a table named test with a single column family name data using defaults for\ntable and column family attributes, enter:\nhbase(main):007:0> create 'test', 'data'\n0 row(s) in 4.3066 seconds\nIf the previous command does not complete successfully, and the shell\ndisplays an error and a stack trace, your install was not successful. Check\nthe master logs under the HBase logs directory for a clue as to where\nthings went awry.\nSee the help output for examples adding table and column family attributes when\nspecifying a schema.\n‖ In standalone mode, HBase master and regionserver are both run in the same JVM.\nInstallation | 349To prove the new table was created successfully, run the list command. This will\noutput all tables in the user space:\nhbase(main):019:0> list\ntest\n1 row(s) in 0.1485 seconds\nTo insert data into three different rows and columns in the data column family, and\nthen list the table content, do the following:\nhbase(main):021:0>\n0 row(s) in 0.0454\nhbase(main):022:0>\n0 row(s) in 0.0035\nhbase(main):023:0>\n0 row(s) in 0.0090\nhbase(main):024:0>\nROW\nrow1\nrow2\nrow3\n3 row(s) in 0.0825\nput 'test', 'row1', 'data:1', 'value1'\nseconds\nput 'test', 'row2', 'data:2', 'value2'\nseconds\nput 'test', 'row3', 'data:3', 'value3'\nseconds\nscan 'test'\nCOLUMN+CELL\ncolumn=data:1, timestamp=1240148026198, value=value1\ncolumn=data:2, timestamp=1240148040035, value=value2\ncolumn=data:3, timestamp=1240148047497, value=value3\nseconds\nNotice how we added three new columns without changing the schema.#\nTo remove the table, you must first disable it before dropping it:\nhbase(main):025:0> disable 'test'\n09/04/19 06:40:13 INFO client.HBaseAdmin: Disabled test\n0 row(s) in 6.0426 seconds\nhbase(main):026:0> drop 'test'\n09/04/19 06:40:17 INFO client.HBaseAdmin: Deleted test\n0 row(s) in 0.0210 seconds\nhbase(main):027:0> list\n0 row(s) in 2.0645 seconds\nShut down your HBase instance by running:\n% stop-hbase.sh\nTo learn how to set up a distributed HBase and point it at a running HDFS, see the\nGetting Started section of the HBase documentation.\nClients\nThere are a number of client options for interacting with an HBase cluster.\n# To quickly load one million 1 k rows, run PerformanceEvaluation as shown here. This will launch a single\nclient running the PerformanceEvaluation sequentialWriter script. It will create a table named TestTable with\na single column family and then populate it. For more on the PerformanceEvaluation scripts, see the HBase\nwiki page on the subject (http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation).\n% hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWriter 1\n350 | Chapter 12: HBaseJava\nHBase, like Hadoop, is written in Java. Later in this chapter, in “Exam-\nple” on page 354, there is sample code for uploading data to an HBase table and for\nreading and updating the uploaded data in HBase tables. In the case of the examples,\nwhere we are interacting with preexisting tables, we use the HBase HTable class as the\ngateway for fetching and updating HBase table instances. Administering your cluster\n(adding, enabling or dropping tables), you would use a different class, HBaseAdmin. Both\nof these classes are in the org.apache.hadoop.hbase.client package.\nMapReduce\nHBase classes and utilities in the org.apache.hadoop.hbase.mapred package facilitate\nusing HBase as a source and/or sink in MapReduce jobs. The TableInputFormat class\nmakes splits on region boundaries so maps are handed a single region to work on. The\nTableOutputFormat will write the result of reduce into HBase. The RowCounter class in\nExample 12-1 can be found in the HBase mapred package. It runs a map task to count\nrows using TableInputFormat.\nExample 12-1. A MapReduce application to count the number of rows in an HBase table\npublic class RowCounter extends Configured implements Tool {\n// Name of this 'program'\nstatic final String NAME = ""rowcounter"";\nstatic class RowCounterMapper\nimplements TableMap<ImmutableBytesWritable, RowResult> {\nprivate static enum Counters {ROWS}\npublic void map(ImmutableBytesWritable row, RowResult value,\nOutputCollector<ImmutableBytesWritable, RowResult> output,\nReporter reporter)\nthrows IOException {\nboolean content = false;\nfor (Map.Entry<byte [], Cell> e: value.entrySet()) {\nCell cell = e.getValue();\nif (cell != null && cell.getValue().length > 0) {\ncontent = true;\nbreak;\n}\n}\nif (!content) {\n// Don't count rows that are all empty values.\nreturn;\n}\n// Give out same value every time. We're only interested in the row/key\nreporter.incrCounter(Counters.ROWS, 1);\n}\npublic void configure(JobConf jc) {\n// Nothing to do.\n}\nClients | 351}\npublic void close() throws IOException {\n// Nothing to do.\n}\npublic JobConf createSubmittableJob(String[] args) throws IOException {\nJobConf c = new JobConf(getConf(), getClass());\nc.setJobName(NAME);\n// Columns are space delimited\nStringBuilder sb = new StringBuilder();\nfinal int columnoffset = 2;\nfor (int i = columnoffset; i < args.length; i++) {\nif (i > columnoffset) {\nsb.append("" "");\n}\nsb.append(args[i]);\n}\n// Second argument is the table name.\nTableMapReduceUtil.initTableMapJob(args[1], sb.toString(),\nRowCounterMapper.class, ImmutableBytesWritable.class, RowResult.class, c);\nc.setNumReduceTasks(0);\n// First arg is the output directory.\nFileOutputFormat.setOutputPath(c, new Path(args[0]));\nreturn c;\n}\nstatic int printUsage() {\nSystem.out.println(NAME +\n"" <outputdir> <tablename> <column1> [<column2>...]"");\nreturn -1;\n}\npublic int run(final String[] args) throws Exception {\n// Make sure there are at least 3 parameters\nif (args.length < 3) {\nSystem.err.println(""ERROR: Wrong number of parameters: "" + args.length);\nreturn printUsage();\n}\nJobClient.runJob(createSubmittableJob(args));\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nHBaseConfiguration c = new HBaseConfiguration();\nint errCode = ToolRunner.run(c, new RowCounter(), args);\nSystem.exit(errCode);\n}\nThis class implements Tool, which is discussed in “GenericOptionsParser, Tool, and\nToolRunner” on page 121, and the HBase TableMap interface, a specialization of\norg.apache.hadoop.mapred.Mapper that sets the map inputs types passed by TableInput\nFormat. The createSubmittableJob() method parses arguments added to the\n352 | Chapter 12: HBaseconfiguration that were passed on the command line figuring the table and columns\nwe are to run RowCounter against. It also invokes the TableMapReduceUtil.initTableMap\nJob() utility method, which among other things such as setting the map class to use,\nsets the input format to TableInputFormat. The map is simple. It checks all columns. If\nall are empty, it doesn’t count the row. Otherwise, it increments Counters.ROWS by one.\nREST and Thrift\nHBase ships with REST and thrift interfaces. These are useful when the interacting\napplication is written in a language other than Java. In both cases, a Java server hosts\nan instance of the HBase client brokering application REST and thrift requests in and\nout of the HBase cluster. This extra work proxying requests and responses means these\ninterfaces are slower than using the Java client directly.\nREST\nTo put up the REST, start it using the following command:\n% hbase-daemon.sh start rest\nThis will start a server instance, by default on port 60050, background it, and catch\nany emissions by the server into a logfile under the HBase logs directory.\nClients can ask for the response to be formatted as JSON or as XML, depending on\nhow the client HTTP Accept header is set. See the REST wiki page for documentation\nand examples of making REST client requests.\nTo stop the REST server, type:\n% hbase-daemon.sh stop rest\nThrift\nSimilarly, start a thrift service by putting up a server to field thrift clients by running\nthe following:\n% hbase-daemon.sh start thrift\nThis will start the server instance, by default on port 9090, background it, and catch\nany emissions by the server into a logfile under the HBase logs directory. The HBase\nthrift documentation* notes the thrift version used generating classes. The HBase thrift\nIDL can be found at src/java/org/apache/hadoop/hbase/thrift/Hbase.thrift in the HBase\nsource code.\nTo stop the thrift server, type:\n% hbase-daemon.sh stop thrift\n* http://hadoop.apache.org/hbase/docs/current/api/org/apache/hadoop/hbase/thrift/package-summary.html\nClients | 353Example\nAlthough HDFS and MapReduce are powerful tools for processing batch operations\nover large datasets, they do not provide ways to read or write individual records effi-\nciently. In this example, we’ll explore HBase as the tool to fill this gap.\nThe existing weather dataset described in previous chapters contains observations for\ntens of thousands of stations over 100 years and this data is growing without bound.\nIn this example, we will build a simple web interface that allows a user to navigate the\ndifferent stations and page through their historical temperature observations in time\norder. For the sake of this example, let us allow that the dataset is massive, that the\nobservations run to the billions, and that the rate at which temperature updates arrive\nis significant—say hundreds to thousands of updates a second from around the world\nacross the whole range of weather stations. Also let us allow that it is a requirement\nthat the web application must display the most up-to-date observation within a second\nor so of receipt.\nThe first size requirement should preclude our use of a simple RDBMS instance and\nmake HBase a candidate store. The second latency requirement rules out plain HDFS.\nA MapReduce job could build initial indices that allowed random-access over all of the\nobservation data but keeping up this index as the updates arrived is not what HDFS\nand MapReduce are good at.\nSchemas\nIn our example, there will be two tables:\nStations\nThis table holds station data. Let the row key be the stationid. Let this table have\na column family info that acts as a key/val dictionary for station information. Let\nthe keys be name, location, and description. This table is static and the info family,\nin this case, closely mirrors a typical RDBMS table design.\nObservations\nThis table holds temperature observations. Let the row key be a composite key of\nstationid + reverse order timestamp. Give this table a column family data that will\ncontain one column airtemp with the observed temperature as the column value.\nOur choice of schema is derived from how we want to most efficiently read from HBase.\nRows and columns are stored in increasing lexicographical order. Though there are\nfacilities for secondary indexing and regular expression matching, they come at a per-\nformance penalty.† It is vital that you understand how you want to most efficiently\nquery your data in order to most effectively store and access it.\n† The bundled HBase secondary index mechanism make use of TransactionalHBase, which is powerful but\nnot performant. If you need good performance, it is recommended, for now, that you manage your own\nsecondary index tables or make use of an external index implementation like Lucene.\n354 | Chapter 12: HBaseFor the stations table, the choice of stationid as key is obvious because we will always\naccess information for a particular station by its id. The observations table, however,\nuses a composite key that adds the observation timestamp at the end. This will group\nall observations for a particular station together, and by using a reverse order timestamp\n(Long.MAX_VALUE - epoch) and storing it as binary, observations for each station will be\nordered with most recent observation first.\nIn the shell, you would define your tables as follows:\nhbase(main):036:0>\n0 row(s) in 0.1304\nhbase(main):037:0>\n0 row(s) in 0.1332\ncreate 'stations', {NAME => 'info', VERSIONS => 1}\nseconds\ncreate 'observations', {NAME => 'data', VERSIONS => 1}\nseconds\nIn both cases, we are interested only in the latest version of a table cell, so set VERSIONS to\n1.\nLoading Data\nThere are a relatively small number of stations, so their static data is easily inserted\nusing any of the available interfaces.\nHowever, let’s assume that there are billions of individual observations to be loaded.\nThis kind of import is normally an extremely complex and long-running database op-\neration, but MapReduce and HBase’s distribution model allow us to make full use of\nthe cluster. Copy the raw input data onto HDFS and then run a MapReduce job that\ncan read the input and write to HBase, eventually reading from and writing to all nodes.\nAt the beginning, as the import runs, your table will have only one region, so writes\nwill go to only one server. As the import progresses and tables split, regions will be\ndistributed across the cluster along with your writes.\nExample 12-2 shows an example MapReduce job that imports observations to HBase\nfrom the same input file used in the previous chapters’ examples.\nExample 12-2. A MapReduce application to import temperature data from HDFS into an HBase table\npublic class HBaseTemperatureImporter extends Configured implements Tool {\n// Inner-class for map\nstatic class HBaseTemperatureMapper<K, V> extends MapReduceBase implements\nMapper<LongWritable, Text, K, V> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\nprivate HTable table;\npublic void map(LongWritable key, Text value,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\nparser.parse(value.toString());\nif (parser.isValidTemperature()) {\nbyte[] rowKey = RowKeyConverter.makeObservationRowKey(parser.getStationId(),\nparser.getObservationDate().getTime());\nExample | 355}\n}\n}\nBatchUpdate bu = new BatchUpdate(rowKey);\nbu.put(""data:airtemp"", Bytes.toBytes(parser.getAirTemperature()));\ntable.commit(bu);\npublic void configure(JobConf jc) {\nsuper.configure(jc);\n// Create the HBase table client once up-front and keep it around\n// rather than create on each map invocation.\ntry {\nthis.table = new HTable(new HBaseConfiguration(jc), ""observations"");\n} catch (IOException e) {\nthrow new RuntimeException(""Failed HTable construction"", e);\n}\n}\npublic int run(String[] args) throws IOException {\nif (args.length != 1) {\nSystem.err.println(""Usage: HBaseTemperatureImporter <input>"");\nreturn -1;\n}\nJobConf jc = new JobConf(getConf(), getClass());\nFileInputFormat.addInputPath(jc, new Path(args[0]));\njc.setMapperClass(HBaseTemperatureMapper.class);\njc.setNumReduceTasks(0);\njc.setOutputFormat(NullOutputFormat.class);\nJobClient.runJob(jc);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new HBaseConfiguration(),\nnew HBaseTemperatureImporter(), args);\nSystem.exit(exitCode);\n}\nHBaseTemperatureImporter has an inner class named HBaseTemperatureMapper that is like\nthe MaxTemperatureMapper class from Chapter 5. The outer class implements Tool and\ndoes the setup to launch the HBaseTemperatureMapper inner class. HBaseTemperatureMap\nper takes the same input as MaxTemperatureMapper and does the same parse—using the\nNcdcRecordParser introduced in Chapter 5—to check for valid temperatures, but rather\nthan add valid temperatures to the output collector as MaxTemperatureMapper does,\ninstead it adds valid temperatures to the observations HBase table into the data:air-\ntemp column. In the configure() method, we create an HTable instance once against\nthe observations table and use it afterward in map invocations talking to HBase.\nThe row key used is created in the makeObservationRowKey() method on RowKey\nConverter from the station ID and observation time:\n356 | Chapter 12: HBasepublic class RowKeyConverter {\nprivate static final int STATION_ID_LENGTH = 12;\n/**\n* @return A row key whose format is: <station_id> <reverse_order_epoch>\n*/\npublic static byte[] makeObservationRowKey(String stationId,\nlong observationTime) {\nbyte[] row = new byte[STATION_ID_LENGTH + Bytes.SIZEOF_LONG];\nBytes.putBytes(row, 0, Bytes.toBytes(stationId), 0, STATION_ID_LENGTH);\nlong reverseOrderEpoch = Long.MAX_VALUE - observationTime;\nBytes.putLong(row, STATION_ID_LENGTH, reverseOrderEpoch);\nreturn row;\n}\n}\nThe conversion takes advantage of the fact that the station ID is a fixed-length string.\nThe Bytes class used in makeObservationRowKey() is from the HBase utility package. It\nincludes methods for converting between byte arrays and common Java and Hadoop\ntypes. In makeObservationRowKey(), the Bytes.putLong() method is used to fill the key\nbyte array. The Bytes.SIZEOF_LONG constant is used for sizing and positioning in the\nrow key array.\nWe can run the program with the following:\n% hbase HBaseTemperatureImporter input/ncdc/all\nOptimization notes\n• Watch for the phenomenon where the import walks in lock-step through the table\nwith all clients in concert pounding one of the table’s regions (and thus, a single\nnode), then moving on to the next, and so on, rather than evenly distributing the\nload over all regions. This is usually brought on by some interaction between sorted\ninput and how the splitter works. Randomizing the ordering of your row keys prior\nto insertion may help. In our example, given the distribution of stationid values\nand how TextInputFormat makes splits, the upload should be sufficiently\ndistributed.‡\n• Only obtain one HTable instance per task. There is a cost instantiating an HTable,\nso if you do this for each insert, you may have a negative impact on performance.\nHence our setup of HTable in the configure() step.\n‡ If a table is new, it will have only one region and initially all updates will be to this single region until it splits.\nThis will happen even if row keys are randomly distributed. This startup phenomenon means uploads run\nslow at first until there are sufficient regions distributed so all cluster members are able to participate in the\nupload. Do not confuse this phenomenon with that of the one described earlier.\nExample | 357• By default, each HTable.commit(BatchUpdate) actually performs the insert without\nany buffering. You can disable HTable auto-flush feature using HTable.setAuto\nFlush(false) and then set the size of configurable write buffer. When the inserts\ncommitted fill the write buffer, it is then flushed. Remember though, you must call\na manual HTable.flushCommits() at the end of each task to ensure that nothing is\nleft unflushed in the buffer. You could do this in an override of the mapper’s\nclose() method.\n• HBase includes TableInputFormat and TableOutputFormat to help with MapReduce\njobs that source and sink HBase (see Example 12-1). One way to write the previous\nexample would have been to use MaxTemperatureMapper from Chapter 5 as is but\nadd a reducer task that takes the output of the MaxTemperatureMapper and feeds it\nto HBase via TableOutputFormat.\nWeb Queries\nTo implement the web application, we will use the HBase Java API directly. Here it\nbecomes clear how important your choice of schema and storage format is.\nThe simplest query will be to get the static station information. This type of query is\nsimple in a traditional database, but HBase gives you additional control and flexibility.\nUsing the info family as a key/value dictionary (column names as keys, column values\nas values), the code would look like this:\npublic Map<String, String> getStationInfo(HTable table, String stationId)\nthrows IOException {\nbyte[][] columns = { Bytes.toBytes(""info:"") };\nRowResult res = table.getRow(Bytes.toBytes(stationId), columns);\nif (res == null) {\nreturn null;\n}\nMap<String, String> resultMap = new HashMap<String, String>();\nresultMap.put(""name"", getValue(res, ""info:name""));\nresultMap.put(""location"", getValue(res, ""info:location""));\nresultMap.put(""description"", getValue(res, ""info:description""));\nreturn resultMap;\n}\nprivate static String getValue(RowResult res, String key) {\nCell c = res.get(key.getBytes());\nif (c == null) {\nreturn """";\n}\nreturn Bytes.toString(c.getValue());\n}\n358 | Chapter 12: HBaseIn this example, getStationInfo() takes an HTable instance and a station ID. To get the\nstation info, we use HTable.getRow(). Use this method when fetching more than one\ncolumn from a row. Passing a column family name info: rather than explicit column\nnames will return all columns on that family.§ The getRow() results are returned in\nRowResult, an implementation of SortedMap with the row key as a data member. It is\nkeyed by the column name as a byte array. Values are Cell data structures that hold a\ntimestamp and the cell content in a byte array. The getStationInfo() method converts\nthe RowResult Map into a more friendly Map of String keys and values.\nWe can already see how there is a need for utility functions when using HBase. There\nare an increasing number of abstractions being built atop HBase to deal with this low-\nlevel interaction, but it’s important to understand how this works and how storage\nchoices make a difference.\nOne of the strengths of HBase over a relational database is that you don’t have to\nprespecify the columns. So, in the future, if each station now has at least these three\nattributes but there are hundreds of optional ones, we can just insert them without\nmodifying the schema. Your applications reading and writing code would of course\nneed to be changed. The example code might change in this case to looping through\nRowResult.entrySet() rather than explicitly grabbing each value from the RowResult.\nWe will make use of HBase scanners for retrieval of observations in our web\napplication.\nHere we are after a Map<ObservationTime, ObservedTemp> result. We will use a\nNavigableMap<Long, Integer> because it is sorted and has a descendingMap() method,\nso we can access observations in both ascending or descending order. The code is in\nExample 12-3.\nExample 12-3. Methods for retrieving a range of rows of weather station observations from an HBase\ntable\npublic NavigableMap<Long, Integer> getStationObservations(HTable table,\nString stationId, long maxStamp, int maxCount) throws IOException {\nbyte[][] columns = { Bytes.toBytes(""data:airtemp"") };\nbyte[] startRow = RowKeyConverter.makeObservationRowKey(stationId, maxStamp);\nRowResult res = null;\nNavigableMap<Long, Integer> resultMap = new TreeMap<Long, Integer>();\nbyte[] airtempColumn = Bytes.toBytes(""data:airtemp"");\nScanner s = table.getScanner(columns, startRow);\nint count = 0;\ntry {\nwhile ((res = s.next()) != null && count++ < maxCount) {\nbyte[] row = res.getRow();\nbyte[] value = res.get(airtempColumn).getValue();\nLong stamp = Long.MAX_VALUE -\nBytes.toLong(row, row.length - Bytes.SIZEOF_LONG, Bytes.SIZEOF_LONG);\n§ A column name without a qualifier designates a column family. The colon is required. If you pass info only\nfor column family name, rather than info:, HBase will complain the column name is incorrectly formatted.\nExample | 359Integer temp = Bytes.toInt(value);\nresultMap.put(stamp, temp);\n}\n}\n} finally {\ns.close();\n}\nreturn resultMap;\n/**\n* Return the last ten observations.\n*/\npublic NavigableMap<Long, Integer> getStationObservations(HTable table,\nString stationId) throws IOException {\nreturn getStationObservations(table, stationId, Long.MAX_VALUE, 10);\n}\nThe getStationObservations() method takes a station ID and a range defined by max\nStamp and a maximum number of rows (maxCount). Note that the NavigableMap that is\nreturned is actually now in descending time order. If you want to read through it in\nascending order, you would make use of NavigableMap.descendingMap().\nThe advantage of storing things as Long.MAX_VALUE - stamp may not be clear in the\nprevious example. It has more use when you want to get the newest observations for a\ngiven offset and limit, which is often the case in web applications. If the observations\nwere stored with the actual stamps, we would be able to get only the oldest observations\nfor a given offset and limit efficiently. Getting the newest would mean getting all of\nthem, and then grabbing them off the end. One of the prime reasons for moving from\nRDBMS to HBase is to allow for these types of “early-out” scenarios.\nScanners\nHBase scanners are like cursors in a traditional database or Java iterators, except—\nunlike the latter—they have to be closed after use. Scanners return rows in order. Users\nobtain a scanner on an HBase table by calling HTable.getScanner(). A number of over-\nloaded variants allow the user to pass a row at which to start the scanner, a row at which\nto stop the scanner on, which columns in a row to return in the row result, and op-\ntionally, a filter to run on the server side.‖ The Scanner interface absent Javadoc is as\nfollows:\npublic interface Scanner extends Closeable, Iterable<RowResult> {\npublic RowResult next() throws IOException;\npublic RowResult [] next(int nbRows) throws IOException;\npublic void close();\n}\n‖ To learn more about the server-side filtering mechanism in HBase, see http://hadoop.apache.org/hbase/\ndocs/current/api/org/apache/hadoop/hbase/filter/package-summary.html.\n360 | Chapter 12: HBaseYou can ask for the next row’s results or a number of rows. Each invocation of\nnext() involves a trip back to the regionserver, so grabbing a bunch of rows at once can\nmake for significant performance savings.#\nHBase Versus RDBMS\nHBase and other column-oriented databases are often compared to more traditional\nand popular relational databases or RDBMSs. Although they differ dramatically in their\nimplementations and in what they set out to accomplish, the fact that they are potential\nsolutions to the same problems means that despite their enormous differences, the\ncomparison is a fair one to make.\nAs described previously, HBase is a distributed, column-oriented data storage system.\nIt picks up where Hadoop left off by providing random reads and writes on top of\nHDFS. It has been designed from the ground up with a focus on scale in every direction:\ntall in numbers of rows (billions), wide in numbers of columns (millions), and to be\nhorizontally partitioned and replicated across thousands of commodity nodes auto-\nmatically. The table schemas mirror the physical storage, creating a system for efficient\ndata structure serialization, storage, and retrieval. The burden is on the application\ndeveloper to make use of this storage and retrieval in the right way.\nStrictly speaking, an RDBMS is a database that follows Codd’s 12 Rules. Typical\nRDBMSs are fixed-schema, row-oriented databases with ACID properties and a so-\nphisticated SQL query engine. The emphasis is on strong consistency, referential in-\ntegrity, abstraction from the physical layer, and complex queries through the SQL lan-\nguage. You can easily create secondary indexes, perform complex inner and outer joins,\ncount, sum, sort, group, and page your data across a number of tables, rows, and\ncolumns.\nFor a majority of small- to medium-volume applications, there is no substitute for the\nease of use, flexibility, maturity, and powerful feature set of available open source\nRDBMS solutions like MySQL and PostgreSQL. However, if you need to scale up in\nterms of dataset size, read/write concurrency, or both, you’ll soon find that the con-\nveniences of an RDBMS come at an enormous performance penalty and make distri-\nbution inherently difficult. The scaling of an RDBMS usually involves breaking Codd’s\nrules, loosening ACID restrictions, forgetting conventional DBA wisdom, and on the\nway losing most of the desirable properties that made relational databases so conven-\nient in the first place.\n# The hbase.client.scanner.caching configuration option is set to 1 by default. Scanners will, under the\ncovers, fetch this many results at a time, bringing them client side, and returning to the server to fetch\nthe next batch only after the current batch has been exhausted. Higher caching values will enable faster\nscanning but will eat up more memory in the client.\nHBase Versus RDBMS | 361Successful Service\nHere is a synopsis of how the typical RDBMS scaling story runs. The following list\npresumes a successful growing service:\nInitial public launch\nMove from local workstation to shared, remote hosted MySQL instance with a\nwell-defined schema.\nService becomes more popular; too many reads hitting the database\nAdd memcached to cache common queries. Reads are now no longer strictly ACID;\ncached data must expire.\nService continues to grow in popularity; too many writes hitting the database\nScale MySQL vertically by buying a beefed up server with 16 cores, 128 GB of RAM,\nand banks of 15 k RPM hard drives. Costly.\nNew features increases query complexity; now we have too many joins\nDenormalize your data to reduce joins. (That’s not what they taught me in DBA\nschool!)\nRising popularity swamps the server; things are too slow\nStop doing any server-side computations.\nSome queries are still too slow\nPeriodically prematerialize the most complex queries, try to stop joining in most\ncases.\nReads are OK, but writes are getting slower and slower\nDrop secondary indexes and triggers (no indexes?).\nAt this point, there are no clear solutions for how to solve your scaling problems. In\nany case, you’ll need to begin to scale horizontally. You can attempt to build some type\nof partitioning on your largest tables, or look into some of the commercial solutions\nthat provide multiple master capabilities.\nCountless applications, businesses, and websites have successfully achieved scalable,\nfault-tolerant, and distributed data systems built on top of RDBMSs and are likely using\nmany of the previous strategies. But what you end up with is something that is no longer\na true RDBMS, sacrificing features and conveniences for compromises and complexi-\nties. Any form of slave replication or external caching introduces weak consistency into\nyour now denormalized data. The inefficiency of joins and secondary indexes means\nalmost all queries become primary key lookups. A multiwriter setup likely means no\nreal joins at all and distributed transactions are a nightmare. There’s now an incredibly\ncomplex network topology to manage with an entirely separate cluster for caching.\nEven with this system and the compromises made, you will still worry about your\nprimary master crashing and the daunting possibility of having 10 times the data and\n10 times the load in a few months.\n362 | Chapter 12: HBaseHBase\nEnter HBase, which has the following characteristics:\nNo real indexes\nRows are stored sequentially, as are the columns within each row. Therefore, no\nissues with index bloat, and insert performance is independent of table size.\nAutomatic partitioning\nAs your tables grow, they will automatically be split into regions and distributed\nacross all available nodes.\nScale linearly and automatically with new nodes\nAdd a node, point it to the existing cluster, and run the regionserver. Regions will\nautomatically rebalance and load will spread evenly.\nCommodity hardware\nClusters are built on $1,000–$5,000 nodes rather than $50,000 nodes. RDBMS are\nhungry I/O, which is the most costly type of hardware.\nFault tolerance\nLots of nodes means each is relatively insignificant. No need to worry about indi-\nvidual node downtime.\nBatch processing\nMapReduce integration allows fully parallel, distributed jobs against your data\nwith locality awareness.\nIf you stay up at night worrying about your database (uptime, scale, or speed), then\nyou should seriously consider making a jump from the RDBMS world to HBase. Utilize\na solution that was intended to scale rather than a solution based on stripping down\nand throwing money at what used to work. With HBase, the software is free, the hard-\nware is cheap, the distribution is intrinsic, and there are no black boxes to step on your\ntoes.\nUse Case: HBase at streamy.com\nStreamy.com is a real-time news aggregator and social sharing platform. With a broad\nfeature set, we started out with a complex implementation on top of PostgreSQL. It’s\na terrific product with a great community and a beautiful codebase. We tried every trick\nin the book to keep things fast as we scaled, going so far as to modify the code directly\nto suit our needs. Originally taking advantage of all RDBMS goodies, we found that\neventually, one by one, we had to let them all go. Along the way, our entire team became\nthe DBA.\nWe did manage to solve many of the issues that we ran into, but there were two that\neventually led to the decision to find another solution from outside the world of\nRDBMS.\nHBase Versus RDBMS | 363Streamy crawls thousands of RSS feeds and aggregates hundreds of millions of items\nfrom them. In addition to having to store these items, one of our more complex queries\nreads a time-ordered list of all items from a set of sources. At the high end, this can run\nto several thousand sources and all of their items all in a single query.\nVery large items tables\nAt first, this was a single items table, but the high number of secondary indexes made\ninserts and updates very slow. We started to divide items up into several one-to-one\nlink tables to store other information, separating static fields from dynamic ones,\ngrouping fields based on how they were queried, and denormalizing everything along\nthe way. Even with these changes, single updates required rewriting the entire record,\nso tracking statistics on items was difficult to scale. The rewriting of records and having\nto update indexes along the way are intrinsic properties of the RDBMS we were using.\nThey could not be decoupled. We partitioned our tables, which was not too difficult\nbecause of the natural partition of time, but the complexity got out of hand fast. We\nneeded another solution!\nVery large sort merges\nPerforming sorted merges of time-ordered lists is common in many Web 2.0 applica-\ntions. An example SQL query might look like this:\nSELECT id, stamp, type FROM streams\nWHERE type IN ('type1','type2','type3','type4',...,'typeN')\nORDER BY stamp DESC LIMIT 10 OFFSET 0;\nAssuming id is a primary key on streams, and that stamp and type have secondary\nindexes, an RDBMS query planner treats this query as follows:\nMERGE (\nSELECT id, stamp, type FROM streams\nWHERE type = 'type1' ORDER BY stamp DESC,\n...,\nSELECT id, stamp, type FROM streams\nWHERE type = 'typeN' ORDER BY stamp DESC\n) ORDER BY stamp DESC LIMIT 10 OFFSET 0;\nThe problem here is that we are after only the top 10 IDs, but the query planner actually\nmaterializes an entire merge and then limits at the end. A simple heapsort across each\nof the types would allow you to “early out” once you have the top 10. In our case, each\ntype could have tens of thousands of IDs in it, so materializing the entire list and sorting\nit was extremely slow and unnecessary. We actually went so far as to write a custom\nPL/Python script that performed a heapsort using a series of queries like the following:\nSELECT id, stamp, type FROM streams\nWHERE type = 'typeN'\nORDER BY stamp DESC LIMIT 1 OFFSET 0;\nIf we ended up taking from typeN (it was the next most recent in the heap), we would\nrun another query:\n364 | Chapter 12: HBaseSELECT id, stamp, type FROM streams\nWHERE type = 'typeN'\nORDER BY stamp DESC LIMIT 1 OFFSET 1;\nIn nearly all cases, this outperformed the native SQL implementation and the query\nplanner’s strategy. In the worst cases for SQL, we were more than an order of magnitude\nfaster using the Python procedure. We found ourselves continually trying to outsmart\nthe query planner.\nAgain, at this point, we really needed another solution.\nLife with HBase\nOur RDBMS-based system was always capable of correctly implementing our require-\nments; the issue was scaling. When you start to focus on scale and performance rather\nthan correctness, you end up short-cutting and optimizing for your domain-specific\nuse cases everywhere possible. Once you start implementing your own solutions to\nyour data problems, the overhead and complexity of an RDBMS gets in your way. The\nabstraction from the storage layer and ACID requirements are an enormous barrier and\nluxury that you cannot always afford when building for scale. HBase is a distributed,\ncolumn-oriented, sorted map store and not much else. The only major part that is\nabstracted from the user is the distribution, and that’s exactly what we don’t want to\ndeal with. Business logic, on the other hand, is very specialized and optimized. With\nHBase not trying to solve all of our problems, we’ve been able to solve them better\nourselves and rely on HBase for scaling our storage, not our logic. It was an extremely\nliberating experience to be able to focus on our applications and logic rather than the\nscaling of the data itself.\nWe currently have tables with hundreds of millions of rows and tens of thousands of\ncolumns; the thought of storing billions of rows and millions of columns is exciting,\nnot scary.\nPraxis\nIn this section, we discuss some of the common issues users run into running an HBase\ninstance that moves beyond basic examples.\nVersions\nFirst, ensure you are running compatible versions of Hadoop and HBase. Compatible\nversions have their major and minor version numbers in common. Although an HBase\n0.18.0 cannot talk to an Hadoop 0.19.0 version cluster—they disagree in their minor\nnumbers—an HBase 0.19.2 can run on a Hadoop 0.19.1 HDFS. Incompatible versions\nwill throw an exception complaining about the version mismatch, if you are lucky. If\nthey cannot talk to each sufficiently to pass versions, you may see your HBase cluster\nhang indefinitely, soon after startup. The mismatch exception or HBase hang can also\nPraxis | 365happen on upgrade if older versions of either HBase or Hadoop can still be found on\nthe CLASSPATH because of imperfect cleanup of the old software.\nLove and Hate: HBase and HDFS\nHBase’s use of HDFS is very different from how its used by MapReduce. In MapReduce,\ngenerally, HDFS files are opened, their content streamed through a map task and then\nclosed. In HBase, data files are opened on cluster startup and kept open so that we\navoid paying the file open costs on each access. Because of this, HBase tends to see\nissues not normally encountered by MapReduce clients:\nRunning out of file descriptors\nBecause we keep files open, on a loaded cluster, it doesn’t take long before we run\ninto system- and Hadoop-imposed limits. For instance, say we have a cluster that\nhas three nodes each running an instance of a datanode and a regionserver and\nwe’re running an upload into a table that is currently at 100 regions and 10 column\nfamilies. Allow that each column family has on average two flush files. Doing the\nmath, we can have 100 × 10 × 2, or 2,000, files open at any one time. Add to this\ntotal miscellaneous other descriptors consumed by outstanding scanners, Java li-\nbraries. Each open file consumes at least one descriptor over on the remote data-\nnode. The default limit on the number of file descriptors per process is 1024. When\nwe exceed the filesystem ulimit, we’ll see the complaint about Too many open\nfiles in logs but often you’ll first see indeterminate behavior in HBase. The fix re-\nquires increasing the file descriptor ulimit count.* You can verify that the HBase\nprocess is running with sufficient file descriptors by looking at the first few lines\nof a regionservers log. It emits vitals on such as the JVM being used and environ-\nment settings such as the file descriptor ulimit.\nRunning out of datanode threads\nSimilarly, the Hadoop datanode has an upper bound of 256 on the number of\nthreads it can run at any one time. Given the same table statistics quoted in the\npreceding bullet, it’s easy to see how we can exceed this upper bound relatively\nearly, given that in the datanode as of this writing each open connection to a file\nblock consumes a thread.† If you look in the datanode log, you’ll see a complaint\nlike xceiverCount 258 exceeds the limit of concurrent xcievers 256 but again, you’ll\nlikely see HBase act erratically before you encounter this log entry. Increase the\ndfs.datanode.max.xcievers (note that the property name is misspelled) count in\nHDFS and restart your cluster.‡\n* See the HBase FAQ (http://wiki.apache.org/hadoop/Hbase/FAQ) for how to up the ulimit on your cluster.\n† See HADOOP-3856 Asynchronous IO Handling in Hadoop and HDFS.\n‡ See the HBase troubleshooting guide (http://wiki.apache.org/hadoop/Hbase/Troubleshooting) for more detail\non this issue.\n366 | Chapter 12: HBaseBad blocks\nThe DFSClient hosted in your long-running regionserver will tend to mark file\nblocks as bad if, on an access, the server is currently heavily loaded. Since blocks\nby default are replicated three times, the regionserver DFSClient will move on to\nthe next replica. But if this replica is accessed during a time of heavy loading, we\nnow have two of the three blocks marked as bad. If the third block is found to be\nbad, we start see complaint No live nodes contain current block in regionserver logs.\nDuring startup, there is lots of churn and contention as regions are opened and\ndeployed. During this time, the No live nodes contain current block can come on\nquickly. At an extreme, set dfs.datanode.socket.write.timeout to zero. Note that\nthis configuration needs to be set in a location that can be seen by the HBase\nDFSClient; set it in the hbase-site.xml or by symlinking the hadoop-site.xml (or hdfs-\nsite.xml in recent versions) into your HBase conf directory.§\nUI\nHBase runs a web server on the master to present a view on the state of your running\ncluster. By default, it listens on port 60010. The master UI displays a list of basic at-\ntributes such as software versions, cluster load, request rates, lists of cluster tables and\nparticipating regionservers. Click on a regionserver in the master UI and you are taken\nto the web server running on the individual regionserver. It lists the regions this server\nis carrying and basic metrics such as resources consumed and request rates.\nMetrics\nHadoop has a metrics system that can be used emitting vitals over a period to a con-\ntext (this is covered in “Metrics” on page 286). Enabling Hadoop metrics, and in par-\nticular tying them to Ganglia, helps the development of view on what is happening on\nyour cluster currently and in the recent past. HBase also adds metrics of its own—\nrequest rates, counts of vitals, resources used—that can be caught by a Hadoop context.\nSee the file hadoop-metrics.properties under the HBase conf directory.‖\nSchema Design\nHBase tables are like those in an RDBMS, except that cells are versioned, rows are\nsorted, and columns can be added on the fly by the client as long as the column family\nthey belong to preexists. The following factors should be considered when designing\nschemas for HBase. The other property to keep in mind when designing schemas is\n§ Again, see the HBase troubleshooting guide (http://wiki.apache.org/hadoop/Hbase/Troubleshooting) for more\ndetail on this issue.\n‖ Yes, this file named for Hadoop, though it’s for setting up HBase metrics.\nPraxis | 367that a defining attribute of column(-family)-oriented stores, like HBase, is that it can\nhost wide and sparsely populated tables at no incurred cost.#\nJoins\nThere is no native database join facility in HBase, but wide tables can make it so that\nthere is no need for database joins pulling from secondary or tertiary tables. A wide\nrow can sometimes be made to hold all data that pertains to a particular primary key.\nRow keys\nTake time designing your row key. In the weather data example in this chapter, the\ncompound row key has a station prefix that served to group temperatures by station.\nThe reversed timestamp suffix made it so temperatures could be scanned ordered from\nmost recent to oldest. A smart compound key can be used clustering data in ways\namenable to how it will be accessed.\nDesigning compound keys, you may have to zero-pad number components so row keys\nsort properly. Otherwise, you will run into the issue where 10 sorts before 2 when only\nbyte-order is considered (02 sorts before 10).\nIf your keys are integers, use a binary representation rather than persist the string ver-\nsion of a number—it consumes less space.\n# “Column-Stores for Wide and Sparse Data” by Daniel J. Abadi.\n368 | Chapter 12: HBaseCHAPTER 13\nZooKeeper\nSo far in this book, we have been studying large-scale data processing. This chapter is\ndifferent: it is about building general distributed applications using Hadoop’s distrib-\nuted coordination service, called ZooKeeper.\nWriting distributed applications is hard. It’s hard primarily because of partial failure.\nWhen a message is sent across the network between two nodes and the network fails,\nthe sender does not know whether the receiver got the message. It may have gotten\nthrough before the network failed, or it may not have. Or perhaps the receiver’s process\ndied. The only way that the sender can find out what happened is to reconnect to the\nreceiver and ask it. This is partial failure: when we don’t even know if an operation\nfailed.\nZooKeeper can’t make partial failures go away, since they are intrinsic to distributed\nsystems. It certainly does not hide partial failures, either.* But what ZooKeeper does\ndo is give you a set of tools to build distributed applications that can safely handle\npartial failures.\nZooKeeper also has the following characteristics:\nZooKeeper is simple\nZooKeeper is, at its core, a stripped-down filesystem that exposes a few simple\noperations, and some extra abstractions such as ordering and notifications.\nZooKeeper is expressive\nThe ZooKeeper primitives are a rich set of building blocks that can be used to build\na large class of coordination data structures and protocols. Examples include: dis-\ntributed queues, distributed locks, and leader election among a group of peers.\n* This is the message of J. Waldo et al., “A Note on Distributed Computing,” (1994), http://research.sun.com/\ntechrep/1994/smli_tr-94-29.pdf. That is, distributed programming is fundamentally different from local\nprogramming, and the differences cannot simply be papered over.\n369ZooKeeper is highly available\nZooKeeper runs on a collection of machines and is designed to be highly available,\nso applications can depend on it. ZooKeeper can help you avoid introducing single\npoints of failure into your system, so you can build a reliable application.\nZooKeeper facilitates loosely coupled interactions\nZooKeeper interactions support participants that do not need to know about one\nanother. For example, ZooKeeper can be used as a rendezvous mechanism so that\nprocesses that otherwise don’t know of each other’s existence (or network details)\ncan discover each other and interact. Coordinating parties may not even be con-\ntemporaneous, since one process may leave a message in ZooKeeper that is read\nby another after the first has shut down.\nZooKeeper is a library\nZooKeeper provides an open source, shared repository of implementations and\nrecipes of common coordination patterns. Individual programmers are spared the\nburden of writing common protocols themselves (which are often difficult to get\nright). Over time the community can add to, and improve, the libraries, which is\nto everyone’s benefit.\nZooKeeper is highly performant, too. At Yahoo!, where it was created, ZooKeeper’s\nthroughput has been benchmarked at approximately 10,000 operations per second for\nwrite-dominant workloads. For workloads where reads dominate, which is the norm,\nthe throughput is several times higher.\nInstalling and Running ZooKeeper\nWhen trying out ZooKeeper for the first time, it’s simplest to run it in standalone mode\nwith a single ZooKeeper server. You can do this on a development machine, for exam-\nple. ZooKeeper requires Java 6 to run, so make sure you have it installed first. If you\nare running ZooKeeper on Windows (Windows is supported only as a development\nplatform, not as a production platform), you need to install Cygwin, too.\nDownload a stable release of ZooKeeper from the Apache ZooKeeper releases page at\nhttp://hadoop.apache.org/zookeeper/releases.html, and unpack the tarball in a suitable\nlocation:\n% tar xzf zookeeper-x.y.z.tar.gz\nZooKeeper provides a few binaries to run and interact with the service, and it’s con-\nvenient to put the directory containing the binaries on your command-line path:\n% export ZOOKEEPER_INSTALL=/home/tom/zookeeper-x.y.z\n% export PATH=$PATH:$ZOOKEEPER_INSTALL/bin\nBefore running the ZooKeeper service, we need to set up a configuration file. The con-\nfiguration file is conventionally called zoo.cfg and placed in the conf subdirectory (al-\n370 | Chapter 13: ZooKeeperthough you can also place it in /etc/zookeeper, or in the directory defined by the\nZOOCFGDIR environment variable, if set). Here’s an example:\ntickTime=2000\ndataDir=/Users/tom/zookeeper\nclientPort=2181\nThis is a standard Java properties file, and the three properties defined in this example\nare the minimum required for running ZooKeeper in standalone mode. Briefly,\ntickTime is the basic time unit in ZooKeeper (specified in milliseconds), dataDir is the\nlocal filesystem location where ZooKeeper stores persistent data, and clientPort is the\nport the ZooKeeper listens on for client connections (2181 is a common choice). You\nshould change dataDir to an appropriate setting for your system.\nWith a suitable configuration defined, we are now ready to start a local ZooKeeper\nserver:\n% zkServer.sh start\nTo check whether ZooKeeper is running, send the ruok command (“Are you OK?”) to\nthe client port using nc (telnet works, too):\n% echo ruok | nc localhost 2181\nimok\nThat’s ZooKeeper saying “I’m OK.” There are other commands, known as the “four-\nletter words,” for interacting with ZooKeeper. Most are queries: dump lists sessions and\nephemeral znodes, envi lists server properties, reqs lists outstanding requests, and\nstat lists service statistics and connected clients. However, you can also update Zoo-\nKeeper’s state: srst resets the service statistics, and kill shuts down ZooKeeper if\nissued from the host running the ZooKeeper server.\nFor more extensive ZooKeeper monitoring, have a look at its JMX support, which is\ncovered in the ZooKeeper documentation (linked from http://hadoop.apache.org/zoo\nkeeper/).\nAn Example\nImagine a group of servers that provide some service to clients. We want clients to be\nable to locate one of the servers, so they can use the service. One of the challenges is\nmaintaining the list of servers in the group.\nThe membership list clearly cannot be stored on a single node in the network, as the\nfailure of that node would mean the failure of the whole system (we would like the list\nto be highly available). Suppose for a moment that we had a robust way of storing the\nlist. We would still have the problem of how to remove a server from the list if it failed.\nSome process needs to be responsible for removing failed servers, but note that it can’t\nbe the servers themselves, since they are no longer running!\nAn Example | 371What we are describing is not a passive distributed data structure, but an active one,\nand one that can change the state of an entry when some external event occurs. Zoo-\nKeeper provides this service, so let’s see how to build this group membership applica-\ntion (as it is known) with it.\nGroup Membership in ZooKeeper\nOne way of understanding ZooKeeper is to think of it as providing a high-availability\nfilesystem. It doesn’t have files and directories, but a unified concept of a node, called\na znode, which acts both as a container of data (like a file) and a container of other\nznodes (like a directory). Znodes form a hierarchical namespace, and a natural way to\nbuild a membership list is to create a parent znode with the name of the group, and\nchild znodes with the name of the group members (servers). This is shown in Fig-\nure 13-1.\nFigure 13-1. ZooKeeper znodes\nIn this example, we won’t store data in any of the znodes, but in a real application, you\ncould imagine storing data about the members in their znodes, such as hostname.\nCreating the Group\nLet’s introduce ZooKeeper’s Java API by writing a program to create a znode for the\ngroup, /zoo in this example. See Example 13-1.\nExample 13-1. A program to create a znode representing a group in ZooKeeper\npublic class CreateGroup implements Watcher {\n372 | Chapter 13: ZooKeeperprivate static final int SESSION_TIMEOUT = 5000;\nprivate ZooKeeper zk;\nprivate CountDownLatch connectedSignal = new CountDownLatch(1);\npublic void connect(String hosts) throws IOException, InterruptedException {\nzk = new ZooKeeper(hosts, SESSION_TIMEOUT, this);\nconnectedSignal.await();\n}\n@Override\npublic void process(WatchedEvent event) { // Watcher interface\nif (event.getState() == KeeperState.SyncConnected) {\nconnectedSignal.countDown();\n}\n}\npublic void create(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\nString createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\nSystem.out.println(""Created "" + createdPath);\n}\npublic void close() throws InterruptedException {\nzk.close();\n}\n}\npublic static void main(String[] args) throws Exception {\nCreateGroup createGroup = new CreateGroup();\ncreateGroup.connect(args[0]);\ncreateGroup.create(args[1]);\ncreateGroup.close();\n}\nWhen the main() method is run, it creates a CreateGroup instance and then calls its\nconnect() method. This method instantiates a new ZooKeeper object, the main class of\nthe client API and the one that maintains the connection between the client and the\nZooKeeper service. The constructor takes three arguments: the first is the host address\n(and optional port, which defaults to 2181) of the ZooKeeper service;† the second is\nthe session timeout in milliseconds (which we set to 5 seconds), explained in more\ndetail later; and the third is an instance of a Watcher object. The Watcher object receives\ncallbacks from ZooKeeper to inform it of various events. In this case, CreateGroup is a\nWatcher, so we pass this to the ZooKeeper constructor.\nWhen a ZooKeeper instance is created, it starts a thread to connect to the ZooKeeper\nservice. The call to the constructor returns immediately, so it is important to wait for\n† For a replicated ZooKeeper service, this parameter is the comma-separated list of servers (host and optional\nport) in the ensemble.\nAn Example | 373the connection to be established before using the ZooKeeper object. We make use of\nJava’s CountDownLatch class (in the java.util.concurrent package) to block until the\nZooKeeper instance is ready. This is where the Watcher comes in. The Watcher interface\nhas a single method:\npublic void process(WatchedEvent event);\nWhen the client has connected to ZooKeeper, the Watcher receives a call to its\nprocess() method with an event indicating that it has connected. On receiving a con-\nnection event (represented by the Watcher.Event.KeeperState enum, with value\nSyncConnected), we decrement the counter in the CountDownLatch, using its count\nDown() method. The latch was created with a count of one, representing the number of\nevents that need to occur before it releases all waiting threads. After calling count\nDown() once, the counter reaches zero and the await() method returns.\nThe connect() method has now returned, and the next method to be invoked on the\nCreateGroup is the create() method. In this method, we create a new ZooKeeper znode\nusing the create() method on the ZooKeeper instance. The arguments it takes are the\npath (represented by a string), the contents of the znode (a byte array, null here), an\naccess control list (or ACL for short, which here is a completely open ACL, allowing\nany client to read or write the znode), and the nature of the znode to be created.\nZnodes may be ephemeral or persistent. An ephemeral znode will be deleted by the\nZooKeeper service when the client that created it disconnects, either by explicitly dis-\nconnecting or if the client terminates for whatever reason. A persistent znode, on the\nother hand, is not deleted when the client disconnects. We want the znode representing\na group to live longer than the lifetime of the program that creates it, so we create a\npersistent znode.\nThe return value of the create() method is the path that was created by ZooKeeper.\nWe use it to print a message that the path was successfully created. We will see how\nthe path returned by create() may differ from the one passed in to the method when\nwe look at sequential znodes.\nTo see the program in action, we need to have ZooKeeper running on the local machine,\nand then we can type:\n% export CLASSPATH=build/classes:$ZOOKEEPER_INSTALL/*:$ZOOKEEPER_INSTALL/lib/*:\\\n$ZOOKEEPER_INSTALL/conf\n% java CreateGroup localhost zoo\nCreated /zoo\nJoining a Group\nThe next part of the application is a program to register a member in a group. Each\nmember will run as a program and join a group. When the program exits, it should be\nremoved from the group, which we can do by creating an ephemeral znode that rep-\nresents it in the ZooKeeper namespace.\n374 | Chapter 13: ZooKeeperThe JoinGroup program implements this idea, and its listing is in Example 13-2. The\nlogic for creating and connecting to a ZooKeeper instance has been refactored into a base\nclass, ConnectionWatcher, and appears in Example 13-3.\nExample 13-2. A program that joins a group\npublic class JoinGroup extends ConnectionWatcher {\npublic void join(String groupName, String memberName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName + ""/"" + memberName;\nString createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE,\nCreateMode.EPHEMERAL);\nSystem.out.println(""Created "" + createdPath);\n}\npublic static void main(String[] args) throws Exception {\nJoinGroup joinGroup = new JoinGroup();\njoinGroup.connect(args[0]);\njoinGroup.join(args[1], args[2]);\n}\n}\n// stay alive until process is killed or thread is interrupted\nThread.sleep(Long.MAX_VALUE);\nExample 13-3. A helper class that waits for the connection to ZooKeeper to be established\npublic class ConnectionWatcher implements Watcher {\nprivate static final int SESSION_TIMEOUT = 5000;\nprotected ZooKeeper zk;\nprivate CountDownLatch connectedSignal = new CountDownLatch(1);\npublic void connect(String hosts) throws IOException, InterruptedException {\nzk = new ZooKeeper(hosts, SESSION_TIMEOUT, this);\nconnectedSignal.await();\n}\n@Override\npublic void process(WatchedEvent event) {\nif (event.getState() == KeeperState.SyncConnected) {\nconnectedSignal.countDown();\n}\n}\n}\npublic void close() throws InterruptedException {\nzk.close();\n}\nThe code for JoinGroup is very similar to CreateGroup. It creates an ephemeral znode as\na child of the group znode in its join() method, then simulates doing work of some\nAn Example | 375kind by sleeping until the process is forcibly terminated. Later, you will see that upon\ntermination, the ephemeral znode is removed by ZooKeeper.\nListing Members in a Group\nNow we need a program to find the members in a group (see Example 13-4).\nExample 13-4. A program to list the members in a group\npublic class ListGroup extends ConnectionWatcher {\npublic void list(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\n}\n}\ntry {\nList<String> children = zk.getChildren(path, false);\nif (children.isEmpty()) {\nSystem.out.printf(""No members in group %s\\n"", groupName);\nSystem.exit(1);\n}\nfor (String child : children) {\nSystem.out.println(child);\n}\n} catch (KeeperException.NoNodeException e) {\nSystem.out.printf(""Group %s does not exist\\n"", groupName);\nSystem.exit(1);\n}\npublic static void main(String[] args) throws Exception {\nListGroup listGroup = new ListGroup();\nlistGroup.connect(args[0]);\nlistGroup.list(args[1]);\nlistGroup.close();\n}\nIn the list() method, we call getChildren() with a znode path and a watch flag to\nretrieve a list of child paths for the znode, which we print out. Placing a watch on a\nznode causes the registered Watcher to be triggered if the znode changes state. Although\nwe’re not using it here, watching a znode’s children would permit a program to get\nnotifications of members joining or leaving the group, or of the group being deleted.\nWe catch KeeperException.NoNodeException, which is thrown in the case when the\ngroup’s znode does not exist.\nLet’s see ListGroup in action. As expected, the zoo group is empty, since we haven’t\nadded any members yet:\n% java ListGroup localhost zoo\nNo members in group zoo\n376 | Chapter 13: ZooKeeperWe can use JoinGroup to add some members. We launch them as background pro-\ncesses, since they don’t terminate on their own (due to the sleep statement):\n%\n%\n%\n%\njava JoinGroup localhost zoo duck &\njava JoinGroup localhost zoo cow &\njava JoinGroup localhost zoo goat &\ngoat_pid=$!\nThe last line saves the process ID of the Java process running the program that adds\ngoat as a member. We need to remember the ID so that we can kill the process in a\nmoment, after checking the members:\n% java ListGroup localhost zoo\ngoat\nduck\ncow\nTo remove a member, we kill its process:\n% kill $goat_pid\nAnd a few seconds later, it has disappeared from the group because the process’s Zoo-\nKeeper session has terminated (the timeout was set to 5 seconds) and its associated\nephemeral node has been removed:\n% java ListGroup localhost zoo\nduck\ncow\nLet’s stand back and see what we’ve built here. We have a way of building up a list of\na group of nodes that are participating in a distributed system. The nodes may have no\nknowledge of each other. A service that wants to use the nodes in the list to perform\nsome work, for example, can discover the nodes without them being aware of the serv-\nice’s existence.\nFinally, note that group membership is not a substitution for handling network errors\nwhen communicating with a node. Even if a node is a group member, communications\nwith it may fail, and such failures must be handled in the usual manner.\nZooKeeper command-line tools\nZooKeeper comes with a command-line tool for interacting with the ZooKeeper name-\nspace. We can use it to list the znodes under the /zoo znode as follows:\n% zkCli.sh localhost ls /zoo\nProcessing ls\nWatchedEvent: Server state change. New state: SyncConnected\n[duck, cow]\nYou can run the command without arguments to display usage instructions.\nAn Example | 377Deleting a Group\nTo round off the example, let’s see how to delete a group. The ZooKeeper class provides\na delete() method that takes a path and a version number. ZooKeeper will delete a\nznode only if the version number specified is the same as the version number of the\nznode it is trying to delete, an optimistic locking mechanism that allows clients to detect\nconflicts over znode modification. You can bypass the version check, however, by using\na version number of –1 to delete the znode regardless of its version number.\nThere is no recursive delete operation in ZooKeeper, so you have to delete child znodes\nbefore parents. This is what we do in the DeleteGroup class, which will remove a group\nand all its members (Example 13-5).\nExample 13-5. A program to delete a group and its members\npublic class DeleteGroup extends ConnectionWatcher {\npublic void delete(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\n}\n}\ntry {\nList<String> children = zk.getChildren(path, false);\nfor (String child : children) {\nzk.delete(path + ""/"" + child, -1);\n}\nzk.delete(path, -1);\n} catch (KeeperException.NoNodeException e) {\nSystem.out.printf(""Group %s does not exist\\n"", groupName);\nSystem.exit(1);\n}\npublic static void main(String[] args) throws Exception {\nDeleteGroup deleteGroup = new DeleteGroup();\ndeleteGroup.connect(args[0]);\ndeleteGroup.delete(args[1]);\ndeleteGroup.close();\n}\nFinally, we can delete the zoo group that we created earlier:\n% java DeleteGroup localhost zoo\n% java ListGroup localhost zoo\nGroup zoo does not exist\nThe ZooKeeper Service\nZooKeeper is a highly available, high-performance coordination service. In this section,\nwe look at the nature of the service it provides: its model, operations, and\nimplementation.\n378 | Chapter 13: ZooKeeperData Model\nZooKeeper maintains a hierarchical tree of nodes called znodes. A znode stores data\nand has an associated ACL. ZooKeeper is designed for coordination (which typically\nuses small data files), not high-volume data storage, so there is a limit of 1 MB on the\namount of data that may be stored in any znode.\nData access is atomic. A client reading the data stored at a znode will never receive only\nsome of the data; the data will be delivered in its entirety (or the read will fail). Similarly,\na write will replace all the data associated with a znode. ZooKeeper guarantees that the\nwrite will either succeed or fail; there is no such thing as a partial write, where only\nsome of the data written by the client is stored. ZooKeeper does not support an append\noperation. These characteristics contrast with HDFS, which is designed for high-\nvolume data storage, with streaming data access, and provides an append operation.\nZnodes are referenced by paths, which in ZooKeeper are represented as slash-delimited\nUnicode character strings, like filesystem paths in Unix. Paths must be absolute, so\nthey must begin with a slash character. Furthermore, they are canonical, which means\nthat each path has a single representation, and so paths do not undergo resolution. For\nexample, in Unix, a file with the path /a/b can equivalently be referred to by the\npath /a/./b, since “.” refers to the current directory at the point it is encountered in the\npath. In ZooKeeper “.” does not have this special meaning, and is actually illegal as a\npath component (as is “..” for the parent of the current directory).\nPath components are composed of Unicode characters, with a few restrictions (these\nare spelled out in the ZooKeeper reference documentation). The string “zookeeper” is\na reserved word, and may not be used as a path component. In particular, ZooKeeper\nuses the /zookeeper subtree to store management information, such as information on\nquotas.\nNote that paths are not URIs, and they are represented in the Java API by a\njava.lang.String, rather than the Hadoop Path class (or by the java.net.URI class, for\nthat matter).\nZnodes have some properties that are very useful for building distributed applications,\nwhich we discuss in the following sections.\nEphemeral znodes\nZnodes can be one of two types: ephemeral or persistent. A znode’s type is set at creation\ntime and may not be changed later. An ephemeral znode is deleted by ZooKeeper when\nthe creating client’s session ends. By contrast, a persistent znode is not tied to the client’s\nsession, and is deleted only when explicitly deleted by a client (not necessarily the one\nthat created it). An ephemeral znode may not have children, not even ephemeral ones.\nEven though ephemeral nodes are tied to a client session, they are visible to all clients\n(subject to their ACL policy, of course).\nThe ZooKeeper Service | 379Ephemeral znodes are ideal for building applications that needs to know when certain\ndistributed resources are available. The example earlier in this chapter uses ephemeral\nznodes to implement a group membership service, so any process can discover the\nmembers of the group at any particular time.\nSequence numbers\nA sequential znode is given a sequence number by ZooKeeper as a part of its name. If\na znode is created with the sequential flag set, then the value of a monotonically in-\ncreasing counter (maintained by the parent znode) is appended to its name.\nIf a client asks to create a sequential znode with the name /a/b-, for example, then the\nznode created may actually have the name /a/b-3.‡ If, later on, another sequential znode\nwith the name /a/b- is created, then it will be given a unique name with a larger value\nof the counter—for example, /a/b-5. In the Java API, the actual path given to sequential\nznodes is communicated back to the client as the return value of the create() call.\nSequence numbers can be used to impose a global ordering on events in a distributed\nsystem, and may be used by the client to infer the ordering. In “A Lock Serv-\nice” on page 398, you will learn how to use sequential znodes to build a shared lock.\nWatches\nWatches allow clients to get notifications when a znode changes in some way. Watches\nare set by operations on the ZooKeeper service, and are triggered by other operations\non the service. For example, a client might call the exists operation on a znode, placing\na watch on it at the same time. If the znode doesn’t exist, then the exists operation\nwill return false. If, some time later, the znode is created by a second client, then the\nwatch is triggered, notifying the first client of the znode’s creation. You will see precisely\nwhich operations trigger others in the next section.\nWatchers are triggered only once.§ To receive multiple notifications, a client needs to\nreregister the watch. If the client in the previous example wishes to receive further\nnotifications for the znode’s existence (to be notified when it is deleted, for example),\nit needs to call the exists operation again to set a new watch.\nThere is an example in “A Configuration Service” on page 391 demonstrating how to\nuse watches to update configuration across a cluster.\nOperations\nThere are nine basic operations in ZooKeeper, listed in Table 13-1.\n‡ It is conventional (but not required) to have a trailing dash on path names for sequential nodes, to make their\nsequence numbers easy to read and parse (by the application).\n§ Except for callbacks for connection events, which do not need re-registration.\n380 | Chapter 13: ZooKeeperTable 13-1. Operations in the ZooKeeper service\nOperation Description\ncreate Creates a znode (the parent znode must already exist)\ndelete Deletes a znode (the znode may not have any children)\nexists Tests whether a znode exists and retrieves its metadata\ngetACL, setACL Gets/sets the ACL for a znode\ngetChildren Gets a list of the children of a znode\ngetData, setData Gets/sets the data associated with a znode\nsync Synchronizes a client’s view of a znode with ZooKeeper\nUpdate operations in ZooKeeper are conditional. A delete or setData operation has to\nspecify the version number of the znode that is being updated (which is found from a\nprevious exists call). If the version number does not match, the update will fail. Up-\ndates are a nonblocking operation, so a client that loses an update (because another\nprocess updated the znode in the meantime) can decide whether to try again or take\nsome other action, and it can do so without blocking the progress of any other process.\nAlthough ZooKeeper can be viewed as a filesystem, there are some filesystem primitives\nthat it does away with in the name of simplicity. Because files are small and are written\nand read in their entirety, there is no need to provide open, close, or seek operations.\nThe sync operation is not like fsync() in POSIX filesystems. As men-\ntioned earlier, writes in ZooKeeper are atomic, and a successful write\noperation is guaranteed to have been written to persistent storage on a\nmajority of ZooKeeper servers. However, it is permissible for reads to\nlag the latest state of ZooKeeper service, and the sync operation exists\nto allow a client to bring itself up-to-date. This topic is covered in more\ndetail in the section on “Consistency” on page 386.\nAPIs\nThere are two core language bindings for ZooKeeper clients, one for Java and one for\nC; there are also contrib bindings for Perl, Python, and REST clients. For each binding,\nthere is a choice between performing operations synchronously or asynchronously.\nWe’ve already seen the synchronous Java API. Here’s the signature for the exists op-\neration, which returns a Stat object encapsulating the znode’s metadata, or null if the\nznode doesn’t exist:\npublic Stat exists(String path, Watcher watcher) throws KeeperException,\nInterruptedException\nThe asynchronous equivalent, which is also found in the ZooKeeper class, looks like this:\npublic void exists(String path, Watcher watcher, StatCallback cb, Object ctx)\nThe ZooKeeper Service | 381In the Java API, all the asynchronous methods have void return types, since the result\nof the operation is conveyed via a callback. The caller passes a callback implementation,\nwhose method is invoked when a response is received from ZooKeeper. In this case,\nthe callback is the StatCallback interface, which has the following method:\npublic void processResult(int rc, String path, Object ctx, Stat stat);\nThe rc argument is the return code, corresponding to the codes defined by KeeperEx\nception. A nonzero code represents an exception, in which case the stat parameter will\nbe null. The path and ctx arguments correspond to the equivalent arguments passed\nby the client to the exists() method, and can be used to identify the request for which\nthis callback is a response. The ctx parameter can be an arbitrary object that may be\nused by the client when the path does not give enough context to disambiguate the\nrequest. If not needed, it may be set to null.\nThere are actually two C shared libraries. The single-threaded library, zookeeper_st,\nsupports only the asynchronous API and is intended for platforms where the pthread\nlibrary is not available or stable. Most developers will use the multithreaded library,\nzookeeper_mt, as it supports both the synchronous and asynchronous APIs. For details\non how to build and use the C API, please refer to the README file in the src/c directory\nof the ZooKeeper distribution.\nShould I Use the Synchronous or Asynchronous API?\nBoth APIs offer the same functionality, so the one you use is largely a matter of style.\nThe asynchronous API is appropriate if you have an event-driven programming model,\nfor example.\nThe asynchronous API allows you to pipeline requests, which in some scenarios can\noffer better throughput. Imagine that you want to read a large batch of znodes and\nprocess them independently. Using the synchronous API, each read would block until\nit returned, whereas with the asynchronous API, you can fire off all the asynchronous\nreads very quickly and process the responses in a separate thread as they come back.\nWatch triggers\nThe read operations exists, getChildren, and getData may have watches set on them,\nand the watches are triggered by write operations: create, delete, and setData. ACL\noperations do not participate in watches. When a watch is triggered, a watch event is\ngenerated, and the watch event’s type depends both on the watch and the operation\nthat triggered it:\n• A watch set on an exists operation will be triggered when the znode being watched\nis created, deleted, or has its data updated.\n382 | Chapter 13: ZooKeeper• A watch set on a getData operation will be triggered when the znode being watched\nis deleted or has its data updated. No trigger can occur on creation, since the znode\nmust already exist for the getData operation to succeed.\n• A watch set on a getChildren operation will be triggered when a child of the znode\nbeing watched is created or deleted, or when the znode itself is deleted. You can\ntell whether the znode or its child was deleted by looking at the watch event type:\nNodeDeleted shows the znode was deleted, and NodeChildrenChanged indicates that\nit was a child that was deleted.\nThe combinations are summarized in Table 13-2.\nTable 13-2. Watch creation operations and their corresponding triggers\nWatch trigger\nWatch creation\ncreate\nznode\nexists\ndelete\nchild\nNodeCreated\nchild\nNodeDeleted \nNodeChildren\nChanged\nNodeData\nChanged\nNodeDeleted \ngetData\ngetChildren\nznode\nsetData\nNodeData\nChanged\nNodeDeleted\nNodeChildren\nChanged\nA watch event includes the path of the znode that was involved in the event, so for\nNodeCreated and NodeDeleted events, you can tell which node was created or deleted\nsimply by inspecting the path. To discover which children have changed after a Node\nChildrenChanged event, you need to call getChildren again to retrieve the new list of\nchildren. Similarly, to discover the new data for a NodeDataChanged event, you need to\ncall getData. In both of these cases, the state of the znodes may have changed between\nreceiving the watch event and performing the read operation, so you should bear this\nin mind when writing applications.\nACLs\nA znode is created with a list of ACLs, which determines who can perform certain\noperations on it.\nACLs depend on authentication, the process by which the client identifies itself to\nZooKeeper. There are a few authentication schemes that ZooKeeper provides:\ndigest\nThe client is identified by a username and password.\nhost\nThe client is identified by his hostname.\nThe ZooKeeper Service | 383ip\nThe client is identified by his IP address.\nClients may authenticate themselves after establishing a ZooKeeper session. Authen-\ntication is optional, although a znode’s ACL may require an authenticated client, in\nwhich case the client must authenticate itself to access the znode. Here is an example\nof using the digest scheme to authenticate with a username and password:\nzk.addAuthInfo(""digest"", ""tom:secret"".getBytes());\nAn ACL is the combination of an authentication scheme, an identity for that scheme,\nand a set of permissions. For example, if we wanted to give clients in the domain\nexample.com read access to a znode, we would set an ACL on the znode with the\nhost scheme, an ID of example.com, and READ permission. In Java, we would create the\nACL object as follows:\nnew ACL(Perms.READ, new Id(""host"", ""example.com""));\nThe full set of permissions are listed in Table 13-3. Note that the exists operation is\nnot governed by an ACL permission, so any client may call exists to find the Stat for\na znode or to discover that a znode does not in fact exist.\nTable 13-3. ACL permissions\nACL permission Permitted operations\nCREATE create (a child znode)\nREAD getChildren\ngetData\nWRITE setData\nDELETE delete (a child znode)\nADMIN setACL\nThere are a number of predefined ACLs defined in the ZooDefs.Ids class, including\nOPEN_ACL_UNSAFE, which gives all permissions (except ADMIN permission) to everyone.\nIn addition, ZooKeeper has a pluggable authentication mechanism, which makes it\npossible to integrate third-party authentication systems if needed.\nImplementation\nThe ZooKeeper service can run in two modes. In standalone mode, there is a single\nZooKeeper server, which is useful for testing due to its simplicity (it can even be em-\nbedded in unit tests), but provides no guarantees of high-availability or resilience. In\nproduction, ZooKeeper runs in replicated mode, on a cluster of machines called an\nensemble. ZooKeeper achieves high-availability through replication, and can provide a\nservice as long as a majority of the machines in the ensemble are up. For example, in a\nfive-node ensemble, any two machines can fail and the service will still work because\n384 | Chapter 13: ZooKeepera majority of three remain. Note that a six-node ensemble can also tolerate only two\nmachines failing, since with three failures the remaining three do not constitute a ma-\njority of the six. For this reason, it is usual to have an odd number of machines in an\nensemble.\nConceptually, ZooKeeper is very simple: all it has to do is ensure that every modification\nto the tree of znodes is replicated to a majority of the ensemble. If a minority of the\nmachines fail, then a minimum of one machine will survive with the latest state. The\nother remaining replicas will eventually catch up with this state.\nThe implementation of this simple idea, however, is nontrivial. ZooKeeper uses a pro-\ntocol called Zab that runs in two phases, which may be repeated indefinitely:\nPhase 1: Leader election\nThe machines in an ensemble go through a process of electing a distinguished\nmember, called the leader. The other machines are termed followers. This phase is\nfinished once a majority (or quorum) of followers have synchronized their state\nwith the leader.\nPhase 2: Atomic broadcast\nAll write requests are forwarded to the leader, which broadcasts the update to the\nfollowers. When a majority have persisted the change, the leader commits the up-\ndate, and the client gets a response saying the update succeeded. The protocol for\nachieving consensus is designed to be atomic, so a change either succeeds or fails.\nIt resembles two-phase commit.\nDoes ZooKeeper Use Paxos?\nNo. ZooKeeper’s Zab protocol is not the same as the well-known Paxos algorithm\n(Leslie Lamport, “Paxos Made Simple,” ACM SIGACT News [Distributed Computing\nColumn] 32, 4 [Whole Number 121, December 2001] 51–58.). Zab is similar, but it\ndiffers in several aspects of its operation, such as relying on TCP for its message ordering\nguarantees.\nZab is described in “A simple totally ordered broadcast protocol” by Benjamin Reed\nand Flavio Junqueira (Proceedings of the Second Workshop on Large-Scale Distributed\nSystems and Middleware [LADIS’08] 2008, IBM TJ Watson Research Center, York-\ntown Heights, NY, USA. To appear in ACM International Conference Proceedings\nSeries, ACM Press, 2009. ISBN: 978-1-60558-296-2).\nGoogle’s Chubby Lock Service (Mike Burrows, “The Chubby Lock Service for Loosely-\nCoupled Distributed Systems,” November 2006, http://labs.google.com/papers/chubby\n.html), which shares similar goals with ZooKeeper, is based on Paxos.\nThe ZooKeeper Service | 385If the leader fails, the remaining machines hold another leader election and continue\nas before with the new leader. If the old leader later recovers, it then starts as a follower.\nLeader election is very fast, around 200 ms according to one published result,‖ so per-\nformance does not noticeably degrade during an election.\nAll machines in the ensemble write updates to disk before updating their in-memory\ncopy of the znode tree. Read requests may be serviced from any machine, and since\nthey involve only a lookup from memory, they are very fast.\nConsistency\nUnderstanding the basis of ZooKeeper’s implementation helps in understanding the\nconsistency guarantees that the service makes. The terms “leader” and “follower” for\nthe machines in an ensemble are apt, for they make the point that a follower may lag\nthe leader by a number of updates. This is a consequence of the fact that only a majority\nand not all of the ensemble needs to have persisted a change before it is committed. A\ngood mental model for ZooKeeper is of clients connected to ZooKeeper servers that\nare following the leader. A client may actually be connected to the leader, but it has no\ncontrol over this, and cannot even know if this is the case.# See Figure 13-2.\nFigure 13-2. Reads are satisfied by followers, while writes are committed by the leader\n‖ Reported by Yahoo! at http://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.html.\n# It is possible to configure ZooKeeper so that the leader does not accept client connections. In this case, its\nonly job is to coordinate updates. Do this by setting the leaderServes property to no. This is recommended\nfor ensembles of more than three servers.\n386 | Chapter 13: ZooKeeperEvery update made to the znode tree is given a globally unique identifier, called a\nzxid (which stands for “ZooKeeper transaction ID”). Updates are ordered, so if zxid\nz1 is less than z2, then z1 happened before z2, according to ZooKeeper, which is the\nsingle authority on ordering in the distributed system.\nThe following guarantees for data consistency flow from ZooKeeper’s design:\nSequential consistency\nUpdates from any particular client are applied in the order that they are sent. This\nmeans that if a client updates the znode z to the value a, and in a later operation,\nit updates z to the value b, then no client will ever see z with value a after it has\nseen it with value b (if no other updates are made to z).\nAtomicity\nUpdates either succeed or fail. This means that if an update fails, no client will ever\nsee it.\nSingle system image\nA client will see the same view of the system regardless of the server it connects to.\nThis means that if a client connects to a new server during the same session, it will\nnot see an older state of the system than the one it saw with the previous server.\nWhen a server fails and a client tries to connect to another in the ensemble, a server\nthat is behind the one that failed will not accept connections from the client until\nit has caught up with the failed server.\nDurability\nOnce an update has succeeded, it will persist and will not be undone. This means\nupdates will survive server failures.\nTimeliness\nThe lag in any client’s view of the system is bounded, so it will not be out of date\nby more than some multiple of tens of seconds. This means that rather than allow\na client to see data that is very stale, a server will shut down, forcing the client to\nswitch to a more up-to-date server.\nFor performance reasons, reads are satisfied from a ZooKeeper’s server’s memory and\ndo not participate in the global ordering of writes. This property can lead to the ap-\npearance of inconsistent ZooKeeper states from clients that communicate through a\nmechanism outside ZooKeeper.\nFor example, client A updates znode z from a to a', A tells B to read z, B reads the value\nof z as a, not a'. This is perfectly compatible with the guarantees that ZooKeeper makes\n(this condition that it does not promise is called “Simultaneously Consistent Cross-\nClient Views”). To prevent this condition from happening, B should call sync on z,\nbefore reading z’s value. The sync operation forces the ZooKeeper server that B is con-\nnected to to “catch up” with the leader, so that when B reads z’s value it will be the one\nthat A set (or a later value).\nThe ZooKeeper Service | 387Slightly confusingly, the sync operation is only available as an asyn-\nchronous call. The reason for this is that you don’t need to wait for it to\nreturn, since ZooKeeper guarantees that any subsequent operation will\nhappen after the sync completes on the server, even if the operation is\nissued before the sync completes.\nSessions\nA ZooKeeper client is configured with the list of servers in the ensemble. On startup,\nit tries to connect to one of the servers in the list. If the connection fails, it tries another\nserver in the list, and so on, until it either successfully connects to one of them, or fails\nif all ZooKeeper servers are unavailable.\nOnce a connection has been made with a ZooKeeper server, the server creates a new\nsession for the client. A session has a timeout period that is decided on by the appli-\ncation that creates it. If the server hasn’t received a request within the timeout period,\nit may expire the session. Once a session has expired, it may not be reopened, and any\nephemeral nodes associated with the session will be lost. Although session expiry is a\ncomparatively rare event, since sessions are long-lived, it is important for applications\nto handle it (which you will see how in “The Resilient ZooKeeper Applica-\ntion” on page 394).\nSessions are kept alive by the client sending ping requests (also known as heartbeats)\nwhenever the session is idle for longer than a certain period. (Pings are automatically\nsent by the ZooKeeper client library, so your code doesn’t need to worry about main-\ntaining the session.) The period is chosen to be low enough to detect server failure\n(manifested by a read timeout) and reconnect to another server within the session\ntimeout period.\nFailover to another ZooKeeper server is handled automatically by the ZooKeeper client,\nand, crucially, sessions (and associated ephemeral znodes) are still valid after another\nserver takes over from the failed one.\nDuring failover, the application will receive notifications of disconnections and con-\nnections to the service. Watch notifications will not be delivered while the client is\ndisconnected, but they will be delivered when the client successfully reconnects. Also,\nif the application tries to perform an operation while the client is reconnecting to\nanother server, the operation will fail. This underlines the importance of handling con-\nnection loss exceptions in real-world ZooKeeper applications (described in “The Re-\nsilient ZooKeeper Application” on page 394).\nTime\nThere are several time parameters in ZooKeeper. The tick time is the fundamental period\nof time in ZooKeeper and is used by servers in the ensemble to define the schedule on\nwhich their interactions run. Other settings are defined in terms of tick time, or are at\n388 | Chapter 13: ZooKeeperleast constrained by it. The session timeout, for example, may not be less than 2 ticks\nor more than 20. If you attempt to set a session timeout outside this range, it will be\nmodified to fall within the range.\nA common tick time setting is 2 seconds (2,000 milliseconds). This translates to an\nallowable session timeout of between 4 and 40 seconds. There are a few considerations\nin selecting a session timeout.\nA low session timeout leads to faster detection of machine failure. In the group mem-\nbership example, the session timeout is the time it takes for a failed machine to be\nremoved from the group. Beware of setting the session timeout too low, however, since\na busy network can cause packets to be delayed and may cause inadvertent session\nexpiry. In such an event, a machine would appear to “flap”: leaving and then rejoining\nthe group repeatedly in a short space of time.\nApplications that create more complex ephemeral state should favor longer session\ntimeouts, as the cost of reconstruction is higher. In some cases, it is possible to design\nthe application so it can restart within the session timeout period, and avoid session\nexpiry. (This might be desirable to perform maintenance or upgrades.) Every session\nis given a unique identity and password by the server, and if these are passed to Zoo-\nKeeper while a connection is being made, it is possible to recover a session (as long as\nit hasn’t expired). An application can therefore arrange a graceful shutdown, whereby\nit stores the session identity and password to stable storage before restarting the proc-\ness, retrieving the stored session identity and password, and recovering the session.\nYou should view this feature as an optimization, which can help avoid expire sessions.\nIt does not remove the need to handle session expiry, which can still occur if a machine\nfails unexpectedly, or even if an application is shut down gracefully but does not restart\nbefore its session expires—for whatever reason.\nAs a general rule, the larger the ZooKeeper ensemble, the larger the session timeout\nshould be. Connection timeouts, read timeouts, and ping periods are all defined inter-\nnally as a function of the number of servers in the ensemble, so as the ensemble grows,\nthese periods decrease. Consider increasing the timeout if you experience frequent\nconnection loss. You can monitor ZooKeeper metrics—such as request latency\nstatistics—using JMX.\nStates\nThe ZooKeeper object transitions through different states in its lifecycle (see Fig-\nure 13-3). You can query its state at any time by using the getState() method:\npublic States getState()\nStates is an enum representing the different states that a ZooKeeper object may be in.\n(Despite the enum’s name, an instance of ZooKeeper may only be in one state at a time.)\nA newly constructed ZooKeeper instance is in the CONNECTING state, while it tries to\nThe ZooKeeper Service | 389Figure 13-3. ZooKeeper state transitions\nestablish a connection with the ZooKeeper service. Once a connection is established,\nit goes into the CONNECTED state.\nA client using the ZooKeeper object can receive notifications of the state transitions by\nregistering a Watcher object. On entering the CONNECTED state, the watcher receives a\nWatchedEvent whose KeeperState value is SyncConnected.\nA ZooKeeper Watcher object serves double duty: it can be used to be\nnotified of changes in the ZooKeeper state (as described in this section),\nand it can be used to be notified of changes in znodes (described in\n“Watch triggers” on page 382). The (default) watcher passed in to the\nZooKeeper object constructor is used for state changes, but znode\nchanges may either use a dedicated instance of Watcher (by passing one\nin to the appropriate read operation), or they may share the default one\nif using the form of the read operation that takes a boolean flag to specify\nwhether to use a watcher.\nThe ZooKeeper instance may disconnect and reconnect to the ZooKeeper service, mov-\ning between the CONNECTED and CONNECTING states. If it disconnects, the watcher receives\na Disconnected event. Note that these state transitions are initiated by the ZooKeeper\ninstance itself, and it will automatically try to reconnect if the connection is lost.\n390 | Chapter 13: ZooKeeperThe ZooKeeper instance may transition to a third state, CLOSED, if either the close()\nmethod is called or the session times out as indicated by a KeeperState of type\nExpired. Once in the CLOSED state, the ZooKeeper object is no longer considered to be\nalive (this can be tested using the isAlive() method on States), and cannot be reused.\nTo reconnect to the ZooKeeper service, the client must construct a new ZooKeeper\ninstance.\nBuilding Applications with ZooKeeper\nHaving covered ZooKeeper in some depth, let’s turn back to writing some useful ap-\nplications with it.\nA Configuration Service\nOne of the most basic services that a distributed application needs is a configuration\nservice so that common pieces of configuration information can be shared by machines\nin a cluster. At the simplest level, ZooKeeper can act as a highly available store for\nconfiguration, allowing application participants to retrieve or update configuration\nfiles. Using ZooKeeper watches, it is possible to create an active configuration service,\nwhere interested clients are notified of changes in configuration.\nLet’s write such a service. We make a couple of assumptions that simplify the imple-\nmentation (they could be removed with a little more work). First, the only configuration\nvalues we need to store are strings, and keys are just znode paths, so we use a znode to\nstore each key-value pair. Second, there is a single client that performs updates at any\none time. Among other things, this model fits with the idea of a master (such as the\nnamenode in HDFS) that wishes to update information that its workers need to follow.\nWe wrap the code up in a class called ActiveKeyValueStore:\npublic class ActiveKeyValueStore extends ConnectionWatcher {\nprivate static final Charset CHARSET = Charset.forName(""UTF-8"");\n}\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), -1);\n}\n}\nThe contract of the write() method is that a key with the given value is written to\nZooKeeper. It hides the difference between creating a new znode and updating an ex-\nisting znode with a new value, by testing first for the znode using the exists operation\nBuilding Applications with ZooKeeper | 391and then performing the appropriate operation. The other detail worth mentioning is\nthe need to convert the string value to a byte array, for which we just use the\ngetBytes() method with a UTF-8 encoding.\nTo illustrate the use of the ActiveKeyValueStore, consider a ConfigUpdater class that\nupdates a configuration property with a value. The listing appears in Example 13-6.\nExample 13-6. An application that updates a property in ZooKeeper at random times\npublic class ConfigUpdater {\npublic static final String PATH = ""/config"";\nprivate ActiveKeyValueStore store;\nprivate Random random = new Random();\npublic ConfigUpdater(String hosts) throws IOException, InterruptedException {\nstore = new ActiveKeyValueStore();\nstore.connect(hosts);\n}\npublic void run() throws InterruptedException, KeeperException {\nwhile (true) {\nString value = random.nextInt(100) + """";\nstore.write(PATH, value);\nSystem.out.printf(""Set %s to %s\\n"", PATH, value);\nTimeUnit.SECONDS.sleep(random.nextInt(10));\n}\n}\n}\npublic static void main(String[] args) throws Exception {\nConfigUpdater configUpdater = new ConfigUpdater(args[0]);\nconfigUpdater.run();\n}\nThe program is simple. A ConfigUpdater has an ActiveKeyValueStore that connects to\nZooKeeper in ConfigUpdater’s constructor. The run() method loops forever, updating\nthe /config znode at random times with random values.\nNext, let’s look at how to read the /config configuration property. First we add a read\nmethod to ActiveKeyValueStore:\npublic String read(String path, Watcher watcher) throws InterruptedException,\nKeeperException {\nbyte[] data = zk.getData(path, watcher, null/*stat*/);\nreturn new String(data, CHARSET);\n}\nThe getData() method of ZooKeeper takes the path, a Watcher, and a Stat object. The\nStat object is filled in with values by getData(), and is used to pass information back\nto the caller. In this way, the caller can get both the data and the metadata for a znode,\nalthough in this case, we pass a null Stat because we are not interested in the metadata.\n392 | Chapter 13: ZooKeeperAs a consumer of the service, ConfigWatcher (see Example 13-7) creates an ActiveKey\nValueStore, and after starting, calls the store’s read() method (in its displayConfig()\nmethod) passing a reference to itself as the watcher. It displays the initial value of the\nconfiguration that it reads.\nExample 13-7. An application that watches for updates of a property in ZooKeeper and prints them\nto the console\npublic class ConfigWatcher implements Watcher {\nprivate ActiveKeyValueStore store;\npublic ConfigWatcher(String hosts) throws IOException, InterruptedException {\nstore = new ActiveKeyValueStore();\nstore.connect(hosts);\n}\npublic void displayConfig() throws InterruptedException, KeeperException {\nString value = store.read(ConfigUpdater.PATH, this);\nSystem.out.printf(""Read %s as %s\\n"", ConfigUpdater.PATH, value);\n}\n@Override\npublic void process(WatchedEvent event) {\nif (event.getType() == EventType.NodeDataChanged) {\ntry {\ndisplayConfig();\n} catch (InterruptedException e) {\nSystem.err.println(""Interrupted. Exiting."");\nThread.currentThread().interrupt();\n} catch (KeeperException e) {\nSystem.err.printf(""KeeperException: %s. Exiting.\\n"", e);\n}\n}\n}\npublic static void main(String[] args) throws Exception {\nConfigWatcher configWatcher = new ConfigWatcher(args[0]);\nconfigWatcher.displayConfig();\n}\n}\n// stay alive until process is killed or thread is interrupted\nThread.sleep(Long.MAX_VALUE);\nWhen the ConfigUpdater updates the znode, ZooKeeper causes the watcher to fire with\nan event type of EventType.NodeDataChanged. ConfigWatcher acts on this event in its\nprocess() method by reading and displaying the latest version of the config.\nBecause watches are one-time signals, we tell ZooKeeper of the new watch each time\nwe call read() on ActiveKeyValueStore—this ensures we see future updates. Further-\nmore, we are not guaranteed to receive every update, since between the receipt of the\nwatch event and the next read, the znode may have been updated, possibly many times,\nBuilding Applications with ZooKeeper | 393and as the client has no watch registered during that period, it is not notified. For the\nconfiguration service, this is not a problem because clients care only about the latest\nvalue of a property, as it takes precedence over previous values, but in general you\nshould be aware of this potential limitation.\nLet’s see the code in action. Launch the ConfigUpdater in one terminal window:\n% java ConfigUpdater localhost\nSet /config to 79\nSet /config to 14\nSet /config to 78\nThen launch the ConfigWatcher in another window immediately afterward:\n% java ConfigWatcher localhost\nRead /config as 79\nRead /config as 14\nRead /config as 78\nThe Resilient ZooKeeper Application\nThe first of the Fallacies of Distributed Computing* states that “The network is relia-\nble.” As they stand, the programs so far have been assuming a reliable network, so when\nthey run on a real network, they can fail in several ways. Let’s examine possible failure\nmodes, and what we can do to correct them so that our programs are resilient in the\nface of failure.\nEvery ZooKeeper operation in the Java API declares two types of exception in its throws\nclause: InterruptedException and KeeperException.\nInterruptedException\nAn InterruptedException is thrown if the operation is interrupted. There is a standard\nJava mechanism for canceling blocking methods, which is to call interrupt() on the\nthread from which the blocking method was called. A successful cancelation will result\nin an InterruptedException. ZooKeeper adheres to this standard, so you can cancel a\nZooKeeper operation in this way. Classes or libraries that use ZooKeeper should usually\npropagate the InterruptedException so that their clients can cancel their operations.†\nAn InterruptedException does not indicate a failure, but rather that the operation has\nbeen canceled, so in the configuration application, it is appropriate to propagate the\nexception, causing the application to terminate.\n* See http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing.\n† For more detail, see the excellent article “Dealing with InterruptedException” by Brian Goetz.\n394 | Chapter 13: ZooKeeperKeeperException\nA KeeperException is thrown if the ZooKeeper server signals an error, or if there is a\ncommunication problem with the server. There are various subclasses of KeeperExcep\ntion for different error cases. For example, KeeperException.NoNodeException is a sub-\nclass of KeeperException that is thrown if you try to perform an operation on a znode\nthat doesn’t exist.\nEvery subclass of KeeperException has a corresponding code with information about\nthe type of error. For example, for KeeperException.NoNodeException the code is Keep\nerException.Code.NONODE (an enum value).\nThere are two ways then to handle KeeperException: either catch KeeperException and\ntest its code to determine what remedying action to take, or catch the equivalent\nKeeperException subclasses and perform the appropriate action in each catch block.\nKeeperExceptions fall into three broad categories.\nState exceptions. A state exception occurs when the operation fails because it cannot be\napplied to the znode tree. State exceptions usually happen because another process is\nmutating a znode at the same time. For example, a setData operation with a version\nnumber will fail with a KeeperException.BadVersionException if the znode is updated\nby another process first, since the version number does not match. The programmer is\nusually aware that this kind of conflict is possible, and will code to deal with it.\nSome state exceptions indicate an error in the program, such as KeeperExcep\ntion.NoChildrenForEphemeralsException, which is thrown when trying to create a child\nznode of an ephemeral znode.\nRecoverable exceptions. Recoverable exceptions are those from which the application can\nrecover within the same ZooKeeper session. A recoverable exception is manifested by\nKeeperException.ConnectionLossException, which means that the connection to Zoo-\nKeeper has been lost. ZooKeeper will try to reconnect, and in most cases the recon-\nnection will succeed and ensure that the session is intact.\nHowever, ZooKeeper cannot tell whether the operation that failed with KeeperExcep\ntion.ConnectionLossException was applied. This is an example of partial failure (which\nwe introduced at the beginning of the chapter). The onus is therefore on the program-\nmer to deal with the uncertainty, and the action that should be taken depends on the\napplication.\nAt this point, it is useful to make a distinction between idempotent and nonidempo-\ntent operations. An idempotent operation is one that may be applied one or more times\nwith the same result, such as a read request or an unconditional setData. These can\nsimply be retried.\nA nonidempotent operation cannot be indiscriminately retried, as the effect of applying\nit multiple times is not the same as applying it once. The program needs a way of\ndetecting whether its update was applied by encoding information in the znode’s path\nBuilding Applications with ZooKeeper | 395name or its data. We shall discuss how to deal with failed nonidempotent operations\nin “Recoverable exceptions” on page 399, when we look at the implementation of a\nlock service.\nUnrecoverable exceptions. In some cases, the ZooKeeper session becomes invalid—\nperhaps because of a timeout or because the session was closed (both get a KeeperEx\nception.SessionExpiredException), or perhaps because authentication failed (Keeper\nException.AuthFailedException). In any case, all ephemeral nodes associated with the\nsession will be lost, so the application needs to rebuild its state before reconnecting to\nZooKeeper.\nA reliable configuration service\nGoing back to the write() method in ActiveKeyValueStore, recall that it is composed\nof an exists operation followed by either a create or a setData:\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), -1);\n}\n}\nTaken as a whole, the write() method is idempotent, so we can afford to uncondi-\ntionally retry it. Here’s a modified version of the write() method that retries in a loop.\nIt is set to try a maximum number of retries (MAX_RETRIES) and sleeps for\nRETRY_PERIOD_SECONDS between each attempt:\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nint retries = 0;\nwhile (true) {\ntry {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), stat.getVersion());\n}\n} catch (KeeperException.SessionExpiredException e) {\nthrow e;\n} catch (KeeperException e) {\nif (retries++ == MAX_RETRIES) {\nthrow e;\n}\n// sleep then retry\nTimeUnit.SECONDS.sleep(RETRY_PERIOD_SECONDS);\n396 | Chapter 13: ZooKeeper}\n}\n}\nThe code is careful not to retry KeeperException.SessionExpiredException, since when\na session expires, the ZooKeeper object enters the CLOSED state, from which it can never\nreconnect (refer to Figure 13-3). We simply rethrow the exception‡ and let the caller\ncreate a new ZooKeeper instance, so that the whole write() method can be retried. A\nsimple way to create a new instance is to create a new ConfigUpdater (which we’ve\nactually renamed ResilientConfigUpdater) to recover from an expired session:\npublic static void main(String[] args) throws Exception {\nwhile (true) {\ntry {\nResilientConfigUpdater configUpdater =\nnew ResilientConfigUpdater(args[0]);\nconfigUpdater.run();\n} catch (KeeperException.SessionExpiredException e) {\n// start a new session\n} catch (KeeperException e) {\n// already retried, so exit\ne.printStackTrace();\nbreak;\n}\n}\n}\nAn alternative way of dealing with session expiry would be to look for a KeeperState\nof type Expired in the watcher (that would be the ConnectionWatcher in the example\nhere), and create a new connection when this is detected. This way, we would just keep\nretrying in the write() method, even if we got a KeeperException.SessionExpiredExcep\ntion, since the connection should eventually be re-established. Regardless of the precise\nmechanics of how we recover from an expired session, the important point is that it is\na different kind of failure from connection loss and needs to be handled differently.\nThere’s actually another failure mode that we’ve ignored here. When\nthe ZooKeeper object is created, it tries to connect to a ZooKeeper server.\nIf the connection fails or times out, then it tries another server in the\nensemble. If, after trying all of the servers in the ensemble, it can’t con-\nnect, then it throws an IOException. The likelihood of all ZooKeeper\nservers being unavailable is low; nevertheless, some applications may\nchoose to retry the operation in a loop until ZooKeeper is available.\nThis is just one strategy for retry handling—there are many others, such as using ex-\nponential backoff where the period between retries is multiplied by a constant each\n‡ Another way of writing the code would be to have a single catch block, just for KeeperException, and a test\nto see whether its code has the value KeeperException.Code.SESSIONEXPIRED. Which method you use is a\nmatter of style, since they both behave in the same way.\nBuilding Applications with ZooKeeper | 397time. The org.apache.hadoop.io.retry package in Hadoop Core is a set of utilities for\nadding retry logic into your code in a reusable way, and may be helpful for building\nZooKeeper applications.\nA Lock Service\nA distributed lock is a mechanism for providing mutual exclusion between a collection\nof processes. At any one time, only a single process may hold the lock. Distributed locks\ncan be used for leader election in a large distributed system, where the leader is the\nprocess that holds the lock at any point in time.\nDo not confuse ZooKeeper’s own leader election with a general leader\nelection service, which can be built using ZooKeeper primitives. Zoo-\nKeeper’s own leader election is not exposed publicly, unlike the type of\ngeneral leader election service we are describing here, which is designed\nto be used by distributed systems that need to agree upon a master\nprocess.\nTo implement a distributed lock using ZooKeeper, we use sequential znodes to impose\nan order on the processes vying for the lock. The idea is simple: first designate a lock\nznode, typically describing the entity being locked on, say /leader; then clients that want\nto acquire the lock create sequential ephemeral znodes as children of the lock znode.\nAt any point in time, the client with the lowest sequence number holds the lock. For\nexample, if two clients create znodes at around the same time, /leader/lock-1\nand /leader/lock-2, then the client that created /leader/lock-1 holds the lock, since its\nznode has the lowest sequence number. The ZooKeeper service is the arbiter of order,\nsince it assigns the sequence numbers.\nThe lock may be released simply by deleting the znode /leader/lock-1; alternatively, if\nthe client process dies, it will be deleted by virtue of it being an ephemeral znode. The\nclient that created /leader/lock-2 will then hold the lock, since it has the next lowest\nsequence number. It will be notified that it has the lock by creating a watch that fires\nwhen znodes go away.\nThe pseudocode for lock acquisition is as follows:\n1. Create an ephemeral sequential znode named lock- under the lock znode and re-\nmember its actual path name (the return value of the create operation).\n2. Get the children of the lock znode and set a watch.\n3. If the path name of the znode created in 1 has the lowest number of the children\nreturned in 2, then the lock has been acquired. Exit.\n4. Wait for the notification from the watch set in 2 and go to step 2.\n398 | Chapter 13: ZooKeeperThe herd effect\nAlthough this algorithm is correct, there are some problems with it. The first problem\nis that this implementation suffers from the herd effect. Consider hundreds or thou-\nsands of clients, all trying to acquire the lock. Each client places a watch on the lock\nznode for changes in its set of children. Every time the lock is released, or another\nprocess starts the lock acquisition process, the watch fires and every client receives a\nnotification. The “herd effect” refers to a large number of clients being notified of the\nsame event, when only a small number of them can actually proceed. In this case, only\none client will successfully acquire the lock, and the process of maintaining and sending\nwatch events to all clients causes traffic spikes, which put pressure on the ZooKeeper\nservers.\nTo avoid the herd effect, we need to refine the condition for notification. The key\nobservation for implementing locks is that a client needs to be notified only when the\nchild znode with the previous sequence number goes away, not when any child znode\nis deleted (or created). In our example, if clients have created the znodes /leader/\nlock-1, /leader/lock-2, and /leader/lock-3, then the client holding /leader/lock-3 only\nneeds to be notified when /leader/lock-2 disappears. It does not need to be notified\nwhen /leader/lock-1 disappears, or when a new znode /leader/lock-4 is added.\nRecoverable exceptions\nAnother problem with the lock algorithm as it stands is that it doesn’t handle the case\nwhen the create operation fails due to connection loss. Recall that in this case we do\nnot know if the operation succeeded or failed. Creating a sequential znode is a\nnonidempotent operation, so we can’t simply retry, since if the first create had succee-\nded, we would have an orphaned znode that would never be deleted (until the client\nsession ended, at least). Deadlock would be the unfortunate result.\nThe problem is that after reconnecting, the client can’t tell whether it created any of\nthe child znodes. By embedding an identifier in the znode name, if it suffers a connection\nloss, it can check to see whether any of the children of the lock node have its identifier\nin their name. If a child contains its identifier, it knows that the create operation suc-\nceeded, and it shouldn’t create another child znode. If no child has the identifier in its\nname, then the client can safely create a new sequential child znode.\nThe client’s session identifier is a long integer that is unique for the ZooKeeper service\nand therefore ideal for the purpose of identifying a client across connection loss events.\nThe session identifier can be obtained by calling the getSessionId() method on the\nZooKeeper Java class.\nThe ephemeral sequential znode should be created with a name of the form lock-\n<sessionId>-, so that when the sequence number is appended by ZooKeeper, the name\nbecomes lock-<sessionId>-<sequenceNumber>. The sequence numbers are unique to the\nparent, not to the name of the child, so this technique allows the child znodes to identify\ntheir creators as well as impose an order of creation.\nBuilding Applications with ZooKeeper | 399Unrecoverable exceptions\nIf a client’s ZooKeeper session expires, the ephemeral znode created by the client will\nbe deleted, effectively relinquishing the lock, or at least forfeiting the client’s turn to\nacquire the lock. The application using the lock should realize that it no longer holds\nthe lock, clean up its state, then start again, by creating a new lock object, and trying\nto acquire it. Notice that it is the application that controls this process, not the lock\nimplementation, since it cannot second-guess how the application needs to clean up\nits state.\nImplementation\nImplementing a distributed lock correctly is a delicate matter, since accounting for all\nof the failure modes is nontrivial. ZooKeeper comes with a production-quality lock\nimplementation in Java called WriteLock (from ZooKeeper 3.2.0 onward) that is very\neasy for clients to use.\nMore Distributed Data Structures and Protocols\nThere are many distributed data structures and protocols that can be built with Zoo-\nKeeper, such as barriers, queues, and two-phase commit. One interesting thing to note\nis that these are synchronous protocols, even though we use asynchronous ZooKeeper\nprimitives (such as notifications) to build them.\nThe ZooKeeper website (http://hadoop.apache.org/zookeeper/) describes several such\ndata structures and protocols in pseudocode. At the time of this writing, standard im-\nplementations were not available as a part of ZooKeeper, but over time it is expected\nthat they will be added to the codebase.\nBookKeeper\nBookKeeper is a highly available and reliable logging service. It can be used to provide\nwrite-ahead logging, which is a common technique for ensuring data integrity in storage\nsystems. In a system using write-ahead logging, every write operation is written to the\ntransaction log before it is applied. Using this procedure, we don’t have to write the\ndata to permanent storage after every write operation because in the event of a system\nfailure, the latest state may be recovered by replaying the transaction log for any writes\nthat had not been applied.\nBookKeeper clients create logs called ledgers, and each record appended to a ledger is\ncalled a ledger entry, which is simply a byte array. Ledgers are managed by bookies,\nwhich are servers that replicate the ledger data. Note that ledger data is not stored in\nZooKeeper, only metadata is.\nTraditionally, the challenge has been to make systems that use write-ahead logging\nrobust in the face of failure of the node writing the transaction log. This is usually done\nby replicating the transaction log in some manner. Hadoop’s HDFS namenode, for\n400 | Chapter 13: ZooKeeperinstance, writes its edit log to multiple disks, one of which is typically an NFS mounted\ndisk. However, in the event of failure of the primary, failover is still manual. By pro-\nviding logging as a highly available service, BookKeeper promises to make failover\ntransparent, since it can tolerate the loss of bookie servers.\nBookKeeper is provided in the contrib directory of the ZooKeeper distribution, where\nyou can find more information on how to use it.\nZooKeeper in Production\nIn production, you should run ZooKeeper in replicated mode. Here we will cover some\nof the considerations for running an ensemble of ZooKeeper servers. However, this\nsection is not exhaustive, so you should consult the ZooKeeper Administrator’s Guide\n(http://hadoop.apache.org/zookeeper/docs/current/) for detailed up-to-date instructions,\nincluding supported platforms, recommended hardware, maintenance procedures, and\nconfiguration properties.\nResilience and Performance\nZooKeeper machines should be located to minimize the impact of machine and network\nfailure. In practice, this means that servers should be spread across racks, power sup-\nplies, and switches, so that the failure of any one of these does not cause the ensemble\nto lose a majority of its servers. ZooKeeper replies on having low-latency connections\nbetween all of the servers in the ensemble, so for that reason an ensemble should be\nconfined to a single data center.\nZooKeeper is a highly available system, and it is critical that it can perform its functions\nin a timely manner. Therefore, ZooKeeper should run on machines that are dedicated\nto ZooKeeper alone. Having other applications contend for resources can cause Zoo-\nKeeper’s performance to degrade significantly.\nConfigure ZooKeeper to keep its transaction log on a different disk drive from its snap-\nshots. By default, both go in the directory specified by the dataDir property, but by\nspecifying a location for dataLogDir, the transaction log will be written there. By having\nits own dedicated device (not just a partition) a ZooKeeper server can maximize the\nrate at which it writes log entries to disk, which is does sequentially, without seeking.\nSince all writes go through the leader, write throughput does not scale by adding servers,\nso it is crucial that writes are as fast as possible.\nIf the process swaps to disk, performance will suffer adversely. This can be avoided by\nsetting the Java heap size to less than the amount of physical memory available on the\nmachine. The ZooKeeper scripts will source a file called java.env from its configuration\ndirectory, and this can be used to set the JVMFLAGS environment variable to set the heap\nsize (and any other desired JVM arguments).\nZooKeeper in Production | 401Configuration\nEach server in the ensemble of ZooKeeper servers has a numeric identifier that is unique\nwithin the ensemble, and must fall between 1 and 255. The server number is specified\nin plain text in a file named myid in the directory specified by the dataDir property.\nSetting each server number is only half of the job. We also need to give all the servers\nall the identities and network locations of the others in the ensemble. The ZooKeeper\nconfiguration file must include a line for each server, of the form:\nserver.n=hostname:port:port\nThe value of n is replaced by the server number. There are two port settings: the first\nis the port that followers use to connect to the leader, and the second is used for leader\nelection. Here is a sample configuration for a three-machine replicated ZooKeeper\nensemble:\ntickTime=2000\ndataDir=/disk1/zookeeper\ndataLogDir=/disk2/zookeeper\nclientPort=2181\ninitLimit=5\nsyncLimit=2\nserver.1=zookeeper1:2888:3888\nserver.2=zookeeper2:2888:3888\nserver.3=zookeeper3:2888:3888\nServers listen on three ports: 2181 for client connections; 2888 for follower connections,\nif they are the leader; and 3888 for other server connections during the leader election\nphase. When a ZooKeeper server starts up, it reads the myid file to determine which\nserver it is, then reads the configuration file to determine the ports it should listen on,\nas well as the network addresses of the other servers in the ensemble.\nClients connecting to this ZooKeeper ensemble should use zookeeper1:2181,zoo\nkeeper2:2181,zookeeper3:2181 as the host string in the constructor for the ZooKeeper\nobject.\nIn replicated mode, there are two extra mandatory properties: initLimit and\nsyncLimit, both measured in multiples of tickTime.\ninitLimit is the amount of time to allow for followers to connect to and sync with the\nleader. If a majority of followers fail to sync within this period, then the leader renounces\nits leadership status and another leader election takes place. If this happens often (and\nyou can discover if this is the case because it is logged), it is a sign that the setting is too\nlow.\nsyncLimit is the amount of time to allow a follower to sync with the leader. If a follower\nfails to sync within this period, it will restart itself. Clients that were attached to this\nfollower will connect to another one.\n402 | Chapter 13: ZooKeeperThese are the minimum settings needed to get up and running with a cluster of Zoo-\nKeeper servers. There are, however, more configuration options, particularly for tuning\nperformance, documented in the ZooKeeper Administrator’s Guide.\nZooKeeper in Production | 403CHAPTER 14\nCase Studies\nHadoop Usage at Last.fm\nLast.fm: The Social Music Revolution\nFounded in 2002, Last.fm is an Internet radio and music community website that offers\nmany services to its users, such as free music streams and downloads, music and event\nrecommendations, personalized charts, and much more. There are about 25 million\npeople who use Last.fm every month, generating huge amounts of data that need to be\nprocessed. One example of this is users transmitting information indicating which\nsongs they are listening to (this is known as “scrobbling”). This data is processed and\nstored by Last.fm, so the user can access it directly (in the form of charts), and it is also\nused to make decisions about users’ musical tastes and compatibility, and artist and\ntrack similarity.\nHadoop at Last.fm\nAs Last.fm’s service developed and the number of users grew from thousands to mil-\nlions, storing, processing and managing all the incoming data became increasingly\nchallenging. Fortunately, Hadoop was quickly becoming stable enough and was en-\nthusiastically adopted as it became clear how many problems it solved. It was first used\nat Last.fm in early 2006 and was put into production a few months later. There were\nseveral reasons for adopting Hadoop at Last.fm:\n• The distributed filesystem provided redundant backups for the data stored on it\n(e.g., web logs, user listening data) at no extra cost.\n• Scalability was simplified through the ability to add cheap, commodity hardware\nwhen required.\n• The cost was right (free) at a time when Last.fm had limited financial resources.\n• The open source code and active community meant that Last.fm could freely mod-\nify Hadoop to add custom features and patches.\n405• Hadoop provided a flexible framework for running distributed computing algo-\nrithms with a relatively easy learning curve.\nHadoop has now become a crucial part of Last.fm’s infrastructure, currently consisting\nof two Hadoop clusters spanning over 50 machines, 300 cores, and 100 TB of disk\nspace. Hundreds of daily jobs are run on the clusters performing operations, such as\nlogfile analysis, evaluation of A/B tests, ad hoc processing, and charts generation. This\ncase study will focus on the process of generating charts, as this was the first usage of\nHadoop at Last.fm and illustrates the power and flexibility that Hadoop provides over\nother approaches when working with very large datasets.\nGenerating Charts with Hadoop\nLast.fm uses user-generated track listening data to produce many different types of\ncharts, such as weekly charts for tracks, per country and per user. A number of Hadoop\nprograms are used to process the listening data and generate these charts, and these\nrun on a daily, weekly, or monthly basis. Figure 14-1 shows an example of how this\ndata is displayed on the site; in this case, the weekly top tracks.\nFigure 14-1. Last.fm top tracks chart\nListening data typically arrives at Last.fm from one of two sources:\n406 | Chapter 14: Case Studies• A user plays a track of her own (e.g., listening to an MP3 file on a PC or other\ndevice), and this information is sent to Last.fm using either the official Last.fm\nclient application or one of many hundreds of third-party applications.\n• A user tunes into one of Last.fm’s Internet radio stations and streams a song to her\ncomputer. The Last.fm player or website can be used to access these streams and\nextra functionality is made available to the user, allowing her to love, skip, or ban\neach track that she listens to.\nWhen processing the received data, we distinguish between a track listen submitted by\na user (the first source above, referred to as a scrobble from here on) and a track listened\nto on the Last.fm radio (the second source, mentioned earlier, referred to as a radio\nlisten from here on). This distinction is very important in order to prevent a feedback\nloop in the Last.fm recommendation system, which is based only on scrobbles. One of\nthe most fundamental Hadoop jobs at Last.fm takes the incoming listening data and\nsummarizes it into a format that can be used for display purposes on the Last.fm website\nas well as input to other Hadoop programs. This is achieved by the Track Statistics\nprogram, which is the example described in the following sections.\nThe Track Statistics Program\nWhen track listening data is submitted to Last.fm, it undergoes a validation and\nconversion phase, the end result of which is a number of space-delimited text files\ncontaining the user ID, the track ID, the number of times the track was scrobbled, the\nnumber of times the track was listened to on the radio, and the number of times it was\nskipped. Table 14-1 contains sample listening data, which is used in the following\nexamples as input to the Track Statistics program (the real data is gigabytes in size and\nincludes many more fields that have been omitted here for simplicity’s sake).\nTable 14-1. Listening data\nUserId TrackId Scrobble Radio Skip\n111115 222 0 1 0\n111113 225 1 0 0\n111117 223 0 1 1\n111115 225 1 0 0\nThese text files are the initial input provided to the Track Statistics program, which\nconsists of two jobs that calculate various values from this data and a third job that\nmerges the results (see Figure 14-2).\nThe Unique Listeners job calculates the total number of unique listeners for a track by\ncounting the first listen by a user and ignoring all other listens by the same user. The\nSum job accumulates the total listens, scrobbles, radio listens, and skips for each track\nby counting these values for all listens by all users. Although the input format of these\nHadoop Usage at Last.fm | 407Figure 14-2. TrackStats jobs\ntwo jobs is identical, two separate jobs are needed, as the Unique Listeners job is re-\nsponsible for emitting values per track per user, and the Sum job emits values per track.\nThe final “Merge” job is responsible for merging the intermediate output of the two\nother jobs into the final result. The end results of running the program are the following\nvalues per track:\n•\n•\n•\n•\n•\nNumber of unique listeners\nNumber of times the track was scrobbled\nNumber of times the track was listened to on the radio\nNumber of times the track was listened to in total\nNumber of times the track was skipped on the radio\nEach job and its MapReduce phases are described in more detail next. Please note that\nthe provided code snippets have been simplified due to space constraints; for download\ndetails for the full code listings, refer to the preface.\nCalculating the number of unique listeners\nThe Unique Listeners job calculates, per track, the number of unique listeners.\nUniqueListenerMapper. The UniqueListenersMapper processes the space-delimited raw lis-\ntening data and emits the user ID associated with each track ID:\npublic void map(LongWritable position, Text rawLine, OutputCollector<IntWritable,\nIntWritable> output, Reporter reporter) throws IOException {\nString[] parts = (rawLine.toString()).split("" "");\nint scrobbles = Integer.parseInt(parts[TrackStatisticsProgram.COL_SCROBBLES]);\nint radioListens = Integer.parseInt(parts[TrackStatisticsProgram.COL_RADIO]);\n408 | Chapter 14: Case Studies}\n// if track somehow is marked with zero plays - ignore\nif (scrobbles <= 0 && radioListens <= 0) {\nreturn;\n}\n// if we get to here then user has listened to track,\n// so output user id against track id\nIntWritable trackId = new IntWritable(\nInteger.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]));\nIntWritable userId = new IntWritable(\nInteger.parseInt(parts[TrackStatisticsProgram.COL_USERID]));\noutput.collect(trackId, userId);\nUniqueListenersReducer. The UniqueListenersReducers receives a list of user IDs per track\nID, and puts these IDs into a Set to remove any duplicates. The size of this set is then\nemitted (i.e., the number of unique listeners) for each track ID. Storing all the reduce\nvalues in a Set runs the risk of running out of memory if there are many values for a\ncertain key. This hasn’t happened in practice, but to overcome this, an extra\nMapReduce step could be introduced to remove all the duplicate values or a secondary\nsort could be used. (For more details, see “Secondary Sort” on page 227.)\npublic void reduce(IntWritable trackId, Iterator<IntWritable> values,\nOutputCollector<IntWritable, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\nSet<Integer> userIds = new HashSet<Integer>();\n// add all userIds to the set, duplicates automatically removed (set contract)\nwhile (values.hasNext()) {\nIntWritable userId = values.next();\nuserIds.add(Integer.valueOf(userId.get()));\n}\n// output trackId -> number of unique listeners per track\noutput.collect(trackId, new IntWritable(userIds.size()));\nTable 14-2 shows the sample input data for the job. The map output appears in Ta-\nble 14-3 and the reduce output in Table 14-4.\nTable 14-2. Job input\nLine of file UserId TrackId\nLongWritable IntWritable 0 \nScrobbled\nRadio play Skip\nIntWritableBoolean Boolean Boolean\n11115 222 0 1 0\n1 11113 225 1 0 0\n2 11117 223 0 1 1\n3 11115 225 1 0 0\nHadoop Usage at Last.fm | 409Table 14-3. Mapper output\nTrackId UserId\nIntWritable IntWritable\n222 11115\n225 11113\n223 11117\n225 11115\nTable 14-4. Reducer output\nTrackId #listeners\nIntWritable IntWritable\n222 1\n225 2\n223 1\nSumming the track totals\nThe Sum job is relatively simple; it just adds up the values we are interested in for each\ntrack.\nSumMapper. The input data is again the raw text files, but in this case, it is handled quite\ndifferently. The desired end result is a number of totals (unique listener count, play\ncount, scrobble count, radio listen count, skip count) associated with each track. To\nsimplify things, we use an intermediate TrackStats object generated using Hadoop\nRecord I/O, which implements WritableComparable (so it can be used as output) to hold\nthese values. The mapper creates a TrackStats object and sets the values on it for each\nline in the file, except for the unique listener count, which is left empty (it will be filled\nin by the final merge job):\npublic void map(LongWritable position, Text rawLine,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\n}\nString[] parts = (rawLine.toString()).split("" "");\nint trackId = Integer.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]);\nint scrobbles = Integer.parseInt(parts[TrackStatisticsProgram.COL_SCROBBLES]);\nint radio = Integer.parseInt(parts[TrackStatisticsProgram.COL_RADIO]);\nint skip = Integer.parseInt(parts[TrackStatisticsProgram.COL_SKIP]);\n// set number of listeners to 0 (this is calculated later)\n// and other values as provided in text file\nTrackStats trackstat = new TrackStats(0, scrobbles + radio, scrobbles, radio, skip);\noutput.collect(new IntWritable(trackId), trackstat);\nSumReducer. In this case, the reducer performs a very similar function to the mapper—\nit sums the statistics per track and returns an overall total:\n410 | Chapter 14: Case Studiespublic void reduce(IntWritable trackId, Iterator<TrackStats> values,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\n}\nTrackStats sum = new TrackStats(); // holds the totals for this track\nwhile (values.hasNext()) {\nTrackStats trackStats = (TrackStats) values.next();\nsum.setListeners(sum.getListeners() + trackStats.getListeners());\nsum.setPlays(sum.getPlays() + trackStats.getPlays());\nsum.setSkips(sum.getSkips() + trackStats.getSkips());\nsum.setScrobbles(sum.getScrobbles() + trackStats.getScrobbles());\nsum.setRadioPlays(sum.getRadioPlays() + trackStats.getRadioPlays());\n}\noutput.collect(trackId, sum);\nTable 14-5 shows the input data for the job (the same as for the Unique Listeners job).\nThe map output appears in Table 14-6 and the reduce output in Table 14-7.\nTable 14-5. Job input\nLine UserId TrackId Scrobbled Radio play Skip\nLongWritable IntWritable IntWritable Boolean Boolean Boolean\n0 11115 222 0 1 0\n1 11113 225 1 0 0\n2 11117 223 0 1 1\n3 11115 225 1 0 0\nTable 14-6. Map output\nTrackId #listeners #plays #scrobbles #radio plays #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 1 1 0 0\n223 0 1 0 1 1\n225 0 1 1 0 0\nTable 14-7. Reduce output\nTrackId #listeners #plays #scrobbles #radio plays #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 2 2 0 0\n223 0 1 0 1 1\nHadoop Usage at Last.fm | 411Merging the results\nThe final job needs to merge the output from the two previous jobs: the number of\nunique listeners per track and the statistics per track. In order to be able to merge these\ndifferent inputs, two different mappers (one for each type of input) are used. The two\nintermediate jobs are configured to write their results to different paths, and the\nMultipleInputs class is used to specify which mapper will process which files. The\nfollowing code shows how the JobConf for the job is set up to do this:\nMultipleInputs.addInputPath(conf, sumInputDir,\nSequenceFileInputFormat.class, IdentityMapper.class);\nMultipleInputs.addInputPath(conf, listenersInputDir,\nSequenceFileInputFormat.class, MergeListenersMapper.class);\nIt is possible to use a single mapper to handle different inputs, but the example solution\nis more convenient and elegant.\nMergeListenersMapper. This mapper is used to process the UniqueListenerJob’s output of\nunique listeners per track. It creates a TrackStats object in a similar manner to the\nSumMapper, but this time, it fills in only the unique listener count per track and leaves\nthe other values empty:\npublic void map(IntWritable trackId, IntWritable uniqueListenerCount,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\nTrackStats trackStats = new TrackStats();\ntrackStats.setListeners(uniqueListenerCount.get());\noutput.collect(trackId, trackStats);\n}\nTable 14-8 shows some input for the mapper; the corresponding output is shown in\nTable 14-9.\nTable 14-8. MergeListenersMapper input\nTrackId #listeners\nIntWritable IntWritable\n222 1\n225 2\n223 1\nTable 14-9. MergeListenersMapper output\nTrackId #listeners #plays #scrobbles #radio #skips\n222 1 0 0 0 0\n225 2 0 0 0 0\n223 2 0 0 0 0\n412 | Chapter 14: Case StudiesIdentityMapper. The IdentityMapper is configured to process the SumJob’s output of\nTrackStats objects and, as no additional processing is required, it directly emits the\ninput data (see Table 14-10).\nTable 14-10. IdentityMapper input and output\nTrackId #listeners #plays #scrobbles #radio #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 2 2 0 0\n223 0 1 0 1 1\nSumReducer. The two mappers above emit values of the same type: a TrackStats object\nper track, with different values filled in. The final reduce phase can reuse the\nSumReducer described earlier to create a TrackStats object per track, sum up all the\nvalues, and emit it (see Table 14-11).\nTable 14-11. Final SumReducer output\nTrackId #listeners #plays #scrobbles #radio #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 1 1 0 1 0\n225 2 2 2 0 0\n223 1 1 0 1 1\nThe final output files are then accumulated and copied to a server where a web service\nmakes the data available to the Last.fm website for display. An example of this is shown\nin Figure 14-3, where the total number of listeners and plays are displayed for a track.\nFigure 14-3. TrackStats result\nHadoop Usage at Last.fm | 413Summary\nHadoop has become an essential part of Last.fm’s infrastructure and is used to generate\nand process a wide variety of datasets ranging from web logs to user listening data. The\nexample covered here has been simplified considerably in order to get the key concepts\nacross; in real-world usage the input data has a more complicated structure and the\ncode that processes it is more complex. Hadoop itself, while mature enough for pro-\nduction use, is still in active development and new features and improvements are\nadded by the Hadoop community every week. We at Last.fm are happy to be part of\nthis community as a contributor of code and ideas, and as end users of a great piece of\nopen source technology.\n—Adrian Woodhead and Marc de Palol\nHadoop and Hive at Facebook\nIntroduction\nHadoop can be used to form core backend batch and near real-time computing infra-\nstructures. It can also be used to store and archive massive datasets. In this case study,\nwe will explore backend data architectures and the role Hadoop can play in them. We\nwill describe hypothetical Hadoop configurations, potential uses of Hive—an open\nsource data warehousing and SQL infrastructure built on top of Hadoop—and the\ndifferent kinds of business and product applications that have been built using this\ninfrastructure.\nHadoop at Facebook\nHistory\nThe amount of log and dimension data in Facebook that needs to be processed and\nstored has exploded as the usage of the site has increased. A key requirement for any\ndata processing platform for this environment is the ability to be able to scale rapidly\nin tandem. Further, engineering resources being limited, the system should be very\nreliable and easy to use and maintain.\nInitially, data warehousing at Facebook was performed entirely on an Oracle instance.\nAfter we started hitting scalability and performance problems, we investigated whether\nthere were open source technologies that could be used in our environment. As part of\nthis investigation, we deployed a relatively small Hadoop instance and started pub-\nlishing some of our core datasets into this instance. Hadoop was attractive because\nYahoo! was using it internally for its batch processing needs and also because we were\nfamiliar with the simplicity and scalability of the MapReduce model as popularized by\nGoogle.\n414 | Chapter 14: Case StudiesOur initial prototype was very successful: the engineers loved the ability to process\nmassive amounts of data in reasonable timeframes, an ability that we just did not have\nbefore. They also loved being able to use their favorite programming language for pro-\ncessing (using Hadoop streaming). Having our core datasets published in one\ncentralized data store was also very convenient. At around the same time, we started\ndeveloping Hive. This made it even easier for users to process data in the Hadoop cluster\nby being able to express common computations in the form of SQL, a language with\nwhich most engineers and analysts are familiar.\nAs a result, the cluster size and usage grew leaps and bounds, and today Facebook is\nrunning the second largest Hadoop cluster in the world. As of this writing, we hold\nmore than 2 PB of data in Hadoop and load more than 10 TB of data into it every day.\nOur Hadoop instance has 2,400 cores and about 9 TB of memory and runs at 100%\nutilization at many points during the day. We are able to scale out this cluster rapidly\nin response to our growth, and we have been able to take advantage of open source by\nmodifying Hadoop where required to suit our needs. We have contributed back to open\nsource, both in the form of contributions to some core components of Hadoop as well\nas by open-sourcing Hive, which is now a Hadoop subproject.\nUse cases\nThere are at least four interrelated but distinct classes of uses for Hadoop at Facebook:\n• Producing daily and hourly summaries over large amounts of data. These summa-\nries are used for a number of different purposes within the company:\n— Reports based on these summaries are used by engineering and nonengineering\nfunctional teams to drive product decisions. These summaries include reports\non growth of the users, page views, and average time spent on the site by the\nusers.\n— Providing performance numbers about advertisement campaigns that are run\non Facebook.\n— Backend processing for site features such as people you may like and applica-\ntions you may like.\n• Running ad hoc jobs over historical data. These analyses help answer questions\nfrom our product groups and executive team.\n• As a de facto long-term archival store for our log datasets.\n• To look up log events by specific attributes (where logs are indexed by such at-\ntributes), which is used to maintain the integrity of the site and protect users against\nspambots.\nData architecture\nFigure 14-4 shows the basic components of our architecture and the data flow within\nthese components.\nHadoop and Hive at Facebook | 415Figure 14-4. Data warehousing architecture at Facebook\nAs shown in Figure 14-4, the following components are used in processing data:\nScribe\nLog data is generated by web servers as well as internal services such as the Search\nbackend. We use Scribe, an open source log collection service developed in Face-\nbook that deposits hundreds of log datasets with daily volume in tens of terabytes\ninto a handful of NFS servers.\nHDFS\nA large fraction of this log data is copied into one central HDFS instance. Dimen-\nsion data is also scraped from our internal MySQL databases and copied over into\nHDFS daily.\nHive/Hadoop\nWe use Hive, a Hadoop subproject developed in Facebook, to build a data ware-\nhouse over all the data collected in HDFS. Files in HDFS, including log data from\nScribe and dimension data from the MySQL tier, are made available as tables with\nlogical partitions. A SQL-like query language provided by Hive is used in conjunc-\ntion with MapReduce to create/publish a variety of summaries and reports, as well\nas to perform historical analysis over these tables.\nTools\nBrowser-based interfaces built on top of Hive allow users to compose and launch\nHive queries (which in turn launch MapReduce jobs) using just a few mouse clicks.\n416 | Chapter 14: Case StudiesTraditional RDBMS\nWe use Oracle and MySQL databases to publish these summaries. The volume of\ndata here is relatively small, but the query rate is high and needs real-time response.\nDataBee\nAn in-house ETL workflow software that is used to provide a common framework\nfor reliable batch processing across all data processing jobs.\nData from the NFS tier storing Scribe data is continuously replicated to the HDFS\ncluster by copier jobs. The NFS devices are mounted on the Hadoop tier and the copier\nprocesses run as map-only jobs on the Hadoop cluster. This makes it easy to scale the\ncopier processes and also makes them fault-resilient. Currently we copy over 6 TB per\nday from Scribe to HDFS in this manner. We also download up to 4 TB of dimension\ndata from our MySQL tier to HDFS every day. These are also conveniently arranged\non the Hadoop cluster, as map-only jobs that copy data out of MySQL boxes.\nHadoop configuration\nThe central philosophy behind our Hadoop deployment is consolidation. We use a\nsingle HDFS instance, and a vast majority of processing is done in a single MapReduce\ncluster (running a single jobtracker). The reasons for this are fairly straightforward:\n• We can minimize the administrative overheads by operating a single cluster.\n• Data does not need to be duplicated. All data is available in a single place for all\nthe use cases described previously.\n• By using the same compute cluster across all departments, we get tremendous\nefficiencies.\n• Our users work in a collaborative environment, so requirements in terms of quality\nof service are not onerous (yet).\nWe also have a single shared Hive metastore (using a MySQL database) that holds\nmetadata about all the Hive tables stored in HDFS.\nHypothetical Use Case Studies\nIn this section, we will describe some typical problems that are common for large web-\nsites, which are difficult to solve through traditional warehousing technologies, simply\nbecause the costs and scales involved are prohibitively high. Hadoop and Hive can\nprovide a more scalable and more cost-effective solution in such situations.\nAdvertiser insights and performance\nOne of the most common uses of Hadoop is to produce summaries from large volumes\nof data. It is very typical of large ad networks such as Facebook ad network, Google\nAdSense, and many others to provide advertisers with standard aggregated statistics\nabout their ads that help the advertisers to tune their campaigns effectively. Computing\nHadoop and Hive at Facebook | 417advertisement performance numbers on large datasets is a very data-intensive opera-\ntion, and the scalability and cost advantages of Hadoop and Hive can really help in\ncomputing these numbers in a reasonable time frame and at a reasonable cost.\nMany ad networks provide standardized CPC- and CPM-based ad-units to the adver-\ntisers. The CPC ads are cost-per-click ads: the advertiser pays the ad network amounts\nthat are dependent on the number of clicks that the particular ad gets from the users\nvisiting the site. The CPM ads, on the other hand, bill the advertisers amounts that are\nproportional to the number of users that see the ad on the site. Apart from these stand-\nardized ad units, in the last few years ads that have more dynamic content that is tailored\nto each individual user have also become common in the online advertisement industry.\nYahoo! does this through SmartAds, whereas Facebook provides its advertisers with\nSocial Ads. The latter allows the advertisers to embed information from a user’s net-\nwork of friends; for example, a Nike ad may refer to a friend of the user who recently\nfanned Nike and shared that information with his friends on Facebook. In addition,\nFacebook also provides Engagement Ad units to the advertisers, wherein the users can\nmore effectively interact with the ad, be it by commenting on it or by playing embedded\nvideos. In general, there is a wide variety of ads that are provided to the advertisers by\nthe online ad networks, and this variety also adds yet another dimension to the various\nkinds of performance numbers that the advertisers are interested in getting about their\ncampaigns.\nAt the most basic level, advertisers are interested in knowing the total and the number\nof unique users that have seen the ad or have clicked on it. For more dynamic ads, they\nmay even be interested in getting the breakdown of these aggregated numbers by the\nkind of dynamic information shown in the ad unit or the kind of engagement action\nundertaken by the users on the ad. For example, a particular advertisement may have\nbeen shown 100,000 times to 30,000 unique users. Similarly a video embedded inside\nan Engagement Ad may have been watched by 100,000 unique users. In addition, these\nperformance numbers are typically reported for each ad, campaign, and account. An\naccount may have multiple campaigns with each campaign running multiple ads on\nthe network. Finally, these numbers are typically reported for different time durations\nby the ad networks. Typical durations are daily, rolling week, month to date, rolling\nmonth, and sometimes even for the entire lifetime of the campaign. Moreover, adver-\ntisers also look at the geographic breakdown of these numbers among other ways of\nslicing and dicing this data, such as what percentage of the total viewers or clickers of\na particular ad are in the Asia Pacific region.\nAs is evident, there are four predominant dimension hierarchies: the account, cam-\npaign, and ad dimension; the time period; the type of interaction; and the user dimen-\nsion. The last of these is used to report unique numbers, whereas the other three are\nthe reporting dimensions. The user dimension is also used to create aggregated geo-\ngraphic profiles for the viewers and clickers of ads. All this information in totality allows\nthe advertisers to tune their campaigns to improve their effectiveness on any given ad\nnetwork. Aside from the multidimensional nature of this set of pipelines, the volumes\n418 | Chapter 14: Case Studiesof data processed and the rate at which this data is growing on a daily basis make this\ndifficult to scale without a technology like Hadoop for large ad networks. As of this\nwriting, for example, the ad log volume that is processed for ad performance numbers\nat Facebook is approximately 1 TB per day of (uncompressed) logs. This volume has\nseen a 30-fold increase since January 2008, when the volumes were in the range of 30\nGB per day. Hadoop’s ability to scale with hardware has been a major factor behind\nthe ability of these pipelines to keep up with this data growth with minor tweaking of\njob configurations. Typically, these configuration changes involve increasing the num-\nber of reducers for the Hadoop jobs that are processing the intensive portions of these\npipelines. The largest of these stages currently run with 400 reducers (an increase of\neight times from the 50 reducers that were being used in January 2008).\nAd hoc analysis and product feedback\nApart from regular reports, another primary use case for a data warehousing solution\nis to be able to support ad hoc analysis and product feedback solutions. Any typical\nwebsite, for example, makes product changes, and it is typical for product managers\nor engineers to understand the impact of a new feature, based on user engagement as\nwell as on the click-through rate on that feature. The product team may even wish to\ndo a deeper analysis on what is the impact of the change based on various regions and\ncountries, such as whether this change increases the click-through rate of the users in\nU.S. or whether it reduces the engagement of users in India. A lot of this type of analysis\ncould be done with Hadoop by using Hive and regular SQL. The measurement of click-\nthrough rate can be easily expressed as a join of the impressions and clicks for the\nparticular link related to the feature. This information can be joined with geographic\ninformation to compute the effect of product changes on different regions. Subse-\nquently one can compute average click-through rate for different geographic regions\nby performing aggregations over them. All of these are easily expressible in Hive using\na couple of SQL queries (that would in turn generate multiple Hadoop jobs). If only an\nestimate were required, the same queries can be run for a sample set of the users using\nsampling functionality natively supported by Hive. Some of this analysis needs the use\nof custom map and reduce scripts in conjunction with the Hive SQL and that is also\neasy to plug into a Hive query.\nA good example of a more complex analysis is estimating the peak number of users\nlogging into the site per minute for the entire past year. This would involve sampling\npage view logs (because the total page view data for a popular website is huge), grouping\nit by time and then finding the number of new users at different time points via a custom\nreduce script. This is a good example where both SQL and MapReduce are required\nfor solving the end user problem and something that is possible to achieve easily with\nHive.\nHadoop and Hive at Facebook | 419Data analysis\nHive and Hadoop can be easily used for training and scoring for data analysis applica-\ntions. These data analysis applications can span multiple domains such as popular\nwebsites, bioinformatics companies, and oil exploration companies. A typical example\nof such an application in the online ad network industry would be the prediction of\nwhat features of an ad makes it more likely to be noticed by the user. The training phase\ntypically would involve identifying the response metric and the predictive features. In\nthis case, a good metric to measure the effectiveness of an ad could be its click-through\nrate. Some interesting features of the ad could be the industry vertical that it belongs\nto, the content of the ad, the placement of the ad on the page, and so on. Hive is easily\nuseful for assembling training data and then feeding the same into a data analysis engine\n(typically R or user programs written in MapReduce). In this particular case, different\nad performance numbers and features can be structured as tables in Hive. One can\neasily sample this data (sampling is required as R can only handle limited data volume)\nand perform the appropriate aggregations and joins using Hive queries to assemble a\nresponse table that contains the most important ad features that determine the effec-\ntiveness of an advertisement. However, since sampling loses information, some of the\nmore important data analysis applications use parallel implementations of popular data\nanalysis kernels using MapReduce framework.\nOnce the model has been trained, it may be deployed for scoring on a daily basis. The\nbulk of the data analysis tasks do not perform daily scoring though. Many of them are\nad hoc in nature and require one-time analysis that can be used as input into product\ndesign process.\nHive\nOverview\nWhen we started using Hadoop, we very quickly became impressed by its scalability\nand availability. However, we were worried about widespread adoption, primarily be-\ncause of the complexity involved in writing MapReduce programs in Java (as well as\nthe cost of training users to write them). We were aware that a lot of engineers and\nanalysts in the company understood SQL as a tool to query and analyze data and that\na lot of them were proficient in a number of scripting languages like PHP and Python.\nAs a result, it was imperative for us to develop software that could bridge this gap\nbetween the languages that the users were proficient in and the languages required to\nprogram Hadoop.\nIt was also evident that a lot of our datasets were structured and could be easily parti-\ntioned. The natural consequence of these requirements was a system that could model\ndata as tables and partitions and that could also provide a SQL-like language for query\nand analysis. Also essential was the ability to plug in customized MapReduce programs\nwritten in the programming language of the user’s choice into the query. This system\n420 | Chapter 14: Case Studieswas called Hive. Hive is a data warehouse infrastructure built on top of Hadoop and\nserves as the predominant tool that is used to query the data stored in Hadoop at\nFacebook. In the following sections, we describe this system in more detail.\nData organization\nData is organized consistently across all datasets and is stored compressed, partitioned,\nand sorted:\nCompression\nAlmost all datasets are stored as sequence files using gzip codec. Older datasets are\nrecompressed to use the bzip codec that gives substantially more compression than\ngzip. Bzip is slower than gzip, but older data is accessed much less frequently and\nthis performance hit is well worth the savings in terms of disk space.\nPartitioning\nMost datasets are partitioned by date. Individual partitions are loaded into Hive,\nwhich loads each partition into a separate HDFS directory. In most cases, this\npartitioning is based simply on datestamps associated with scribe logfiles. How-\never, in some cases, we scan data and collate them based on timestamp available\ninside a log entry. Going forward, we are also going to be partitioning data on\nmultiple attributes (for example, country and date).\nSorting\nEach partition within a table is often sorted (and hash-partitioned) by unique ID\n(if one is present). This has a few key advantages:\n• It is easy to run sampled queries on such datasets.\n• We can build indexes on sorted data.\n• Aggregates and joins involving unique IDs can be done very efficiently on such\ndatasets.\nLoading data into this long-term format is done by daily MapReduce jobs (and is dis-\ntinct from the near real-time data import processes).\nQuery language\nThe Hive Query language is very SQL-like. It has traditional SQL constructs like joins,\ngroup bys, where, select, from clauses and from clause subqueries. It tries to convert\nSQL commands into a set of MapReduce jobs. Apart from the normal SQL clauses, it\nhas a bunch of other extensions, like the ability to specify custom mapper and reducer\nscripts in the query itself, the ability to insert into multiple tables, partitions, HDFS or\nlocal files while doing a single scan of the data and the ability to run the query on data\nsamples rather than the full dataset (this ability is fairly useful while testing queries).\nThe Hive metastore stores the metadata for a table and provides this metadata to the\nHive compiler for converting SQL commands to MapReduce jobs. Through partition\nHadoop and Hive at Facebook | 421pruning, map-side aggregations, and other features, the compiler tries to create plans\nthat can optimize the runtime for the query.\nData pipelines using Hive\nAdditionally, the ability provided by Hive in terms of expressing data pipelines in SQL\ncan and has provided the much needed flexibility in putting these pipelines together in\nan easy and expedient manner. This is especially useful for organizations and products\nthat are still evolving and growing. Many of the operations needed in processing data\npipelines are the well-understood SQL operations like join, group by, and distinct ag-\ngregations. With Hive’s ability to convert SQL into a series of Hadoop MapReduce\njobs, it becomes fairly easy to create and maintain these pipelines. We illustrate these\nfacets of Hive in this section by using an example of a hypothetical ad network and\nshowing how some typical aggregated reports needed by the advertisers can be com-\nputed using Hive. As an example, assuming that an online ad network stores informa-\ntion on ads in a table named dim_ads and stores all the impressions served to that ad in\na table named impression_logs in Hive, with the latter table being partitioned by date,\nthe daily impression numbers (both unique and total by campaign, that are routinely\ngiven by ad networks to the advertisers) for 2008-12-01 are expressible as the following\nSQL in Hive:\nSELECT a.campaign_id, count(1), count(DISTINCT b.user_id)\nFROM dim_ads a JOIN impression_logs b ON(b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01'\nGROUP BY a.campaign_id;\nThis would also be the typical SQL statement that one could use in other RDBMSs such\nas Oracle, DB2, and so on.\nIn order to compute the daily impression numbers by ad and account from the same\njoined data as earlier, Hive provides the ability to do multiple group bys simultaneously\nas shown in the following query (SQL-like but not strictly SQL):\nFROM(\nSELECT a.ad_id, a.campaign_id, a.account_id, b.user_id\nFROM dim_ads a JOIN impression_logs b ON (b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01') x\nINSERT OVERWRITE DIRECTORY 'results_gby_adid'\nSELECT x.ad_id, count(1), count(DISTINCT x.user_id) GROUP BY x.ad_id\nINSERT OVERWRITE DIRECTORY 'results_gby_campaignid'\nSELECT x.campaign_id, count(1), count(DISTINCT x.user_id) GROUP BY x.campaign_id\nINSERT OVERWRITE DIRECTORY 'results_gby_accountid'\nSELECT x.account_id, count(1), count(DISTINCT x.user_id) GROUP BY x.account_id;\nIn one of the optimizations that is being added to Hive, the query can be converted into\na sequence of Hadoop MapReduce jobs that are able to scale with data skew. Essen-\ntially, the join is converted into one MapReduce job and the three group bys are con-\nverted into four MapReduce jobs with the first one generating a partial aggregate on\nunique_id. This is especially useful because the distribution of impression_logs over\nunique_id is much more uniform as compared to ad_id (typically in an ad network, a\n422 | Chapter 14: Case Studiesfew ads dominate in that they are shown more uniformly to the users). As a result,\ncomputing the partial aggregation by unique_id allows the pipeline to distribute the\nwork more uniformly to the reducers. The same template can be used to compute\nperformance numbers for different time periods by simply changing the date predicate\nin the query.\nComputing the lifetime numbers can be more tricky though, as using the strategy de-\nscribed previously, one would have to scan all the partitions of the impression_logs\ntable. Therefore, in order to compute the lifetime numbers, a more viable strategy is to\nstore the lifetime counts on a per ad_id, unique_id grouping every day in a partition of\nan intermediate table. The data in this table combined with the next days\nimpression_logs can be used to incrementally generate the lifetime ad performance\nnumbers. As an example, in order to get the impression numbers for 2008-12-01, the\nintermediate table partition for 2008-11-30 is used. The Hive queries that can be used\nto achieve this are as follows:\nINSERT OVERWRITE lifetime_partial_imps PARTITION(dateid='2008-12-01')\nSELECT x.ad_id, x.user_id, sum(x.cnt)\nFROM (\nSELECT a.ad_id, a.user_id, a.cnt\nFROM lifetime_partial_imps a\nWHERE a.dateid = '2008-11-30'\nUNION ALL\nSELECT b.ad_id, b.user_id, 1 as cnt\nFROM impression_log b\nWHERE b.dateid = '2008-12-01'\n) x\nGROUP BY x.ad_id, x.user_id;\nThis query computes the partial sums for 2008-12-01, which can be used for computing\nthe 2008-12-01 numbers as well as the 2008-12-02 numbers (not shown here). The\nSQL is converted to a single Hadoop MapReduce job that essentially computes the\ngroup by on the combined stream of inputs. This SQL can be followed by the following\nHive query, which computes the actual numbers for different groupings (similar to the\none in the daily pipelines):\nFROM(\nSELECT a.ad_id, a.campaign_id, a.account_id, b.user_id, b.cnt\nFROM dim_ads a JOIN lifetime_partial_imps b ON (b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01') x\nINSERT OVERWRITE DIRECTORY 'results_gby_adid'\nSELECT x.ad_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.ad_id\nINSERT OVERWRITE DIRECTORY 'results_gby_campaignid'\nSELECT x.campaign_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.campaign_id\nINSERT OVERWRITE DIRECTORY 'results_gby_accountid'\nSELECT x.account_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.account_id;\nHive and Hadoop are batch processing systems that cannot serve the computed data\nwith the same latency as a usual RDBMS such as Oracle or MySQL. Therefore, on many\noccasions, it is still useful to load the summaries generated through Hive and Hadoop\nHadoop and Hive at Facebook | 423to a more traditional RDBMS for serving this data to users through different BI tools\nor even though a web portal.\nProblems and Future Work\nFair sharing\nHadoop clusters typically run a mix of production daily jobs that need to finish com-\nputation within a reasonable time frame as well as ad hoc jobs that may be of different\npriorities and sizes. In typical installations, these jobs tend to run overnight, when\ninterference from ad hoc jobs run by users is minimal. However, overlap between large\nad hoc and production jobs is often unavoidable and, without adequate safeguards,\ncan impact the latency of production jobs. ETL processing also contains several near\nreal-time jobs that must be performed at hourly intervals (these include processes to\ncopy Scribe data from NFS servers as well as hourly summaries computed over some\ndatasets). It also means that a single rogue job can bring down the entire cluster and\nput production processes at risk.\nThe fair-sharing Hadoop jobscheduler, developed at Facebook and contributed back\nto Hadoop, provides a solution to many of these issues. It reserves guaranteed compute\nresources for specific pools of jobs while at the same time letting idle resources be used\nby everyone. It also prevents large jobs from hogging cluster resources by allocating\ncompute resources in a fair manner across these pools. Memory can become one of the\nmost contended resources in the cluster. We have made some changes to Hadoop so\nthat if the JobTracker is low on memory, Hadoop job submissions are throttled. This\ncan allow the user processes to run with reasonable per-process memory limits, and it\nis possible to put in place some monitoring scripts in order to prevent MapReduce jobs\nfrom impacting HDFS daemons (due primarily to high memory consumption) running\non the same node. Log directories are stored in separate disk partitions and cleaned\nregularly, and we think it can also be useful to put MapReduce intermediate storage in\nseparate disk partitions as well.\nSpace management\nCapacity management continues to be a big challenge—utilization is increasing at a\nfast rate with growth of data. Many growing companies with growing datasets have the\nsame pain. In many situations, much of this data is temporary in nature. In such cases,\none can use retention settings in Hive and also recompress older data in bzip format to\nsave on space. Although configurations are largely symmetrical from a disk storage\npoint of view, adding a separate tier of high-storage-density machines to hold older\ndata may prove beneficial. This will make it cheaper to store archival data in Hadoop.\nHowever, access to such data should be transparent. We are currently working on a\ndata archival layer to make this possible and to unify all the aspects of dealing with\nolder data.\n424 | Chapter 14: Case StudiesScribe-HDFS integration\nCurrently, Scribe writes to a handful of NFS filers from where data is picked up and\ndelivered to HDFS by custom copier jobs as described earlier. We are working on\nmaking Scribe write directly to another HDFS instance. This will make it very easy to\nscale and administer Scribe. Due to the high uptime requirements for Scribe, its target\nHDFS instance is likely to be different from the production HDFS instance (so that it\nis isolated from any load/downtime issues due to user jobs).\nImprovements to Hive\nHive is still under active development. A number of key features are being worked on\nsuch as order by and having clause support, more aggregate functions, more built in\nfunctions, datetime, data type, and so on. At the same time, a number of performance\noptimizations are being worked on, such as predicate pushdown and common subex-\npression elimination. On the integration side, JDBC and ODBC drivers are being de-\nveloped in order to integrate with OLAP and BI tools. With all these optimizations, we\nhope that we can unlock the power of MapReduce and Hadoop and bring it closer to\nnonengineering communities as well within Facebook. For more information on this\nproject, please visit http://hadoop.apache.org/hive/.\n—Joydeep Sen Sarma and Ashish Thusoo\nNutch Search Engine\nBackground\nNutch is a framework for building scalable Internet crawlers and search engines. It’s\nan Apache Software Foundation project, and a subproject of Lucene, and it’s available\nunder the Apache 2.0 license.\nWe won’t go deeply into the anatomy of a web crawler as such—the purpose of this\ncase study is to show how Hadoop can be used to implement various complex pro-\ncessing tasks typical for a search engine. Interested readers can find plenty of Nutch-\nspecific information on the official site of the project (http://lucene.apache.org/nutch).\nSuffice to say that in order to create and maintain a search engine, one needs the fol-\nlowing subsystems:\nDatabase of pages\nThis database keeps track of all pages known to the crawler and their status, such\nas the last time it visited the page, its fetching status, refresh interval, content\nchecksum, etc. In Nutch terminology, this database is called CrawlDb.\nNutch Search Engine | 425List of pages to fetch\nAs crawlers periodically refresh their view of the Web, they download new pages\n(previously unseen) or refresh pages that they think already expired. Nutch calls\nsuch a list of candidate pages prepared for fetching a fetchlist.\nRaw page data\nPage content is downloaded from remote sites, and stored locally in the original\nuninterpreted format, as a byte array. This data is called the page content in Nutch.\nParsed page data\nPage content is then parsed using a suitable parser—Nutch provides parsers for\ndocuments in many popular formats, such as HTML, PDF, Open Office and Mi-\ncrosoft Office, RSS, and others.\nLink graph database\nThis database is necessary to compute link-based page ranking scores, such as\nPageRank. For each URL known to Nutch, it contains a list of other URLs pointing\nto it, and their associated anchor text (from HTML <a href="".."">anchor\ntext</a> elements). This database is called LinkDb.\nFull-text search index\nThis is a classical inverted index, built from the collected page metadata and from\nthe extracted plain-text content. It is implemented using the excellent Lucene li-\nbrary (http://lucene.apache.org/java).\nWe briefly mentioned before that Hadoop began its life as a component in Nutch,\nintended to improve its scalability and to address clear performance bottlenecks caused\nby a centralized data processing model. Nutch was also the first public proof-of-concept\napplication ported to the framework that would later become Hadoop, and the effort\nrequired to port Nutch algorithms and data structures to Hadoop proved to be sur-\nprisingly small. This probably encouraged the following development of Hadoop as a\nseparate subproject with the aim of providing a reusable framework for applications\nother than Nutch.\nCurrently nearly all Nutch tools process data by running one or more MapReduce jobs.\nData Structures\nThere are several major data structures maintained in Nutch, and they all make use of\nHadoop I/O classes and formats. Depending on the purpose of the data, and the way\nit’s accessed once it’s created, the data is kept either using Hadoop map files or\nsequence files.\nSince the data is produced and processed by MapReduce jobs, which in turn run several\nmap and reduce tasks, its on-disk layout corresponds to the common Hadoop output\nformats, that is, MapFileOutputFormat and SequenceFileOutputFormat. So to be precise,\nwe should say that data is kept in several partial map files or sequence files, with as\n426 | Chapter 14: Case Studiesmany parts as there were reduce tasks in the job that created the data. For simplicity,\nwe omit this distinction in the following sections.\nCrawlDb\nCrawlDb stores the current status of each URL, as a map file of <url, CrawlDatum>,\nwhere keys use Text and values use a Nutch-specific CrawlDatum class (which imple-\nments the Writable interface).\nIn order to provide a quick random access to the records (sometimes useful for diag-\nnostic reasons, when users want to examine individual records in CrawlDb), this data\nis stored in map files and not in sequence files.\nCrawlDb is initially created using the Injector tool, which simply converts a plain-text\nrepresentation of the initial list of URLs (called the seed list) to a map file in the format\ndescribed earlier. Subsequently it is updated with the information from the fetched and\nparsed pages—more on that shortly.\nLinkDb\nThis database stores the incoming link information for every URL known to Nutch. It\nis a map file of <url, Inlinks>, where Inlinks is a list of URL and anchor text data. It’s\nworth noting that this information is not immediately available during page collection,\nbut the reverse information is available, namely that of outgoing links from a page. The\nprocess of inverting this relationship is implemented as a MapReduce job, described\nshortly.\nSegments\nSegments in Nutch parlance correspond to fetching and parsing a batch of URLs.\nFigure 14-5 presents how segments are created and processed.\nA segment (which is really a directory in a filesystem) contains the following parts\n(which are simply subdirectories containing MapFileOutputFormat or SequenceFileOut\nputFormat data):\ncontent\nContains the raw data of downloaded pages, as a map file of <url, Content>. Nutch\nuses a map file here, because it needs fast random access in order to present a cached\nview of a page.\ncrawl_generate\nContains the list of URLs to be fetched, together with their current status retrieved\nfrom CrawlDb, as a sequence file of <url, CrawlDatum>. This data uses sequence\nfile, first because it’s processed sequentially, and second because we couldn’t sat-\nisfy the map file invariants of sorted keys. We need to spread URLs that belong to\nthe same host as far apart as possible to minimize the load per target host, and this\nmeans that records are sorted more or less randomly.\nNutch Search Engine | 427crawl_fetch\nContains status reports from the fetching, that is, whether it was successful, what\nwas the response code, etc. This is stored in a map file of <url, CrawlDatum>.\ncrawl_parse\nThe list of outlinks for each successfully fetched and parsed page is stored here so\nthat Nutch can expand its crawling frontier by learning new URLs.\nparse_data\nMetadata collected during parsing; among others, the list of outgoing links (out-\nlinks) for a page. This information is crucial later on to build an inverted graph (of\nincoming links—inlinks).\nparse_text\nPlain-text version of the page, suitable for indexing in Lucene. These are stored as\na map file of <url, ParseText> so that Nutch can access them quickly when\nbuilding summaries (snippets) to display the list of search results.\nNew segments are created from CrawlDb when the Generator tool is run (1 in Fig-\nure 14-5), and initially contain just a list of URLs to fetch (the crawl_generate subdir-\nectory). As this list is processed in several steps, the segment collects output data from\nthe processing tools in a set of subdirectories.\nFigure 14-5. Segments\nFor example, the content part is populated by a tool called Fetcher, which downloads\nraw data from URLs on the fetchlist (2). This tool also saves the status information in\ncrawl_fetch so that this data can be used later on for updating the status of the page in\nCrawlDb.\nThe remaining parts of the segment are populated by the Parse segment tool (3), which\nreads the content section, selects appropriate content parser based on the declared (or\n428 | Chapter 14: Case Studiesdetected) MIME type, and saves the results of parsing in three parts: crawl_parse,\nparse_data, and parse_text. This data is then used to update the CrawlDb with new\ninformation (4) and to create the LinkDb (5).\nSegments are kept around until all pages present in them are expired. Nutch applies a\nconfigurable maximum time limit, after which a page is forcibly selected for refetching;\nthis helps the operator phase out all segments older than this limit (because he can be\nsure that by that time all pages in this segment would have been refetched).\nSegment data is used to create Lucene indexes ([6]—primarily the parse_text and\nparse_data parts), but it also provides a data storage mechanism for quick retrieval of\nplain text and raw content data. The former is needed so that Nutch can generate\nsnippets (fragments of document text best matching a query); the latter provides the\nability to present a “cached view” of the page. In both cases, this data is accessed directly\nfrom map files in response to requests for snippet generation or for cached content. In\npractice, even for large collections the performance of accessing data directly from map\nfiles is quite sufficient.\nSelected Examples of Hadoop Data Processing in Nutch\nThe following sections present relevant details of some Nutch tools to illustrate how\nthe MapReduce paradigm is applied to a concrete data processing task in Nutch.\nLink inversion\nHTML pages collected during crawling contain HTML links, which may point either\nto the same page (internal links) or to other pages. HTML links are directed from source\npage to target page. See Figure 14-6.\nFigure 14-6. Link inversion\nHowever, most algorithms for calculating a page’s importance (or quality) need the\nopposite information, that is, what pages contain outlinks that point to the current\npage. This information is not readily available when crawling. Also, the indexing proc-\ness benefits from taking into account the anchor text on inlinks so that this text may\nsemantically enrich the text of the current page.\nNutch Search Engine | 429As mentioned earlier, Nutch collects the outlink information and then uses this data\nto build a LinkDb, which contains this reversed link data in the form of inlinks and\nanchor text.\nThis section presents a rough outline of the implementation of the LinkDb tool—many\ndetails have been omitted (such as URL normalization and filtering) in order to present\na clear picture of the process. What’s left gives a classical example of why the\nMapReduce paradigm fits so well with the key data transformation processes required\nto run a search engine. Large search engines need to deal with massive web graph data\n(many pages with a lot of outlinks/inlinks), and the parallelism and fault tolerance\noffered by Hadoop make this possible. Additionally, it’s easy to express the link inver-\nsion using the map-sort-reduce primitives, as illustrated next.\nThe snippet below presents the job initialization of the LinkDb tool:\nJobConf job = new JobConf(configuration);\nFileInputFormat.addInputPath(job, new Path(segmentPath, ""parse_data""));\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(LinkDb.class);\njob.setReducerClass(LinkDb.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(Inlinks.class);\njob.setOutputFormat(MapFileOutputFormat.class);\nFileOutputFormat.setOutputPath(job, newLinkDbPath);\nAs we can see, the source data for this job is the list of fetched URLs (keys) and the\ncorresponding ParseData records that contain among others the outlink information\nfor each page, as an array of outlinks. An outlink contains both the target URL and the\nanchor text.\nThe output from the job is again a list of URLs (keys), but the values are instances of\ninlinks, which is simply a specialized Set of inlinks that contain target URLs and anchor\ntext.\nPerhaps surprisingly, URLs are typically stored and processed as plain text and not as\njava.net.URL or java.net.URI instances. There are several reasons for this: URLs ex-\ntracted from downloaded content usually need normalization (e.g., converting host-\nnames to lowercase, resolving relative paths), are often broken or invalid, or refer to\nunsupported protocols. Many normalization and filtering operations are better ex-\npressed as text patterns that span several parts of a URL. Also, for the purpose of link\nanalysis, we may still want to process and count invalid URLs.\nLet’s take a closer look now at the map() and reduce() implementations—in this case,\nthey are simple enough to be implemented in the body of the same class:\npublic void map(Text fromUrl, ParseData parseData,\nOutputCollector<Text, Inlinks> output, Reporter reporter) {\n...\nOutlink[] outlinks = parseData.getOutlinks();\nInlinks inlinks = new Inlinks();\nfor (Outlink out : outlinks) {\n430 | Chapter 14: Case Studies}\n}\ninlinks.clear(); // instance reuse to avoid excessive GC\nString toUrl = out.getToUrl();\nString anchor = out.getAnchor();\ninlinks.add(new Inlink(fromUrl, anchor));\noutput.collect(new Text(toUrl), inlinks);\nYou can see from this listing that for each Outlink our map() implementation produces\na pair of <toUrl, Inlinks>, where Inlinks contains just a single Inlink containing\nfromUrl and the anchor text. The direction of the link has been inverted.\nSubsequently, these one-element-long Inlinks are aggregated in the reduce() method:\npublic void reduce(Text toUrl, Iterator<Inlinks> values,\nOutputCollector<Text, Inlinks> output, Reporter reporter) {\nInlinks result = new Inlinks();\nwhile (values.hasNext()) {\nresult.add(values.next());\n}\noutput.collect(toUrl, result);\n}\nFrom this code, it’s obvious that we have got exactly what we wanted—that is, a list\nof all fromUrls that point to our toUrl, together with their anchor text. The inversion\nprocess has been accomplished.\nThis data is then saved using the MapFileOutputFormat and becomes the new version of\nLinkDb.\nGeneration of fetchlists\nLet’s take a look now at a more complicated use case. Fetchlists are produced from the\nCrawlDb (which is a map file of <url, crawlDatum>, with the crawlDatum containing a\nstatus of this URL), and they contain URLs ready to be fetched, which are then pro-\ncessed by the Nutch Fetcher tool. Fetcher is itself a MapReduce application (described\nshortly). This means that the input data (partitioned in N parts) will be processed by\nN map tasks—the Fetcher tool enforces that SequenceFileInputFormat should not fur-\nther split the data in more parts than there are already input partitions. We mentioned\nearlier briefly that fetchlists need to be generated in a special way so that the data in\neach part of the fetchlist (and consequently processed in each map task) meets certain\nrequirements:\n1. All URLs from the same host need to end up in the same partition. This is required\nso that Nutch can easily implement in-JVM host-level blocking to avoid over-\nwhelming target hosts.\n2. URLs from the same host should be as far apart as possible (i.e., well mixed with\nURLs from other hosts) in order to minimize the host-level blocking.\nNutch Search Engine | 4313. There should be no more than x URLs from any single host so that large sites with\nmany URLs don’t dominate smaller sites (and URLs from smaller sites still have a\nchance to be scheduled for fetching).\n4. URLs with high scores should be preferred over URLs with low scores.\n5. There should be at most y URLs in total in the fetchlist.\n6. The number of output partitions should match the optimum number of fetching\nmap tasks.\nIn this case, two MapReduce jobs are needed to satisfy all these requirements, as illus-\ntrated in Figure 14-7. Again, in the following listings, we are going to skip some details\nof these steps for the sake of brevity.\nFigure 14-7. Generation of fetchlists\nStep 1: Select, sort by score, limit by URL count per host. In this step, Nutch runs a MapReduce\njob to select URLs that are considered eligible for fetching, and to sort them by their\nscore (a floating-point value assigned to each URL, e.g., a PageRank score). The input\ndata comes from CrawlDb, which is a map file of <url, datum>. The output from this\njob is a sequence file with <score, <url, datum>>, sorted in descending order by score.\nFirst, let’s look at the job setup:\nFileInputFormat.addInputPath(job, crawlDbPath);\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(Selector.class);\njob.setPartitionerClass(Selector.class);\njob.setReducerClass(Selector.class);\nFileOutputFormat.setOutputPath(job, tempDir);\njob.setOutputFormat(SequenceFileOutputFormat.class);\njob.setOutputKeyClass(FloatWritable.class);\njob.setOutputKeyComparatorClass(DecreasingFloatComparator.class);\njob.setOutputValueClass(SelectorEntry.class);\nThe Selector class implements three functions: mapper, reducer, and partitioner. The\nlast function is especially interesting: Selector uses a custom Partitioner to assign\nURLs from the same host to the same reduce task so that we can satisfy criteria 3–5\n432 | Chapter 14: Case Studiesfrom the previous list. If we didn’t override the default partitioner, URLs from the same\nhost would end up in different partitions of the output, and we wouldn’t be able to\ntrack and limit the total counts, because MapReduce tasks don’t communicate between\nthemselves. As it is now, all URLs that belong to the same host will end up being\nprocessed by the same reduce task, which means we can control how many URLs per\nhost are selected.\nIt’s easy to implement a custom partitioner so that data that needs to be processed in\nthe same task ends up in the same partition. Let’s take a look first at how the\nSelector class implements the Partitioner interface (which consists of a single\nmethod):\n/** Partition by host. */\npublic int getPartition(FloatWritable key, Writable value, int numReduceTasks) {\nreturn hostPartitioner.getPartition(((SelectorEntry)value).url, key, numReduceTasks);\n}\nThe method returns an integer number from 0 to numReduceTasks - 1. It simply replaces\nthe key with the original URL from SelectorEntry to pass the URL (instead of score)\nto an instance of PartitionUrlByHost, where the partition number is calculated:\n/** Hash by hostname. */\npublic int getPartition(Text key, Writable value, int numReduceTasks) {\nString urlString = key.toString();\nURL url = null;\ntry {\nurl = new URL(urlString);\n} catch (MalformedURLException e) {\nLOG.warn(""Malformed URL: '"" + urlString + ""'"");\n}\nint hashCode = (url == null ? urlString : url.getHost()).hashCode();\n// make hosts wind up in different partitions on different runs\nhashCode ^= seed;\n}\nreturn (hashCode & Integer.MAX_VALUE) % numReduceTasks;\nAs you can see from the code snippet, the partition number is a function of only the\nhost part of the URL, which means that all URLs that belong to the same host will end\nup in the same partition.\nThe output from this job is sorted in decreasing order by score. Since there are many\nrecords in CrawlDb with the same score, we couldn’t use MapFileOutputFormat because\nwe would violate the map file’s invariant of strict key ordering.\nObservant readers will notice that as we had to use something other than the original\nkeys, but we still want to preserve the original key-value pairs, we use here a Selector\nEntry class to pass the original key-value pairs to the next step of processing.\nSelector.reduce() keeps track of the total number of URLs and the maximum number\nof URLs per host, and simply discards excessive records. Please note that the enforce-\nment of the total count limit is necessarily approximate. We calculate the limit for the\nNutch Search Engine | 433current task as the total limit divided by the number of reduce tasks. But we don’t know\nfor sure from within the task that it is going to get an equal share of URLs; indeed, in\nmost cases, it doesn’t because of the uneven distribution of URLs among hosts. How-\never, for Nutch this approximation is sufficient.\nStep 2: Invert, partition by host, sort randomly. In the previous step, we ended up with a se-\nquence file of <score, selectorEntry>. Now we have to produce a sequence file of\n<url, datum> and satisfy criteria 1, 2, and 6 just described. The input data for this step\nis the output data produced in step 1.\nThe following is a snippet showing the setup of this job:\nFileInputFormat.addInputPath(job, tempDir);\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(SelectorInverseMapper.class);\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(SelectorEntry.class);\njob.setPartitionerClass(PartitionUrlByHost.class);\njob.setReducerClass(PartitionReducer.class);\njob.setNumReduceTasks(numParts);\nFileOutputFormat.setOutputPath(job, output);\njob.setOutputFormat(SequenceFileOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(CrawlDatum.class);\njob.setOutputKeyComparatorClass(HashComparator.class);\nThe SelectorInverseMapper class simply discards the current key (the score value), ex-\ntracts the original URL and uses it as a key, and uses the SelectorEntry as the value.\nCareful readers may wonder why we don’t go one step further, extracting also the\noriginal CrawlDatum and using it as the value—more on this shortly.\nThe final output from this job is a sequence file of <Text, CrawlDatum>, but our output\nfrom the map phase uses <Text, SelectorEntry>. We have to specify that we use dif-\nferent key/value classes for the map output, using the setMapOutputKeyClass() and\nsetMapOutputValueClass() setters—otherwise, Hadoop assumes that we use the same\nclasses as declared for the reduce output (this conflict usually would cause a job to fail).\nThe output from the map phase is partitioned using PartitionUrlByHost class so that\nit again assigns URLs from the same host to the same partition. This satisfies require-\nment 1.\nOnce the data is shuffled from map to reduce tasks, it’s sorted by Hadoop according\nto the output key comparator, in this case the HashComparator. This class uses a simple\nhashing scheme to mix URLs in a way that is least likely to put URLs from the same\nhost close to each other.\nIn order to meet requirement 6, we set the number of reduce tasks to the desired number\nof Fetcher map tasks (the numParts mentioned earlier), keeping in mind that each reduce\npartition will be used later on to create a single Fetcher map task.\n434 | Chapter 14: Case StudiesPartitionReducer class is responsible for the final step, that is, to convert <url,\nselectorEntry> to <url, crawlDatum>. A surprising side effect of using HashCompara\ntor is that several URLs may be hashed to the same hash value, and Hadoop will call\nreduce() method passing only the first such key—all other keys considered equal will\nbe discarded. Now it becomes clear why we had to preserve all URLs in SelectorEn\ntry records, because now we can extract them from the iterated values. Here is the\nimplementation of this method:\npublic void reduce(Text key, Iterator<SelectorEntry> values,\nOutputCollector<Text, CrawlDatum> output, Reporter reporter) throws IOException {\n// when using HashComparator, we get only one input key in case of hash collisions\n// so use only URLs extracted from values\nwhile (values.hasNext()) {\nSelectorEntry entry = values.next();\noutput.collect(entry.url, entry.datum);\n}\n}\nFinally, the output from reduce tasks is stored as a SequenceFileOutputFormat in a Nutch\nsegment directory, in a crawl_generate subdirectory. This output satisfies all criteria\nfrom 1 to 6.\nFetcher: A multi-threaded MapRunner in action\nThe Fetcher application in Nutch is responsible for downloading the page content from\nremote sites. As such, it is important that the process uses every opportunity for par-\nallelism, in order to minimize the time it takes to crawl a fetchlist.\nThere is already one level of parallelism present in Fetcher—multiple parts of the input\nfetchlists are assigned to multiple map tasks. However, in practice this is not sufficient:\nsequential download of URLs, from different hosts (see the earlier section on HashCom\nparator), would be a tremendous waste of time. For this reason the Fetcher map tasks\nprocess this data using multiple worker threads.\nHadoop uses the MapRunner class to implement the sequential processing of input data\nrecords. The Fetcher class implements its own MapRunner that uses multiple threads to\nprocess input records in parallel.\nLet’s begin with the setup of the job:\njob.setSpeculativeExecution(false);\nFileInputFormat.addInputPath(job, ""segment/crawl_generate"");\njob.setInputFormat(InputFormat.class);\njob.setMapRunnerClass(Fetcher.class);\nFileOutputFormat.setOutputPath(job, segment);\njob.setOutputFormat(FetcherOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(NutchWritable.class);\nNutch Search Engine | 435First, we turn off speculative execution. We can’t run several map tasks downloading\ncontent from the same hosts because it would violate the host-level load limits (such\nas the number of concurrent requests and the number of requests per second).\nNext, we use a custom InputFormat implementation that prevents Hadoop from split-\nting partitions of input data into smaller chunks (splits), thus creating more map tasks\nthan there are input partitions. This again ensures that we control host-level access\nlimits.\nOutput data is stored using a custom OutputFormat implementation, which creates sev-\neral output map files and sequence files created using data contained in NutchWrita\nble values. The NutchWritable class is a subclass of GenericWritable, able to pass in-\nstances of several different Writable classes declared in advance.\nThe Fetcher class implements the MapRunner interface, and we set this class as the job’s\nMapRunner implementation. The relevant parts of the code are listed here:\npublic void run(RecordReader<Text, CrawlDatum> input,\nOutputCollector<Text, NutchWritable> output,\nReporter reporter) throws IOException {\nint threadCount = getConf().getInt(""fetcher.threads.fetch"", 10);\nfeeder = new QueueFeeder(input, fetchQueues, threadCount * 50);\nfeeder.start();\n}\nfor (int i = 0; i < threadCount; i++) {\nnew FetcherThread(getConf()).start();\n}\ndo {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException e) {}\nreportStatus(reporter);\n} while (activeThreads.get() > 0);\n// spawn threads\n// wait for threads to exit\nFetcher reads many input records in advance, using the QueueFeeder thread that puts\ninput records into a set of per-host queues. Then several FetcherThread instances are\nstarted, which consume items from per-host queues, while QueueFeeder keeps reading\ninput data to keep the queues filled. Each FetcherThread consumes items from any\nnonempty queue.\nIn the meantime, the main thread of the map task spins around waiting for all threads\nto finish their job. Periodically it reports the status to the framework to ensure that\nHadoop doesn’t consider this task to be dead and kill it. Once all items are processed,\nthe loop is finished and the control is returned to Hadoop, which considers this map\ntask to be completed.\nIndexer: Using custom OutputFormat\nThis is an example of a MapReduce application that doesn’t produce sequence file or\nmap file output—instead, the output from this application is a Lucene index. Again,\n436 | Chapter 14: Case Studiesas MapReduce applications may consist of several reduce tasks, the output from this\napplication may consist of several partial Lucene indexes.\nNutch Indexer tool uses information from CrawlDb, LinkDb, and Nutch segments\n(fetch status, parsing status, page metadata, and plain-text data), so the job setup sec-\ntion involves adding several input paths:\nFileInputFormat.addInputPath(job, crawlDbPath);\nFileInputFormat.addInputPath(job, linkDbPath);\n// add segment data\nFileInputFormat.addInputPath(job, ""segment/crawl_fetch"");\nFileInputFormat.addInputPath(job, ""segment/crawl_parse"");\nFileInputFormat.addInputPath(job, ""segment/parse_data"");\nFileInputFormat.addInputPath(job, ""segment/parse_text"");\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(Indexer.class);\njob.setReducerClass(Indexer.class);\nFileOutputFormat.setOutputPath(job, indexDir);\njob.setOutputFormat(OutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(LuceneDocumentWrapper.class);\nAll corresponding records for a URL dispersed among these input locations need to be\ncombined to create Lucene documents to be added to the index.\nThe Mapper implementation in Indexer simply wraps input data, whatever its source\nand implementation class, in a NutchWritable, so that the reduce phase may receive\ndata from different sources, using different classes, and still be able to consistently\ndeclare a single output value class (as NutchWritable) from both map and reduce steps.\nThe Reducer implementation iterates over all values that fall under the same key (URL),\nunwraps the data (fetch CrawlDatum, CrawlDb CrawlDatum, LinkDb Inlinks, Parse\nData and ParseText) and, using this information, builds a Lucene document, which is\nthen wrapped in a Writable LuceneDocumentWrapper and collected. In addition to all\ntextual content (coming either from the plain-text data or from metadata), this docu-\nment also contains a PageRank-like score information (obtained from CrawlDb data).\nNutch uses this score to set the boost value of Lucene document.\nThe OutputFormat implementation is the most interesting part of this tool:\npublic static class OutputFormat extends\nFileOutputFormat<WritableComparable, LuceneDocumentWrapper> {\npublic RecordWriter<WritableComparable, LuceneDocumentWrapper>\ngetRecordWriter(final FileSystem fs, JobConf job,\nString name, final Progressable progress) throws IOException {\nfinal Path out = new Path(FileOutputFormat.getOutputPath(job), name);\nfinal IndexWriter writer = new IndexWriter(out.toString(),\nnew NutchDocumentAnalyzer(job), true);\nreturn new RecordWriter<WritableComparable, LuceneDocumentWrapper>() {\nboolean closed;\npublic void write(WritableComparable key, LuceneDocumentWrapper value)\nNutch Search Engine | 437}\nthrows IOException {\nDocument doc = value.get();\nwriter.addDocument(doc);\nprogress.progress();\n// unwrap & index doc\npublic void close(final Reporter reporter) throws IOException {\n// spawn a thread to give progress heartbeats\nThread prog = new Thread() {\npublic void run() {\nwhile (!closed) {\ntry {\nreporter.setStatus(""closing"");\nThread.sleep(1000);\n} catch (InterruptedException e) { continue; }\ncatch (Throwable e) { return; }\n}\n}\n};\n}\n}\ntry {\nprog.start();\n// optimize & close index\nwriter.optimize();\nwriter.close();\n} finally {\nclosed = true;\n}\n};\nWhen an instance of RecordWriter is requested, the OutputFormat creates a new Lucene\nindex by opening an IndexWriter. Then, for each new output record collected in the\nreduce method, it unwraps the Lucene document from LuceneDocumentWrapper value\nand adds it to the index.\nWhen a reduce task is finished, Hadoop will try to close the RecordWriter. In this case,\nthe process of closing may take a long time, because we would like to optimize the\nindex before closing. During this time, Hadoop may conclude that the task is hung,\nsince there are no progress updates, and it may attempt to kill it. For this reason, we\nfirst start a background thread to give reassuring progress updates, and then proceed\nto perform the index optimization. Once the optimization is completed, we stop the\nprogress updater thread. The output index is now created, optimized, and is closed,\nand ready for use in a searcher application.\nSummary\nThis short overview of Nutch necessarily omits many details, such as error handling,\nlogging, URL filtering and normalization, dealing with redirects or other forms of\n“aliased” pages (such as mirrors), removing duplicate content, calculating PageRank\n438 | Chapter 14: Case Studiesscoring, etc. You can find this and much more information on the official page of the\nproject and on the wiki (http://wiki.apache.org/nutch).\nToday, Nutch is used by many organizations and individual users. Still, operating a\nsearch engine requires nontrivial investment in hardware, integration, and customiza-\ntion, and the maintenance of the index, so in most cases Nutch is used to build com-\nmercial vertical- or field-specific search engines.\nNutch is under active development, and the project follows closely new releases of\nHadoop. As such, it will continue to be a practical example of a real-life application\nthat uses Hadoop at its core, with excellent results.\n—Andrzej Białecki\nLog Processing at Rackspace\nRackspace Hosting has always provided managed systems for enterprises, and in that\nvein, Mailtrust became Rackspace’s mail division in the Fall 2007. Rackspace currently\nhosts email for over 1 million users and thousands of companies on hundreds of servers.\nRequirements/The Problem\nTransferring the mail generated by Rackspace customers through the system generates\na considerable “paper” trail, in the form of around 150 GB per day of logs in various\nformats. It is extremely helpful to aggregate that data for growth planning purposes\nand to understand how customers use our applications, and the records are also a boon\nfor troubleshooting problems in the system.\nIf an email fails to be delivered, or a customer is unable to log in, it is vital that our\ncustomer support team is able to find enough information about the problem to begin\nthe debugging process. To make it possible to find that information quickly, we cannot\nleave the logs on the machines that generated them or in their original format. Instead,\nwe use Hadoop to do a considerable amount of processing, with the end result being\nLucene indexes that customer support can query.\nLogs\nTwo of our highest volume log formats are produced by the Postfix mail transfer agent\nand Microsoft Exchange Server. All mail that travels through our systems touches\nPostfix at some point, and the majority of messages travel through multiple Postfix\nservers. The Exchange environment is independent by necessity, but one class of Postfix\nmachines acts as an added layer of protection, and uses SMTP to transfer messages\nbetween mailboxes hosted in each environment.\nThe messages travel through many machines, but each server only knows enough about\nthe destination of the mail to transfer it to the next responsible server. Thus, in order\nto build the complete history of a message, our log processing system needs to have a\nLog Processing at Rackspace | 439global view of the system. This is where Hadoop helps us immensely: as our system\ngrows, so does the volume of logs. For our log processing logic to stay viable, we had\nto ensure that it would scale, and MapReduce was the perfect framework for that\ngrowth.\nBrief History\nEarlier versions of our log processing system were based on MySQL, but as we gained\nmore and more logging machines, we reached the limits of what a single MySQL server\ncould process. The database schema was already reasonably denormalized, which\nwould have made it less difficult to shard, but MySQL’s partitioning support was still\nvery weak at that point in time. Rather than implementing our own sharding and pro-\ncessing solution around MySQL, we chose to use Hadoop.\nChoosing Hadoop\nAs soon as you shard the data in a RDBMS system, you lose a lot of the advantages of\nSQL for performing analysis of your dataset. Hadoop gives us the ability to easily proc-\ness all of our data in parallel using the same algorithms we would for smaller datasets.\nCollection and Storage\nLog collection\nThe servers generating the logs we process are distributed across multiple data centers,\nbut we currently have a single Hadoop cluster, located in one of those data centers (see\nFigure 14-8). In order to aggregate the logs and place them into the cluster, we use the\nUnix syslog replacement syslog-ng and some simple scripts to control the creation of\nfiles in Hadoop.\nWithin a data center, syslog-ng is used to transfer logs from a source machine to a load-\nbalanced set of collector machines. On the collectors, each type of log is aggregated into\na single stream, and lightly compressed with gzip (step A in Figure 14-8). From remote\ncollectors, logs can be transferred through an SSH tunnel cross-data center to collectors\nthat are local to the Hadoop cluster (step B).\n440 | Chapter 14: Case StudiesFigure 14-8. Hadoop data flow at Rackspace\nOnce the compressed log stream reaches a local collector, it can be written to Hadoop\n(step C). We currently use a simple Python script that buffers input data to disk, and\nperiodically pushes the data into the Hadoop cluster using the Hadoop command-line\ninterface. The script copies the log buffers to input folders in Hadoop when they reach\na multiple of the Hadoop block size, or when enough time has passed.\nThis method of securely aggregating logs from different data centers was developed\nbefore SOCKS support was added to Hadoop via the hadoop.rpc.socket.fac\ntory.class.default parameter and SocksSocketFactory class. By using SOCKS support\nand the HDFS API directly from remote collectors, we could eliminate one disk write\nand a lot of complexity from the system. We plan to implement a replacement using\nthese features in future development sprints.\nOnce the raw logs have been placed in Hadoop, they are ready for processing by our\nMapReduce jobs.\nLog storage\nOur Hadoop cluster currently contains 15 datanodes with commodity CPUs and three\n500 GB disks each. We use a default replication factor of three for files that need to\nsurvive for our archive period of six months, and two for anything else.\nThe Hadoop namenode uses hardware identical to the datanodes. To provide reason-\nably high availability, we use two secondary namenodes, and a virtual IP that can easily\nbe pointed at any of the three machines with snapshots of the HDFS. This means that\nin a failover situation, there is potential for us to lose up to 30 minutes of data, de-\npending on the ages of the snapshots on the secondary namenodes. This is acceptable\nfor our log processing application, but other Hadoop applications may require lossless\nfailover by using shared storage for the namenode’s image.\nLog Processing at Rackspace | 441MapReduce for Logs\nProcessing\nIn distributed systems, the sad truth of unique identifiers is that they are rarely actually\nunique. All email messages have a (supposedly) unique identifier called a message-id\nthat is generated by the host where they originated, but a bad client could easily send\nout duplicates. In addition, since the designers of Postfix could not trust the message-\nid to uniquely identify the message, they were forced to come up with a separate ID\ncalled a queue-id, which is guaranteed to be unique only for the lifetime of the message\non a local machine.\nAlthough the message-id tends to be the definitive identifier for a message, in Postfix\nlogs, it is necessary to use queue-ids to find the message-id. Looking at the second line\nin Example 14-1 (which is formatted to better fit the page), you will see the hex string\n1DBD21B48AE, which is the queue-id of the message that the log line refers to. Because\ninformation about a message (including its message-id) is output as separate lines when\nit is collected (potentially hours apart), it is necessary for our parsing code to keep state\nabout messages.\nExample 14-1. Postfix log lines\nNov 12 17:36:54 gate8.gate.sat.mlsrvr.com postfix/smtpd[2552]: connect from hostname\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/qmgr[9489]: 1DBD21B48AE:\nfrom=<mapreduce@rackspace.com>, size=5950, nrcpt=1 (queue active)\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/smtpd[28085]: disconnect from\nhostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/smtpd[22593]: too many errors\nafter DATA from hostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/smtpd[22593]: disconnect from\nhostname\nNov 12 17:36:54 gate10.gate.sat.mlsrvr.com postfix/smtpd[10311]: connect from\nhostname\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/smtp[28107]: D42001B48B5:\nto=<mapreduce@rackspace.com>, relay=hostname[ip], delay=0.32, delays=0.28/0/0/0.04,\ndsn=2.0.0, status=sent (250 2.0.0 Ok: queued as 1DBD21B48AE)\nNov 12 17:36:54 gate20.gate.sat.mlsrvr.com postfix/smtpd[27168]: disconnect from\nhostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/qmgr[1209]: 645965A0224: removed\nNov 12 17:36:54 gate2.gate.sat.mlsrvr.com postfix/smtp[15928]: 732196384ED: to=<m\napreduce@rackspace.com>, relay=hostname[ip], conn_use=2, delay=0.69, delays=0.04/\n0.44/0.04/0.17, dsn=2.0.0, status=sent (250 2.0.0 Ok: queued as 02E1544C005)\nNov 12 17:36:54 gate2.gate.sat.mlsrvr.com postfix/qmgr[13764]: 732196384ED: removed\nNov 12 17:36:54 gate1.gate.sat.mlsrvr.com postfix/smtpd[26394]: NOQUEUE: reject: RCP\nT from hostname 554 5.7.1 <mapreduce@rackspace.com>: Client host rejected: The\nsender's mail server is blocked; from=<mapreduce@rackspace.com> to=<mapred\nuce@rackspace.com> proto=ESMTP helo=<mapreduce@rackspace.com>\nFrom a MapReduce perspective, each line of the log is a single key-value pair. In phase\n1, we need to map all lines with a single queue-id key together, and then reduce them\nto determine if the log message values indicate that the queue-id is complete.\n442 | Chapter 14: Case StudiesSimilarly, once we have a completed queue-id for a message, we need to group by the\nmessage-id in phase 2. We Map each completed queue-id with its message-id as key,\nand a list of its log lines as the value. In Reduce, we determine whether all of the queue-\nids for the message-id indicate that the message left our system.\nTogether, the two phases of the mail log MapReduce job and their InputFormat and\nOutputFormat form a type of staged event-driven architecture (SEDA). In SEDA, an ap-\nplication is broken up into multiple “stages,” that are separated by queues. In a Hadoop\ncontext, a queue could be either an input folder in HDFS that a MapReduce job con-\nsumes from, or the implicit queue that MapReduce forms between the Map and Reduce\nsteps.\nIn Figure 14-9, the arrows between stages represent the queues, with a dashed arrow\nbeing the implicit MapReduce queue. Each stage can send a key-value pair (SEDA calls\nthem events or messages) to another stage via these queues.\nFigure 14-9. MapReduce chain\nPhase 1: Map. During the first phase of our Mail log processing job, the inputs to the Map\nstage are either a line number key and log message value or a queue-id key to an array\nof log-message values. The first type of input is generated when we process a raw logfile\nLog Processing at Rackspace | 443from the queue of input files, and the second type is an intermediate format that rep-\nresents the state of a queue-id we have already attempted to process, but which was\nrequeued because it was incomplete.\nIn order to accomplish this dual input, we implemented a Hadoop InputFormat that\ndelegates the work to an underlying SequenceFileRecordReader or LineRecordReader,\ndepending on the file extension of the input FileSplit. The two input formats come\nfrom different input folders (queues) in HDFS.\nPhase 1: Reduce. During this phase, the Reduce stage determines whether the queue-id\nhas enough lines to be considered completed. If the queue-id is completed, we output\nthe message-id as key, and a HopWritable object as value. Otherwise, the queue-id is set\nas the key, and the array of log lines is requeued to be Mapped with the next set of raw\nlogs. This will continue until we complete the queue-id, or until it times out.\nThe HopWritable object is a POJO that implements Hadoop’s\nWritable interface. It completely describes a message from the viewpoint\nof a single server, including the sending address and IP, attempts to\ndeliver the message to other servers, and typical message header\ninformation.\nThis split output is accomplished with an OutputFormat implementation that is some-\nwhat symmetrical with our dual InputFormat. Our MultiSequenceFileOutputFormat was\nimplemented before the Hadoop API added a MultipleSequenceFileOutputFormat in\nr0.17.0, but fulfills the same type of goal: we needed our Reduce output pairs to go to\ndifferent files depending on characteristics of their keys.\nPhase 2: Map. In the next stage of the Mail log processing job, the input is a message-id\nkey, with a HopWritable value from the previous phase. This stage does not contain any\nlogic: instead, it simply combines the inputs from the first phase using the standard\nSequenceFileInputFormat and IdentityMapper.\nPhase 2: Reduce. In the final Reduce stage, we want to see whether all of the HopWrita\nbles we have collected for the message-id represent a complete message path through\nour system. A message path is essentially a directed graph (which is typically acyclic,\nbut it may contain loops if servers are misconfigured). In this graph, a vertex is a server,\nwhich can be labeled with multiple queue-ids, and attempts to deliver the message from\none server to another are edges. For this processing, we use the JGraphT graph library.\nFor output, we again use the MultiSequenceFileOutputFormat. If the Reducer decides\nthat all of the queue-ids for a message-id create a complete message path, then the\nmessage is serialized and queued for the SolrOutputFormat. Otherwise, the\nHopWritables for the message are queued for phase 2: Map stage to be reprocessed with\nthe next batch of queue-ids.\n444 | Chapter 14: Case StudiesThe SolrOutputFormat contains an embedded Apache Solr instance—in the fashion that\nwas originally recommended by the Solr wiki—to generate an index on local disk.\nClosing the OutputFormat then involves compressing the disk index to the final desti-\nnation for the output file. This approach has a few advantages over using Solr’s HTTP\ninterface or using Lucene directly:\n• We can enforce a Solr schema\n• Map and Reduce remain idempotent\n• Indexing load is removed from the Search nodes\nWe currently use the default HashPartitioner class to decide which Reduce task will\nreceive particular keys, which means that the keys are semirandomly distributed. In a\nfuture iteration of the system, we’d like to implement a new Partitioner to split by\nsending address instead (our most common search term). Once the indexes are split\nby sender, we can use the hash of the address to determine where to merge or query\nfor an index, and our search API will only need to communicate with the relevant nodes.\nMerging for near-term search\nAfter a set of MapReduce phases have completed, a different set of machines are notified\nof the new indexes, and can pull them for merging. These Search nodes are running\nApache Tomcat and Solr instances to host completed indexes, along with a service to\npull and merge the indexes to local disk (step D in Figure 14-8).\nEach compressed file from SolrOutputFormat is a complete Lucene index, and Lucene\nprovides the IndexWriter.addIndexes() methods for quickly merging multiple indexes.\nOur MergeAgent service decompresses each new index into a Lucene RAMDirectory or\nFSDirectory (depending on size), merges them to local disk, and sends a <commit/>\nrequest to the Solr instance hosting the index to make the changed index visible to\nqueries.\nSharding. The Query/Management API is a thin layer of PHP code that handles sharding\nthe output indexes across all of the Search nodes. We use a simple implementation of\nconsistent hashing to decide which Search nodes are responsible for each index file.\nCurrently, indexes are sharded by their creation time, and then by their hashed file-\nname, but we plan to replace the filename hash with a sending address hash at some\npoint in the future (see phase 2: Reduce).\nBecause HDFS already handles replication of the Lucene indexes, there is no need to\nkeep multiple copies available in Solr. Instead, in a failover situation, the Search node\nis completely removed, and other nodes become responsible for merging the indexes.\nSearch results. With this system, we’ve achieved a 15-minute turnaround time from log\ngeneration to availability of a search result for our Customer Support team.\nOur search API supports the full Lucene query syntax, so we commonly see complex\nqueries like:\nLog Processing at Rackspace | 445sender:""mapreduce@rackspace.com"" -recipient:""hadoop@rackspace.com""\nrecipient:""@rackspace.com"" short-status:deferred timestamp:[1228140900 TO 2145916799]\nEach result returned by a query is a complete serialized message path, which indicates\nwhether individual servers and recipients received the message. We currently display\nthe path as a 2D graph (Figure 14-10) that the user can interact with to expand points\nof interest, but there is a lot of room for improvement in the visualization of this data.\nFigure 14-10. Data tree\nArchiving for analysis\nIn addition to providing short-term search for Customer Support, we are also interested\nin performing analysis of our log data.\nEvery night, we run a series of MapReduce jobs with the day’s indexes as input. We\nimplemented a SolrInputFormat that can pull and decompress an index, and emit each\ndocument as a key-value pair. With this InputFormat, we can iterate over all message\npaths for a day, and answer almost any question about our mail system, including:\n• Per domain data (viruses, spam, connections, recipients)\n• Most effective spam rules\n446 | Chapter 14: Case Studies•\n•\n•\n•\nLoad generated by specific users\nReasons for message bounces\nGeographical sources of connections\nAverage latency between specific machines\nSince we have months of compressed indexes archived in Hadoop, we are also able to\nretrospectively answer questions that our nightly log summaries leave out. For instance,\nwe recently wanted to determine the top sending IP addresses per month, which we\naccomplished with a simple one-off MapReduce job.\n—Stu Hood\nCascading\nCascading is an open source Java library and application programming interface (API)\nthat provides an abstraction layer for MapReduce. It allows developers to build com-\nplex, mission-critical data processing applications that run on Hadoop clusters.\nThe Cascading project began in the summer of 2007. Its first public release, version\n0.1, launched in January 2008. Version 1.0 was released in January 2009. Binaries,\nsource code, and add-on modules can be downloaded from the project website, http:\n//www.cascading.org/.\n“Map” and “Reduce” operations offer powerful primitives. However, they tend to be\nat the wrong level of granularity for creating sophisticated, highly composable code\nthat can be shared among different developers. Moreover, many developers find it dif-\nficult to “think” in terms of MapReduce when faced with real-world problems.\nTo address the first issue, Cascading substitutes the “keys” and “values” used in Map-\nReduce with simple field names and a data tuple model, where a tuple is simply a list\nof values. For the second issue, Cascading departs from Map and Reduce operations\ndirectly by introducing higher-level abstractions as alternatives: Functions, Filters, Ag-\ngregators, and Buffers.\nOther alternatives began to emerge at about the same time as the project’s initial public\nrelease, but Cascading was designed to complement them. Consider that most of these\nalternative frameworks impose pre- and post-conditions, or other expectations.\nFor example, in several other MapReduce tools, you must preformat, filter, or import\nyour data into the Hadoop Filesystem (HDFS) prior to running the application. That\nstep of preparing the data must be performed outside of the programming abstraction.\nIn contrast, Cascading provides means to prepare and manage your data as integral\nparts of the programming abstraction.\nThis case study begins with an introduction to the main concepts of Cascading, then\nfinishes with an overview of how ShareThis uses Cascading in its infrastructure.\nCascading | 447Please see the Cascading User Guide on the project website for a more in-depth pre-\nsentation of the Cascading processing model.\nFields, Tuples, and Pipes\nThe MapReduce model uses keys and values to link input data to the Map function,\nthe Map function to the Reduce function, and the Reduce function to the output data.\nBut as we know, real-world Hadoop applications are usually more than one MapRe-\nduce job chained together. Consider the canonical word count example implemented\nin MapReduce. If you needed to sort the numeric counts in descending order, not an\nunlikely requirement, it would need to be done in a second MapReduce job.\nSo, in the abstract, keys and values not only bind Map to Reduce, but Reduce to the\nnext Map, and then to the next Reduce, and so on (Figure 14-11). That is, key-value\npairs are sourced from input files and stream through chains of Map and Reduce op-\nerations and finally rest in an output file. When you implement enough of these chained\nMapReduce applications, you start to see a well-defined set of key/value manipulations\nused over and over again to modify the key/value data stream.\nFigure 14-11. Counting and sorting in MapReduce\nCascading simplifies this by abstracting away keys and values and replacing them with\ntuples that have corresponding field names, similar in concept to tables and column\nnames in a relational database. And during processing, streams of these Fields and\nTuples are then manipulated as they pass through user-defined operations linked to-\ngether by Pipes (Figure 14-12).\n448 | Chapter 14: Case StudiesFigure 14-12. Pipes linked by Fields and Tuples\nSo, MapReduce keys and values are reduced to:\nFields\nFields are a collection of either String names (like “first_name”), numeric positions\n(like 2, or –1, for the third and last position, respectively), or a combination of\nboth, very much like column names. So Fields are used to declare the names of\nvalues in a Tuple, and to select values by name from a Tuple. The later is like a\nSQL select call.\nTuple\nA Tuple is simply an array of java.lang.Comparable objects. A Tuple is very much\nlike a database row or record.\nAnd the Map and Reduce operations are abstracted behind one or more Pipe instances\n(Figure 14-13):\nEach\nThe Each pipe processes a single input Tuple at a time. It may apply either a Func\ntion or a Filter operation (described shortly) to the input tuple.\nGroupBy\nThe GroupBy pipe groups tuples on grouping fields. It behaves just like the SQL\ngroup by statement. It can also merge multiple input tuple streams into a single\nstream, if they all share the same field names.\nCoGroup\nThe CoGroup pipe both joins multiple tuple streams together by common field\nnames, and it also groups the tuples by the common grouping fields. All standard\njoin types (inner, outer, etc.) and custom joins can be used across two or more\ntuple streams.\nEvery\nThe Every pipe processes a single grouping of tuples at a time, where the group\nwas grouped by a GroupBy or CoGroup pipe. The Every pipe may apply either an\nAggregator or a Buffer operation to the grouping.\nCascading | 449SubAssembly\nThe SubAssembly pipe allows for nesting of assemblies inside a single pipe, which\ncan in turn be nested in more complex assemblies.\nFigure 14-13. Pipe types\nAll these pipes are chained together by the developer into “pipe assemblies” in which\neach assembly can have many input Tuple streams (sources) and many output Tuple\nstreams (sinks) (see Figure 14-14).\nFigure 14-14. A simple PipeAssembly\nOn the surface, this might seem more complex than the traditional MapReduce model.\nAnd admittedly there are more concepts here than Map, Reduce, Key, and Value. But\nin practice, there are many more concepts that must all work in tandem to provide\ndifferent behaviors.\nFor example, if a developer wanted to provide a “secondary sorting” of reducer values,\nshe would need to implement Map, Reduce, a “composite” Key (two Keys nested in a\nparent Key), Value, Partitioner, an “output value grouping” Comparator, and an “out-\nput key” Comparator, all of which would be coupled to one another in varying ways,\nand very likely nonreusable in subsequent applications.\n450 | Chapter 14: Case StudiesIn Cascading, this would be one line of code: new GroupBy(<previous>, <grouping\nfields>, <secondary sorting fields>), where previous is the pipe that came before.\nOperations\nAs mentioned earlier, Cascading departs from MapReduce by introducing alternative\noperations that either are applied to individual Tuples or groups of Tuples (Fig-\nure 14-15):\nFunction\nA Function operates on individual input Tuples and may return zero or more output\nTuples for every one input. Functions are applied by the Each pipe.\nFilter\nA Filter is a special kind of function that returns a boolean value indicating\nwhether the current input Tuple should be removed from the Tuple stream. A\nFunction could serve this purpose, but the Filter is optimized for this case, and\nmany filters can be grouped by “logical” filters like And, Or, Xor, and Not, rapidly\ncreating more complex filtering operations.\nAggregator\nAn Aggregator performs some operation against a group of Tuples, where the\ngrouped Tuples are grouped by a common set of field values. For example, all\nTuples having the same “last-name” value. Common Aggregator implementations\nwould be Sum, Count, Average, Max, and Min.\nBuffer\nA Buffer is similar to the Aggregator, except it is optimized to act as a “sliding\nwindow” across all the Tuples in a unique grouping. This is useful when the de-\nveloper needs to efficiently insert missing values in an ordered set of Tuples (like\na missing date or duration), or create a running average. Usually Aggregator is the\noperation of choice when working with groups of Tuples, since many\nAggregators can be chained together very efficiently, but sometimes a Buffer is the\nbest tool for the job.\nCascading | 451Figure 14-15. Operation types\nOperations are bound to pipes when the pipe assembly is created (Figure 14-16).\nFigure 14-16. An assembly of operations\nThe Each and Every pipes provide a simple mechanism for selecting some or all values\nout of an input tuple before being passed to its child operation. And there is a simple\nmechanism for merging the operation results with the original input Tuple to create\nthe output Tuple. Without going into great detail, this allows for each operation to\nonly care about argument Tuple values and fields, not the whole set of fields in the\ncurrent input Tuple. Subsequently, operations can be reusable across applications the\nsame way Java methods can be reusable.\nFor example, in Java, a method declared as concatenate(String first, String\nsecond) is more abstract than concatenate(Person person). In the second case, the\nconcatenate() function must “know” about the Person object; in the first case, it is\nagnostic to where the data came from. Cascading operations exhibit this same quality.\nTaps, Schemes, and Flows\nIn many of the previous diagrams, there are references to “sources” and “sinks.” In\nCascading, all data is read from or written to Tap instances, but is converted to and\nfrom Tuple instances via Scheme objects:\n452 | Chapter 14: Case StudiesTap\nA Tap is responsible for the “how” and “where” parts of accessing data. For exam-\nple, is the data on HDFS or the local filesystem? In Amazon S3 or over HTTP?\nScheme\nA Scheme is responsible for reading raw data and converting it to a Tuple and/or\nwriting a Tuple out into raw data, where this “raw” data can be lines of text, Ha-\ndoop binary sequence files, or some proprietary format.\nNote that Taps are not part of a pipe assembly, and so they are not a type of Pipe.\nBut they are connected with pipe assemblies when they are made cluster-executable.\nWhen a pipe assembly is connected with the necessary number of source and sink Tap\ninstances, we get a Flow. A Flow is created when a pipe assembly is connected with its\nrequired number of source and sink taps, and the Taps either emit or capture the field\nnames the pipe assembly expects. That is, if a Tap emits a Tuple with the field name\n“line” (by reading data from a file on HDFS), the head of the pipe assembly must be\nexpecting a “line” value as well. Otherwise, the process that connects the pipe assembly\nwith the Taps will immediately fail with an error.\nSo pipe assemblies are really data process definitions, and are not “executable” on their\nown. They must be connected to source and sink Tap instances before they can run on\na cluster. This separation between Taps and pipe assemblies is part of what makes\nCascading so powerful.\nIf you think of pipe assemblies like a Java class, then a Flow is like a Java Object instance\n(Figure 14-17). That is, the same pipe assembly can be “instantiated” many times into\nnew Flows, in the same application, without fear of any interference between them.\nThis allows pipe assemblies to be created and shared like standard Java libraries.\nFigure 14-17. A Flow\nCascading | 453Cascading in Practice\nNow that we know what Cascading is and have a good idea how it works, what does\nan application written in Cascading look like? See Example 14-2.\nExample 14-2. Word count and sort\nScheme sourceScheme =\nnew TextLine(new Fields(""line""));\nTap source =\nnew Hfs(sourceScheme, inputPath);\nScheme sinkScheme = new TextLine();\nTap sink =\nnew Hfs(sinkScheme, outputPath, SinkMode.REPLACE);\nPipe assembly = new Pipe(""wordcount"");\nString regexString = ""(?<!\\\\pL)(?=\\\\pL)[^ ]*(?<=\\\\pL)(?!\\\\pL)"";\nFunction regex = new RegexGenerator(new Fields(""word""), regexString);\nassembly =\nnew Each(assembly, new Fields(""line""), regex);\nassembly =\nnew GroupBy(assembly, new Fields(""word""));\nAggregator count = new Count(new Fields(""count""));\nassembly = new Every(assembly, count);\nassembly =\nnew GroupBy(assembly, new Fields(""count""), new Fields(""word""));\nFlowConnector flowConnector = new FlowConnector();\nFlow flow =\nflowConnector.connect(""word-count"", source, sink, assembly);\nflow.complete();\nWe create a new Scheme that can read simple text files, and emits a new Tuple for\neach line in a field named “line,” as declared by the Fields instance.\nWe create a new Scheme that can write simple text files, and expects a Tuple with any\nnumber of fields/values. If more than one value, they will be tab-delimited in the\noutput file.\nWe create source and sink Tap instances that reference the input file and output\ndirectory, respectively. The sink Tap will overwrite any file that may already exist.\nWe construct the head of our pipe assembly, and name it “wordcount.” This name\nis used to bind the source and sink taps to the assembly. Multiple heads or tails\nwould require unique names.\nWe construct an Each pipe with a function that will parse the “line” field into a new\nTuple for each word encountered.\n454 | Chapter 14: Case StudiesWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\nvalue in the field “word.”\nWe construct an Every pipe with an Aggregator that will count the number of\nTuples in every unique word group. The result is stored in a field named “count.”\nWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\nvalue in the field “count,” and secondary sort each value in the field “word.” The\nresult will be a list of “count” and “word” values with “count” sorted in increasing\norder.\nWe connect the pipe assembly to its sources and sinks into a Flow, and then execute\nthe Flow on the cluster.\nIn the example, we count the words encountered in the input document, and we sort\nthe counts in their natural order (ascending). And if some words have the same “count”\nvalue, these words are sorted in their natural order (alphabetical).\nOne obvious problem with this example is that some words might have uppercase\nletters; for example, “the” and “The” when the word comes at the beginning of a sen-\ntence. So we might decide to insert a new operation to force all the words to\nlowercase, but we realize that all future applications that need to parse words from\ndocuments should have the same behavior, so we decide to create a reusable pipe\nSubAssembly, just like we would by creating a subroutine in a traditional application\n(see Example 14-3).\nExample 14-3. Creating a SubAssembly\npublic class ParseWordsAssembly extends SubAssembly\n{\npublic ParseWordsAssembly(Pipe previous)\n{\nString regexString = ""(?<!\\\\pL)(?=\\\\pL)[^ ]*(?<=\\\\pL)(?!\\\\pL)"";\nFunction regex = new RegexGenerator(new Fields(""word""), regexString);\nprevious = new Each(previous, new Fields(""line""), regex);\nString exprString = ""word.toLowerCase()"";\nFunction expression =\nnew ExpressionFunction(new Fields(""word""), exprString, String.class);\nprevious = new Each(previous, new Fields(""word""), expression);\n}\nsetTails(previous);\n}\nWe subclass the SubAssembly class, which is itself a kind of Pipe.\nWe create a Java expression function that will call toLowerCase() on the String value\nin the field named “word.” We must also pass in the Java type the expression expects\n“word” to be, in this case, String. (http://www.janino.net/ is used under the covers.)\nCascading | 455We must tell the SubAssembly superclass where the tail ends of our pipe subassembly\nare.\nFirst, we create a SubAssembly pipe to hold our “parse words” pipe assembly. Since this\nis a Java class, it can be reused in any other application, as long as there is an incoming\nfield named “word” (Example 14-4). Note that there are ways to make this function\neven more generic, but they are covered in the Cascading User Guide.\nExample 14-4. Extending word count and sort with a SubAssembly\nScheme sourceScheme = new TextLine(new Fields(""line""));\nTap source = new Hfs(sourceScheme, inputPath);\nScheme sinkScheme = new TextLine(new Fields(""word"", ""count""));\nTap sink = new Hfs(sinkScheme, outputPath, SinkMode.REPLACE);\nPipe assembly = new Pipe(""wordcount"");\nassembly =\nnew ParseWordsAssembly(assembly);\nassembly = new GroupBy(assembly, new Fields(""word""));\nAggregator count = new Count(new Fields(""count""));\nassembly = new Every(assembly, count);\nassembly = new GroupBy(assembly, new Fields(""count""), new Fields(""word""));\nFlowConnector flowConnector = new FlowConnector();\nFlow flow = flowConnector.connect(""word-count"", source, sink, assembly);\nflow.complete();\nWe replace the Each from the previous example with our ParseWordsAssembly pipe.\nFinally, we just substitute in our new SubAssembly right where the previous Every and\nword parser function was used in the previous example. This nesting can continue as\ndeep as necessary.\nFlexibility\nTake a step back and see what this new model has given us. Or better yet, what it has\ntaken away.\nYou see, we no longer think in terms of MapReduce jobs, or Mapper and Reducer\ninterface implementations, and how to bind or link subsequent MapReduce jobs to the\nones that precede them. During runtime, the Cascading “planner” figures out the op-\ntimal way to partition the pipe assembly into MapReduce jobs, and manages the link-\nages between them (Figure 14-18).\n456 | Chapter 14: Case StudiesFigure 14-18. How a Flow translates to chained MapReduce jobs\nBecause of this, developers can build applications of arbitrary granularity. They can\nstart with a small application that just filters a logfile, but then can iteratively build up\nmore features into the application as needed.\nSince Cascading is an API and not a syntax like strings of SQL, it is more flexible. First\noff, developers can create domain-specific languages (DSLs) using their favorite lan-\nguage, like Groovy, JRuby, Jython, Scala, and others (see the project site for examples).\nSecond, developers can extend various parts of Cascading, like allowing custom Thrift\nor JSON objects to be read and written to and allowing them to be passed through the\nTuple stream.\nHadoop and Cascading at ShareThis\nShareThis is a sharing network that makes it simple to share any online content. With\nthe click of a button on a web page or browser plug-in, ShareThis allows users to\nseamlessly access their contacts and networks from anywhere online and share the\ncontent through email, IM, Facebook, Digg, mobile SMS, etc. without ever leaving the\ncurrent page. Publishers can deploy the ShareThis button to tap the service’s universal\nsharing capabilities to drive traffic, stimulate viral activity, and track the sharing of\nonline content. ShareThis also simplifies social media services by reducing clutter on\nweb pages and providing instant distribution of content across social networks, affiliate\ngroups, and communities.\nAs ShareThis users share pages and information through the online widgets, a contin-\nuous stream of events enter the ShareThis network. These events are first filtered and\nprocessed, and then handed to various backend systems, including AsterData,\nHypertable, and Katta.\nThe volume of these event can be huge, too large to process with traditional systems.\nThis data can also be very “dirty” thanks to “injection attacks” from rogue systems,\nbrowser bugs, or faulty widgets. For this reason, ShareThis chose to deploy Hadoop as\nthe preprocessing and orchestration frontend to their backend systems. They also chose\nto use Amazon Web Services to host their servers, on the Elastic Computing Cloud\nCascading | 457(EC2), and provide long term storage, on the Simple Storage Service (S3), with an eye\ntoward leveraging Elastic MapReduce (EMR).\nIn this overview, we will focus on the “log processing pipeline” (Figure 14-19). The log\nprocessing pipeline simply takes data stored in an S3 bucket, processes it (described\nshortly), and stores the results back into another bucket. Simple Queue Service (SQS)\nis used to coordinate the events that mark the start and completion of data processing\nruns. Downstream other processes pull data that load AsterData, pull URL lists from\nHypertable to source a web crawl, or pull crawled page data to create Lucene indexes\nfor use by Katta. Note that Hadoop is central to the ShareThis architecture. It is used\nto coordinate the processing and movement of data between architectural components.\nFigure 14-19. The ShareThis log processing pipeline\nWith Hadoop as the frontend, all the event logs can be parsed, filtered, cleaned, and\norganized by a set of rules before ever being loaded into the AsterData cluster or used\nby any other component. AsterData is a clustered data warehouse that can support\nlarge datasets and allow for complex ad hoc queries using a standard SQL syntax.\nShareThis chose to clean and prepare the incoming datasets on the Hadoop cluster and\nthen to load that data into the AsterData cluster for ad hoc analysis and reporting.\nThough possible with AsterData, it made a lot of sense to use Hadoop as the first stage\nin the processing pipeline to offset load on the main data warehouse.\nCascading was chosen as the primary data processing API to simplify the development\nprocess, codify how data is coordinated between architectural components, and pro-\nvide the developer-facing interface to those components. This represents a departure\nfrom more “traditional” Hadoop use cases, which essentially just query stored data.\nInstead, Cascading and Hadoop together provide better and simpler structure to the\ncomplete solution, end-to-end, and thus more value to the users.\n458 | Chapter 14: Case StudiesFor developers, Cascading made it easy to start with a simple unit test (by subclassing\ncascading.ClusterTestCase) that did simple text parsing and then to layer in more\nprocessing rules while keeping the application logically organized for maintenance.\nCascading aided this organization in a couple of ways. First, standalone operations\n(Functions, Filters, etc.) could be written and tested independently. Second, the ap-\nplication was segmented into stages: one for parsing, one for rules, and a final stage for\nbinning/collating the data, all via the SubAssembly base class described earlier.\nThe data coming from the ShareThis loggers looks a lot like Apache logs with date/\ntimestamps, share URLs, referrer URLs, and a bit of metadata. To use the data for\nanalysis downstream the URLs needed to be un-packed (parsing query-string data,\ndomain names, etc.). So a top-level SubAssembly was created to encapsulate the parsing,\nand child SubAssemblies were nested inside to handle specific fields if they were suf-\nficiently complex to parse.\nThe same was done for applying rules. As every Tuple passed through the rules SubAs\nsembly, it was marked as “bad” if any of the rules were triggered. Along with the “bad”\ntag, a description of why the record was bad was added to the Tuple for later review.\nFinally, a splitter SubAssembly was created to do two things. First, to allow for the\ntuple stream to split into two, one stream for “good” data and one for “bad” data.\nSecond, the splitter binned the data into intervals, such as every hour. To do this, only\ntwo operations were necessary. The first to create the interval from the timestamp value\nalready present in the stream, and the second to use the interval and good/bad metadata\nto create a directory path; for example, “05/good/” where “05” is 5am and “good”\nmeans the tuple passed all the rules. This path would then be used by the Cascading\nTemplateTap, a special Tap that can dynamically output tuple streams to different\nlocations based on values in the Tuple. In this case, the TemplateTap used the “path”\nvalue to create the final output path.\nThe developers also created a fourth SubAssembly, this one to apply Cascading Asser-\ntions during unit testing. These assertions double-checked that rules and parsing Sub-\nAssemblies did their job.\nIn the unit test in Example 14-5, we see the splitter isn’t being tested, but it is added in\nanother integration test not shown.\nExample 14-5. Unit testing a Flow\npublic void testLogParsing() throws IOException\n{\nHfs source = new Hfs(new TextLine(new Fields(""line"")), sampleData);\nHfs sink =\nnew Hfs(new TextLine(), outputPath + ""/parser"", SinkMode.REPLACE);\nPipe pipe = new Pipe(""parser"");\n// split ""line"" on tabs\npipe = new Each(pipe, new Fields(""line""), new RegexSplitter(""\\t""));\nCascading | 459pipe = new LogParser(pipe);\npipe = new LogRules(pipe);\n// testing only assertions\npipe = new ParserAssertions(pipe);\nFlow flow = new FlowConnector().connect(source, sink, pipe);\nflow.complete(); // run the test flow\n// verify there are 98 tuples, 2 fields, and matches the regex pattern\n// for TextLine schemes the tuples are { ""offset"", ""line }\nvalidateLength(flow, 98, 2, Pattern.compile(""^[0-9]+(\\\\t[^\\\\t]*){19}$""));\n}\nFor integration and deployment, many of the features built into Cascading allowed for\neasier integration with external systems and for greater process tolerance.\nIn production, all the SubAssemblies are joined and planned into a Flow, but instead\nof just source and sink Taps, trap Taps were planned in (Figure 14-20). Normally when\nan operation throws an exception from a remote Mapper or Reducer task, the Flow\nwill fail and kill all its managed MapReduce jobs. When a Flow has traps, any excep-\ntions are caught and the data causing the exception is saved to the Tap associated with\nthe current trap. Then the next Tuple is processed without stopping the Flow. Sometimes\nyou want your Flows to fail on errors, but in this case, the ShareThis developers knew\nthey could go back and look at the “failed” data and update their unit tests while the\nproduction system kept running. Losing a few hours of processing time was worse than\nlosing a couple of bad records.\nUsing Cascading’s event listeners, Amazon SQS could be integrated. When a Flow\nfinishes, a message is sent to notify other systems that there is data ready to be picked\nup from Amazon S3. On failure, a different message is sent, alerting other processes.\nFigure 14-20. The ShareThis log processing Flow\n460 | Chapter 14: Case StudiesThe remaining downstream processes pick up where the log processing pipeline leaves\noff on different independent clusters. The log processing pipeline today runs once a\nday, so there is no need to keep a 100-node cluster sitting around for the 23 hours it\nhas nothing to do. So it is decommissioned and recommissioned 24 hours later.\nIn the future, it would be trivial to increase this interval on smaller clusters to every 6\nhours, or 1 hour, as the business demands. Independently, other clusters are booting\nand shutting down at different intervals based on the needs of the business unit re-\nsponsible for that component. For example, the web crawler component (using Bixo,\na Cascading-based web-crawler toolkit developed by EMI and ShareThis) may run\ncontinuously on a small cluster with a companion Hypertable cluster. This on-demand\nmodel works very well with Hadoop where each cluster can be tuned for the kind of\nworkload it is expected to handle.\nSummary\nHadoop is a very powerful platform for processing and coordinating the movement of\ndata across various architectural components. Its only drawback is that the primary\ncomputing model is MapReduce.\nCascading aims to help developers build powerful applications quickly and simply,\nthrough a well-reasoned API, without needing to think in MapReduce, while leaving\nthe heavy lifting of data distribution, replication, distributed process management, and\nliveness to Hadoop.\nRead more about Cascading, join the online community, and download sample appli-\ncations by visiting the project website at http://www.cascading.org/.\n—Chris K Wensel\nTeraByte Sort on Apache Hadoop\nThis article is reproduced from the http://sortbenchmark.org/YahooHadoop.pdf, which\nwas written in May 2008. Jim Gray and his successors define a family of benchmarks to\nfind the fastest sort programs every year. TeraByte Sort and other sort benchmarks are\nlisted with winners over the years at http://sortbenchmark.org/. In April 2009, Arun\nMurthy and I won the minute sort (where the aim is to sort as much data as possible in\nunder one minute) by sorting 500 GB in 59 seconds on 1,406 Hadoop nodes. We also\nsorted a terabyte in 62 seconds on the same cluster. The cluster we used in 2009 was\nsimilar to the hardware listed below, except that the network was much better with only\n2-to-1 oversubscription between racks instead of 5-to-1 in the previous year. We also used\nLZO compression on the intermediate data between the nodes. We also sorted a petabyte\n(1015 bytes) in 975 minutes on 3,658 nodes, for an average rate of 1.03 TB/minute. See\nhttp://developer.yahoo.net/blogs/hadoop/2009/05/hadoop_sorts_a_petabyte_in_162\n.html for more details about the 2009 results.\nTeraByte Sort on Apache Hadoop | 461Apache Hadoop is an open source software framework that dramatically simplifies\nwriting distributed data-intensive applications. It provides a distributed filesystem,\nwhich is modelled after the Google File System,* and a MapReduce† implementation\nthat manages distributed computation. Since the primary primitive of MapReduce is a\ndistributed sort, most of the custom code is glue to get the desired behavior.\nI wrote three Hadoop applications to run the terabyte sort:\n1. TeraGen is a MapReduce program to generate the data.\n2. TeraSort samples the input data and uses MapReduce to sort the data into a total\norder.\n3. TeraValidate is a MapReduce program that validates the output is sorted.\nThe total is around 1,000 lines of Java code, which will be checked in to the Hadoop\nexample directory.\nTeraGen generates output data that is byte-for-byte equivalent to the C version including\nthe newlines and specific keys. It divides the desired number of rows by the desired\nnumber of tasks and assigns ranges of rows to each map. The map jumps the random\nnumber generator to the correct value for the first row and generates the following rows.\nFor the final run, I configured TeraGen to use 1,800 tasks to generate a total of 10 billion\nrows in HDFS, with a block size of 512 MB.\nTeraSort is a standard MapReduce sort, except for a custom partitioner that uses a\nsorted list of N−1 sampled keys that define the key range for each reduce. In particular,\nall keys such that sample[i−1] <= key < sample[i] are sent to reduce i. This guarantees\nthat the output of reduce i are all less than the output of reduce i+1. To speed up the\npartitioning, the partitioner builds a two-level trie that quickly indexes into the list of\nsample keys based on the first two bytes of the key. TeraSort generates the sample keys\nby sampling the input before the job is submitted and writing the list of keys into HDFS.\nI wrote an input and output format, which are used by all 3 applications, that read and\nwrite the text files in the right format. The output of the reduce has replication set to\n1, instead of the default 3, because the contest does not require the output data be\nreplicated on to multiple nodes. I configured the job with 1,800 maps and 1,800 reduces\nand io.sort.mb, io.sort.factor, fs.inmemory.size.mb, and task heap size sufficient that\ntransient data was never spilled to disk other at the end of the map. The sampler used\n100,000 keys to determine the reduce boundaries, although as can be seen in Fig-\nure 14-21, the distribution between reduces was hardly perfect and would benefit from\nmore samples. You can see the distribution of running tasks over the job run in Fig-\nure 14-22.\n* S. Ghemawat, H. Gobioff, and S.-T. Leung. “The Google File System.” In 19th Symposium on Operating\nSystems Principles (October 2003), Lake George, NY: ACM.\n† J. Dean and S. Ghemawat. “MapReduce: Simplified Data Processing on Large Clusters.” In Sixth Symposium\non Operating System Design and Implementation (December 2004), San Francisco, CA.\n462 | Chapter 14: Case StudiesFigure 14-21. Plot of reduce output size versus finish time\nFigure 14-22. Number of tasks in each phase across time\nTeraByte Sort on Apache Hadoop | 463TeraValidate ensures that the output is globally sorted. It creates one map per file in\nthe output directory, and each map ensures that each key is less than or equal to the\nprevious one. The map also generates records with the first and last keys of the file, and\nthe reduce ensures that the first key of file i is greater than the last key of file i−1. Any\nproblems are reported as output of the reduce with the keys that are out of order.\nThe cluster I ran on was:\n•\n•\n•\n•\n•\n•\n•\n•\n•\n910 nodes\n2 quad core Xeons at 2.0ghz per a node\n4 SATA disks per a node\n8 G RAM per a node\n1 gigabit Ethernet on each node\n40 nodes per a rack\n8 gigabit Ethernet uplinks from each rack to the core\nRed Hat Enterprise Linux Server release 5.1 (kernel 2.6.18)\nSun Java JDK 1.6.0_05-b13\nThe sort completed in 209 seconds (3.48 minutes). I ran Hadoop trunk (pre-0.18.0) with\npatches for HADOOP-3443 and HADOOP-3446, which were required to remove in-\ntermediate writes to disk. Although I had the 910 nodes mostly to myself, the network\ncore was shared with another active 2,000-node cluster, so the times varied a lot de-\npending on the other activity.\n—Owen O’Malley, Yahoo!\n464 | Chapter 14: Case StudiesAPPENDIX A\nInstalling Apache Hadoop\nIt’s easy to install Hadoop on a single machine to try it out. (For installation on a cluster,\nplease refer to Chapter 9.) The quickest way is to download and run a binary release\nfrom an Apache Software Foundation Mirror.\nIn this appendix, we cover how to install Hadoop Core, HDFS, and MapReduce. In-\nstructions for installing Pig, HBase, and ZooKeeper are included in the relevant chapter\n(Chapters 11, 12, and 13).\nPrerequisites\nHadoop is written in Java, so you will need to have Java installed on your machine,\nversion 6 or later. Sun's JDK is the one most widely used with Hadoop, although others\nhave been reported to work.\nHadoop runs on Unix and on Windows. Linux is the only supported production plat-\nform, but other flavors of Unix (including Mac OS X) can be used to run Hadoop for\ndevelopment. Windows is only supported as a development platform, and additionally\nrequires Cygwin to run. During the Cygwin installation process, you should include\nthe openssh package if you plan to run Hadoop in pseudo-distributed mode (see fol-\nlowing explanation).\nInstallation\nStart by deciding which user you’d like to run Hadoop as. For trying out Hadoop or\ndeveloping Hadoop programs, it is simplest to run Hadoop on a single machine using\nyour own user account.\nDownload a stable release, which is packaged as a gzipped tar file, from the Apache\nHadoop releases page (http://hadoop.apache.org/core/releases.html) and unpack it\nsomewhere on your filesystem:\n% tar xzf hadoop-x.y.z.tar.gz\n465Before you can run Hadoop, you need to tell it where Java is located on your system.\nIf you have the JAVA_HOME environment variable set to point to a suitable Java installa-\ntion, that will be used, and you don’t have to configure anything further. Otherwise,\nyou can set the Java installation that Hadoop uses by editing conf/hadoop-env.sh, and\nspecifying the JAVA_HOME variable. For example, on my Mac I changed the line to read:\nexport JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6.0/Home\nto point to version 1.6.0 of Java. On Ubuntu, the equivalent line is:\nexport JAVA_HOME=/usr/lib/jvm/java-6-sun\nIt’s very convenient to create an environment variable that points to the Hadoop in-\nstallation directory (HADOOP_INSTALL, say) and to put the Hadoop binary directory on\nyour command-line path. For example:\n% export HADOOP_INSTALL=/home/tom/hadoop-x.y.z\n% export PATH=$PATH:$HADOOP_INSTALL/bin\nCheck that Hadoop runs by typing:\n% hadoop version\nHadoop 0.20.0\nSubversion https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.20 -r 763504\nCompiled by ndaley on Thu Apr 9 05:18:40 UTC 2009\nConfiguration\nEach component in Hadoop is configured using an XML file. Core properties go in\ncore-site.xml, HDFS properties go in hdfs-site.xml, and MapReduce properties go in\nmapred-site.xml. These files are all located in the conf subdirectory.\nIn earlier versions of Hadoop, there was a single site configuration file\nfor the Core, HDFS, and MapReduce components, called hadoop-\nsite.xml. From release 0.20.0 onward this file has been split into three:\none for each component. The property names have not changed, just\nthe configuration file they have to go in. You can see the default settings\nfor all the properties that are governed by these configuration files by\nlooking in the docs directory of your Hadoop installation for HTML files\ncalled core-default.html, hdfs-default.html, and mapred-default.html.\nHadoop can be run in one of three modes:\nStandalone (or local) mode\nThere are no daemons running and everything runs in a single JVM. Standalone\nmode is suitable for running MapReduce programs during development, since it\nis easy to test and debug them.\n466 | Appendix A: Installing Apache HadoopPseudo-distributed mode\nThe Hadoop daemons run on the local machine, thus simulating a cluster on a\nsmall scale.\nFully distributed mode\nThe Hadoop daemons run on a cluster of machines. This setup is described in\nChapter 9.\nTo run Hadoop in a particular mode, you need to do two things: set the appropriate\nproperties, and start the Hadoop daemons. Table A-1 shows the minimal set of prop-\nerties to configure each mode. In standalone mode, the local filesystem and the local\nMapReduce job runner are used, while in the distributed modes the HDFS and Map-\nReduce daemons are started.\nTable A-1. Key configuration properties for different modes\nComponent Property Standalone Pseudo-distributed Fully distributed\nCore fs.default.name file:/// (default) hdfs://localhost/ hdfs://namenode/\nHDFS dfs.replication N/A 1 3 (default)\nMapReduce mapred.job.tracker local (default) localhost:8021 jobtracker:8021\nYou can read more about configuration in “Hadoop Configuration” on page 251.\nStandalone Mode\nIn standalone mode, there is no further action to take, since the default properties are\nset for standalone mode, and there are no daemons to run.\nPseudo-Distributed Mode\nThe configuration files should be created with the following contents, and placed in\nthe conf directory (although you can place configuration files in any directory as long\nas you start the daemons with the --config option).\n<?xml version=""1.0""?>\n<!-- core-site.xml -->\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://localhost/</value>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- hdfs-site.xml -->\n<configuration>\n<property>\n<name>dfs.replication</name>\n<value>1</value>\nConfiguration | 467</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- mapred-site.xml -->\n<configuration>\n<property>\n<name>mapred.job.tracker</name>\n<value>localhost:8021</value>\n</property>\n</configuration>\nConfiguring SSH\nIn pseudo-distributed mode, we have to start daemons, and to do that, we need to have\nSSH installed. Hadoop doesn’t actually distinguish between pseudo-distributed and\nfully distributed modes: it merely starts daemons on the set of hosts in the cluster\n(defined by the slaves file) by SSH-ing to each host and starting a daemon process.\nPseudo-distributed mode is just a special case of fully distributed mode in which the\n(single) host is localhost, so we need to make sure that we can SSH to localhost and log\nin without having to enter a password.\nFirst, make sure that SSH is installed and a server is running. On Ubuntu, for example,\nthis is achieved with:\n% sudo apt-get install ssh\nOn Windows with Cygwin, you can set up an SSH server (after having\ninstalled the openssh package) by running ssh-host-config -y.\nThen to enable password-less login, generate a new SSH key with an empty passphrase:\n% ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n% cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nTest this with:\n% ssh localhost\nYou should be logged in without having to type a password.\nFormatting the HDFS filesystem\nBefore it can be used, a brand-new HDFS installation needs to be formatted. The for-\nmatting process creates an empty filesystem by creating the storage directories and the\ninitial versions of the namenode’s persistent data structures. Datanodes are not in-\nvolved in the initial formatting process, since the namenode manages all of the filesys-\ntem’s metadata, and datanodes can join or leave the cluster dynamically. For the same\nreason, you don’t need to say how large a filesystem to create, since this is determined\n468 | Appendix A: Installing Apache Hadoopby the number of datanodes in the cluster, which can be increased as needed, long after\nthe filesystem was formatted.\nFormatting HDFS is quick to do. Just type the following:\n% hadoop namenode -format\nStarting and stopping the daemons\nTo start the HDFS and MapReduce daemons, type:\n% start-dfs.sh\n% start-mapred.sh\nIf you have placed configuration files outside the default conf directory,\nstart the daemons with the --config option, which takes an absolute\npath to the configuration directory:\n% start-dfs.sh --config path-to-config-directory\n% start-mapred.sh --config path-to-config-directory\nThree daemons will be started on your local machine: a namenode, a secondary name-\nnode, and a datanode. You can check whether the daemons started successfully by\nlooking at the logfiles in the logs directory (in the Hadoop installation directory), or by\nlooking at the web UIs, at http://localhost:50030/ for the jobtracker, and at http://lo-\ncalhost:50070/ for the namenode. You can also use Java’s jps command to see whether\nthey are running.\nStopping the daemons is done in the obvious way:\n% stop-dfs.sh\n% stop-mapred.sh\nFully Distributed Mode\nSetting up a cluster of machines brings many additional considerations, so this mode\nis covered in Chapter 9.\nConfiguration | 469APPENDIX B\nCloudera’s Distribution for Hadoop\nby Matt Massie and Todd Lipcon, Cloudera\nCloudera’s Distribution for Hadoop is based on the most recent stable version of\nApache Hadoop with numerous patches, backports, and updates. Cloudera shares this\ndistribution in a number of different formats: compressed tar files, RPMs, Debian\npackages, and Amazon EC2 AMIs. Cloudera’s Distribution for Hadoop is free, released\nunder the Apache 2.0 license and available at http://www.cloudera.com/hadoop/.\nCloudera has an online configurator at http://www.cloudera.com/configurator to make\nsetting up a Hadoop cluster easy (Figure B-1). The configurator has a simple wizard-\nlike interface that asks targeted questions about your cluster. When you’ve finished,\nthe configurator generates customized Hadoop packages and places them in a package\nrepository for you. You can manage any number of clusters and return at a later time\nto update your active configurations.\nTo simplify package management, Cloudera shares RPMs from a yum repository and\nDebian packages from an apt repository. Cloudera’s Distribution for Hadoop enables\nyou to install and configure Hadoop on each machine in your cluster by running a\nsingle, simple command. Kickstart users benefit even more by being able to commission\nentire Hadoop clusters automatically without any manual intervention.\nPrerequisites\nCloudera’s Distribution for Hadoop requires Sun Java 6 or later to be installed. The\nSun Java Debian and RPM packages require that you agree to the Sun license before\nuse. For a Debian-based system, you will want to enable the non-free apt repository,\nas it contains the sun-java6-* packages. For a Red Hat–based system, download the\nSun Java RPM package from http://java.sun.com/javase/downloads/.\nBefore you can use your favorite package manager (e.g., yum, apt-get, aptitude) to\ninstall Cloudera packages, you’ll need to add the Cloudera repositories to your list of\nyum and/or apt sources.\n471Figure B-1. Cloudera’s on-line configurator makes it easy to set up a Hadoop cluster\nPlease refer to http://www.cloudera.com/hadoop/ for up-to-date instructions on the sim-\nplest way to satisfy these prerequisites.\n472 | Appendix B: Cloudera’s Distribution for HadoopStandalone Mode\nTo install Hadoop standalone mode, run the following command on Red Hat–based\nsystems:\n% yum install hadoop\nOr on Debian-based systems, run the command:\n% apt-get install hadoop\nThe hadoop package include a man page. To read the man page, run the command:\n% man hadoop\nIf you want to install the full Hadoop documentation on a machine, install the hadoop-\ndocs package. On Red Hat–based systems, run the command:\n% yum install hadoop-docs\nTo install the documentation on Debian-based systems, run the command:\n% apt-get install hadoop-doc\nPseudo-Distributed Mode\nTo install Hadoop in pseudo-distributed mode, run the following command on Red\nHat–based systems:\n% yum install hadoop-conf-pseudo\nOr on Debian-based systems, run the command:\n% apt-get install hadoop-conf-pseudo\nOnce you’ve installed the Hadoop pseudo-distributed configuration package, you start\nthe Hadoop services by running the same command on both Red Hat– and Debian-\nbased systems:\n% for x in namenode secondarynamenode datanode jobtracker tasktracker ;\ndo /etc/init.d/hadoop-$x start ; done\nThere is no need to worry about creating a hadoop user or formatting HDFS, as that is\nhandled automatically by the hadoop-conf-pseudo package. You can use Hadoop im-\nmediately after installing the package and starting the Hadoop services. The hadoop-\nconf-pseudo package will also ensure that your Hadoop services are started at system\nboot.\nFully Distributed Mode\nFor details about deploying a fully distributed Hadoop cluster, visit Cloudera’s Distri-\nbution for Hadoop web page at http://www.cloudera.com/hadoop/.\nFully Distributed Mode | 473When you run Cloudera’s online configurator, it creates a personalized apt or yum re-\npository to hold the configuration packages for every cluster you manage. For example,\nlet’s say you gave one of your clusters the name mycluster. To see a list of all the con-\nfiguration packages for mycluster, run the following command on Red Hat–based\nsystems:\n% yum search hadoop-conf-mycluster\nor on Debian-based systems, run the command:\n% apt-cache search hadoop-conf-mycluster\nThese commands will return a list of configuration packages for the mycluster cluster.\nThe number and types of configuration packages depends on how you answered the\nquestions posed by the Cloudera configurator. Some of the packages will be generated\nfor specific hosts in your cluster; others will be for groups or classes of machines in\nyour cluster. For host-specific configurations, the fully qualified hostname will be\nadded to the package name. For example, there may be a configuration for myhost.mydo\nmain in the mycluster cluster. To install Hadoop on myhost.mydomain on Red Hat–based\nsystems, run the command:\n% yum install hadoop-conf-mycluster-myhost.mydomain\nor on Debian-based systems, run the command:\n% apt-get install hadoop-conf-mycluster-myhost.mydomain\nThe Hadoop configuration packages will ensure that your services are set up to run at\nsystem boot.\nHadoop-Related Packages\nCloudera’s Distribution for Hadoop allows you to easily deploy tools built on top of\nHadoop like Hive and Pig. Hive is a data warehouse infrastructure that allows you to\nquery data in Hadoop with a query language based on SQL. For more information on\nHive, see “Hadoop and Hive at Facebook” on page 414. Pig is a platform for analyzing\nlarge datasets using a high-level language; it is covered in Chapter 11.\nTo install Hive and Pig on Red Hat–based systems, run the command:\n% yum install hadoop-hive hadoop-pig\nTo install Hive and Pig on Debian-based systems, run the command:\n% apt-get install hadoop-hive hadoop-pig\nMore Hadoop-related packages will be added to Cloudera’s Distribution for Hadoop\nover time.\n474 | Appendix B: Cloudera’s Distribution for HadoopAPPENDIX C\nPreparing the NCDC Weather Data\nThis section gives a runthrough of the steps taken to prepare the raw weather data files\nso they are in a form that is amenable for analysis using Hadoop. If you want to get a\ncopy of the data to process using Hadoop, you can do so by following the instructions\ngiven at the website which accompanies this book at http://hadoopbook.com/. The rest\nof this section explains how the raw weather data files were processed.\nThe raw data is provided as a collection of tar files, compressed with bzip2. Each year\nof readings comes in a separate file. Here’s a partial directory listing of the files:\n1901.tar.bz2\n1902.tar.bz2\n1903.tar.bz2\n...\n2000.tar.bz2\nEach tar file contains a file for each weather station’s readings for the year, compressed\nwith gzip. (The fact that the files in the archive are compressed makes the bzip2 com-\npression on the archive itself redundant.) For example:\n% tar jxf 1901.tar.bz2\n% ls -l 1901 | head\n011990-99999-1950.gz\n011990-99999-1950.gz\n...\n011990-99999-1950.gz\nSince there are tens of thousands of weather stations, the whole dataset is made up of\nlarge number of relatively small files. It’s generally easier and more efficient to process\na smaller number of relatively large files in Hadoop (see “Small files and CombineFi-\nleInputFormat” on page 190), so in this case, I concatenated the decompressed files for\na whole year into a single file, named by the year. I did this using a MapReduce program,\nto take advantage of its parallel processing capabilities. Let’s take a closer look at the\nprogram.\n475The program has only a map function: no reduce function is needed since the map does\nall the file processing in parallel with no combine stage. The processing can be done\nwith a Unix script so the Streaming interface to MapReduce is appropriate in this case;\nsee Example C-1.\nExample C-1. Bash script to process raw NCDC data files and store in HDFS\n#!/usr/bin/env bash\n# NLineInputFormat gives a single line: key is offset, value is S3 URI\nread offset s3file\n# Retrieve file from S3 to local disk\necho ""reporter:status:Retrieving $s3file"" >&2\n$HADOOP_INSTALL/bin/hadoop fs -get $s3file .\n# Un-bzip and un-tar the local file\ntarget=`basename $s3file .tar.bz2`\nmkdir -p $target\necho ""reporter:status:Un-tarring $s3file to $target"" >&2\ntar jxf `basename $s3file` -C $target\n# Un-gzip each station file and concat into one file\necho ""reporter:status:Un-gzipping $target"" >&2\nfor file in $target/*/*\ndo\ngunzip -c $file >> $target.all\necho ""reporter:status:Processed $file"" >&2\ndone\n# Put gzipped version into HDFS\necho ""reporter:status:Gzipping $target and putting in HDFS"" >&2\ngzip -c $target.all | $HADOOP_INSTALL/bin/hadoop fs -put - gz/$target.gz\nThe input is a small text file (ncdc_files.txt) listing all the files to be processed (the files\nstart out on S3, so the files are referenced using S3 URIs that Hadoop understands).\nHere is a sample:\ns3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2\ns3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2\n...\ns3n://hadoopbook/ncdc/raw/isd-2000.tar.bz2\nBy specifying the input format to be NLineInputFormat, each mapper receives one line\nof input, which contains the file it has to process. The processing is explained in the\nscript, but, briefly, it unpacks the bzip2 file, and then concatenates each station file into\na single file for the whole year. Finally, the file is gzipped and copied into HDFS. Note\nthe use of hadoop fs -put - to consume from standard input.\nStatus messages are echoed to standard error with a reporter:status prefix so that they\nget interpreted as a MapReduce status update. This tells Hadoop that the script is\nmaking progress, and is not hanging.\n476 | Appendix C: Preparing the NCDC Weather DataThe script to run the Streaming job is as follows:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-D mapred.reduce.tasks=0 \\\n-D mapred.map.tasks.speculative.execution=false \\\n-D mapred.task.timeout=12000000 \\\n-input ncdc_files.txt \\\n-inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \\\n-output output \\\n-mapper load_ncdc_map.sh \\\n-file load_ncdc_map.sh\nI set the number of reduce tasks to zero, since this is a map-only job. I also turned off\nspeculative execution so duplicate tasks didn’t write the same files (although the ap-\nproach discussed in “Task side-effect files” on page 173 would have worked, too). The\ntask timeout was set high so that Hadoop didn’t kill tasks that are taking a long time\n(for example, when unarchiving files, or copying to HDFS, when no progress is\nreported).\nLast, the files were archived on S3 by copying them from HDFS using distcp.\nPreparing the NCDC Weather Data | 477Index\nA\nack queue in HDFS, 67\nACLs (access control lists)\nfor Hadoop services, 264\nZooKeeper, 374, 383\npermissions, 384\nActiveKeyValueStore class (example), 391\nad hoc analysis and product feedback\n(hypothetical use case), 419\nadministration procedures, routine, 292–293\nadvertiser insights and performance\n(hypothetical use case), 418\naggregate functions, 321\nalgebraic functions, 321\naliases for relations, 306\nALL and ANY ALL groups, 338\nalter command (HBase), 349\nAmazon Elastic Compute Cloud (EC2), 269,\n458\nAmazon Simple Storage Service (S3), 458\nAmazon Web Services, Public Data Sets, 2\nanalysis of data, 3\nAnt, packaging program in JAR file, 132\nANY keyword, 338\nApache Commons Logging API, 142\nApache Hadoop, 465–469\nconfiguration, 466–469\nmodes and properties for, 466\npseudo-distributed mode, 467–469\nstandalone mode, 467\nhome page, xvi\ninstallation, 465\nprerequisites, 465\nTeraByte sort on, 462\nApache Hadoop project, 10, 12\nApache Lucene project, 9\nApache Nutch, 9\nApache Thrift services, 49\nApache ZooKeeper, 372\n(see also ZooKeeper)\nreleases page, 370\nAPIs in ZooKeeper, 381\narchive files, copying to tasks, 239\narchive tool, 72\narchives (see HAR files)\nArrayWritable class, 95\nAsterData, 457\nAstrometry.net project, 2\nasynchronous API in ZooKeeper, 381\natomic broadcast phase, 385\nattempt IDs, 133\naudit logging, 280\naudit logs (HDFS), 143\nauthorization, service-level, 264\nautofs tool, 250\nAvro, 12, 103\nawk, using to analyze weather data, 17\nB\nbackups of data, 292\nbad blocks, HBase using HDFS, 367\nbad_files directory, 77\nbag type, 316\nbalancer tool, 284, 293\nBaldeschwieler, Eric, 11\nbenchmarks, 267\nother widely used Hadoop benchmarks,\n269\nWe’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n479TeraByteSort and other sort benchmarks,\n461\nTestDFSIO, benchmarking HDFS, 267\nbinary input, 199\nbinary output, 203\nblacklisting of tasktracker, 161\nblock compression in sequence files, 109\nblocks, 43\nbenefits for distributed filesystem, 43\nBookKeeper logging service, 400\nbuffer size for I/O operations, 264\nburning a Hadoop cluster, 266\nBY clauses, JOIN or COGROUP statements,\n336\nbytearray type, 317, 325\nByteArrayOutputStream objects, 87\nBytes class, 357\nBytesWritable class, 94\nBzip2 compression, 78\nC\nC language, ZooKeeper client binding, 381\nC++\ncompiling and running MapReduce\nprogram, 38\nmap and reduce functions, 36\ncaching\nCodecPool class, 82\ndistributed cache mechanism, 239\nHBase Scanners, 361\nCafarella, Mike, 344\ncartesian product, 337\nCascading, 447–461\napplication example, word count and sort,\n454\ncreating a SubAssembly, 455\nextending word count and sort application\nwith a SubAssembly, 456\nfields, tuples, and pipes, 448\nflexibility for application development, 456\nHadoop and Cascading at ShareThis, 457–\n461\noperations, 451\nproject website, 461\nTaps, Schemes, and Flows, 452\ncascading.ClusterTestCase class, 459\ncase sensitivity (Pig Latin), 311\ncells, versioning in HBase, 344\nChainMapper class, 243\n480 | Index\nChainReducer class, 243\nchararray type, 305, 315\ntrimming whitespace from values, 326\ncheckpointing process for filesystem metadata,\n275\nChecksumFileSystem class, 77\nchecksums, 75\nclient side, 76\nChubby Lock Service, 385\nChukwa, 285\ndefined, 13\nclientPort property (ZooKeeper), 371\nCloudera’s Distribution for Hadoop, 471–474\nfully distributed mode, 473\non-line configurator, 471\npackages related to Hadoop, 474\nprerequisites, 471\npseudo-distributed mode, 473\nstandalone mode, 473\ncluster membership, properties for, 264\nclusters, 245–271\naddresses for namenode and jobtracker,\nhadoopcluster.xml file, 119\nbalancing, 71\nbenchmarking, 266–269\nHadoop configuration, 251–266\ndaemon addresses and ports, 263\nenvironment settings, 254–258\nimportant daemon properties, 258–263\nmanaging, 252\nother properties, 264\nHadoop in the cloud, 269\nHBase cluster members, 346\nHBase, configuration files, 346\nmini-clusters, 131\nnetwork topology, 247\nrack awareness, 248\nRAID and, 246\nrunning a job on, 132–145\ndebugging the job, 139–145\nlaunching the job, 132\nMapReduce web UI, 134–136\npackaging program as JAR file, 132\nretrieving results, 136\nrunning Hadoop on Amazon EC2, 269\nlaunching a cluster, 270\nMapReduce job, 271\nsetup, 270\nterminating a cluster, 271setup and installation, 249–251\ncreating Hadoop user, 250\ninstalling Hadoop, 250\ninstalling Java, 249\nspecification, 245\nSSH configuration, 251\nuser jobs as benchmarks, 269\nusername, setting for a cluster, 120\nCodd’s rules, 361\ncode examples from this book, download site,\nxvi\nCodecPool class, 82\ncodecs, 79–83\ncompression codec properties, 81\ninferring using CompressionCodecFactory,\n80\nCOGROUP statement, 335\njoin key in BY clause, 336\nusing combination of COGROUP, INNER,\nand FLATTEN, 335\ncoherency model (filesystem), 68\ncollections, Writable collection types, 95\ncolumn families (HBase), 344, 359\nCombineFileInputFormat class, 190\ncombiner functions, 29\nsetting in Streaming Ruby program, 35\nspecifying, 31\ncommand line\nrunning jobs from, helper classes for, 121–\n124\nrunning local job driver, 128\nZooKeeper tool, 377\nconf switch, 120\ncommands\nHBase shell, 349\nPig Latin, 313\nZooKeeper, 371\ncomments in Pig Latin, 310\ncommit logs, HBase regionservers, 347\ncommodity hardware, 42\nComparable interface, 88\ncomparators, 88\ncustom comparators in Java secondary sort\nprogram, 231\ncustom RawComparator, 100\nFirstComparator custom class (example),\n237\nKeyFieldBasedComparator, 232\nRawComparator class, 220\ncomparison functions, 321\nComparisonFunc class, 339\ncompletion, job, 158\nCompositeContext class, 289\nCompositeInputFormat class, 234\ncompression, 77–86\ncodecs, 79–83\ndetails in sequence files, 109\ndetermining which format to use, 84\ninput splits and, 83\nlisting of compression formats, 78\nmap output written to disk, 164\nnative libraries for, 81\nusing in MapReduce, 84\nCompressionCodec interface, 79\ncompressing and decompressing streams,\n79\nCompressionCodecFactory class, 80\ncompressors and decompressors, reusing, 82\nconf command line switch, 120\nlaunching cluster job, 132\nConfigUpdater class (example), 392\nConfigurable interface, 121\nconfiguration API, 116–118\ncombining resources to define, 117\nexample file (configuration-1.xml), 116\nvariable expansion, 118\nConfiguration class, 53, 116\nconfiguration files, 118–120\nHadoop, 251\nHadoop site configuration files, typical set,\n258–260\nHBase, 346\nzoo.cfg, 371\nconfiguration service (ZooKeeper), 391–394\nreliable service, 396–398\nconfiguration tuning, shuffle and sort, 166\nmap side, 167\nconfiguration, ZooKeeper in production, 402\nConfigured class, 121\nConfigWatcher class (example), 393\nconnection events, ZooKeeper, 374\nConnectionWatcher class (example), 375\nconsistency, ZooKeeper service, 386\ncontext objects, new Java MapReduce API, 25\ncontexts for metrics, 286\ncopy phase of reduce tasks, 164\nCore, 12\ncore-default.xml file, 117\nIndex | 481core-site.xml file, 117, 252\ncounters, 211–218\nbuilt-in, 211–213\nmetrics versus, 286\nspilled records, 166\nuser-defined Java counters, 213–217\ndynamic, 215\nMaxTemperatureWithCounters class\n(example), 213\nreadable names, 215\nretrieving, 216\nuser-defined Streaming counters, 218\nCounters class, 217\ncounting in MapReduce, 448\nCrawlDb (Nutch), 425, 427\nCRC-32 (cyclic redundancy check), 75\nCreateGroup objects, 373\nCROSS operator, 337\ncustom Writable, implementing, 96–101\ncustom comparators, 100\nimplementing RawComparator for speed,\n99\nCutLoadFunc (example UDF), 327–330\nCutting, Doug, 9, 11\nD\ndata analysis, 3\nhypothetical use case, 420\ndata backups, 292\ndata integrity, 75–77\nChecksumFileSystem, 77\nin HDFS, 75\nLocalFileSystem, 76\ndata locality, 7\ndata locality optimization, 28\ndata pipelines using Hive, 422\ndata processing operators (Pig), 331–340\ncombining and splitting data, 339\nfiltering data, 331–334\ngrouping and joining data, 334–338\nsorting data, 338\ndata queue, 67\ndata structures, file-based, 103–114\nMapFile class, 110–114\nSequenceFile class, 103–110\ndata types\nconfiguration properties, 116, 117\nJava primitives, ObjectWritable wrapper\nfor, 95\n482 | Index\nJava primitives, Writable wrappers for, 89\nleveraging in Pig filter UDF, 325\nMapReduce, 175–184\nconflicts in, 178\ndefault MapReduce job, 178\nPig Latin, 315\ndata, sources of, 1\ndata-local tasks, 156\ndatabase input and output, 201\ndatabases, 309\n(see also RDBMS)\ncomparison with Pig, 308\nstorage and retrieval of data, 343\nDataBlockScanner, 76\nDataByteArray class, 325\ndataDir property (ZooKeeper), 371, 401\ndataLogDir property (ZooKeeper), 401\ndatanodes, 44\nblock distribution over, balancing, 284\nblock scanner, 283\nclient reading data from, 63\ncommissioning, 294\ndecommissioning, 295\ndirectory structure, 277\npermitted to connect to namenode, 294\nrole in client file write to HDFS, 67\nrunning out of datanode threads, 366\nstorage of replicas on, 67\nverification of data, 76\nwriting of file to, visibility to other readers,\n69\nDataOutput interface, 88\nDataOutputStream objects, 87\nDataStreamer class, 67\nDataType class, 325\nDBInputFormat class, 201\nDBOutputFormat class, 201\ndebugging jobs, 139–145\nhandling malformed data, 143\nusing remote debugger, 144\nDEFINE operator, 324\ndelete operations in ZooKeeper, 381\nDESCRIBE operator, 306\ndeserialization\ndefined, 86\nexample, 88\nDeserializer objects, 101\ndevelopment environment, configuring, 118–\n124helper classes for running jobs from\ncommand line, 121–124\ndfs.block.size property, 265\ndfs.data.dir property, 260\ndfs.datanode.du.reserved property, 265\ndfs.datanode.http.address property, 264\ndfs.datanode.ipc property, 263\ndfs.hosts property, 264, 294\ndfs.http.address property, 71, 264\ndfs.name.dir property, 260\ndfs.permissions property, 47\ndfs.replication property, 45\ndfs.replication.min property, 67\ndfs.secondary.http.address property, 264\ndfsadmin tool, 280\nchecking progress of upgrade, 298\ncommands, listed, 280\nsafe mode commands, 279\nDFSClient class, bad blocks on HDFS and,\n367\nDFSInputStream class, 63\nDFSOutputStream class, 66\ndiagnostic operators (Pig Latin), 313\ndirectories\ncreating, 57\ndeleting, 62\nlisting in HDFS, 46\nspecified as input path, 187\ntemporary directory for MapReduce output,\n173\ndisable command (HBase shell), 349\ndistcp tool, 70, 271\ncluster balance and, 71\nusing for backups, 293\ndistributed cache, 239\nDistributedCache API, 242\nusing to share metadata file for station\nnames (example), 240–242\ndistributed computation, 6\ndistributed filesystems, 41\nDistributedFileSystem class, 47, 51, 63\n(see also FileSystem class)\nrole in client write to HDFS, 66\nsetVerifyChecksum( ) method, 76\ndistributive functions, 31\nDNSToSwitchMapping interface, 248\ndomain-specific languages (DSLs), 457\ndryrun option (Pig), 342\ndsh tool, 258\nDSLs (domain-specific languages), 457\ndump command, 371\nDUMP operator, 306\nDUMP statement, order and, 339\ndynamic counters, 215\ndynamic parameters, 342\nE\nedit log, HDFS, 260, 274\nenable command (HBase shell), 349\nensemble (ZooKeeper), 385\nenums, 215\nenvi command (ZooKeeper), 371\nenvironment properties, task, 172\nenvironment settings, 254–258\nJava, 256\nmemory, 254\nSSH, 257\nsystem logfiles, 256\nenvironment variables\nHADOOP_CLASSPATH, 24\nsetting for Makefile, C++ MapReduce\nprogram, 38\neval functions, 321\nUDF (user-defined function), 326–327\nEvalFunc class, 323\ngetArgToFuncMapping( ) method, 325\nEventType.NodeDataChanged, 393\nexceptions in ZooKeeper, 394\nInterruptedException, 394\nKeeperException, 395\nlocks and, 399\nexclude file, 295\nprecedence in HDFS, 295\nexists operation in ZooKeeper, 381\nsignature for, 381\nwatches on, 382\nexpressions (Pig Latin), 314\nF\nFacebook, Hadoop and Hive at, 414–417\nfailover, ZooKeeper service, 388\nfailures, 159–161\njobtracker, 161\npartial failure, distributed applications, 369\ntask, 160\nskipping bad records, 171\ntasktracker, 161\nIndex | 483Fair Scheduler, 162\nfair sharing for jobs, 424\nFetcher application, multi-threaded\nMapRunner, 435\nfetchlists (Nutch)\ndefined, 426\ngeneration of, 431–438\nfields, 449\nFieldSelectionMapReduce class, 243\nfile descriptors, running out of, 366\nfile mode, 46\nfile permissions, 47\nfile-based data structures (see data structures,\nfile-based)\nFileContext class, 287\nFileInputFormat class, 186\ncomputeSplitSize( ) method, 189\ninput splits, 188\nstatic methods to set JobConf's input paths,\n187\nFileOutputFormat class, 22, 23, 173\nfiles\ncopying local file to Hadoop filesystem and\nshowing progress, 56\ncopying to and from HDFS, 45\ndeleting, 62\nlisting on HDFS, 46\nparallel copying with distcp, 70\nprocessing whole file as a record, 192\nsmall, packaging in SequenceFile, 103\nworking with small files, using\nCombineFileInputFormat, 190\nFileStatus class, 58, 59\nfilesystem blocks (see blocks)\nfilesystem check (see fsck utility)\nFileSystem class, 47, 51–62\nconcrete implementations, listed, 47\ncreating directories, 57\ndeleting files or directories, 62\nexists( ) method, 59\ngetting file metadata in FileStatus object,\n58\nlisting files, 59\nmethods for processing globs, 60\nreading data from Hadoop URLs, 51\nreading data using, 52–55\nwriting data, 56\nfilesystem commands (Pig Latin), 314\nFilesystem in Userspace (FUSE), 50\n484 | Index\nFileSystemCat class (example), 53\nfilesystems, 41\ndefault Hadoop configuration for default\nfilesystem, 119\nHadoop, 47–51\nlisted, 47\nHBase persistence to, 346\nraw filesystem underlying FileSystem, 77\nZooKeeper as a filesystem, 381\nfilter functions, 321\nUDF (user-defined function), 322–326\nleveraging types, 325\nFILTER operator, 331\nFilterFunc class, 323\nfiltering, 331–334\ninput path, 188\nserver-side filtering in HBase, 360\nusing FOREACH ... GENERATE operator,\n331\nusing STREAM operator, 333\nfinal properties, 117\nFLATTEN expression, 335\nFlows, 453\ntranslation into chained MapReduce jobs,\n456\nflush files, HBase regions, 348\nfollowers (ZooKeeper), 385\nFOREACH ... GENERATE operator, 331\nFOREACH statements, 307\nfragment replicate join, 334\nfs and jt command line options, 128\nfs command, 46\nconf option, 120\ntext option, 108\ngetmerge option, 138\nfs.checkpoint.dir property, 261\nfs.default.name property, 260, 263\nfs.trash.interval property, 265\nfsck utility, 44, 281\nchecking HDFS upgrade, 298\nfinding blocks for a file, 283\nhandling files with corrupt or missing\nblocks, 282\nrunning before HDFS upgrades, 297\nrunning regularly for filesystem\nmaintenance, 293\nFSDataInputStream class, 54, 63\nimplementation of PositionedReadable\ninterface, 55FSDataOutputStream class, 57, 66\nsync( ) method, 69\nfsimage file, 274\nfsync system call in Unix, 69\nFTP interface to HDFS, 51\nFTPFileSystem interface, 47, 51\nfully-distributed mode, 467\ninstalling Cloudera’s Distribution for\nHadoop, 473\nFuncSpec objects, 325\nfunctions in Pig, 320\nbuilt-in functions, listed, 321\nresolution of function calls, 324\ntypes of functions, 321\nUDFs (user-defined functions), 322–331\nFUSE (Filesystem in Userspace), 50\nG\nGanglia, 288\nusing with altering system to monitor\nHadoop clusters, 291\nGangliaContext class, 288\nGENERATE statement, 332\nGenericOptionsParser class, 121\nlisting of supported options, 122\nusing distributed cache via, 242\nfs and jt options, 128\nGenericWritable class, 95\nNutchWritable subclass, 436\ngetChildren operation, watches on, 383\ngetData operation, watches on, 383\nGFS (Google filesystem), 9\nglobbing, 60\nfile globs and their expansions, 61\nglob characters and their meanings, 60\nGoogle\nBigtable, 344\nChubby Lock Service, 385\nGFS, 9\nMapReduce, 8\ngraph-based problems, 8\nGrid Computing, 6\nGROUP function, 306\ngroup names\nsetting, 120\ngrouping data\nCOGROUP statement, 335\nGROUP operator, 338\nPARALLEL clause for grouping operators in\nreduce phase, 340\ngroups\ncreating in ZooKeeper (example), 372–374\ndeleting in ZooKeeper, 378\njoining in ZooKeeper, 374–376\nlisting members in ZooKeeper, 376–377\nmembership in ZooKeeper, 372\nGrunt, 304\ngzip compression, 78\nH\nHadoop\nApache Hadoop project and subprojects,\n12\nconfiguration, 251–266\ndaemon addresses and ports, 263\nenvironment settings, 254–258\nfiles controlling, 251\nimportant daemon properties, 258–263\nmanagement of, 252\nother properties, 264\ndownloading and installing, 250\nHBase subproject, 344\nhistory of, 9–12\nstorage and analysis of data, 3\nHadoop and Cascading at ShareThis, 457–461\nHadoop and Hive at Facebook, 414\ndata warehousing architecture at Facebook,\n415\nHadoop configuration, 417\nHadoop use cases, 415\nHadoop, history at Facebook, 414\nHadoop Distributed Filesystem (see HDFS)\nHadoop in the cloud, 269\nHadoop in the Cloud\non Amazon EC2, 269\nHadoop mode (Pig), 303\nHadoop usage at Last.fm, 405–414\ngenerating charts with Hadoop, 406\nHadoop at Last.fm, 405\nTrack Statistics Program, 407–413\nHadoop Workflow Scheduler (HWS), 151\nhadoop-conf-pseudo package, 473\nhadoop-env.sh file, 252\nhadoop-metrics.properties file, 252\nhadoop.job.ugi property, 120\nhadoop.security.authorization property, 264\nhadoop.tmp.dir property, 261\nIndex | 485HadoopPipes::runTask method, 38\nHADOOP_CLASSPATH environment\nvariable, 24\nHADOOP_INSTALL environment variable,\n38\nHADOOP_LOG_DIR setting, 256\nHADOOP_MASTER setting, 257\nhanging tasks, 160\n.har file extension, 72\nHAR files (Hadoop Archives), 72\nlimitations of, 73\nhardware\ncommodity hardware, Hadoop on, 42\nspecification for Hadoop clusters, 245\nHarFileSystem, 47\nHashComparator objects, 434\nHashPartitioner class, 98, 181\ndefault, using to sort SequenceFile with\nIntWritable keys, 219\nHBase, 343–368\nbrief history of, 344\nclients, 350–354\nJava, 351\nusing REST and thrift, 353\ndata model, 344\nlocking, 345\nregions, 345\ndefined, 13\nexample, 354–360\nloading data from HDFS into table, 355–\n358\nschemas for Stations and Observations\ntables, 354\nweb queries, 358–360\nimplementation\nclients, slaves, and coordinating master,\n346\noperation of HBase, 346\ninstallation, 348–350\ntesting, 349\nlisting command-line options, 348\npractical issues running HBase instance,\n365–368\nHBase and HDFS, 366\nmetrics, 367\nschema design, 368\nUI, 367\nversions of Hadoop and HBase, 366\nRDBMS versus, 361\n486 | Index\nHBase characteristics, 363\nsuccessful RDBMS service, 362–365\nuse case, HBase at streamy.com, 363\nshutting down an instance, 350\nTableInputFormat and\nTableOutputFormat, 202\nHBaseAdmin class, 351\nHDFS (Hadoop Distributed Filesystem), 3, 41–\n73\naudit logging, 280\nbackups, 292\nbalancer for block distribution over\ndatanodes, 284\nbenchmarking with TestDFSIO, 267\nblock size, property for, 265\nblocks, 43\nclient reading file from, 63\nclient writing files to, 66\ncoherency model, 68\ncommand-line interface, 45\nbasic filesystem operations, 45\ndata integrity, 75\ndatanode block scanner, 283\ndefined, 12\ndesign of, 41\ndfsadmin tool, 280\nfile permissions, 47\nformatting, 469\nfsck utility, 281\nHAR files (Hadoop Archives), 71\nHBase persistence to, 346\nHBase use of, problems with, 366\nHTTP and FTP interfaces, 50\nimportant daemon properties, 260\ninclude and exclude file precedence, 295\ninstallation, MapReduce installation and,\n250\nkeeping clusters balanced, 71\nnamenodes and datanodes, 44\nparallel copying with distcp, 70\npersistent data structures, 273–278\ndatanode directory structure, 277\nfilesystem image and edit log, 274\nnamenode directory structure, 273\nsecondary namenode directory\nstructure, 276\nrelationship between input splits and\nblocks, 197\nsafe mode, 278Scribe integration with, 425\nstarting and stopping the daemon, 469\nupgrading, 297\nchecking the upgrade, 298\nchecking upgrade progress, 298\nfinalizing the upgrade, 299\nrolling back the upgrade, 298\nstarting the upgrade, 298\nsteps in procedure, 297\nwriting of reduce phase output to, 166\nhdfs-default.xml file, 121\nhdfs-site.xml file, 121, 252\nhelp command (HBase), 349\nherd effect, 399\nHftpFileSystem, 47\nHigh Performance Computing (HPC), 6\nhistory of Hadoop, 9–12\nHive, 309, 474\ndefined, 13\nat Facebook, 416\nimprovements to, 425\nuse case study, 421–424\ndata organization, 421\ndata pipelines using Hive, 422\nHive Query Language, 422\nHPC (High Performance Computing), 6\nHPROF profiler, 147\nHsftpFileSystem, 47\nHTable class, 351\ngetRow( ) method, 359\ngetScanner( ) method, 360\noptimization in HBase application, 357\nHTTP Accept header, 353\nHTTP interface to HDFS, 51\nHTTP job completion notification, 159\nHTTP server properties, 263\nHWS (Hadoop Workflow Scheduler), 151\nHypertable, 457\nhypothetical use case studies, 417\nad hoc analysis and product feedback, 419\nadvertiser insights and performance, 418\ndata analysis, 420\nI\nI/O (input/output), 75–114, 201\n(see also input formats; output formats)\ncompression\nusing in MapReduce, 84\ndata integrity, 75–77\nfile compression, 77–86\nfile-based data structures, 103–114\ninput formats, 184–202\nmap function, Java MapReduce, 21\nreduce function, Java MapReduce, 22\nserialization, 86–103\nsetting types for MapReduce jobs, 178\nStreaming MapReduce jobs, 182\nwriting output from map and reduce tasks,\n173\nidempotent and nonidempotent operations,\n395\nidentifiers (IDs), 133\nzxid, 387\nIdentityMapper class, 180\nTrack Statistics Program, 413\nIdentityReducer class, 182\nIDL (interface description language), 102\nILLUSTRATE operator, 307\nimage analysis, 8\ninclude file, 294\nprecedence in HDFS, 295\nIndexer tool in Nutch, custom OutputFormat,\n437\nindexing for Text class, 91\nindexing, secondary HBase index, 354\nIndexWriter class, addIndexes( ) methods,\n445\ninitialization, MapReduce jobs, 155\ninitLimit property, 402\nINNER keyword, 335\ninput formats\nbinary input, 199\ndatabase input, 201\ninput splits and records, 185–196\nmultiple inputs, 200\ntext input, 196–199\ninput paths\nproperties for input paths and filters, 188\nsetting with FileInputFormat methods, 187\ninput splits, 27, 185\ncontrolling size of, examples, 189\ncreation by FileInputFormat, 188\nfile split properties, 192\npreventing splitting, 191\nrelationship to HDFS blocks, 197\nsupport by compression formats, 78, 83\nInputFormat interface, 185\nhierarchy of classes implementing, 186\nIndex | 487InputSampler class, 225\nInputSplit interface, 185\nintegrity (see data integrity)\ninter-process communication, use of\nserialization, 86\ninterface description language (IDL), 102\nInterruptedException class, 394\nIntervalSampler objects, 226\nIntSumReducer class, 243\nIntWritable class, 21, 87\nobtaining comparator for and comparing,\n89\nreusing instances, 148\nInverseMapper class, 243\nio.bytes.per.checksum property, 75, 76\nio.compression.codecs property, 81\nio.file.buffer.size property, 167, 264\nio.serializations property, 101\nio.sort.factor property, 167, 168\nio.sort.mb property, 167\nio.sort.record.percent property, 167\nio.sort.spill.percent property, 167\nIOUtils class, closeStream( ) method, 52\nIsolationRunner class, 145\nitems tables, very large, 364\nJ\nJAR files\nadding to classpath of mapper and reducer\ntasks, 239\ncopying to tasks, 239\nin Streaming MapReduce API, 35\npackaging program as, 132\nJava\nenums, 215\nHBase client, 351–353\ninstalling, 249\nrunning Pig programs from, 304\nSun's JDK, 465\nJava API documentation for Hadoop, xvi\nJava Management Extensions (see JMX)\nJava MapReduce, 20–27\napplication to run MapReduce job, 22\nMaxTemperatureMapper class (example),\n20\nMaxTemperatureReducer class (example),\n21\nnew API in Hadoop 0.20.0, 25–27\nStreaming versus, 33\n488 | Index\ntesting running of MapReduce job, 23\nJava Object Serialization, 101\nJava Virtual Machines (see JVMs)\njava.env file, 401\njava.library.path property, 82\nJavaSerialization class, 101\nJAVA_HOME setting, 256\nfor HBase, 348\nJBOD (Just a Bunch of Disks) configuration,\n246\nJConsole tool, viewing MBeans in running\nJVM, 290\nJMX (Java Management Extensions), 289\nenabling remote access to, 290\nretrieving MBean attribute values, 291\njob history, 134\njob history logs (MapReduce), 143\njob IDs, 25, 133\ngetting for new job, 154\njob page, 136\njob run, anatomy of, 153–159\njob completion, 158\njob initialization, 155\njob submission, 153\nprogress and status updates, 157\ntask assignment, 155\ntask execution, 156\njob schedulers, 266\njob scheduling, 162\njob.end.notification.url property, 159\nJobClient class\nDistributedCache and, 242\ngetJob( ) method, 158\ngetSplits( ) method, 186\nrunJob( ) method, 23, 132, 153, 215\nsetJobPriority( ) method, 162\nsubmitJob( ) method, 154\nJobConf class, 22\nmethods to get lists of available cache files,\n242\nsetNumTasksToExecutePerJvm( ) method,\n170\nsetOutputKeyComparatorClass( ) method,\n220\nsetter methods for MapReduce types, 176\nusing for side data distribution, 238\nJobControl class, 151\njobs, 27\ndecomposing problem into, 149killing, 161\nrunning dependent jobs, 151\ntuning, 145–149\nuser jobs as cluster benchmarks, 269\njobtracker, 9, 27\ncluster specifications and, 247\nfailure of, 161\nrunning on localhost, 119\ntasktrackers connecting to, 294\nJobTracker class, 9, 153\ngetNewJobId( ) method, 154\nsubmitJob( ) method, 155\njobtracker page, 134\nJOIN statement\nCOGROUP versus, 335\njoin key in BY clause, 336\njoins, 233–238\nCOGROUP versus JOIN statements, 335\ndataset size and partitioning, 233\nexample of join in Pig, 336\nHBase and, 368\ninner join of two datasets, 233\nmap-side, 233\nPARALLEL clause for joining operators in\nreduce phase, 340\nin Pig, 334\nreduce-side, 235\nusing CROSS operator, 337\nJRuby IRB interpreter, 349\njt and fs command line options, 128\nJUnit 4 assertions, 88\nJust a Bunch of Disks (JBOD) configuration,\n246\nJVMFLAGS environment variable, 401\nJVMs (Java Virtual Machines)\nexit of child JVM in task failure, 160\nlaunch by TaskRunner, 156\nmemory given to JVMs running map and\nreduce tasks, 166\nmemory, setting, 266\nreuse for subsequent tasks, 170\nK\nKatta, 457\nKeeperException class, 395\nrecoverable exceptions, 395\nstate exceptions, 395\nunrecoverable exceptions, 396\nKeeperException.NoNodeException, 376\nKeeperState objects, 390\nKellerman, Jim, 344\nKeyFieldBasedComparator objects, 232\nKeyFieldBasedPartitioner objects, 232\nkeys and values\nin C++ Pipes MapReduce API, 37\nin Streaming MapReduce API, 33\nsorting in MapReduce, 227\nin Streaming, 183\nKeyValueTextInputFormat class, 197\nkill command, 371\nKosmosFileSystem, 47\nL\nLast.fm, 405\n(see also Hadoop usage at Last.fm)\nLazyOutputFormat class, 210\nleader election phase, 385, 398\nZooKeeper server numbers and, 402\nLIMIT statement, limiting number of results,\n339\nlinear chain of jobs, 151\nlink inversion, 429–431\nLinkDb\nimplementation, 430\nLinkDb (Nutch), 427\nLinux\nautomated installation tools, 249\nHadoop on, 465\nsetting up NFS on, 250\nlist command (HBase), 350\nlists, Writable collection implementations, 96\nload functions, 321\nUDF (user defined function)\nadvanced loading with Slicer, 330\nusing a schema, 329\nUDF (user-defined function), 327–331\nLOAD operator, 305\nloading data into HBase table, 355\nlocal job runner, 127–131\nfixing mapper, 129\nrunning the driver, 128\ntesting the driver, 130\nwriting driver to run job, 127\nlocal mode (Pig), 302\nLocalFileSystem class, 47\nclient-side checksumming, 76\nlock service (ZooKeeper), 398–400\nherd effect, 399\nIndex | 489implementation, 400\npseudocode for lock acquisition, 398\nrecoverable exceptions and, 399\nunrecoverable exceptions and, 400\nlocking in HBase, 345\nlog processing at Rackspace, 439–447\nbrief history, 440\nchoosing Hadoop, 440\ncollection and storage, 440\nMapReduce for logs, 442–447\nrequirements/problem, 439\nlog4j.properties file, 252\nlogging, 285\naudit logging, 280\nBookKeeper service, 400\ncompression format for logfiles, 84\ngetting stack traces, 286\nHadoop user logs, 142\nin Java, using Apache Commons Logging\nAPI, 142\nsetting levels, 286\nShareThis log processing, 458–461\nsystem logfiles produced by Hadoop, 256\nusing SequenceFile for logfiles, 103\nlogical plan for Pig Latin statements, 311\nLong.MAX_VALUE stamp, 360\nLongSumReducer class, 243\nLongWritable class, 21\nlow-latency data access, HDFS and, 42\nLucene library, 426\nLucene project, 9\nM\nmachine learning algorithms, 8\nMailtrust (see Rackspace)\nmaintenance, 292–299\ncommissioning and decommissioning\nnodes, 293\nroutine administrative procedures, 292\nupgrades, 296–299\nMakefile, C++ MapReduce program, 38\nmalformed data, handling by mapper\napplication, 143\nmap functions\ncompressing output, 85\ngeneral form, 175\nsecondary sort in Python, 231\nmap tasks, 27\n490 | Index\nconfiguration properties for shuffle tuning,\n167\nshuffle and sort, 163\nskipping bad records, 171\nmap type (Pig), 316\nmap-side joins, 233\nmap.input.file property, 195\nMapFile class, 110–114\napplication for partitioned MapFile\nlookups, 221–223\nconverting SequenceFile to, 113\nreading with MapFile.Reader instance, 112\nwriting with MapFile.Writer instance, 110\nMapFile.Reader objects, 222\nMapFileOutputFormat class, 203\nstatic methods for lookups against\nMapReduce output, 221\nMapper interface, 20, 21\nconfigure( ) method, 192\nHBase TableMap interface and, 353\nmappers, 7\nadding debugging to, 139\ndefault mapper, IdentityMapper, 180\ngetting information about file input splits,\n192\nhandling malformed data, 143\nparser class for, 129\ntagging station and weather records in\nreduce-side join, 235\nunit testing, 124–126\nusing utility parser class, 130\nmapred-default.xml file, 121\nmapred-site.xml file, 121, 252\nmapred.child.java.opts property, 262, 266\nmapred.child.ulimit property, 266\nmapred.combiner.class property, 178\nmapred.compress.map.output property, 167\nmapred.hosts property, 264, 294\nmapred.inmem.merge.threshold property,\n167, 168\nmapred.input.dir property, 188\nmapred.input.format.class property, 176\nmapred.input.pathFilter.class property, 188\nmapred.job.id property, 172\nmapred.job.priority property, 162\nmapred.job.reduce.input.buffer.percent\nproperty, 167, 168\nmapred.job.reuse.jvm.num.tasks property,\n170mapred.job.shuffle.input.buffer.percent\nproperty, 168\nmapred.job.shuffle.merge.percent property,\n168\nmapred.job.tracker property, 128, 262, 263\nmapred.job.tracker.http.address property, 264\nmapred.jobtracker.taskScheduler property,\n162\nmapred.line.input.format.linespermap\nproperty, 198\nmapred.local.dir property, 145, 262\nmapred.map.max.attempts property, 160\nmapred.map.output.compression.codec\nproperty, 167\nmapred.map.runner.class property, 178\nmapred.map.tasks.speculative.execution\nproperty, 169\nmapred.mapoutput.key.class property, 176\nmapred.mapper.class property, 178\nmapred.max.split.size property, 188\nmapred.min.split.size property, 188\nmapred.output.compression.type property, 85\nmapred.output.format.class property, 178\nmapred.output.key.class property, 176\nmapred.output.key.comparator.class property,\n178\nmapred.output.value.class property, 176\nmapred.output.value.groupfn.class property,\n178\nmapred.partitioner.class property, 178\nmapred.reduce.copy.backoff property, 168\nmapred.reduce.max.attempts property, 160\nmapred.reduce.parallel.copies property, 168\nmapred.reduce.tasks property, 155\nmapred.reduce.tasks.speculative.execution\nproperty, 169\nmapred.reducer.class property, 178\nmapred.submit.replication property, 154\nmapred.system.dir property, 262\nmapred.task.id property, 172\nmapred.task.is.map property, 172\nmapred.task.partition property, 172\nmapred.task.tracker.http.address property,\n264\nmapred.task.tracker.report.address property,\n263\nmapred.tasktracker.map.tasks.maximum\nproperty, 122, 262\nmapred.tasktracker.reduce.tasks.maximum\nproperty, 262\nmapred.textoutputformat.separator property,\n183\nmapred.tip.id property, 172\nmapred.userlog.limit.kb property, 142\nmapred.usrlog.retain.hours property, 142\nmapred.work.output.dir property, 174\nMapReduce programming in Hadoop, 15–39\nanalysis of data, 4\napplication counting rows in HBase table,\n351–353\napplication importing data from HDFS into\nHBase table, 355–358\nbenchmarking MapReduce with sort, 268\nCascading and, 447\ncombiner functions, 29\ncomparison to other systems, 4\nGrid Computing, 6\nRDBMS, 4\nvolunteer computing, 8\ncompression and input splits, 83\ncompression, using, 84–86\ncontrol script starting daemons, 253\ncounters, 211–218\ncounting and sorting in, 448\ndata flow, 19\ndata flow for large inputs, 27\ndefinition of MapReduce, 12\ndeveloping an application, 115–151\nconfiguration API, 116–118\nconfiguring development environment,\n118–124\nrunning job on a cluster, 132–145\nrunning locally on test data, 127–131\ntranslating problem into MapReduce\nworkflow, 149–151\ntuning a job, 145–149\nwriting unit test, 124–127\nenvironment settings, 255\nfailures, 159–161\nHadoop Pipes, 36–39\nHadoop Streaming, 32–36\nHAR files as input, 72, 73\nhow Flow translates into chained\nMapReduce jobs, 456\nimportant daemon properties, 261\ninput formats, 184–202\ninstallation, HDFS installation and, 250\nIndex | 491introduction of MapReduce, 10\nJava MapReduce, 20–27\njob scheduling, 162, 266\njoins, 233–238\nmap and reduce functions, 18\nMapReduce library classes, listed, 243\nMapReduce types, 175–184\nnew Java MapReduce API, 25–27\noutput formats, 202–210\nrunning a job, 153–159\nrunning a job on Amazon EC2, 271\nrunning distributed job, 32\nshuffle and sort, 163–168\nside data distribution, 238–242\nsorting, 218–233\nstarting and stopping the daemon, 469\ntask execution, 168–174\nusing for logs at Rackspace, 442–447\nweather dataset, 15\nMapRunnable interface\nMapRunner implementation, 181\nMultithreadedMapRunner\nimplementation, 186\nMapRunner class, 181, 186\nFetcher application in Nutch, 435\nMapWritable class, 95\nexample with different types for keys and\nvalues, 96\nmaster node (HBase), 346\nmasters file, 252\nMAX function, resolution of, 324\nMBeans, 289\nretrieving attribute values with JMX, 291\nmemory\nenvironment settings for, 254\nlimits for tasks, 266\nmemory buffers\nmap task, 163\nreduce tasktracker, 165\nmerges\nmap task file output in reduce task, 165\nvery large sort merges, 364\nMessage Passing Interface (MPI), 6\n.META. table, 346\nmetadata\nencapsulation in FileStatus class, 58\nHDFS blocks and, 43\nHDFS, upgrading, 297\npassing to tasks, 238\n492 | Index\nznode, 381\nmetrics, 286–289\nCompositeContext class, 289\ncontexts for, 286\ncounters versus, 286\nFileContext class, 287\nGangliaContext class, 288\nHadoop and HBase, 367\nmonitoring in ZooKeeper, 389\nNullContextWithUpdateThread class, 288\nMetricsContext interface, 287\nmin.num.spills.for.combine property, 167\nMiniDFSCluster class, 131\nMiniMPCluster class, 131\nmock object frameworks, 124\nmonitoring, 285–291\nlogging, 285\nmetrics, 286–289\nusing Java Management Extensions (JMX),\n289\nMPI (Message Passing Interface), 6\nmulti named output, 209\nMultipleInputs class, 200\nspecifying which mapper processes which\nfiles, 412\nuse in reduce-side joins, 235\nMultipleOutputFormat class, 203\ndifferences from MultipleOutputs, 210\nweather dataset partitioning (example),\n205–207\nMultipleOutputs class, 203\ndifferences from MultipleOutputFormat,\n210\nusing to partition weather dataset\n(example), 207–209\nMultithreadedMapRunner objects, 186\nMyLifeBits project, 2\nN\nnamenodes, 44\nchoosing of datanodes to store replicas on,\n67\ncluster specifications and, 247\ndatanode permitted to connect to, 294\ndirectory structure, 273\nfilesystem image and edit log, 274\nrole in client file write to HDFS, 66\nrole in client reading data from HDFS, 63\nrunning in safe mode, 278entering and leaving safe mode, 279\nrunning on localhost, 119\nsecondary, directory structure, 276\nNativeS3FileSystem, 47\nNavigableMap class, 359\nNCDC (National Climatic Data Center) data\nformat, 15\nNCDC weather data, preparing, 475–477\nNDFS (Nutch Distributed Filesystem), 9\nnetwork addresses, Hadoop daemons, 263\nnetwork topology\nAmazon EC2, 270\nHadoop and, 64, 247\nreplication factor and, 68\nNew York Times, use of Hadoop, 10\nNFS filesystem, 250\nNLineInputFormat class, 174, 198\nspecifying for NCDC files, 476\nnodes\ncommissioning and decommissioning, 293\ncommissioning new nodes, 293\ndecommissioning, 295\nznodes, 372\nnormalization of data, 6\nnull values, Pig Latin schemas and, 318\nNullContext class, 287\nNullContextWithUpdateThread class, 288\nNullWritable class, 95\nNumberFormatException, 125\nNutch Distributed Filesystem (NDFS), 9\nNutch search engine, 9, 425–439\nbackground, 425\ndata structures, 426–429\nHadoop data processing examples in, 429–\n438\ngeneration of fetchlists, 431–438\nlink inversion, 429–431\nsummary, 439\nNutchWritable class, 436\nO\nObjectWritable class, 95\noptimization notes\nHBase application, 357\ntuning a MapReduce job, 145–149\nORDER operator, 339\nOUTER keyword, 335\noutput formats, 202–210\nbinary output, 203\ndatabase output, 201\nlazy output, 210\nmultiple outputs, 203–210\ntext output, 202\nOutputCollector class, 21\ncreating mock replacement, 125\nmock replacement for, 124\npurpose of, 175\nOutputCommitter objects, 173\nOutputFormat class\ncustom implementation used by Nutch\nIndexer, 437\nOutputFormat interface\nclass hierarchy, 202\nP\nPARALLEL clause for operators running in\nreduce phase, 340\nPARALLEL keyword, 332\nparam option (Pig), 341\nparameter substitution, 341\ndynamic parameters, 342\nprocessing, 342\nparameter sweep, 198\nparam_file option (Pig), 341\nparsers, writing parser class for use with\nmapper, 129\npartial failure, 7\nZooKeeper and, 369, 395\npartial sort, 219–223\nPartitioner interface, 433\npartitioners\nHashPartitioner, 98, 181, 219\nKeyFieldBasedPartitioner, 232\nKeyPartitioner custom class, 237\nTotalOrderPartitioner, 225\nPartitionReducer class, 435\npartitions\nmap task output, 164\nnumber rigidly fixed by application, 204\npartitioner respecting total order of output,\n223–227\npartitioning weather dataset (example),\n203\nPartitionUrlByHost class (Nutch), 433\nPathFilter interface, 62, 188\npaths, znode, 379\npattern matching\nfile globs, 60\nIndex | 493using PathFilter, 62\nPaxos, 385\nperformance, ZooKeeper, 401\npermissions for file and directories, 47\nphysical plan for Pig statement execution, 311\nPig, 301–342, 474\ncomparison with databases, 308\ncomponents of, 301\ndata processing operators, 331–340\ndefined, 13\nexample program finding maximum\ntemperature by year, 305–307\nexecution types or modes, 302\nHadoop mode, 303\nlocal mode, 303\ngenerating examples, 307\nGrunt, 304\ninstalling, 302\nparallelism, 340\nparameter substitution, 341\nPig Latin editors, 305\nrunning programs, 304\nUDFs (user-defined functions), 322–331\nPig Latin, 309–322\ncomments, 310\nexpressions, 314\nfunctions, 320\nkeywords, 311\nschemas, 317–320\nstatements, 310, 311–314\ncommands, 313\ndiagnostic operators, 313\nrelational operators, 312\nUDF (user-defined function), 313\ntypes, 315\nPiggy Bank, functions in, 322\nPigStorage function, 331\nuse by STREAM operator, 333\nPipes, 36–39\nassembly of operations, 452\ncompiling and running C++ MapReduce\nprogram, 38\ncreating SubAssembly pipe (Cascading),\n456\nPipe types in Cascading, 449\nrelationship of executable to tasktracker and\nits child, 156\nusing Unix pipes to test Ruby map function\nin Streaming, 34\n494 | Index\nPLATFORM environment variable, 38\nports\nconfiguration in ZooKeeper, 402\nHadoop daemons, 263\nZooKeeper client connections, 371\nPositionedReadable interface, 55\nPostfix log lines, 442\npriority, setting for jobs, 162\nproblems and future work (Facebook), 424\nprofiling tasks, 146–149\nHPROF profiler, 147\nother profilers, 149\nprogress\nMapReduce jobs and tasks, 157\nshowing in file copying, 56\nProgressable interface, 56\nproperties\nconfiguration, 116\nconfiguration for different modes, 467\nconfiguration of MapReduce types, 176\nconfiguration tuning for shuffle, 166\nmap side, 167\nreduce side, 168\ncontrolling size of input splits, 188\nfile split, 192\nHTTP server, 263\nimportant HDFS daemon properties, 261\nimportant MapReduce daemon properties,\n262\ninput path and filter, 188\nmap output compression, 85\nRPC server, 263\nsafe mode, 279\nspeculative execution, 169\nStreaming separator properties for key-value\npairs, 183\nsystem, 117\ntask environment, 172\ntask JVM reuse, 170\nZooKeeper configuration, 371\npseudo-distributed mode, 38, 465, 467\nconfiguration files, 467\nconfiguring SSH, 468\nformatting HDFS filesystem, 469\ninstalling Cloudera’s Distribution for\nHadoop, 473\nstarting and stopping daemons, 469\nPublic Data Sets, Amazon Web Services, 2\nPythonmap function for secondary sort, 231\nPython, map and reduce functions, 35\nQ\nquery languages\nHive Query Language, 422\nPig, SQL, and Hive, 308\nquorum (ZooKeeper), 385\nR\nrack awareness, clusters and, 248\nrack-local tasks, 156\nRackspace, 439\n(see also log processing at Rackspace)\nMailtrust division, 4\nRAID (Redundant Array of Independent\nDisks), Hadoop clusters and, 246\nRandomSampler objects, 226\nRandomWriter objects, 268\nRawComparator class, 88\ncontrolling sort order for keys, 220\ncustom implementation, 100\nimplementing (example), 99\nRawLocalFileSystem class, 77\nRDBMS (Relational DataBase Management\nSystems), 4\ncomparison to MapReduce, 5\nHBase versus, 361–365\nHBase characteristics, scaling and, 363\ntypical RDBMS scaling story for\nsuccessful service, 362\nuse case, HBase at streamy.com, 363\nPig versus, 308\nread operations in ZooKeeper, 382\nreading/writing data in parallel to/from\nmultiple disks, 3\nrecord compression in sequence files, 109\nRecordReader class, 186\nWholeFileRecordReader custom\nimplementation, 193\nrecords, 185\ncorrupt, skipping in task execution, 171\nlogical records for TextInputFormat, 196\nprocessing a whole file as a record, 192\nrecoverable exceptions in ZooKeeper, 395,\n399\nreduce functions\ngeneral form, 175\nsecondary sort in Python, 232\nreduce tasks, 27\nconfiguration properties for shuffle tuning,\n168\nnumber of, 28\nshuffle and sort, 164\nskipping bad records, 171\nreduce-side joins, 235\napplication to join weather records with\nstation names, 237\nmappers for tagging station and weather\nrecords, 235\nReducer interface, implementation (example),\n21\nreducers, 7\ndefault reducer, IdentityReducer, 182\njoining tagged station records with tagged\nweather records (example), 236\nspecifying number in Pig, 340\nwriting unit test for, 126\nRegexMapper class, 243\nregions in HBase tables, 345\nregionservers (HBase), 346\ncommit log, 347\nREGISTER operator, 324\nregular expressions, using with PathFilter, 62\nrelational operators (Pig Latin), 312\nrelations (Pig), 306\nbags versus, 316\npropagation of schemas to new relations,\n320\nschema associated with, 317\nremote debugging, 144\nremote procedure calls (RPCs), 86\nreplicas, placement of, 67\nreplicated mode (ZooKeeper), 385, 401\nreplication factor, 44, 46, 154\nReporter class\ndynamic counters, 215\npurpose of, 175\nreqs command, 371\nreserved storage space, property for, 265\nREST interface for HBase, 353\nretries, ZooKeeper object, write( ) method,\n396\nROOT table, 346\nrow keys, design in HBase, 368\nRowCounter class, 351\nRowKeyConverter class (example), 356\nIndex | 495RowResult class, 359\nnext( ) method, 361\nRPC server properties, 263\nRPCs (remote procedure calls), 86\nrsync tool, 252\ndistributing configuration files to all nodes\nof a cluster, 257\nRuby, map and reduce functions, in Streaming\nMapReduce API, 33\nRunningJob objects, 158\nretrieving a counter, 217\nruok command (ZooKeeper), 371\nS\nS3FileSystem, 47\nsafe mode, 278\nentering and leaving, 279\nproperties, 279\nSampler interface, 225\nsamplers, 226\nScanner interface, 360\nScanners (HBase), 359\nscheduling, job, 162\nFair Scheduler, 162\nschemas (HBase), 361\ndefining for tables, 349\ndesign of, 368\nStations and Observations tables (example),\n354\nschemas (Pig Latin), 317–320\nmerging, 320\nusing in load UDF, 329\nvalidation and nulls, 318\nSchemes (Cascading), 453\nScribe-HDFS integration, 425\nScriptBasedMapping class, 249\nsearch engines, 10\n(see also Nutch search engine)\nApache Lucene and Nutch projects, 9\nbuilding web search engine from scratch, 9\nsecondary namenode, 45\nsecondary sort, 227\n(see also sorting)\nin reduce-side joins, 235\nSEDA (staged event-driven architecture), 443\nSeekable interface, 54\nsegments (in Nutch), 427\nSelector class (Nutch), 433\nSelectorInverseMapper class (Nutch), 434\n496 | Index\nsemi-structured data, 5\nseparators, key-value pairs\nkey.value.separator.in.input.line property,\n197\nin Streaming, 183\nSequenceFile class, 103–110\ncharacteristics of, 200\nconverting to MapFile, 113\ndisplaying with command-line interface,\n108\nformat, 109\nreading with SequenceFile.Reader instance,\n105\nsorting and merging sequence files, 108\nusing WholeFileInputFormat to package\nfiles into, 194\nwriting with SequenceFile.Writer instance,\n103\nSequenceFileAsBinaryInputFormat class, 200\nSequenceFileAsBinaryOutputFormat class,\n203\nSequenceFileAsTextInputFormat class, 200\nSequenceFileInputFormat class, 200\nSequenceFileOutputFormat class, 203\nsequential znodes, 380\nusing in distributed lock implementation,\n398\nserialization, 86–103\nframeworks for, 101\nJava Object Serialization, 101\nserialization IDL, 102\nrelations to and from program IO streams,\n333\nof side data in job configuration, 238\nuse in remote procedure calls (RPCs), 86\nWritable classes, 89\nWritable interface, 87–89\nSerialization interface, 101\nSerializer objects, 101\nservers, ZooKeeper, numeric identifier, 402\nservice-level authorization, 264\nsession IDs (ZooKeeper client), 399\nsessions (ZooKeeper), 388\nSETI@home, 8\nsets, emulation of, 96\nsharding, 445\nshared-nothing architecture, 7\nShareThis, Hadoop and Cascading at, 457–\n461shell, filesystem, 49\nshell, launching for HBase, 349\nshuffle and sort, 163, 218\n(see also sorting)\nconfiguration tuning, 166\nmap side, 163\nreduce tasks, 164\nside data\ndefined, 238\ndistribution using distributed cache, 239–\n242\ndistribution using job configuration, 238\nside effects, task side-effect files, 173\nsingle named output, 209\nSkipBadRecords class, 172\nskipping mode, 171\nslaves file, 252\nSlicer interface, 330\nSocksSocketFactory class, 441\nSolrInputFormat objects, 446\nSolrOutputFormat objects, 445\nsort merges, very large, 364\nsort phase of reduce tasks, 165\nSortedMap interface, 359\nSortedMapWritable class, 95\nsorting, 218–233\n(see also shuffle and sort)\nbenchmarking MapReduce with, 268\nin Pig, 338\nin MapReduce, 448\npartial sorts, 219–223\napplication for partitioned MapFile\nlookups, 221–223\nsorting sequence file with IntWritable\nkeys, 219\npreparing for, converting weather data into\nSequenceFile format, 218\nsecondary sort, 227–233\nin Streaming, 231\nJava code for, 228–231\nTeraByte sort on Apache Hadoop, 461\ntotal sort, 223–227\nspace management, 424\nspeculative execution, 169\nspills, task memory buffers, 163\nreduce task, 165\nSPLIT operator, 319, 340\nsplits (see input splits)\nSQL\ndata pipelines in, 422\nPig Latin versus, 308\nsrst command, 371\nSSH\nconfiguration, 251\nconfiguring for pseudo-distributed mode,\n468\nenvironmental settings, 257\nstack traces, 286\nstaged event-driven architecture (SEDA), 443\nstandalone mode, 466\ninstalling Cloudera’s Distribution for\nHadoop, 473\nZooKeeper service, 385\nstandby namenode, 45\nstat command, 371\nStat objects, 381\nStatCallback interface, 382\nstate exceptions in ZooKeeper, 395\nstatements (Pig Latin), 310, 311–314\ncommands, 313\ndiagnostic operators, 313\nrelational operators, 312\nUDF (user-defined function), 313\nStates enum, 390\nstates, ZooKeeper object, 389\nstatus\nMapReduce jobs and tasks, 157\npropagation of updates through\nMapReduce system, 158\nstorage and analysis of data, 3\nstore functions, 321\nPigStorage, 331\nSTORE statement, order and, 339\nSTREAM operator, 333\nStreaming, 32–36\ndefault MapReduce job, 182\ndistributed cache and, 239\nenvironment variables, 173\nkeys and values, 183\nPython map and reduce functions, 35\nrelationship of executable to tasktracker and\nits child, 156\nRuby map and reduce functions, 33\nscript to process raw NCDC files and store\nin HDFS, 477\nsecondary sort, 231\ntask failures, 160\nuser-defined counters, 218\nIndex | 497streaming data access in HDFS, 42\nstreaming in Pig, custom processing script,\n333\nstreams, compressing and decompressing with\nCompressionCodec, 79\nStreamXmlRecordReader class, 199\nString class, 91\nconversion of Text objects to Strings, 94\nText class versus, 92\nznode paths, 379\nStringifier class, 238\nstructured data, 5\nSubAssembly class (Cascading), 455\nsubmission of a job, 153\nsuper-user, 47\nsync markers in SequenceFiles, 109\nsync operation in ZooKeeper, 381\nsync( ) method, FSDataOutputStream class,\n69\nsynchronous API in ZooKeeper, 381\nsyncLimit property, 402\nsystem daemon logs, 143\nsystem properties, 117\nconfiguration properties defined in terms of,\n118\nT\ntab character, 33, 34\nTableInputFormat class, 202, 351\nTableMap interface, 353\nTableMapReduceUtil class,\ninitTableMapJob( ) method, 353\nTableOutputFormat class, 202, 351\ntables\ncreating in HBase, 349\ndescription of HBase tables, 344\nremoving in HBase, 350\nTaps (Cascading), 453\ntask details page, 141\ntask execution, 156, 168–174\nenvironment, 172\nStreaming environment variables, 173\ntask side-effect files, 173\nJVM reuse, 170\nskipping bad records, 171\nspeculative, 169\nStreaming and Pipes, 156\ntask IDs, 25, 133\ntask logs (MapReduce), 143\n498 | Index\nTaskRunner objects, 156\ntasks\nassignment to tasktracker, 155\ncreating list of tasks to run, 155\nfailures, 160\nkilling attempts, 161\nmap and reduce, 27\nmaximum number of attempts to run, 160\nmemory limits for, 266\nprofiling, 146–149\nprogress of, 157\nstatus of, 158\ntasks page, 140\nTaskTracker class, 153\ntasktracker.http.threads property, 167\ntasktrackers, 27, 153\nblacklisted, 161\nfailure of, 161\npermitted to connect to jobtracker, 294\nreducers fetching map output from, 164\nTCP/IP server, 263\ntemporary directory for MapReduce task\noutputs (datanodes), 173\nTeraByte sort on Apache Hadoop, 461\nTeraGen application, 462\nTeraSort application, 462\nTeraValidate application, 464\nTestDFSIO, benchmarking HDFS, 267\ntesting\nunit testing log flow at ShareThis, 459\nwriting unit test for mapper, 124–126\nwriting unit test for reducer, 126\nTestInputFormat objects, skipping bad\nrecords, 171\nText class, 21, 91–94\nconversion of SequenceFile keys and values\nto, 200\nconverting Text objects to Strings, 94\nindexing, 91\niterating over Unicode characters in Text\nobjects, 93\nmutability of Text objects, 94\nreusing Text objects, 148\nString class versus, 92\ntext input, 196–199, 196\n(see also TextInputFormat class)\nKeyValueTextInputFormat class, 197\nNLineInputFormat class, 198\nXML, 199text output, 202\nTextInputFormat class, 196\ndefault Streaming job and, 182\nnonsplittable example, 191\nTextOutputFormat class, 202\ndefault output format of MapReduce jobs,\n182\nthreads\ncopier threads for reduce task, 164\ndatanode, running out of, 366\nnumber of worker threads serving map\noutput file partitions, 164\nThrift API, 49\ninstallation and usage instructions, 49\nthrift service, using with HBase, 353\ntick time, 388\ntickTime property, 402\ntickTime property (ZooKeeper), 371\ntime parameters in ZooKeeper, 388\ntimeout period for tasks, 160\nTokenCounterMapper class, 243\nTool interface, 121, 353\nexample implementation\n(ConfigurationPrinter), 121\nToolRunner class, 122\nlisting of supported options, 122\ntopology.node.switch.mapping.impl property,\n248\ntotal sort, 223–227\nTotalOrderPartitioner class, 225\nTrack Statistics Program (Hadoop at Last.fm),\n407\nIdentityMapper, 413\nMergeListenerMapper, 412\nmerging results from previous jobs, 412\nresults, 413\nSumMapper, 410\nsumming track totals, 410\nSumReducer, 410, 413\nUnique Listeners job, 408\nUniqueListenerMapper, 408\nUniqueListenerReducer, 409\ntrash, 265\nexpunging, 265\nTrim UDF (example), 326–327\ntuning jobs, 145–149\nchecklist for, 145\nprofiling tasks, 146–149\nTupleFactory class, 329\nTuples (Cascading), 449\ntuples (Pig), 306\nTwoDArrayWritable class, 95\nU\nUDF statements (Pig Latin), 313\nUDFs (user-defined functions) in Pig, 322–331\neval UDF, 326–327\nfilter UDF, 322–326\nleveraging types, 325\nload UDF, 327–331\nUI, 367\n(see also web UI for MapReduce)\nHBase, 367\nulimit count for file descriptors, 366\nUnicode, 92\niteration over characters in Text object, 93\nznode paths, 379\nUNION statement, 339\nUnix\nHadoop on, 465\nproduction platform for Hadoop, 246\nstreams, 32\nUnix tools, analyzing weather data, 17\nunrecoverable exceptions in ZooKeeper, 396,\n400\nunstructured data, 5\nupdate operations in ZooKeeper, 381\nupgrades, 296–299\nchecking, 298\nclean up after, 296\nfinalizing, 299\nHDFS data and metadata, 297\nrolling back, 298\nstarting, 298\nversion compatibility, 296\nwaiting for completion of, 298\nURIs\nadding fragment identifiers to file URIs with\nDistributedCache, 242\nremapping file URIs to\nRawLocalFileSystem, 77\nS3, 476\nznode paths versus, 379\nURLCat class (example), 52\nURLs, reading data from, 51\nuser identity, setting, 120\nuser, creating for Hadoop, 250\nUTF-8 character encoding, 91\nIndex | 499Utf8StorageConverter class, 330\nV\nvalidation, Pig Latin schemas and, 318\nvariable expansion, 118\nversioned cells in HBase, 344\nversions\nHadoop and HBase, compatibility, 366\nHadoop components, compatibility of, 296\nvery large files, 41\nvoid return types in ZooKeeper, 382\nvolunteer computing, 8\nW\nWalters, Chad, 344\nWatcher interface\nConnectionWatcher class (example), 375\nCreateGroup (example), 373\nfunctions of, 390\nprocess( ) method, 374\nWatcher.Event.KeeperState enum, 374\nwatches, 380\ncreation operations and corresponding\ntriggers, 383\non read operations, 382\nweather dataset, 15\nanalyzing with Unix tools, 17\nNCDC format, 15\nweb page for this book, xviii\nweb queries in HBase, 358–360\nmethods retrieving range of rows from\nHBase table, 359\nusing Scanners, 359\nweb search engines\nApache Lucene and Nutch, 9\nbuilding from scratch, 9\nweb UI for MapReduce, 134–136, 139\njob page, 136\njobtracker page, 134\ntask details page, 141\ntasks page, 140\nWebDAV, 50\nwebinterface.private.actions property, 141\nWebMap, 11\nwebtable, 343\nwhoami command, 120\nWholeFileInputFormat class, 192\n500 | Index\nusing to package small files into\nSequenceFiles, 194\nWindows, Hadoop on, 465\nwork units, 8\nworkflows, MapReduce, 149–151\ndecomposing problem into MapReduce\njobs, 149\nrunning dependent jobs, 151\nWritable classes\nBytesWritable, 94\ncollections, 95\nimplementing custom, 96–101\nNullWritable, 95\nObjectWritable and GenericWritable, 95\nText, 91–94\nwrappers for Java primitives, 89\nWritable interface, 87–89\nWritableComparable interface, 88, 220\nWritableSerialization class, 101\nwrite operations in ZooKeeper, 382\nWriteLock class, 400\nwriters, multiple, HDFS and, 42\nX\nXML, text input as XML documents, 199\nY\nYahoo!, Hadoop at, 10\nZ\nZab protocol, 385\nzettabytes, 1\nznodes, 372, 379\nACLs (access control lists), 383\ndeleting, 378\ndeletion of, watch event types and, 383\nephemeral, 379\nephemeral and persistent, 374\npaths, 379\nprogram creating znode to represent group,\n372–374\nsequence numbers, 380\nversion number, 381\nwatches on, 380\nzoo.cfg file, 371\nZOOCFGDIR environment variable, 371\nZooDefs.Ids class, 384\nZooKeeper, 369–403Administrator's Guide, 401\nbuilding applications with, 391–401\nconfiguration service, 391–394\ndistributed data structures and\nprotocols, 400\nlock service, 398–400\nresilient application, 394–398\ncharacteristics of, 369\ncommand-line tool, 377\ndefined, 13\nexample, 371–378\ncreating a group, 372–374\ndeleting a group, 378\ngroup membership, 372\njoining a group, 374–376\nlisting group members, 376–377\ninstalling and running, 370\ncommands, 371\nsetting up configuration file, 371\nstarting local ZooKeeper server, 371\nin production, 401\nconfiguration, 402\nresilience and performance, 401\nservice, 378–391\nconsistency, 386\ndata model, 379\nimplementation, 385\noperations, 380\nsessions, 388\nstates, 389\nuse in HBase, 346\nwebsite, descriptions of data structures and\nprotocols, 400\nzookeeper reserved word, 379\nzookeeper_mt library, 382\nzookeeper_st library, 382\nzxid, 387\nIndex | 501About the Author\nTom White has been an Apache Hadoop committer since February 2007, and is a\nmember of the Apache Software Foundation. He works for Cloudera, a company that\noffers Hadoop support and training. Previously, he was an independent Hadoop con-\nsultant, working with companies to set up, use, and extend Hadoop. He has written\nnumerous articles for oreilly.com, java.net, and IBM’s developerWorks, and has spo-\nken about Hadoop at several conferences. Tom has a B.A. from the University of Cam-\nbridge and an M.A. in philosophy of science from the University of Leeds, UK He lives\nin Powys, Wales, with his family.\nColophon\nThe animal on the cover of Hadoop: The Definitive Guide is an African elephant. They\nare the largest land animals on earth (slightly larger than their cousin, the Asian ele-\nphant) and can be identified by their ears, which have been said to look somewhat like\nthe continent of Asia. Males stand 12 feet tall at the shoulder and weigh 12,000 pounds,\nbut they can get as big as 15,000 pounds, whereas females stand 10 feet tall and weigh\n8,000–11,000 pounds.\nThey have four molars; each weighs about 11 pounds and measures about 12 inches\nlong. As the front pair wears down and drops out in pieces, the back pair shifts forward,\nand two new molars emerge in the back of the mouth. They replace their teeth six times\nthroughout their lives, and between 40–60 years of age, they will lose all of their teeth\nand likely die of starvation (a common cause of death). Their tusks are teeth—actually\nit is the second set of incisors that becomes the tusks, which they use for digging for\nroots and stripping the bark off trees for food, fighting each other during mating season,\nand defending themselves against predators. Their tusks weigh between 50–100\npounds and are between 5–8 feet long.\nAfrican elephants live throughout sub-Saharan Africa. Most of the continent’s ele-\nphants live on savannas and in dry woodlands. In some regions, they can be found in\ndesert areas; in others, they are found in mountains.\nElephants are fond of water. They shower by sucking water into their trunks and\nspraying it all over themselves; afterward, they spray their skin with a protective coating\nof dust. An elephant’s trunk is actually a long nose used for smelling, breathing, trum-\npeting, drinking, and grabbing things, especially food. The trunk alone contains about\n100,000 different muscles. African elephants have two finger-like features on the end\nof their trunks that they can use to grab small items. They feed on roots, grass, fruit,\nand bark. An adult elephant can consume up to 300 pounds of food in a single day.\nThese hungry animals do not sleep much—they roam great distances while foraging\nfor the large quantities of food that they require to sustain their massive bodies.Having a baby elephant is a serious commitment. Elephants have longer pregnancies\nthan any other mammal: almost 22 months. At birth, elephants already weigh approx-\nimately 200 pounds and stand about 3 feet tall.\nThis species plays an important role in the forest and savanna ecosystems in which they\nlive. Many plant species are dependent on passing through an elephant’s digestive tract\nbefore they can germinate; it is estimated that at least a third of tree species in west\nAfrican forests rely on elephants in this way. Elephants grazing on vegetation also affect\nthe structure of habitats and influence bush fire patterns. For example, under natural\nconditions, elephants make gaps through the rainforest, enabling the sunlight to enter,\nwhich allows the growth of various plant species. This in turn facilitates a more abun-\ndant and more diverse fauna of smaller animals. As a result of the influence elephants\nhave over many plants and animals, they are often referred to as a keystone species\nbecause they are vital to the long-term survival of the ecosystems in which they live.\nThe cover image is from the Dover Pictorial Archive. The cover font is Adobe ITC\nGaramond. The text font is Linotype Birka; the heading font is Adobe Myriad Con-\ndensed; and the code font is LucasFont’s TheSansMonoCondensed.\n----------------------------------- SOURCE END -------------------------------------",1369115540609
error,log,,,"Problems occurred when invoking code from plug-in: ""org.eclipse.jdt.ui"".",1369115540611
error,log,,,"Error in JDT Core during reconcile",1369115540611
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.undo",1369115546454
error,log,,,"Exception occurred during compilation unit conversion:\n----------------------------------- SOURCE BEGIN -------------------------------------\npackage ch02;\n\nHadoop: The Definitive Guide\nTom White\nforeword by Doug Cutting\nBeijing • Cambridge • Farnham • Köln • Sebastopol • Taipei • TokyoHadoop: The Definitive Guide\nby Tom White\nCopyright © 2009 Tom White. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\nare also available for most titles (http://my.safaribooksonline.com). For more information, contact our\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\nEditor: Mike Loukides\nProduction Editor: Loranah Dimant\nProofreader: Nancy Kotary\nIndexer: Ellen Troutman Zaig\nCover Designer: Karen Montgomery\nInterior Designer: David Futato\nIllustrator: Robert Romano\nPrinting History:\nJune 2009:\nFirst Edition.\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\nO’Reilly Media, Inc. Hadoop: The Definitive Guide, the image of an African elephant, and related trade\ndress are trademarks of O’Reilly Media, Inc.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\ntrademark claim, the designations have been printed in caps or initial caps.\nWhile every precaution has been taken in the preparation of this book, the publisher and author assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\ntained herein.\nTM\nThis book uses RepKoverTM, a durable and flexible lay-flat binding.\nISBN: 978-0-596-52197-4\n[M]\n1243455573For Eliane, Emilia, and LottieTable of Contents\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\n1. Meet Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nData!\nData Storage and Analysis\nComparison with Other Systems\nRDBMS\nGrid Computing\nVolunteer Computing\nA Brief History of Hadoop\nThe Apache Hadoop Project\n1\n3\n4\n4\n6\n8\n9\n12\n2. MapReduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nA Weather Dataset\nData Format\nAnalyzing the Data with Unix Tools\nAnalyzing the Data with Hadoop\nMap and Reduce\nJava MapReduce\nScaling Out\nData Flow\nCombiner Functions\nRunning a Distributed MapReduce Job\nHadoop Streaming\nRuby\nPython\nHadoop Pipes\nCompiling and Running\n15\n15\n17\n18\n18\n20\n27\n27\n29\n32\n32\n33\n35\n36\n38\nv3. The Hadoop Distributed Filesystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nThe Design of HDFS\nHDFS Concepts\nBlocks\nNamenodes and Datanodes\nThe Command-Line Interface\nBasic Filesystem Operations\nHadoop Filesystems\nInterfaces\nThe Java Interface\nReading Data from a Hadoop URL\nReading Data Using the FileSystem API\nWriting Data\nDirectories\nQuerying the Filesystem\nDeleting Data\nData Flow\nAnatomy of a File Read\nAnatomy of a File Write\nCoherency Model\nParallel Copying with distcp\nKeeping an HDFS Cluster Balanced\nHadoop Archives\nUsing Hadoop Archives\nLimitations\n41\n42\n42\n44\n45\n45\n47\n49\n51\n51\n52\n56\n57\n58\n62\n63\n63\n66\n68\n70\n71\n71\n72\n73\n4. Hadoop I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nData Integrity\nData Integrity in HDFS\nLocalFileSystem\nChecksumFileSystem\nCompression\nCodecs\nCompression and Input Splits\nUsing Compression in MapReduce\nSerialization\nThe Writable Interface\nWritable Classes\nImplementing a Custom Writable\nSerialization Frameworks\nFile-Based Data Structures\nSequenceFile\nMapFile\nvi | Table of Contents\n75\n75\n76\n77\n77\n79\n83\n84\n86\n87\n89\n96\n101\n103\n103\n1105. Developing a MapReduce Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nThe Configuration API\nCombining Resources\nVariable Expansion\nConfiguring the Development Environment\nManaging Configuration\nGenericOptionsParser, Tool, and ToolRunner\nWriting a Unit Test\nMapper\nReducer\nRunning Locally on Test Data\nRunning a Job in a Local Job Runner\nTesting the Driver\nRunning on a Cluster\nPackaging\nLaunching a Job\nThe MapReduce Web UI\nRetrieving the Results\nDebugging a Job\nUsing a Remote Debugger\nTuning a Job\nProfiling Tasks\nMapReduce Workflows\nDecomposing a Problem into MapReduce Jobs\nRunning Dependent Jobs\n116\n117\n117\n118\n118\n121\n123\n124\n126\n127\n127\n130\n132\n132\n132\n134\n136\n138\n144\n145\n146\n149\n149\n151\n6. How MapReduce Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nAnatomy of a MapReduce Job Run\nJob Submission\nJob Initialization\nTask Assignment\nTask Execution\nProgress and Status Updates\nJob Completion\nFailures\nTask Failure\nTasktracker Failure\nJobtracker Failure\nJob Scheduling\nThe Fair Scheduler\nShuffle and Sort\nThe Map Side\nThe Reduce Side\n153\n153\n155\n155\n156\n156\n158\n159\n159\n161\n161\n161\n162\n163\n163\n164\nTable of Contents | viiConfiguration Tuning\nTask Execution\nSpeculative Execution\nTask JVM Reuse\nSkipping Bad Records\nThe Task Execution Environment\n166\n168\n169\n170\n171\n172\n7. MapReduce Types and Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nMapReduce Types\nThe Default MapReduce Job\nInput Formats\nInput Splits and Records\nText Input\nBinary Input\nMultiple Inputs\nDatabase Input (and Output)\nOutput Formats\nText Output\nBinary Output\nMultiple Outputs\nLazy Output\nDatabase Output\n175\n178\n184\n185\n196\n199\n200\n201\n202\n202\n203\n203\n210\n210\n8. MapReduce Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\nCounters\nBuilt-in Counters\nUser-Defined Java Counters\nUser-Defined Streaming Counters\nSorting\nPreparation\nPartial Sort\nTotal Sort\nSecondary Sort\nJoins\nMap-Side Joins\nReduce-Side Joins\nSide Data Distribution\nUsing the Job Configuration\nDistributed Cache\nMapReduce Library Classes\n211\n211\n213\n218\n218\n218\n219\n223\n227\n233\n233\n235\n238\n238\n239\n243\n9. Setting Up a Hadoop Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\nCluster Specification\nviii | Table of Contents\n245Network Topology\nCluster Setup and Installation\nInstalling Java\nCreating a Hadoop User\nInstalling Hadoop\nTesting the Installation\nSSH Configuration\nHadoop Configuration\nConfiguration Management\nEnvironment Settings\nImportant Hadoop Daemon Properties\nHadoop Daemon Addresses and Ports\nOther Hadoop Properties\nPost Install\nBenchmarking a Hadoop Cluster\nHadoop Benchmarks\nUser Jobs\nHadoop in the Cloud\nHadoop on Amazon EC2\n247\n249\n249\n250\n250\n250\n251\n251\n252\n254\n258\n263\n264\n266\n266\n267\n269\n269\n269\n10. Administering Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\nHDFS\nPersistent Data Structures\nSafe Mode\nAudit Logging\nTools\nMonitoring\nLogging\nMetrics\nJava Management Extensions\nMaintenance\nRoutine Administration Procedures\nCommissioning and Decommissioning Nodes\nUpgrades\n273\n273\n278\n280\n280\n285\n285\n286\n289\n292\n292\n293\n296\n11. Pig . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\nInstalling and Running Pig\nExecution Types\nRunning Pig Programs\nGrunt\nPig Latin Editors\nAn Example\nGenerating Examples\n302\n302\n304\n304\n305\n305\n307\nTable of Contents | ixComparison with Databases\nPig Latin\nStructure\nStatements\nExpressions\nTypes\nSchemas\nFunctions\nUser-Defined Functions\nA Filter UDF\nAn Eval UDF\nA Load UDF\nData Processing Operators\nLoading and Storing Data\nFiltering Data\nGrouping and Joining Data\nSorting Data\nCombining and Splitting Data\nPig in Practice\nParallelism\nParameter Substitution\n308\n309\n310\n311\n314\n315\n317\n320\n322\n322\n325\n327\n331\n331\n331\n334\n338\n339\n340\n340\n341\n12. HBase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\nHBasics\nBackdrop\nConcepts\nWhirlwind Tour of the Data Model\nImplementation\nInstallation\nTest Drive\nClients\nJava\nREST and Thrift\nExample\nSchemas\nLoading Data\nWeb Queries\nHBase Versus RDBMS\nSuccessful Service\nHBase\nUse Case: HBase at streamy.com\nPraxis\nVersions\nx | Table of Contents\n343\n344\n344\n344\n345\n348\n349\n350\n351\n353\n354\n354\n355\n358\n361\n362\n363\n363\n365\n365Love and Hate: HBase and HDFS\nUI\nMetrics\nSchema Design\n366\n367\n367\n367\n13. ZooKeeper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\nInstalling and Running ZooKeeper\nAn Example\nGroup Membership in ZooKeeper\nCreating the Group\nJoining a Group\nListing Members in a Group\nDeleting a Group\nThe ZooKeeper Service\nData Model\nOperations\nImplementation\nConsistency\nSessions\nStates\nBuilding Applications with ZooKeeper\nA Configuration Service\nThe Resilient ZooKeeper Application\nA Lock Service\nMore Distributed Data Structures and Protocols\nZooKeeper in Production\nResilience and Performance\nConfiguration\n370\n371\n372\n372\n374\n376\n378\n378\n379\n380\n384\n386\n388\n389\n391\n391\n394\n398\n400\n401\n401\n402\n14. Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\nHadoop Usage at Last.fm\nLast.fm: The Social Music Revolution\nHadoop at Last.fm\nGenerating Charts with Hadoop\nThe Track Statistics Program\nSummary\nHadoop and Hive at Facebook\nIntroduction\nHadoop at Facebook\nHypothetical Use Case Studies\nHive\nProblems and Future Work\nNutch Search Engine\n405\n405\n405\n406\n407\n414\n414\n414\n414\n417\n420\n424\n425\nTable of Contents | xiBackground\nData Structures\nSelected Examples of Hadoop Data Processing in Nutch\nSummary\nLog Processing at Rackspace\nRequirements/The Problem\nBrief History\nChoosing Hadoop\nCollection and Storage\nMapReduce for Logs\nCascading\nFields, Tuples, and Pipes\nOperations\nTaps, Schemes, and Flows\nCascading in Practice\nFlexibility\nHadoop and Cascading at ShareThis\nSummary\nTeraByte Sort on Apache Hadoop\n425\n426\n429\n438\n439\n439\n440\n440\n440\n442\n447\n448\n451\n452\n454\n456\n457\n461\n461\nA. Installing Apache Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\nB. Cloudera’s Distribution for Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\nC. Preparing the NCDC Weather Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479\nxii | Table of ContentsForeword\nHadoop got its start in Nutch. A few of us were attempting to build an open source\nweb search engine and having trouble managing computations running on even a\nhandful of computers. Once Google published its GFS and MapReduce papers, the\nroute became clear. They’d devised systems to solve precisely the problems we were\nhaving with Nutch. So we started, two of us, half-time, to try to recreate these systems\nas a part of Nutch.\nWe managed to get Nutch limping along on 20 machines, but it soon became clear that\nto handle the Web’s massive scale, we’d need to run it on thousands of machines and,\nmoreover, that the job was bigger than two half-time developers could handle.\nAround that time, Yahoo! got interested, and quickly put together a team that I joined.\nWe split off the distributed computing part of Nutch, naming it Hadoop. With the help\nof Yahoo!, Hadoop soon grew into a technology that could truly scale to the Web.\nIn 2006, Tom White started contributing to Hadoop. I already knew Tom through an\nexcellent article he’d written about Nutch, so I knew he could present complex ideas\nin clear prose. I soon learned that he could also develop software that was as pleasant\nto read as his prose.\nFrom the beginning, Tom’s contributions to Hadoop showed his concern for users and\nfor the project. Unlike most open source contributors, Tom is not primarily interested\nin tweaking the system to better meet his own needs, but rather in making it easier for\nanyone to use.\nInitially, Tom specialized in making Hadoop run well on Amazon’s EC2 and S3 serv-\nices. Then he moved on to tackle a wide variety of problems, including improving the\nMapReduce APIs, enhancing the website, and devising an object serialization frame-\nwork. In all cases, Tom presented his ideas precisely. In short order, Tom earned the\nrole of Hadoop committer and soon thereafter became a member of the Hadoop Project\nManagement Committee.\nTom is now a respected senior member of the Hadoop developer community. Though\nhe’s an expert in many technical corners of the project, his specialty is making Hadoop\neasier to use and understand.\nxiiiGiven this, I was very pleased when I learned that Tom intended to write a book about\nHadoop. Who could be better qualified? Now you have the opportunity to learn about\nHadoop from a master—not only of the technology, but also of common sense and\nplain talk.\n—Doug Cutting\nShed in the Yard, California\nxiv | ForewordPreface\nMartin Gardner, the mathematics and science writer, once said in an interview:\nBeyond calculus, I am lost. That was the secret of my column’s success. It took me so\nlong to understand what I was writing about that I knew how to write in a way most\nreaders would understand.*\nIn many ways, this is how I feel about Hadoop. Its inner workings are complex, resting\nas they do on a mixture of distributed systems theory, practical engineering, and com-\nmon sense. And to the uninitiated, Hadoop can appear alien.\nBut it doesn’t need to be like this. Stripped to its core, the tools that Hadoop provides\nfor building distributed systems—for data storage, data analysis, and coordination—\nare simple. If there’s a common theme, it is about raising the level of abstraction—to\ncreate building blocks for programmers who just happen to have lots of data to store,\nor lots of data to analyze, or lots of machines to coordinate, and who don’t have the\ntime, the skill, or the inclination to become distributed systems experts to build the\ninfrastructure to handle it.\nWith such a simple and generally applicable feature set, it seemed obvious to me when\nI started using it that Hadoop deserved to be widely used. However, at the time (in\nearly 2006), setting up, configuring, and writing programs to use Hadoop was an art.\nThings have certainly improved since then: there is more documentation, there are\nmore examples, and there are thriving mailing lists to go to when you have questions.\nAnd yet the biggest hurdle for newcomers is understanding what this technology is\ncapable of, where it excels, and how to use it. That is why I wrote this book.\nThe Apache Hadoop community has come a long way. Over the course of three years,\nthe Hadoop project has blossomed and spun off half a dozen subprojects. In this time,\nthe software has made great leaps in performance, reliability, scalability, and manage-\nability. To gain even wider adoption, however, I believe we need to make Hadoop even\neasier to use. This will involve writing more tools; integrating with more systems; and\n* “The science of fun,” Alex Bellos, The Guardian, May 31, 2008, http://www.guardian.co.uk/science/\n2008/may/31/maths.science.\nxvwriting new, improved APIs. I’m looking forward to being a part of this, and I hope\nthis book will encourage and enable others to do so, too.\nAdministrative Notes\nDuring discussion of a particular Java class in the text, I often omit its package name,\nto reduce clutter. If you need to know which package a class is in, you can easily look\nit up in Hadoop’s Java API documentation for the relevant subproject, linked to from\nthe Apache Hadoop home page at http://hadoop.apache.org/. Or if you’re using an IDE,\nit can help using its auto-complete mechanism.\nSimilarly, although it deviates from usual style guidelines, program listings that import\nmultiple classes from the same package may use the asterisk wildcard character to save\nspace (for example: import org.apache.hadoop.io.*).\nThe sample programs in this book are available for download from the website that\naccompanies this book: http://www.hadoopbook.com/. You will also find instructions\nthere for obtaining the datasets that are used in examples throughout the book, as well\nas further notes for running the programs in the book, and links to updates, additional\nresources, and my blog.\nWhat’s in This Book?\nThe rest of this book is organized as follows. Chapter 2 provides an introduction to\nMapReduce. Chapter 3 looks at Hadoop filesystems, and in particular HDFS, in depth.\nChapter 4 covers the fundamentals of I/O in Hadoop: data integrity, compression,\nserialization, and file-based data structures.\nThe next four chapters cover MapReduce in depth. Chapter 5 goes through the practical\nsteps needed to develop a MapReduce application. Chapter 6 looks at how MapReduce\nis implemented in Hadoop, from the point of view of a user. Chapter 7 is about the\nMapReduce programming model, and the various data formats that MapReduce can\nwork with. Chapter 8 is on advanced MapReduce topics, including sorting and joining\ndata.\nChapters 9 and 10 are for Hadoop administrators, and describe how to set up and\nmaintain a Hadoop cluster running HDFS and MapReduce.\nChapters 11, 12, and 13 present Pig, HBase, and ZooKeeper, respectively.\nFinally, Chapter 14 is a collection of case studies contributed by members of the Apache\nHadoop community.\nxvi | PrefaceConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, databases, data types, environment variables,\nstatements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter-\nmined by context.\nThis icon signifies a tip, suggestion, or general note.\nThis icon indicates a warning or caution.\nUsing Code Examples\nThis book is here to help you get your job done. In general, you may use the code in\nthis book in your programs and documentation. You do not need to contact us for\npermission unless you’re reproducing a significant portion of the code. For example,\nwriting a program that uses several chunks of code from this book does not require\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\nrequire permission. Answering a question by citing this book and quoting example\ncode does not require permission. Incorporating a significant amount of example code\nfrom this book into your product’s documentation does require permission.\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\nauthor, publisher, and ISBN. For example: “Hadoop: The Definitive Guide, by Tom\nWhite. Copyright 2009 Tom White, 978-0-596-52197-4.”\nIf you feel your use of code examples falls outside fair use or the permission given above,\nfeel free to contact us at permissions@oreilly.com.\nPreface | xviiSafari® Books Online\nWhen you see a Safari® Books Online icon on the cover of your favorite\ntechnology book, that means the book is available online through the\nO’Reilly Network Safari Bookshelf.\nSafari offers a solution that’s better than e-books. It’s a virtual library that lets you easily\nsearch thousands of top tech books, cut and paste code samples, download chapters,\nand find quick answers when you need the most accurate, current information. Try it\nfor free at http://my.safaribooksonline.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at:\nhttp://www.oreilly.com/catalog/9780596521974\nThe author also has a site for this book at:\nhttp://www.hadoopbook.com/\nTo comment or ask technical questions about this book, send email to:\nbookquestions@oreilly.com\nFor more information about our books, conferences, Resource Centers, and the\nO’Reilly Network, see our website at:\nhttp://www.oreilly.com\nAcknowledgments\nI have relied on many people, both directly and indirectly, in writing this book. I would\nlike to thank the Hadoop community, from whom I have learned, and continue to learn,\na great deal.\nIn particular, I would like to thank Michael Stack and Jonathan Gray for writing the\nchapter on HBase. Also thanks go to Adrian Woodhead, Marc de Palol, Joydeep Sen\nSarma, Ashish Thusoo, Andrzej Białecki, Stu Hood, Chris K Wensel, and Owen\nxviii | PrefaceO’Malley for contributing case studies for Chapter 14. Matt Massie and Todd Lipcon\nwrote Appendix B, for which I am very grateful.\nI would like to thank the following reviewers who contributed many helpful suggestions\nand improvements to my drafts: Raghu Angadi, Matt Biddulph, Christophe Bisciglia,\nRyan Cox, Devaraj Das, Alex Dorman, Chris Douglas, Alan Gates, Lars George, Patrick\nHunt, Aaron Kimball, Peter Krey, Hairong Kuang, Simon Maxen, Olga Natkovich,\nBenjamin Reed, Konstantin Shvachko, Allen Wittenauer, Matei Zaharia, and Philip\nZeyliger. Ajay Anand kept the review process flowing smoothly. Philip (“flip”) Kromer\nkindly helped me with the NCDC weather dataset featured in the examples in this book.\nSpecial thanks to Owen O’Malley and Arun C Murthy for explaining the intricacies of\nthe MapReduce shuffle to me. Any errors that remain are, of course, to be laid at my\ndoor.\nI am particularly grateful to Doug Cutting for his encouragement, support, and friend-\nship, and for contributing the foreword.\nThanks also go to the many others with whom I have had conversations or email\ndiscussions over the course of writing the book.\nHalfway through writing this book, I joined Cloudera, and I want to thank my\ncolleagues for being incredibly supportive in allowing me the time to write, and to get\nit finished promptly.\nI am grateful to my editor, Mike Loukides, and his colleagues at O’Reilly for their help\nin the preparation of this book. Mike has been there throughout to answer my ques-\ntions, to read my first drafts, and to keep me on schedule.\nFinally, the writing of this book has been a great deal of work, and I couldn’t have done\nit without the constant support of my family. My wife, Eliane, not only kept the home\ngoing, but also stepped in to help review, edit, and chase case studies. My daughters,\nEmilia and Lottie, have been very understanding, and I’m looking forward to spending\nlots more time with all of them.\nPreface | xixCHAPTER 1\nMeet Hadoop\nIn pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log,\nthey didn’t try to grow a larger ox. We shouldn’t be trying for bigger computers, but for\nmore systems of computers.\n—Grace Hopper\nData!\nWe live in the data age. It’s not easy to measure the total volume of data stored elec-\ntronically, but an IDC estimate put the size of the “digital universe” at 0.18 zettabytes\nin 2006, and is forecasting a tenfold growth by 2011 to 1.8 zettabytes.* A zettabyte is\n1021 bytes, or equivalently one thousand exabytes, one million petabytes, or one billion\nterabytes. That’s roughly the same order of magnitude as one disk drive for every person\nin the world.\nThis flood of data is coming from many sources. Consider the following:†\n• The New York Stock Exchange generates about one terabyte of new trade data per\nday.\n• Facebook hosts approximately 10 billion photos, taking up one petabyte of storage.\n• Ancestry.com, the genealogy site, stores around 2.5 petabytes of data.\n• The Internet Archive stores around 2 petabytes of data, and is growing at a rate of\n20 terabytes per month.\n• The Large Hadron Collider near Geneva, Switzerland, will produce about 15\npetabytes of data per year.\n* From Gantz et al., “The Diverse and Exploding Digital Universe,” March 2008 (http://www.emc.com/\ncollateral/analyst-reports/diverse-exploding-digital-universe.pdf).\n† http://www.intelligententerprise.com/showArticle.jhtml?articleID=207800705, http://mashable.com/2008/10/\n15/facebook-10-billion-photos/, http://blog.familytreemagazine.com/insider/Inside+Ancestrycoms+TopSecret\n+Data+Center.aspx, and http://www.archive.org/about/faqs.php, http://www.interactions.org/cms/?pid=\n1027032.\n1So there’s a lot of data out there. But you are probably wondering how it affects you.\nMost of the data is locked up in the largest web properties (like search engines), or\nscientific or financial institutions, isn’t it? Does the advent of “Big Data,” as it is being\ncalled, affect smaller organizations or individuals?\nI argue that it does. Take photos, for example. My wife’s grandfather was an avid\nphotographer, and took photographs throughout his adult life. His entire corpus of\nmedium format, slide, and 35mm film, when scanned in at high-resolution, occupies\naround 10 gigabytes. Compare this to the digital photos that my family took last year,\nwhich take up about 5 gigabytes of space. My family is producing photographic data\nat 35 times the rate my wife’s grandfather’s did, and the rate is increasing every year as\nit becomes easier to take more and more photos.\nMore generally, the digital streams that individuals are producing are growing apace.\nMicrosoft Research’s MyLifeBits project gives a glimpse of archiving of personal infor-\nmation that may become commonplace in the near future. MyLifeBits was an experi-\nment where an individual’s interactions—phone calls, emails, documents—were cap-\ntured electronically and stored for later access. The data gathered included a photo\ntaken every minute, which resulted in an overall data volume of one gigabyte a\nmonth. When storage costs come down enough to make it feasible to store continuous\naudio and video, the data volume for a future MyLifeBits service will be many times that.\nThe trend is for every individual’s data footprint to grow, but perhaps more importantly\nthe amount of data generated by machines will be even greater than that generated by\npeople. Machine logs, RFID readers, sensor networks, vehicle GPS traces, retail\ntransactions—all of these contribute to the growing mountain of data.\nThe volume of data being made publicly available increases every year too. Organiza-\ntions no longer have to merely manage their own data: success in the future will be\ndictated to a large extent by their ability to extract value from other organizations’ data.\nInitiatives such as Public Data Sets on Amazon Web Services, Infochimps.org, and\ntheinfo.org exist to foster the “information commons,” where data can be freely (or in\nthe case of AWS, for a modest price) shared for anyone to download and analyze.\nMashups between different information sources make for unexpected and hitherto\nunimaginable applications.\nTake, for example, the Astrometry.net project, which watches the Astrometry group\non Flickr for new photos of the night sky. It analyzes each image, and identifies which\npart of the sky it is from, and any interesting celestial bodies, such as stars or galaxies.\nAlthough it’s still a new and experimental service, it shows the kind of things that are\npossible when data (in this case, tagged photographic images) is made available and\nused for something (image analysis) that was not anticipated by the creator.\nIt has been said that “More data usually beats better algorithms,” which is to say that\nfor some problems (such as recommending movies or music based on past preferences),\n2 | Chapter 1: Meet Hadoophowever fiendish your algorithms are, they can often be beaten simply by having more\ndata (and a less sophisticated algorithm).‡\nThe good news is that Big Data is here. The bad news is that we are struggling to store\nand analyze it.\nData Storage and Analysis\nThe problem is simple: while the storage capacities of hard drives have increased mas-\nsively over the years, access speeds—the rate at which data can be read from drives—\nhave not kept up. One typical drive from 1990 could store 1370 MB of data and had a\ntransfer speed of 4.4 MB/s,§ so you could read all the data from a full drive in around\nfive minutes. Almost 20 years later one terabyte drives are the norm, but the transfer\nspeed is around 100 MB/s, so it takes more than two and a half hours to read all the\ndata off the disk.\nThis is a long time to read all data on a single drive—and writing is even slower. The\nobvious way to reduce the time is to read from multiple disks at once. Imagine if we\nhad 100 drives, each holding one hundredth of the data. Working in parallel, we could\nread the data in under two minutes.\nOnly using one hundredth of a disk may seem wasteful. But we can store one hundred\ndatasets, each of which is one terabyte, and provide shared access to them. We can\nimagine that the users of such a system would be happy to share access in return for\nshorter analysis times, and, statistically, that their analysis jobs would be likely to be\nspread over time, so they wouldn’t interfere with each other too much.\nThere’s more to being able to read and write data in parallel to or from multiple disks,\nthough.\nThe first problem to solve is hardware failure: as soon as you start using many pieces\nof hardware, the chance that one will fail is fairly high. A common way of avoiding data\nloss is through replication: redundant copies of the data are kept by the system so that\nin the event of failure, there is another copy available. This is how RAID works, for\ninstance, although Hadoop’s filesystem, the Hadoop Distributed Filesystem (HDFS),\ntakes a slightly different approach, as you shall see later.\nThe second problem is that most analysis tasks need to be able to combine the data in\nsome way; data read from one disk may need to be combined with the data from any\nof the other 99 disks. Various distributed systems allow data to be combined from\nmultiple sources, but doing this correctly is notoriously challenging. MapReduce pro-\nvides a programming model that abstracts the problem from disk reads and writes,\n‡ The quote is from Anand Rajaraman writing about the Netflix Challenge (http://anand.typepad.com/\ndatawocky/2008/03/more-data-usual.html).\n§ These specifications are for the Seagate ST-41600n.\nData Storage and Analysis | 3transforming it into a computation over sets of keys and values. We will look at the\ndetails of this model in later chapters, but the important point for the present discussion\nis that there are two parts to the computation, the map and the reduce, and it’s the\ninterface between the two where the “mixing” occurs. Like HDFS, MapReduce has\nreliability built-in.\nThis, in a nutshell, is what Hadoop provides: a reliable shared storage and analysis\nsystem. The storage is provided by HDFS, and analysis by MapReduce. There are other\nparts to Hadoop, but these capabilities are its kernel.\nComparison with Other Systems\nThe approach taken by MapReduce may seem like a brute-force approach. The premise\nis that the entire dataset—or at least a good portion of it—is processed for each query.\nBut this is its power. MapReduce is a batch query processor, and the ability to run an\nad hoc query against your whole dataset and get the results in a reasonable time is\ntransformative. It changes the way you think about data, and unlocks data that was\npreviously archived on tape or disk. It gives people the opportunity to innovate with\ndata. Questions that took too long to get answered before can now be answered, which\nin turn leads to new questions and new insights.\nFor example, Mailtrust, Rackspace’s mail division, used Hadoop for processing email\nlogs. One ad hoc query they wrote was to find the geographic distribution of their users.\nIn their words:\nThis data was so useful that we’ve scheduled the MapReduce job to run monthly and we\nwill be using this data to help us decide which Rackspace data centers to place new mail\nservers in as we grow.‖\nBy bringing several hundred gigabytes of data together and having the tools to analyze\nit, the Rackspace engineers were able to gain an understanding of the data that they\notherwise would never have had, and, furthermore, they were able to use what they\nhad learned to improve the service for their customers. You can read more about how\nRackspace uses Hadoop in Chapter 14.\nRDBMS\nWhy can’t we use databases with lots of disks to do large-scale batch analysis? Why is\nMapReduce needed?\nThe answer to these questions comes from another trend in disk drives: seek time is\nimproving more slowly than transfer rate. Seeking is the process of moving the disk’s\nhead to a particular place on the disk to read or write data. It characterizes the latency\nof a disk operation, whereas the transfer rate corresponds to a disk’s bandwidth.\n‖ http://blog.racklabs.com/?p=66\n4 | Chapter 1: Meet HadoopIf the data access pattern is dominated by seeks, it will take longer to read or write large\nportions of the dataset than streaming through it, which operates at the transfer rate.\nOn the other hand, for updating a small proportion of records in a database, a tradi-\ntional B-Tree (the data structure used in relational databases, which is limited by the\nrate it can perform seeks) works well. For updating the majority of a database, a B-Tree\nis less efficient than MapReduce, which uses Sort/Merge to rebuild the database.\nIn many ways, MapReduce can be seen as a complement to an RDBMS. (The differences\nbetween the two systems are shown in Table 1-1.) MapReduce is a good fit for problems\nthat need to analyze the whole dataset, in a batch fashion, particularly for ad hoc anal-\nysis. An RDBMS is good for point queries or updates, where the dataset has been in-\ndexed to deliver low-latency retrieval and update times of a relatively small amount of\ndata. MapReduce suits applications where the data is written once, and read many\ntimes, whereas a relational database is good for datasets that are continually updated.\nTable 1-1. RDBMS compared to MapReduce\nTraditional RDBMS MapReduce\nData size Gigabytes Petabytes\nAccess Interactive and batch Batch\nUpdates Read and write many times Write once, read many times\nStructure Static schema Dynamic schema\nIntegrity High Low\nScaling Nonlinear Linear\nAnother difference between MapReduce and an RDBMS is the amount of structure in\nthe datasets that they operate on. Structured data is data that is organized into entities\nthat have a defined format, such as XML documents or database tables that conform\nto a particular predefined schema. This is the realm of the RDBMS. Semi-structured\ndata, on the other hand, is looser, and though there may be a schema, it is often ignored,\nso it may be used only as a guide to the structure of the data: for example, a spreadsheet,\nin which the structure is the grid of cells, although the cells themselves may hold any\nform of data. Unstructured data does not have any particular internal structure: for\nexample, plain text or image data. MapReduce works well on unstructured or semi-\nstructured data, since it is designed to interpret the data at processing time. In other\nwords, the input keys and values for MapReduce are not an intrinsic property of the\ndata, but they are chosen by the person analyzing the data.\nComparison with Other Systems | 5Relational data is often normalized to retain its integrity, and remove redundancy.\nNormalization poses problems for MapReduce, since it makes reading a record a non-\nlocal operation, and one of the central assumptions that MapReduce makes is that it\nis possible to perform (high-speed) streaming reads and writes.\nA web server log is a good example of a set of records that is not normalized (for ex-\nample, the client hostnames are specified in full each time, even though the same client\nmay appear many times), and this is one reason that logfiles of all kinds are particularly\nwell-suited to analysis with MapReduce.\nMapReduce is a linearly scalable programming model. The programmer writes two\nfunctions—a map function and a reduce function—each of which defines a mapping\nfrom one set of key-value pairs to another. These functions are oblivious to the size of\nthe data or the cluster that they are operating on, so they can be used unchanged for a\nsmall dataset and for a massive one. More importantly, if you double the size of the\ninput data, a job will run twice as slow. But if you also double the size of the cluster, a\njob will run as fast as the original one. This is not generally true of SQL queries.\nOver time, however, the differences between relational databases and MapReduce sys-\ntems are likely to blur. Both as relational databases start incorporating some of the ideas\nfrom MapReduce (such as Aster Data’s and Greenplum’s databases), and, from the\nother direction, as higher-level query languages built on MapReduce (such as Pig and\nHive) make MapReduce systems more approachable to traditional database\nprogrammers.#\nGrid Computing\nThe High Performance Computing (HPC) and Grid Computing communities have\nbeen doing large-scale data processing for years, using such APIs as Message Passing\nInterface (MPI). Broadly, the approach in HPC is to distribute the work across a cluster\nof machines, which access a shared filesystem, hosted by a SAN. This works well for\npredominantly compute-intensive jobs, but becomes a problem when nodes need to\naccess larger data volumes (hundreds of gigabytes, the point at which MapReduce really\nstarts to shine), since the network bandwidth is the bottleneck, and compute nodes\nbecome idle.\n# In January 2007, David J. DeWitt and Michael Stonebraker caused a stir by publishing “MapReduce: A major\nstep backwards” (http://www.databasecolumn.com/2008/01/mapreduce-a-major-step-back.html), in which\nthey criticized MapReduce for being a poor substitute for relational databases. Many commentators argued\nthat it was a false comparison (see, for example, Mark C. Chu-Carroll’s “Databases are hammers; MapReduce\nis a screwdriver,” http://scienceblogs.com/goodmath/2008/01/databases_are_hammers_mapreduc.php), and\nDeWitt and Stonebraker followed up with “MapReduce II” (http://www.databasecolumn.com/2008/01/\nmapreduce-continued.html), where they addressed the main topics brought up by others.\n6 | Chapter 1: Meet HadoopMapReduce tries to colocate the data with the compute node, so data access is fast\nsince it is local.* This feature, known as data locality, is at the heart of MapReduce and\nis the reason for its good performance. Recognizing that network bandwidth is the most\nprecious resource in a data center environment (it is easy to saturate network links by\ncopying data around), MapReduce implementations go to great lengths to preserve it\nby explicitly modelling network topology. Notice that this arrangement does not pre-\nclude high-CPU analyses in MapReduce.\nMPI gives great control to the programmer, but requires that he or she explicitly handle\nthe mechanics of the data flow, exposed via low-level C routines and constructs, such\nas sockets, as well as the higher-level algorithm for the analysis. MapReduce operates\nonly at the higher level: the programmer thinks in terms of functions of key and value\npairs, and the data flow is implicit.\nCoordinating the processes in a large-scale distributed computation is a challenge. The\nhardest aspect is gracefully handling partial failure—when you don’t know if a remote\nprocess has failed or not—and still making progress with the overall computation.\nMapReduce spares the programmer from having to think about failure, since the\nimplementation detects failed map or reduce tasks and reschedules replacements on\nmachines that are healthy. MapReduce is able to do this since it is a shared-nothing\narchitecture, meaning that tasks have no dependence on one other. (This is a slight\noversimplification, since the output from mappers is fed to the reducers, but this is\nunder the control of the MapReduce system; in this case, it needs to take more care\nrerunning a failed reducer than rerunning a failed map, since it has to make sure it can\nretrieve the necessary map outputs, and if not, regenerate them by running the relevant\nmaps again.) So from the programmer’s point of view, the order in which the tasks run\ndoesn’t matter. By contrast, MPI programs have to explicitly manage their own check-\npointing and recovery, which gives more control to the programmer, but makes them\nmore difficult to write.\nMapReduce might sound like quite a restrictive programming model, and in a sense it\nis: you are limited to key and value types that are related in specified ways, and mappers\nand reducers run with very limited coordination between one another (the mappers\npass keys and values to reducers). A natural question to ask is: can you do anything\nuseful or nontrivial with it?\nThe answer is yes. MapReduce was invented by engineers at Google as a system for\nbuilding production search indexes because they found themselves solving the same\nproblem over and over again (and MapReduce was inspired by older ideas from the\nfunctional programming, distributed computing, and database communities), but it\nhas since been used for many other applications in many other industries. It is pleasantly\nsurprising to see the range of algorithms that can be expressed in MapReduce, from\n* Jim Gray was an early advocate of putting the computation near the data. See “Distributed Computing\nEconomics,” March 2003, http://research.microsoft.com/apps/pubs/default.aspx?id=70001.\nComparison with Other Systems | 7image analysis, to graph-based problems, to machine learning algorithms.† It can’t\nsolve every problem, of course, but it is a general data-processing tool.\nYou can see a sample of some of the applications that Hadoop has been used for in\nChapter 14.\nVolunteer Computing\nWhen people first hear about Hadoop and MapReduce, they often ask, “How is it\ndifferent from SETI@home?” SETI, the Search for Extra-Terrestrial Intelligence, runs\na project called SETI@home in which volunteers donate CPU time from their otherwise\nidle computers to analyze radio telescope data for signs of intelligent life outside earth.\nSETI@home is the most well-known of many volunteer computing projects; others in-\nclude the Great Internet Mersenne Prime Search (to search for large prime numbers)\nand Folding@home (to understand protein folding, and how it relates to disease).\nVolunteer computing projects work by breaking the problem they are trying to solve\ninto chunks called work units, which are sent to computers around the world to be\nanalyzed. For example, a SETI@home work unit is about 0.35 MB of radio telescope\ndata, and takes hours or days to analyze on a typical home computer. When the analysis\nis completed, the results are sent back to the server, and the client gets another work\nunit. As a precaution to combat cheating, each work unit is sent to three different\nmachines, and needs at least two results to agree to be accepted.\nAlthough SETI@home may be superficially similar to MapReduce (breaking a problem\ninto independent pieces to be worked on in parallel), there are some significant differ-\nences. The SETI@home problem is very CPU-intensive, which makes it suitable for\nrunning on hundreds of thousands of computers across the world,‡ since the time to\ntransfer the work unit is dwarfed by the time to run the computation on it. Volunteers\nare donating CPU cycles, not bandwidth.\nMapReduce is designed to run jobs that last minutes or hours on trusted, dedicated\nhardware running in a single data center with very high aggregate bandwidth inter-\nconnects. By contrast, SETI@home runs a perpetual computation on untrusted ma-\nchines on the Internet with highly variable connection speeds and no data locality.\n† Apache Mahout (http://lucene.apache.org/mahout/) is a project to build machine learning libraries (such as\nclassification and clustering algorithms) that run on Hadoop.\n‡ In January 2008, SETI@home was reported at http://www.planetary.org/programs/projects/setiathome/\nsetiathome_20080115.html to be processing 300 gigabytes a day, using 320,000 computers (most of which\nare not dedicated to SETI@home; they are used for other things, too).\n8 | Chapter 1: Meet HadoopA Brief History of Hadoop\nHadoop was created by Doug Cutting, the creator of Apache Lucene, the widely used\ntext search library. Hadoop has its origins in Apache Nutch, an open source web search\nengine, itself a part of the Lucene project.\nThe Origin of the Name “Hadoop”\nThe name Hadoop is not an acronym; it’s a made-up name. The project’s creator, Doug\nCutting, explains how the name came about:\nThe name my kid gave a stuffed yellow elephant. Short, relatively easy to spell and\npronounce, meaningless, and not used elsewhere: those are my naming criteria.\nKids are good at generating such. Googol is a kid’s term.\nSubprojects and “contrib” modules in Hadoop also tend to have names that are unre-\nlated to their function, often with an elephant or other animal theme (“Pig,” for exam-\nple). Smaller components are given more descriptive (and therefore more mundane)\nnames. This is a good principle, as it means you can generally work out what something\ndoes from its name. For example, the jobtracker§ keeps track of MapReduce jobs.\nBuilding a web search engine from scratch was an ambitious goal, for not only is the\nsoftware required to crawl and index websites complex to write, but it is also a challenge\nto run without a dedicated operations team, since there are so many moving parts. It’s\nexpensive too: Mike Cafarella and Doug Cutting estimated a system supporting a 1-\nbillion-page index would cost around half a million dollars in hardware, with a monthly\nrunning cost of $30,000.‖ Nevertheless, they believed it was a worthy goal, as it would\nopen up and ultimately democratize search engine algorithms.\nNutch was started in 2002, and a working crawler and search system quickly emerged.\nHowever, they realized that their architecture wouldn’t scale to the billions of pages on\nthe Web. Help was at hand with the publication of a paper in 2003 that described the\narchitecture of Google’s distributed filesystem, called GFS, which was being used in\nproduction at Google.# GFS, or something like it, would solve their storage needs for\nthe very large files generated as a part of the web crawl and indexing process. In par-\nticular, GFS would free up time being spent on administrative tasks such as managing\nstorage nodes. In 2004, they set about writing an open source implementation, the\nNutch Distributed Filesystem (NDFS).\n§ In this book, we use the lowercase form, “jobtracker,” to denote the entity when it’s being referred to\ngenerally, and the CamelCase form JobTracker to denote the Java class that implements it.\n‖ Mike Cafarella and Doug Cutting, “Building Nutch: Open Source Search,” ACM Queue, April 2004, http://\nqueue.acm.org/detail.cfm?id=988408.\n# Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, “The Google File System,” October 2003, http:\n//labs.google.com/papers/gfs.html.\nA Brief History of Hadoop | 9In 2004, Google published the paper that introduced MapReduce to the world.* Early\nin 2005, the Nutch developers had a working MapReduce implementation in Nutch,\nand by the middle of that year all the major Nutch algorithms had been ported to run\nusing MapReduce and NDFS.\nNDFS and the MapReduce implementation in Nutch were applicable beyond the realm\nof search, and in February 2006 they moved out of Nutch to form an independent\nsubproject of Lucene called Hadoop. At around the same time, Doug Cutting joined\nYahoo!, which provided a dedicated team and the resources to turn Hadoop into a\nsystem that ran at web scale (see sidebar). This was demonstrated in February 2008\nwhen Yahoo! announced that its production search index was being generated by a\n10,000-core Hadoop cluster.†\nIn January 2008, Hadoop was made its own top-level project at Apache, confirming its\nsuccess and its diverse, active community. By this timem Hadoop was being used by\nmany other companies besides Yahoo!, such as Last.fm, Facebook, and the New York\nTimes (some applications are covered in the case studies in Chapter 14 and on the\nHadoop wiki.\nIn one well-publicized feat, the New York Times used Amazon’s EC2 compute cloud\nto crunch through four terabytes of scanned archives from the paper converting them\nto PDFs for the Web.‡ The processing took less than 24 hours to run using 100 ma-\nchines, and the project probably wouldn’t have been embarked on without the com-\nbination of Amazon’s pay-by-the-hour model (which allowed the NYT to access a large\nnumber of machines for a short period), and Hadoop’s easy-to-use parallel program-\nming model.\nIn April 2008, Hadoop broke a world record to become the fastest system to sort a\nterabyte of data. Running on a 910-node cluster, Hadoop sorted one terabyte in 209\nseconds (just under 31⁄2 minutes), beating the previous year’s winner of 297 seconds\n(described in detail in “TeraByte Sort on Apache Hadoop” on page 461). In November\nof the same year, Google reported that its MapReduce implementation sorted one ter-\nabyte in 68 seconds.§ As this book was going to press (May 2009), it was announced\nthat a team at Yahoo! used Hadoop to sort one terabyte in 62 seconds.\n* Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters ,” December\n2004, http://labs.google.com/papers/mapreduce.html.\n† “Yahoo! Launches World’s Largest Hadoop Production Application,” 19 February 2008, http://developer\n.yahoo.net/blogs/hadoop/2008/02/yahoo-worlds-largest-production-hadoop.html.\n‡ Derek Gottfrid, “Self-service, Prorated Super Computing Fun!,” 1 November 2007, http://open.blogs.nytimes\n.com/2007/11/01/self-service-prorated-super-computing-fun/.\n§ “Sorting 1PB with MapReduce,” 21 November 2008, http://googleblog.blogspot.com/2008/11/sorting-1pb\n-with-mapreduce.html.\n10 | Chapter 1: Meet HadoopHadoop at Yahoo!\nBuilding Internet-scale search engines requires huge amounts of data and therefore\nlarge numbers of machines to process it. Yahoo! Search consists of four primary com-\nponents: the Crawler, which downloads pages from web servers; the WebMap, which\nbuilds a graph of the known Web; the Indexer, which builds a reverse index to the best\npages; and the Runtime, which answers users’ queries. The WebMap is a graph that\nconsists of roughly 1 trillion (1012) edges each representing a web link and 100 billion\n(1011) nodes each representing distinct URLs. Creating and analyzing such a large graph\nrequires a large number of computers running for many days. In early 2005, the infra-\nstructure for the WebMap, named Dreadnaught, needed to be redesigned to scale up\nto more nodes. Dreadnaught had successfully scaled from 20 to 600 nodes, but required\na complete redesign to scale up further. Dreadnaught is similar to MapReduce in many\nways, but provides more flexibility and less structure. In particular, each fragment in a\nDreadnaught job can send output to each of the fragments in the next stage of the job,\nbut the sort was all done in library code. In practice, most of the WebMap phases were\npairs that corresponded to MapReduce. Therefore, the WebMap applications would\nnot require extensive refactoring to fit into MapReduce.\nEric Baldeschwieler (Eric14) created a small team and we starting designing and\nprototyping a new framework written in C++ modeled after GFS and MapReduce to\nreplace Dreadnaught. Although the immediate need was for a new framework for\nWebMap, it was clear that standardization of the batch platform across Yahoo! Search\nwas critical and by making the framework general enough to support other users, we\ncould better leverage investment in the new platform.\nAt the same time, we were watching Hadoop, which was part of Nutch, and its progress.\nIn January 2006, Yahoo! hired Doug Cutting, and a month later we decided to abandon\nour prototype and adopt Hadoop. The advantage of Hadoop over our prototype and\ndesign was that it was already working with a real application (Nutch) on 20 nodes.\nThat allowed us to bring up a research cluster two months later and start helping real\ncustomers use the new framework much sooner than we could have otherwise. Another\nadvantage, of course, was that since Hadoop was already open source, it was easier\n(although far from easy!) to get permission from Yahoo!’s legal department to work in\nopen source. So we set up a 200-node cluster for the researchers in early 2006 and put\nthe WebMap conversion plans on hold while we supported and improved Hadoop for\nthe research users.\nHere’s a quick timeline of how things have progressed:\n• 2004—Initial versions of what is now Hadoop Distributed Filesystem and Map-\nReduce implemented by Doug Cutting and Mike Cafarella.\n• December 2005—Nutch ported to the new framework. Hadoop runs reliably on\n20 nodes.\n• January 2006—Doug Cutting joins Yahoo!.\n• February 2006—Apache Hadoop project officially started to support the stand-\nalone development of MapReduce and HDFS.\nA Brief History of Hadoop | 11• February 2006—Adoption of Hadoop by Yahoo! Grid team.\n• April 2006—Sort benchmark (10 GB/node) run on 188 nodes in 47.9 hours.\n• May 2006—Yahoo! set up a Hadoop research cluster—300 nodes.\n• May 2006—Sort benchmark run on 500 nodes in 42 hours (better hardware than\nApril benchmark).\n• October 2006—Research cluster reaches 600 nodes.\n• December 2006—Sort benchmark run on 20 nodes in 1.8 hours, 100 nodes in 3.3\nhours, 500 nodes in 5.2 hours, 900 nodes in 7.8 hours.\n• January 2007—Research cluster reaches 900 nodes.\n• April 2007—Research clusters—2 clusters of 1000 nodes.\n• April 2008—Won the 1 terabyte sort benchmark in 209 seconds on 900 nodes.\n• October 2008—Loading 10 terabytes of data per a day on to research clusters.\n• March 2009—17 clusters with a total of 24,000 nodes.\n• April 2009—Won the minute sort by sorting 500 GB in 59 seconds (on 1400 nodes)\nand the 100 terabyte sort in 173 minutes (on 3400 nodes).\n—Owen O’Malley\nThe Apache Hadoop Project\nToday, Hadoop is a collection of related subprojects that fall under the umbrella of\ninfrastructure for distributed computing. These projects are hosted by the Apache Soft\nware Foundation, which provides support for a community of open source software\nprojects. Although Hadoop is best known for MapReduce and its distributed filesystem\n(HDFS, renamed from NDFS), the other subprojects provide complementary services,\nor build on the core to add higher-level abstractions. The subprojects, and where they\nsit in the technology stack, are shown in Figure 1-1 and described briefly here:\nCore\nA set of components and interfaces for distributed filesystems and general I/O\n(serialization, Java RPC, persistent data structures).\nAvro\nA data serialization system for efficient, cross-language RPC, and persistent data\nstorage. (At the time of this writing, Avro had been created only as a new subpro-\nject, and no other Hadoop subprojects were using it yet.)\nMapReduce\nA distributed data processing model and execution environment that runs on large\nclusters of commodity machines.\nHDFS\nA distributed filesystem that runs on large clusters of commodity machines.\n12 | Chapter 1: Meet HadoopPig\nA data flow language and execution environment for exploring very large datasets.\nPig runs on HDFS and MapReduce clusters.\nHBase\nA distributed, column-oriented database. HBase uses HDFS for its underlying\nstorage, and supports both batch-style computations using MapReduce and point\nqueries (random reads).\nZooKeeper\nA distributed, highly available coordination service. ZooKeeper provides primitives\nsuch as distributed locks that can be used for building distributed applications.\nHive\nA distributed data warehouse. Hive manages data stored in HDFS and provides a\nquery language based on SQL (and which is translated by the runtime engine to\nMapReduce jobs) for querying the data.\nChukwa\nA distributed data collection and analysis system. Chukwa runs collectors that\nstore data in HDFS, and it uses MapReduce to produce reports. (At the time of this\nwriting, Chukwa had only recently graduated from a “contrib” module in Core to\nits own subproject.)\nFigure 1-1. Hadoop subprojects\nThe Apache Hadoop Project | 13CHAPTER 2\nMapReduce\nMapReduce is a programming model for data processing. The model is simple, yet not\ntoo simple to express useful programs in. Hadoop can run MapReduce programs writ-\nten in various languages; in this chapter, we shall look at the same program expressed\nin Java, Ruby, Python, and C++. Most importantly, MapReduce programs are inher-\nently parallel, thus putting very large-scale data analysis into the hands of anyone with\nenough machines at their disposal. MapReduce comes into its own for large datasets,\nso let’s start by looking at one.\nA Weather Dataset\nFor our example, we will write a program that mines weather data. Weather sensors\ncollecting data every hour at many locations across the globe gather a large volume of\nlog data, which is a good candidate for analysis with MapReduce, since it is semi-\nstructured and record-oriented.\nData Format\nThe data we will use is from the National Climatic Data Center (NCDC, http://www\n.ncdc.noaa.gov/). The data is stored using a line-oriented ASCII format, in which each\nline is a record. The format supports a rich set of meteorological elements, many of\nwhich are optional or with variable data lengths. For simplicity, we shall focus on the\nbasic elements, such as temperature, which are always present and are of fixed width.\nExample 2-1 shows a sample line with some of the salient fields highlighted. The line\nhas been split into multiple lines to show each field: in the real file, fields are packed\ninto one line with no delimiters.\n15Example 2-1. Format of a National Climate Data Center record\n0057\n332130\n99999\n19500101\n0300\n4\n+51317\n+028783\nFM-12\n+0171\n99999\nV020\n320\n1\nN\n0072\n1\n00450\n1\nC\nN\n010000\n1\nN\n9\n-0128\n1\n-0139\n1\n10268\n1\n#\n#\n#\n#\nUSAF weather station identifier\nWBAN weather station identifier\nobservation date\nobservation time\n# latitude (degrees x 1000)\n# longitude (degrees x 1000)\n# elevation (meters)\n# wind direction (degrees)\n# quality code\n# sky ceiling height (meters)\n# quality code\n# visibility distance (meters)\n# quality code\n#\n#\n#\n#\n#\n#\nair temperature (degrees Celsius x 10)\nquality code\ndew point temperature (degrees Celsius x 10)\nquality code\natmospheric pressure (hectopascals x 10)\nquality code\nData files are organized by date and weather station. There is a directory for each year\nfrom 1901 to 2001, each containing a gzipped file for each weather station with its\nreadings for that year. For example, here are the first entries for 1990:\n% ls raw/1990 | head\n010010-99999-1990.gz\n010014-99999-1990.gz\n010015-99999-1990.gz\n010016-99999-1990.gz\n010017-99999-1990.gz\n010030-99999-1990.gz\n010040-99999-1990.gz\n010080-99999-1990.gz\n010100-99999-1990.gz\n010150-99999-1990.gz\nSince there are tens of thousands of weather stations, the whole dataset is made up of\na large number of relatively small files. It’s generally easier and more efficient to process\na smaller number of relatively large files, so the data was preprocessed so that each\n16 | Chapter 2: MapReduceyear’s readings were concatenated into a single file. (The means by which this was\ncarried out is described in Appendix C.)\nAnalyzing the Data with Unix Tools\nWhat’s the highest recorded global temperature for each year in the dataset? We will\nanswer this first without using Hadoop, as this information will provide a performance\nbaseline, as well as a useful means to check our results.\nThe classic tool for processing line-oriented data is awk. Example 2-2 is a small script\nto calculate the maximum temperature for each year.\nExample 2-2. A program for finding the maximum recorded temperature by year from NCDC weather\nrecords\n#!/usr/bin/env bash\nfor year in all/*\ndo\necho -ne `basename $year .gz`""\\t""\ngunzip -c $year | \\\nawk '{ temp = substr($0, 88, 5) + 0;\nq = substr($0, 93, 1);\nif (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp }\nEND { print max }'\ndone\nThe script loops through the compressed year files, first printing the year, and then\nprocessing each file using awk. The awk script extracts two fields from the data: the air\ntemperature and the quality code. The air temperature value is turned into an integer\nby adding 0. Next, a test is applied to see if the temperature is valid (the value 9999\nsignifies a missing value in the NCDC dataset), and if the quality code indicates that\nthe reading is not suspect or erroneous. If the reading is OK, the value is compared with\nthe maximum value seen so far, which is updated if a new maximum is found. The\nEND block is executed after all the lines in the file have been processed, and prints the\nmaximum value.\nHere is the beginning of a run:\n% ./max_temperature.sh\n1901\n317\n1902\n244\n1903\n289\n1904\n256\n1905\n283\n...\nThe temperature values in the source file are scaled by a factor of 10, so this works out\nas a maximum temperature of 31.7°C for 1901 (there were very few readings at the\nbeginning of the century, so this is plausible). The complete run for the century took\n42 minutes in one run on a single EC2 High-CPU Extra Large Instance.\nAnalyzing the Data with Unix Tools | 17To speed up the processing, we need to run parts of the program in parallel. In theory,\nthis is straightforward: we could process different years in different processes, using all\nthe available hardware threads on a machine. There are a few problems with this,\nhowever.\nFirst, dividing the work into equal-size pieces isn’t always easy or obvious. In this case,\nthe file size for different years varies widely, so some processes will finish much earlier\nthan others. Even if they pick up further work, the whole run is dominated by the\nlongest file. An alternative approach is to split the input into fixed-size chunks and\nassign each chunk to a process.\nSecond, combining the results from independent processes can need further processing.\nIn this case, the result for each year is independent of other years and may be combined\nby concatenating all the results, and sorting by year. If using the fixed-size chunk ap-\nproach, the combination is more delicate. For this example, data for a particular year\nwill typically be split into several chunks, each processed independently. We’ll end up\nwith the maximum temperature for each chunk, so the final step is to look for the\nhighest of these maximums, for each year.\nThird, you are still limited by the processing capacity of a single machine. If the best\ntime you can achieve is 20 minutes with the number of processors you have, then that’s\nit. You can’t make it go faster. Also, some datasets grow beyond the capacity of a single\nmachine. When we start using multiple machines, a whole host of other factors come\ninto play, mainly falling in the category of coordination and reliability. Who runs the\noverall job? How do we deal with failed processes?\nSo, though it’s feasible to parallelize the processing, in practice it’s messy. Using a\nframework like Hadoop to take care of these issues is a great help.\nAnalyzing the Data with Hadoop\nTo take advantage of the parallel processing that Hadoop provides, we need to express\nour query as a MapReduce job. After some local, small-scale testing, we will be able to\nrun it on a cluster of machines.\nMap and Reduce\nMapReduce works by breaking the processing into two phases: the map phase and the\nreduce phase. Each phase has key-value pairs as input and output, the types of which\nmay be chosen by the programmer. The programmer also specifies two functions: the\nmap function and the reduce function.\nThe input to our map phase is the raw NCDC data. We choose a text input format that\ngives us each line in the dataset as a text value. The key is the offset of the beginning\nof the line from the beginning of the file, but as we have no need for this, we ignore it.\n18 | Chapter 2: MapReduceOur map function is simple. We pull out the year and the air temperature, since these\nare the only fields we are interested in. In this case, the map function is just a data\npreparation phase, setting up the data in such a way that the reducer function can do\nits work on it: finding the maximum temperature for each year. The map function is\nalso a good place to drop bad records: here we filter out temperatures that are missing,\nsuspect, or erroneous.\nTo visualize the way the map works, consider the following sample lines of input data\n(some unused columns have been dropped to fit the page, indicated by ellipses):\n0067011990999991950051507004...9999999N9+00001+99999999999...\n0043011990999991950051512004...9999999N9+00221+99999999999...\n0043011990999991950051518004...9999999N9-00111+99999999999...\n0043012650999991949032412004...0500001N9+01111+99999999999...\n0043012650999991949032418004...0500001N9+00781+99999999999...\nThese lines are presented to the map function as the key-value pairs:\n(0, 0067011990999991950051507004...9999999N9+00001+99999999999...)\n(106, 0043011990999991950051512004...9999999N9+00221+99999999999...)\n(212, 0043011990999991950051518004...9999999N9-00111+99999999999...)\n(318, 0043012650999991949032412004...0500001N9+01111+99999999999...)\n(424, 0043012650999991949032418004...0500001N9+00781+99999999999...)\nThe keys are the line offsets within the file, which we ignore in our map function. The\nmap function merely extracts the year and the air temperature (indicated in bold text),\nand emits them as its output. (The temperature values have been interpreted as\nintegers.)\n(1950,\n(1950,\n(1950,\n(1949,\n(1949,\n0)\n22)\n−11)\n111)\n78)\nThe output from the map function is processed by the MapReduce framework before\nbeing sent to the reduce function. This processing sorts and groups the key-value pairs\nby key. So, continuing the example, our reduce function sees the following input:\n(1949, [111, 78])\n(1950, [0, 22, −11])\nEach year appears with a list of all its air temperature readings. All the reduce function\nhas to do now is iterate through the list and pick up the maximum reading:\n(1949, 111)\n(1950, 22)\nThis is the final output: the maximum global temperature recorded in each year.\nThe whole data flow is illustrated in Figure 2-1. At the bottom of the diagram is a Unix\npipeline, which mimics the whole MapReduce flow, and which we will see again later\nin the chapter when we look at Hadoop Streaming.\nAnalyzing the Data with Hadoop | 19Figure 2-1. MapReduce logical data flow\nJava MapReduce\nHaving run through how the MapReduce program works, the next step is to express it\nin code. We need three things: a map function, a reduce function, and some code to\nrun the job. The map function is represented by an implementation of the Mapper\ninterface, which declares a map() method. Example 2-3 shows the implementation of\nour map function.\nExample 2-3. Mapper for maximum temperature example\nimport java.io.IOException;\nimport\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.LongWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.MapReduceBase;\norg.apache.hadoop.mapred.Mapper;\norg.apache.hadoop.mapred.OutputCollector;\norg.apache.hadoop.mapred.Reporter;\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate static final int MISSING = 9999;\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature;\nif (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs\nairTemperature = Integer.parseInt(line.substring(88, 92));\n} else {\nairTemperature = Integer.parseInt(line.substring(87, 92));\n}\nString quality = line.substring(92, 93);\nif (airTemperature != MISSING && quality.matches(""[01459]"")) {\noutput.collect(new Text(year), new IntWritable(airTemperature));\n}\n20 | Chapter 2: MapReduceThe Mapper interface is a generic type, with four formal type parameters that specify the\ninput key, input value, output key, and output value types of the map function. For the\npresent example, the input key is a long integer offset, the input value is a line of text,\nthe output key is a year, and the output value is an air temperature (an integer). Rather\nthan use built-in Java types, Hadoop provides its own set of basic types that are opti-\nmized for network serialization. These are found in the org.apache.hadoop.io package.\nHere we use LongWritable, which corresponds to a Java Long, Text (like Java String),\nand IntWritable (like Java Integer).\nThe map() method is passed a key and a value. We convert the Text value containing\nthe line of input into a Java String, then use its substring() method to extract the\ncolumns we are interested in.\nThe map() method also provides an instance of OutputCollector to write the output to.\nIn this case, we write the year as a Text object (since we are just using it as a key), and\nthe temperature wrapped in an IntWritable. We write an output record only if the\ntemperature is present and the quality code indicates the temperature reading is OK.\nThe reduce function is similarly defined using a Reducer, as illustrated in Example 2-4.\nExample 2-4. Reducer for maximum temperature example\nimport java.io.IOException;\nimport java.util.Iterator;\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.MapReduceBase;\norg.apache.hadoop.mapred.OutputCollector;\norg.apache.hadoop.mapred.Reducer;\norg.apache.hadoop.mapred.Reporter;\npublic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nint maxValue = Integer.MIN_VALUE;\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(key, new IntWritable(maxValue));\nAgain, four formal type parameters are used to specify the input and output types, this\ntime for the reduce function. The input types of the reduce function must match the\noutput type of the map function: Text and IntWritable. And in this case, the output\ntypes of the reduce function are Text and IntWritable, for a year and its maximum\nAnalyzing the Data with Hadoop | 21temperature, which we find by iterating through the temperatures and comparing each\nwith a record of the highest found so far.\nThe third piece of code runs the MapReduce job (see Example 2-5).\nExample 2-5. Application to find the maximum temperature in the weather dataset\nimport java.io.IOException;\nimport\nimport\nimport\nimport\nimport\nimport\nimport\norg.apache.hadoop.fs.Path;\norg.apache.hadoop.io.IntWritable;\norg.apache.hadoop.io.Text;\norg.apache.hadoop.mapred.FileInputFormat;\norg.apache.hadoop.mapred.FileOutputFormat;\norg.apache.hadoop.mapred.JobClient;\norg.apache.hadoop.mapred.JobConf;\npublic class MaxTemperature {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperature <input path> <output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperature.class);\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\n}\n}\nJobClient.runJob(conf);\nA JobConf object forms the specification of the job. It gives you control over how the\njob is run. When we run this job on a Hadoop cluster, we will package the code into a\nJAR file (which Hadoop will distribute round the cluster). Rather than explicitly specify\nthe name of the JAR file, we can pass a class in the JobConf constructor, which Hadoop\nwill use to locate the relevant JAR file by looking for the JAR file containing this class.\nHaving constructed a JobConf object, we specify the input and output paths. An input\npath is specified by calling the static addInputPath() method on FileInputFormat, and\nit can be a single file, a directory (in which case, the input forms all the files in that\ndirectory), or a file pattern. As the name suggests, addInputPath() can be called more\nthan once to use input from multiple paths.\n22 | Chapter 2: MapReduceThe output path (of which there is only one) is specified by the static setOutput\nPath() method on FileOutputFormat. It specifies a directory where the output files from\nthe reducer functions are written. The directory shouldn’t exist before running the job,\nas Hadoop will complain and not run the job. This precaution is to prevent data loss\n(it can be very annoying to accidentally overwrite the output of a long job with\nanother).\nNext, we specify the map and reduce types to use via the setMapperClass() and\nsetReducerClass() methods.\nThe setOutputKeyClass() and setOutputValueClass() methods control the output types\nfor the map and the reduce functions, which are often the same, as they are in our case.\nIf they are different, then the map output types can be set using the methods\nsetMapOutputKeyClass() and setMapOutputValueClass().\nThe input types are controlled via the input format, which we have not explicitly set\nsince we are using the default TextInputFormat.\nAfter setting the classes that define the map and reduce functions, we are ready to run\nthe job. The static runJob() method on JobClient submits the job and waits for it to\nfinish, writing information about its progress to the console.\nA test run\nAfter writing a MapReduce job, it’s normal to try it out on a small dataset to flush out\nany immediate problems with the code. First install Hadoop in standalone mode—\nthere are instructions for how to do this in Appendix A. This is the mode in which\nHadoop runs using the local filesystem with a local job runner. Let’s test it on the five-\nline sample discussed earlier (the output has been slightly reformatted to fit the page):\n% export HADOOP_CLASSPATH=build/classes\n% hadoop MaxTemperature input/ncdc/sample.txt output\n09/04/07 12:34:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=Job\nTracker, sessionId=\n09/04/07 12:34:35 WARN mapred.JobClient: Use GenericOptionsParser for parsing the\narguments. Applications should implement Tool for the same.\n09/04/07 12:34:35 WARN mapred.JobClient: No job jar file set. User classes may not\nbe found. See JobConf(Class) or JobConf#setJar(String).\n09/04/07 12:34:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n09/04/07 12:34:35 INFO mapred.JobClient: Running job: job_local_0001\n09/04/07 12:34:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n09/04/07 12:34:35 INFO mapred.MapTask: numReduceTasks: 1\n09/04/07 12:34:35 INFO mapred.MapTask: io.sort.mb = 100\n09/04/07 12:34:35 INFO mapred.MapTask: data buffer = 79691776/99614720\n09/04/07 12:34:35 INFO mapred.MapTask: record buffer = 262144/327680\n09/04/07 12:34:35 INFO mapred.MapTask: Starting flush of map output\n09/04/07 12:34:36 INFO mapred.MapTask: Finished spill 0\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task:attempt_local_0001_m_000000_0 is\ndone. And is in the process of commiting\n09/04/07 12:34:36 INFO mapred.LocalJobRunner: file:/Users/tom/workspace/htdg/input/n\ncdc/sample.txt:0+529\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task 'attempt_local_0001_m_000000_0' done.\nAnalyzing the Data with Hadoop | 2309/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.Merger: Merging 1 sorted segments\n09/04/07 12:34:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments\nleft of total size: 57 bytes\n09/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task:attempt_local_0001_r_000000_0 is done\n. And is in the process of commiting\n09/04/07 12:34:36 INFO mapred.LocalJobRunner:\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task attempt_local_0001_r_000000_0 is\nallowed to commit now\n09/04/07 12:34:36 INFO mapred.FileOutputCommitter: Saved output of task\n'attempt_local_0001_r_000000_0' to file:/Users/tom/workspace/htdg/output\n09/04/07 12:34:36 INFO mapred.LocalJobRunner: reduce > reduce\n09/04/07 12:34:36 INFO mapred.TaskRunner: Task 'attempt_local_0001_r_000000_0' done.\n09/04/07 12:34:36 INFO mapred.JobClient: map 100% reduce 100%\n09/04/07 12:34:36 INFO mapred.JobClient: Job complete: job_local_0001\n09/04/07 12:34:36 INFO mapred.JobClient: Counters: 13\n09/04/07 12:34:36 INFO mapred.JobClient: FileSystemCounters\n09/04/07 12:34:36 INFO mapred.JobClient:\nFILE_BYTES_READ=27571\n09/04/07 12:34:36 INFO mapred.JobClient:\nFILE_BYTES_WRITTEN=53907\n09/04/07 12:34:36 INFO mapred.JobClient: Map-Reduce Framework\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce input groups=2\n09/04/07 12:34:36 INFO mapred.JobClient:\nCombine output records=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap input records=5\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce shuffle bytes=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce output records=2\n09/04/07 12:34:36 INFO mapred.JobClient:\nSpilled Records=10\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap output bytes=45\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap input bytes=529\n09/04/07 12:34:36 INFO mapred.JobClient:\nCombine input records=0\n09/04/07 12:34:36 INFO mapred.JobClient:\nMap output records=5\n09/04/07 12:34:36 INFO mapred.JobClient:\nReduce input records=5\nWhen the hadoop command is invoked with a classname as the first argument, it\nlaunches a JVM to run the class. It is more convenient to use hadoop than straight\njava since the former adds the Hadoop libraries (and their dependencies) to the class-\npath, and picks up the Hadoop configuration too. To add the application classes to the\nclasspath, we’ve defined an environment variable called HADOOP_CLASSPATH, which the\nhadoop script picks up.\nWhen running in local (standalone) mode, the programs in this book\nall assume that you have set the HADOOP_CLASSPATH in this way. The com-\nmands should be run from the directory that the example code is in-\nstalled in.\nThe output from running the job provides some useful information. (The warning\nabout the job JAR file not being found is expected, since we are running in local mode\nwithout a JAR. We won’t see this warning when we run on a cluster.) For example, we\ncan see that the job was given an ID of job_local_0001, and it ran one map task and\none reduce task (with the IDs attempt_local_0001_m_000000_0 and\n24 | Chapter 2: MapReduceattempt_local_0001_r_000000_0). Knowing the job and task IDs can be very useful when\ndebugging MapReduce jobs.\nThe last section of the output, entitled “Counters,” shows the statistics that Hadoop\ngenerates for each job it runs. These are very useful for checking whether the amount\nof data processed is what you expected. For example, we can follow the number of\nrecords that went through the system: five map inputs produced five map outputs, then\nfive reduce inputs in two groups produced two reduce outputs.\nThe output was written to the output directory, which contains one output file per\nreducer. The job had a single reducer, so we find a single file, named part-00000:\n% cat output/part-00000\n1949\n111\n1950\n22\nThis result is the same as when we went through it by hand earlier. We interpret this\nas saying that the maximum temperature recorded in 1949 was 11.1°C, and in 1950 it\nwas 2.2°C.\nThe new Java MapReduce API\nRelease 0.20.0 of Hadoop included a new Java MapReduce API, sometimes referred to\nas “Context Objects,” designed to make the API easier to evolve in the future. The new\nAPI is type-incompatible with the old, however, so applications need to be rewritten\nto take advantage of it.*\nThere are several notable differences between the two APIs:\n• The new API favors abstract classes over interfaces, since these are easier to evolve.\nFor example, you can add a method (with a default implementation) to an abstract\nclass without breaking old implementations of the class. In the new API, the\nMapper and Reducer interfaces are now abstract classes.\n• The new API is in the org.apache.hadoop.mapreduce package (and subpackages).\nThe old API is found in org.apache.hadoop.mapred.\n• The new API makes extensive use of context objects that allow the user code to\ncommunicate with the MapReduce system. The MapContext, for example, essen-\ntially unifies the role of the JobConf, the OutputCollector, and the Reporter.\n• The new API supports both a “push” and a “pull” style of iteration. In both APIs,\nkey-value record pairs are pushed to the mapper, but in addition, the new API\nallows a mapper to pull records from within the map() method. The same goes for\nthe reducer. An example of how the “pull” style can be useful is processing records\nin batches, rather than one by one.\n* At the time of this writing, not all of the MapReduce libraries in Hadoop have been ported to work with the\nnew API. This book uses the old API for this reason. However, a copy of all of the examples in this book,\nrewritten to use the new API, will be made available on the book’s website.\nAnalyzing the Data with Hadoop | 25• Configuration has been unified. The old API has a special JobConf object for job\nconfiguration, which is an extension of Hadoop’s vanilla Configuration object\n(used for configuring daemons; see “The Configuration API” on page 116). In the\nnew API, this distinction is dropped, so job configuration is done through a\nConfiguration.\n• Job control is performed through the Job class, rather than JobClient, which no\nlonger exists in the new API.\nExample 2-6 shows the MaxTemperature application rewritten to use the new API. The\ndifferences are highlighted in bold.\nExample 2-6. Application to find the maximum temperature in the weather dataset using the new\ncontext objects MapReduce API\npublic class NewMaxTemperature {\nstatic class NewMaxTemperatureMapper\nextends Mapper<LongWritable, Text, Text, IntWritable> {\nprivate static final int MISSING = 9999;\npublic void map(LongWritable key, Text value, Context context)\nthrows IOException, InterruptedException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature;\nif (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs\nairTemperature = Integer.parseInt(line.substring(88, 92));\n} else {\nairTemperature = Integer.parseInt(line.substring(87, 92));\n}\nString quality = line.substring(92, 93);\nif (airTemperature != MISSING && quality.matches(""[01459]"")) {\ncontext.write(new Text(year), new IntWritable(airTemperature));\n}\nstatic class NewMaxTemperatureReducer\nextends Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterable<IntWritable> values,\nContext context)\nthrows IOException, InterruptedException {\n}\n}\nint maxValue = Integer.MIN_VALUE;\nfor (IntWritable value : values) {\nmaxValue = Math.max(maxValue, value.get());\n}\ncontext.write(key, new IntWritable(maxValue));\n26 | Chapter 2: MapReducepublic static void main(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.println(""Usage: NewMaxTemperature <input path> <output path>"");\nSystem.exit(-1);\n}\nJob job = new Job();\njob.setJarByClass(NewMaxTemperature.class);\nFileInputFormat.addInputPath(job, new Path(args[0]));\nFileOutputFormat.setOutputPath(job, new Path(args[1]));\njob.setMapperClass(NewMaxTemperatureMapper.class);\njob.setReducerClass(NewMaxTemperatureReducer.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(IntWritable.class);\n}\n}\nSystem.exit(job.waitForCompletion(true) ? 0 : 1);\nScaling Out\nYou’ve seen how MapReduce works for small inputs; now it’s time to take a bird’s-eye\nview of the system and look at the data flow for large inputs. For simplicity, the exam-\nples so far have used files on the local filesystem. However, to scale out, we need to\nstore the data in a distributed filesystem, typically HDFS (which you’ll learn about in\nthe next chapter), to allow Hadoop to move the MapReduce computation to each\nmachine hosting a part of the data. Let’s see how this works.\nData Flow\nFirst, some terminology. A MapReduce job is a unit of work that the client wants to be\nperformed: it consists of the input data, the MapReduce program, and configuration\ninformation. Hadoop runs the job by dividing it into tasks, of which there are two types:\nmap tasks and reduce tasks.\nThere are two types of nodes that control the job execution process: a jobtracker and\na number of tasktrackers. The jobtracker coordinates all the jobs run on the system by\nscheduling tasks to run on tasktrackers. Tasktrackers run tasks and send progress re-\nports to the jobtracker, which keeps a record of the overall progress of each job. If a\ntasks fails, the jobtracker can reschedule it on a different tasktracker.\nHadoop divides the input to a MapReduce job into fixed-size pieces called input\nsplits, or just splits. Hadoop creates one map task for each split, which runs the user-\ndefined map function for each record in the split.\nScaling Out | 27Having many splits means the time taken to process each split is small compared to the\ntime to process the whole input. So if we are processing the splits in parallel, the pro-\ncessing is better load-balanced if the splits are small, since a faster machine will be able\nto process proportionally more splits over the course of the job than a slower machine.\nEven if the machines are identical, failed processes or other jobs running concurrently\nmake load balancing desirable, and the quality of the load balancing increases as the\nsplits become more fine-grained.\nOn the other hand, if splits are too small, then the overhead of managing the splits and\nof map task creation begins to dominate the total job execution time. For most jobs, a\ngood split size tends to be the size of a HDFS block, 64 MB by default, although this\ncan be changed for the cluster (for all newly created files), or specified when each file\nis created.\nHadoop does its best to run the map task on a node where the input data resides in\nHDFS. This is called the data locality optimization. It should now be clear why the\noptimal split size is the same as the block size: it is the largest size of input that can be\nguaranteed to be stored on a single node. If the split spanned two blocks, it would be\nunlikely that any HDFS node stored both blocks, so some of the split would have to be\ntransferred across the network to the node running the map task, which is clearly less\nefficient than running the whole map task using local data.\nMap tasks write their output to local disk, not to HDFS. Why is this? Map output is\nintermediate output: it’s processed by reduce tasks to produce the final output, and\nonce the job is complete the map output can be thrown away. So storing it in HDFS,\nwith replication, would be overkill. If the node running the map task fails before the\nmap output has been consumed by the reduce task, then Hadoop will automatically\nrerun the map task on another node to recreate the map output.\nReduce tasks don’t have the advantage of data locality—the input to a single reduce\ntask is normally the output from all mappers. In the present example, we have a single\nreduce task that is fed by all of the map tasks. Therefore the sorted map outputs have\nto be transferred across the network to the node where the reduce task is running, where\nthey are merged and then passed to the user-defined reduce function. The output of\nthe reduce is normally stored in HDFS for reliability. As explained in Chapter 3, for\neach HDFS block of the reduce output, the first replica is stored on the local node, with\nother replicas being stored on off-rack nodes. Thus, writing the reduce output does\nconsume network bandwidth, but only as much as a normal HDFS write pipeline\nconsumes.\nThe whole data flow with a single reduce task is illustrated in Figure 2-2. The dotted\nboxes indicate nodes, the light arrows show data transfers on a node, and the heavy\narrows show data transfers between nodes.\nThe number of reduce tasks is not governed by the size of the input, but is specified\nindependently. In “The Default MapReduce Job” on page 178, you will see how to\nchoose the number of reduce tasks for a given job.\n28 | Chapter 2: MapReduceFigure 2-2. MapReduce data flow with a single reduce task\nWhen there are multiple reducers, the map tasks partition their output, each creating\none partition for each reduce task. There can be many keys (and their associated values)\nin each partition, but the records for every key are all in a single partition. The parti-\ntioning can be controlled by a user-defined partitioning function, but normally the\ndefault partitioner—which buckets keys using a hash function—works very well.\nThe data flow for the general case of multiple reduce tasks is illustrated in Figure 2-3.\nThis diagram makes it clear why the data flow between map and reduce tasks is collo-\nquially known as “the shuffle,” as each reduce task is fed by many map tasks. The\nshuffle is more complicated than this diagram suggests, and tuning it can have a big\nimpact on job execution time, as you will see in “Shuffle and Sort” on page 163.\nFinally, it’s also possible to have zero reduce tasks. This can be appropriate when you\ndon’t need the shuffle since the processing can be carried out entirely in parallel (a few\nexamples are discussed in “NLineInputFormat” on page 198). In this case, the only\noff-node data transfer is when the map tasks write to HDFS (see Figure 2-4).\nCombiner Functions\nMany MapReduce jobs are limited by the bandwidth available on the cluster, so it pays\nto minimize the data transferred between map and reduce tasks. Hadoop allows the\nuser to specify a combiner function to be run on the map output—the combiner func-\ntion’s output forms the input to the reduce function. Since the combiner function is an\noptimization, Hadoop does not provide a guarantee of how many times it will call it\nfor a particular map output record, if at all. In other words, calling the combiner func-\ntion zero, one, or many times should produce the same output from the reducer.\nScaling Out | 29Figure 2-3. MapReduce data flow with multiple reduce tasks\nFigure 2-4. MapReduce data flow with no reduce tasks\nThe contract for the combiner function constrains the type of function that may be\nused. This is best illustrated with an example. Suppose that for the maximum temper-\nature example, readings for the year 1950 were processed by two maps (because they\nwere in different splits). Imagine the first map produced the output:\n30 | Chapter 2: MapReduce(1950, 0)\n(1950, 20)\n(1950, 10)\nAnd the second produced:\n(1950, 25)\n(1950, 15)\nThe reduce function would be called with a list of all the values:\n(1950, [0, 20, 10, 25, 15])\nwith output:\n(1950, 25)\nsince 25 is the maximum value in the list. We could use a combiner function that, just\nlike the reduce function, finds the maximum temperature for each map output. The\nreduce would then be called with:\n(1950, [20, 25])\nand the reduce would produce the same output as before. More succinctly, we may\nexpress the function calls on the temperature values in this case as follows:\nmax(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25\nNot all functions possess this property.† For example, if we were calculating mean\ntemperatures, then we couldn’t use the mean as our combiner function, since:\nmean(0, 20, 10, 25, 15) = 14\nbut:\nmean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15\nThe combiner function doesn’t replace the reduce function. (How could it? The reduce\nfunction is still needed to process records with the same key from different maps.) But\nit can help cut down the amount of data shuffled between the maps and the reduces,\nand for this reason alone it is always worth considering whether you can use a combiner\nfunction in your MapReduce job.\nSpecifying a combiner function\nGoing back to the Java MapReduce program, the combiner function is defined using\nthe Reducer interface, and for this application, it is the same implementation as the\nreducer function in MaxTemperatureReducer. The only change we need to make is to set\nthe combiner class on the JobConf (see Example 2-7).\n† Functions with this property are called distributive in the paper “Data Cube: A Relational Aggregation\nOperator Generalizing Group-By, Cross-Tab, and Sub-Totals,” Gray et al. (1995).\nScaling Out | 31Example 2-7. Application to find the maximum temperature, using a combiner function for efficiency\npublic class MaxTemperatureWithCombiner {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperatureWithCombiner <input path> "" +\n""<output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperatureWithCombiner.class);\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\n}\n}\nJobClient.runJob(conf);\nRunning a Distributed MapReduce Job\nThe same program will run, without alteration, on a full dataset. This is the point of\nMapReduce: it scales to the size of your data and the size of your hardware. Here’s one\ndata point: on a 10-node EC2 cluster running High-CPU Extra Large Instances, the\nprogram took six minutes to run.‡\nWe’ll go through the mechanics of running programs on a cluster in Chapter 5.\nHadoop Streaming\nHadoop provides an API to MapReduce that allows you to write your map and reduce\nfunctions in languages other than Java. Hadoop Streaming uses Unix standard streams\nas the interface between Hadoop and your program, so you can use any language that\ncan read standard input and write to standard output to write your MapReduce\nprogram.\n‡ This is a factor of seven faster than the serial run on one machine using awk. The main reason it wasn’t\nproportionately faster is because the input data wasn’t evenly partitioned. For convenience, the input files\nwere gzipped by year, resulting in large files for the later years in dataset, when the number of weather records\nwas much higher.\n32 | Chapter 2: MapReduceStreaming is naturally suited for text processing (although as of version 0.21.0 it can\nhandle binary streams, too), and when used in text mode, it has a line-oriented view of\ndata. Map input data is passed over standard input to your map function, which pro-\ncesses it line by line and writes lines to standard output. A map output key-value pair\nis written as a single tab-delimited line. Input to the reduce function is in the same\nformat—a tab-separated key-value pair—passed over standard input. The reduce func-\ntion reads lines from standard input, which the framework guarantees are sorted by\nkey, and writes its results to standard output.\nLet’s illustrate this by rewriting our MapReduce program for finding maximum tem-\nperatures by year in Streaming.\nRuby\nThe map function can be expressed in Ruby as shown in Example 2-8.\nExample 2-8. Map function for maximum temperature in Ruby\n#!/usr/bin/env ruby\nSTDIN.each_line do |line|\nval = line\nyear, temp, q = val[15,4], val[87,5], val[92,1]\nputs ""#{year}\\t#{temp}"" if (temp != ""+9999"" && q =~ /[01459]/)\nend\nThe program iterates over lines from standard input by executing a block for each line\nfrom STDIN (a global constant of type IO). The block pulls out the relevant fields from\neach input line, and, if the temperature is valid, writes the year and the temperature\nseparated by a tab character \\t to standard output (using puts).\nIt’s worth drawing out a design difference between Streaming and the\nJava MapReduce API. The Java API is geared toward processing your\nmap function one record at a time. The framework calls the map()\nmethod on your Mapper for each record in the input, whereas with\nStreaming the map program can decide how to process the input—for\nexample, it could easily read and process multiple lines at a time since\nit’s in control of the reading. The user’s Java map implementation is\n“pushed” records, but it’s still possible to consider multiple lines at a\ntime by accumulating previous lines in an instance variable in the\nMapper.§ In this case, you need to implement the close() method so that\nyou know when the last record has been read, so you can finish pro-\ncessing the last group of lines.\n§ Alternatively, you could use “pull” style processing in the new MapReduce API—see “The new\nJava MapReduce API” on page 25.\nHadoop Streaming | 33Since the script just operates on standard input and output, it’s trivial to test the script\nwithout using Hadoop, simply using Unix pipes:\n% cat input/ncdc/sample.txt | src/main/ch02/ruby/max_temperature_map.rb\n1950\n+0000\n1950\n+0022\n1950\n-0011\n1949\n+0111\n1949\n+0078\nThe reduce function shown in Example 2-9 is a little more complex.\nExample 2-9. Reduce function for maximum temperature in Ruby\n#!/usr/bin/env ruby\nlast_key, max_val = nil, 0\nSTDIN.each_line do |line|\nkey, val = line.split(""\\t"")\nif last_key && last_key != key\nputs ""#{last_key}\\t#{max_val}""\nlast_key, max_val = key, val.to_i\nelse\nlast_key, max_val = key, [max_val, val.to_i].max\nend\nend\nputs ""#{last_key}\\t#{max_val}"" if last_key\nAgain, the program iterates over lines from standard input, but this time we have to\nstore some state as we process each key group. In this case, the keys are weather station\nidentifiers, and we store the last key seen and the maximum temperature seen so far\nfor that key. The MapReduce framework ensures that the keys are ordered, so we know\nthat if a key is different from the previous one, we have moved into a new key group.\nIn contrast to the Java API, where you are provided an iterator over each key group, in\nStreaming you have to find key group boundaries in your program.\nFor each line we pull out the key and value, then if we’ve just finished a group (last_key\n&& last_key != key), we write the key and the maximum temperature for that group,\nseparated by a tab character, before resetting the maximum temperature for the new\nkey. If we haven’t just finished a group, we just update the maximum temperature for\nthe current key.\nThe last line of the program ensures that a line is written for the last key group in the\ninput.\nWe can now simulate the whole MapReduce pipeline with a Unix pipeline (which is\nequivalent to the Unix pipeline shown in Figure 2-1):\n% cat input/ncdc/sample.txt | src/main/ch02/ruby/max_temperature_map.rb | \\\nsort | src/main/ch02/ruby/max_temperature_reduce.rb\n1949\n111\n1950\n22\n34 | Chapter 2: MapReduceThe output is the same as the Java program, so the next step is to run it using Hadoop\nitself.\nThe hadoop command doesn’t support a Streaming option; instead, you specify the\nStreaming JAR file along with the jar option. Options to the Streaming program specify\nthe input and output paths, and the map and reduce scripts. This is what it looks like:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-mapper src/main/ch02/ruby/max_temperature_map.rb \\\n-reducer src/main/ch02/ruby/max_temperature_reduce.rb\nWhen running on a large dataset on a cluster, we should set the combiner, using the\n-combiner option.\nFrom release 0.21.0, the combiner can be any Streaming command. For earlier releases,\nthe combiner had to be written in Java, so as a workaround it was common to do manual\ncombining in the mapper, without having to resort to Java. In this case, we could change\nthe mapper to be a pipeline:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/all \\\n-output output \\\n-mapper ""ch02/ruby/max_temperature_map.rb | sort | ch02/ruby/max_temperature_reduce.rb"" \\\n-reducer src/main/ch02/ruby/max_temperature_reduce.rb \\\n-file src/main/ch02/ruby/max_temperature_map.rb \\\n-file src/main/ch02/ruby/max_temperature_reduce.rb\nNote also the use of -file, which we use when running Streaming programs on the\ncluster to ship the scripts to the cluster.\nPython\nStreaming supports any programming language that can read from standard input, and\nwrite to standard output, so for readers more familiar with Python, here’s the same\nexample again.‖ The map script is in Example 2-10, and the reduce script is in Exam-\nple 2-11.\nExample 2-10. Map function for maximum temperature in Python\n#!/usr/bin/env python\nimport re\nimport sys\nfor line in sys.stdin:\nval = line.strip()\n(year, temp, q) = (val[15:19], val[87:92], val[92:93])\n‖ As an alternative to Streaming, Python programmers should consider Dumbo (http://www.last.fm/dumbo),\nwhich makes the Streaming MapReduce interface more Pythonic, and easier to use.\nHadoop Streaming | 35if (temp != ""+9999"" and re.match(""[01459]"", q)):\nprint ""%s\\t%s"" % (year, temp)\nExample 2-11. Reduce function for maximum temperature in Python\n#!/usr/bin/env python\nimport sys\n(last_key, max_val) = (None, 0)\nfor line in sys.stdin:\n(key, val) = line.strip().split(""\\t"")\nif last_key and last_key != key:\nprint ""%s\\t%s"" % (last_key, max_val)\n(last_key, max_val) = (key, int(val))\nelse:\n(last_key, max_val) = (key, max(max_val, int(val)))\nif last_key:\nprint ""%s\\t%s"" % (last_key, max_val)\nWe can test the programs and run the job in the same way we did in Ruby. For example,\nto run a test:\n% cat input/ncdc/sample.txt | src/main/ch02/python/max_temperature_map.py | \\\nsort | src/main/ch02/python/max_temperature_reduce.py\n1949\n111\n1950\n22\nHadoop Pipes\nHadoop Pipes is the name of the C++ interface to Hadoop MapReduce. Unlike Stream-\ning, which uses standard input and output to communicate with the map and reduce\ncode, Pipes uses sockets as the channel over which the tasktracker communicates with\nthe process running the C++ map or reduce function. JNI is not used.\nWe’ll rewrite the example running through the chapter in C++, and then we’ll see how\nto run it using Pipes. Example 2-12 shows the source code for the map and reduce\nfunctions in C++.\nExample 2-12. Maximum temperature in C++\n#include <algorithm>\n#include <limits>\n#include <string>\n#include ""hadoop/Pipes.hh""\n#include ""hadoop/TemplateFactory.hh""\n#include ""hadoop/StringUtils.hh""\nclass MaxTemperatureMapper : public HadoopPipes::Mapper {\npublic:\nMaxTemperatureMapper(HadoopPipes::TaskContext& context) {\n36 | Chapter 2: MapReduce}\nvoid map(HadoopPipes::MapContext& context) {\nstd::string line = context.getInputValue();\nstd::string year = line.substr(15, 4);\nstd::string airTemperature = line.substr(87, 5);\nstd::string q = line.substr(92, 1);\nif (airTemperature != ""+9999"" &&\n(q == ""0"" || q == ""1"" || q == ""4"" || q == ""5"" || q == ""9"")) {\ncontext.emit(year, airTemperature);\n}\n}\n};\nclass MapTemperatureReducer : public HadoopPipes::Reducer {\npublic:\nMapTemperatureReducer(HadoopPipes::TaskContext& context) {\n}\nvoid reduce(HadoopPipes::ReduceContext& context) {\nint maxValue = INT_MIN;\nwhile (context.nextValue()) {\nmaxValue = std::max(maxValue, HadoopUtils::toInt(context.getInputValue()));\n}\ncontext.emit(context.getInputKey(), HadoopUtils::toString(maxValue));\n}\n};\nint main(int argc, char *argv[]) {\nreturn HadoopPipes::runTask(HadoopPipes::TemplateFactory<MaxTemperatureMapper,\nMapTemperatureReducer>());\n}\nThe application links against the Hadoop C++ library, which is a thin wrapper for\ncommunicating with the tasktracker child process. The map and reduce functions are\ndefined by extending the Mapper and Reducer classes defined in the HadoopPipes name-\nspace and providing implementations of the map() and reduce() methods in each case.\nThese methods take a context object (of type MapContext or ReduceContext), which\nprovides the means for reading input and writing output, as well as accessing job con-\nfiguration information via the JobConf class. The processing in this example is very\nsimilar to the Java equivalent.\nUnlike the Java interface, keys and values in the C++ interface are byte buffers, repre-\nsented as Standard Template Library (STL) strings. This makes the interface simpler,\nalthough it does put a slightly greater burden on the application developer, who has to\nconvert to and from richer domain-level types. This is evident in MapTemperatureRe\nducer where we have to convert the input value into an integer (using a convenience\nmethod in HadoopUtils) and then the maximum value back into a string before it’s\nwritten out. In some cases, we can save on doing the conversion, such as in MaxTemper\natureMapper where the airTemperature value is never converted to an integer since it is\nnever processed as a number in the map() method.\nHadoop Pipes | 37The main() method is the application entry point. It calls HadoopPipes::runTask, which\nconnects to the Java parent process and marshals data to and from the Mapper or\nReducer. The runTask() method is passed a Factory so that it can create instances of the\nMapper or Reducer. Which one it creates is controlled by the Java parent over the socket\nconnection. There are overloaded template factory methods for setting a combiner,\npartitioner, record reader, or record writer.\nCompiling and Running\nNow we can compile and link our program using the Makefile in Example 2-13.\nExample 2-13. Makefile for C++ MapReduce program\nCC = g++\nCPPFLAGS = -m32 -I$(HADOOP_INSTALL)/c++/$(PLATFORM)/include\nmax_temperature: max_temperature.cpp\n$(CC) $(CPPFLAGS) $< -Wall -L$(HADOOP_INSTALL)/c++/$(PLATFORM)/lib -lhadooppipes \\\n-lhadooputils -lpthread -g -O2 -o $@\nThe Makefile expects a couple of environment variables to be set. Apart from\nHADOOP_INSTALL (which you should already have set if you followed the installation\ninstructions in Appendix A), you need to define PLATFORM, which specifies the operating\nsystem, architecture, and data model (e.g., 32- or 64-bit). I ran it on a 32-bit Linux\nsystem with the following:\n% export PLATFORM=Linux-i386-32\n% make\nOn successful completion, you’ll find the max_temperature executable in the current\ndirectory.\nTo run a Pipes job, we need to run Hadoop in pseudo-distributed mode (where all the\ndaemons run on the local machine), for which there are setup instructions in Appen-\ndix A. Pipes doesn’t run in standalone (local) mode, since it relies on Hadoop’s dis-\ntributed cache mechanism, which works only when HDFS is running.\nWith the Hadoop daemons now running, the first step is to copy the executable to\nHDFS so that it can be picked up by tasktrackers when they launch map and reduce\ntasks:\n% hadoop fs -put max_temperature bin/max_temperature\nThe sample data also needs to be copied from the local filesystem into HDFS:\n% hadoop fs -put input/ncdc/sample.txt sample.txt\n38 | Chapter 2: MapReduceNow we can run the job. For this, we use the Hadoop pipes command, passing the URI\nof the executable in HDFS using the -program argument:\n% hadoop pipes \\\n-D hadoop.pipes.java.recordreader=true \\\n-D hadoop.pipes.java.recordwriter=true \\\n-input sample.txt \\\n-output output \\\n-program bin/max_temperature\nWe specify two properties using the -D option: hadoop.pipes.java.recordreader and\nhadoop.pipes.java.recordwriter, setting both to true to say that we have not specified\na C++ record reader or writer, but that we want to use the default Java ones (which are\nfor text input and output). Pipes also allows you to set a Java mapper, reducer,\ncombiner, or partitioner. In fact, you can have a mixture of Java or C++ classes within\nany one job.\nThe result is the same as the other versions of the same program that we ran.\nHadoop Pipes | 39CHAPTER 3\nThe Hadoop Distributed Filesystem\nWhen a dataset outgrows the storage capacity of a single physical machine, it becomes\nnecessary to partition it across a number of separate machines. Filesystems that manage\nthe storage across a network of machines are called distributed filesystems. Since they\nare network-based, all the complications of network programming kick in, thus making\ndistributed filesystems more complex than regular disk filesystems. For example, one\nof the biggest challenges is making the filesystem tolerate node failure without suffering\ndata loss.\nHadoop comes with a distributed filesystem called HDFS, which stands for Hadoop\nDistributed Filesystem. (You may sometimes see references to “DFS”—informally or in\nolder documentation or configuration—which is the same thing.) HDFS is Hadoop’s\nflagship filesystem and is the focus of this chapter, but Hadoop actually has a general-\npurpose filesystem abstraction, so we’ll see along the way how Hadoop integrates with\nother storage systems (such as the local filesystem and Amazon S3).\nThe Design of HDFS\nHDFS is a filesystem designed for storing very large files with streaming data access\npatterns, running on clusters on commodity hardware. Let’s examine this statement in\nmore detail:\nVery large files\n“Very large” in this context means files that are hundreds of megabytes, gigabytes,\nor terabytes in size. There are Hadoop clusters running today that store petabytes\nof data.*\nStreaming data access\nHDFS is built around the idea that the most efficient data processing pattern is a\nwrite-once, read-many-times pattern. A dataset is typically generated or copied\n* “Scaling Hadoop to 4000 nodes at Yahoo!,” http://developer.yahoo.net/blogs/hadoop/2008/09/scaling_hadoop\n_to_4000_nodes_a.html.\n41from source, then various analyses are performed on that dataset over time. Each\nanalysis will involve a large proportion, if not all, of the dataset, so the time to read\nthe whole dataset is more important than the latency in reading the first record.\nCommodity hardware\nHadoop doesn’t require expensive, highly reliable hardware to run on. It’s designed\nto run on clusters of commodity hardware (commonly available hardware available\nfrom multiple vendors†) for which the chance of node failure across the cluster is\nhigh, at least for large clusters. HDFS is designed to carry on working without a\nnoticeable interruption to the user in the face of such failure.\nIt is also worth examining the applications for which using HDFS does not work so\nwell. While this may change in the future, these are areas where HDFS is not a good fit\ntoday:\nLow-latency data access\nApplications that require low-latency access to data, in the tens of milliseconds\nrange, will not work well with HDFS. Remember HDFS is optimized for delivering\na high throughput of data, and this may be at the expense of latency. HBase\n(Chapter 12) is currently a better choice for low-latency access.\nLots of small files\nSince the namenode holds filesystem metadata in memory, the limit to the number\nof files in a filesystem is governed by the amount of memory on the namenode. As\na rule of thumb, each file, directory, and block takes about 150 bytes. So, for ex-\nample, if you had one million files, each taking one block, you would need at least\n300 MB of memory. While storing millions of files is feasible, billions is beyond\nthe capability of current hardware.\nMultiple writers, arbitrary file modifications\nFiles in HDFS may be written to by a single writer. Writes are always made at the\nend of the file. There is no support for multiple writers, or for modifications at\narbitrary offsets in the file. (These might be supported in the future, but they are\nlikely to be relatively inefficient.)\nHDFS Concepts\nBlocks\nA disk has a block size, which is the minimum amount of data that it can read or write.\nFilesystems for a single disk build on this by dealing with data in blocks, which are an\nintegral multiple of the disk block size. Filesystem blocks are typically a few kilobytes\nin size, while disk blocks are normally 512 bytes. This is generally transparent to the\n† See Chapter 9 for a typical machine specification.\n42 | Chapter 3: The Hadoop Distributed Filesystemfilesystem user who is simply reading or writing a file—of whatever length. However,\nthere are tools to do with filesystem maintenance, such as df and fsck, that operate on\nthe filesystem block level.\nHDFS too has the concept of a block, but it is a much larger unit—64 MB by default.\nLike in a filesystem for a single disk, files in HDFS are broken into block-sized chunks,\nwhich are stored as independent units. Unlike a filesystem for a single disk, a file in\nHDFS that is smaller than a single block does not occupy a full block’s worth of un-\nderlying storage. When unqualified, the term “block” in this book refers to a block in\nHDFS.\nWhy Is a Block in HDFS So Large?\nHDFS blocks are large compared to disk blocks, and the reason is to minimize the cost\nof seeks. By making a block large enough, the time to transfer the data from the disk\ncan be made to be significantly larger than the time to seek to the start of the block.\nThus the time to transfer a large file made of multiple blocks operates at the disk transfer\nrate.\nA quick calculation shows that if the seek time is around 10ms, and the transfer rate is\n100 MB/s, then to make the seek time 1% of the transfer time, we need to make the\nblock size around 100 MB. The default is actually 64 MB, although many HDFS in-\nstallations use 128 MB blocks. This figure will continue to be revised upward as transfer\nspeeds grow with new generations of disk drives.\nThis argument shouldn’t be taken too far, however. Map tasks in MapReduce normally\noperate on one block at a time, so if you have too few tasks (fewer than nodes in the\ncluster), your jobs will run slower than they could otherwise.\nHaving a block abstraction for a distributed filesystem brings several benefits. The first\nbenefit is the most obvious: a file can be larger than any single disk in the network.\nThere’s nothing that requires the blocks from a file to be stored on the same disk, so\nthey can take advantage of any of the disks in the cluster. In fact, it would be possible,\nif unusual, to store a single file on an HDFS cluster whose blocks filled all the disks in\nthe cluster.\nSecond, making the unit of abstraction a block rather than a file simplifies the storage\nsubsystem. Simplicity is something to strive for all in all systems, but is important for\na distributed system in which the failure modes are so varied. The storage subsystem\ndeals with blocks, simplifying storage management (since blocks are a fixed size, it is\neasy to calculate how many can be stored on a given disk), and eliminating metadata\nconcerns (blocks are just a chunk of data to be stored—file metadata such as permis-\nsions information does not need to be stored with the blocks, so another system can\nhandle metadata orthogonally).\nFurthermore, blocks fit well with replication for providing fault tolerance and availa-\nbility. To insure against corrupted blocks and disk and machine failure, each block is\nHDFS Concepts | 43replicated to a small number of physically separate machines (typically three). If a block\nbecomes unavailable, a copy can be read from another location in a way that is trans-\nparent to the client. A block that is no longer available due to corruption or machine\nfailure can be replicated from their alternative locations to other live machines to bring\nthe replication factor back to the normal level. (See “Data Integrity” on page 75 for\nmore on guarding against corrupt data.) Similarly, some applications may choose to\nset a high replication factor for the blocks in a popular file to spread the read load on\nthe cluster.\nLike its disk filesystem cousin, HDFS’s fsck command understands blocks. For exam-\nple, running:\n% hadoop fsck -files -blocks\nwill list the blocks that make up each file in the filesystem. (See also “Filesystem check\n(fsck)” on page 281.)\nNamenodes and Datanodes\nA HDFS cluster has two types of node operating in a master-worker pattern: a name-\nnode (the master) and a number of datanodes (workers). The namenode manages the\nfilesystem namespace. It maintains the filesystem tree and the metadata for all the files\nand directories in the tree. This information is stored persistently on the local disk in\nthe form of two files: the namespace image and the edit log. The namenode also knows\nthe datanodes on which all the blocks for a given file are located, however, it does not\nstore block locations persistently, since this information is reconstructed from\ndatanodes when the system starts.\nA client accesses the filesystem on behalf of the user by communicating with the name-\nnode and datanodes. The client presents a POSIX-like filesystem interface, so the user\ncode does not need to know about the namenode and datanode to function.\nDatanodes are the work horses of the filesystem. They store and retrieve blocks when\nthey are told to (by clients or the namenode), and they report back to the namenode\nperiodically with lists of blocks that they are storing.\nWithout the namenode, the filesystem cannot be used. In fact, if the machine running\nthe namenode were obliterated, all the files on the filesystem would be lost since there\nwould be no way of knowing how to reconstruct the files from the blocks on the\ndatanodes. For this reason, it is important to make the namenode resilient to failure,\nand Hadoop provides two mechanisms for this.\nThe first way is to back up the files that make up the persistent state of the filesystem\nmetadata. Hadoop can be configured so that the namenode writes its persistent state\nto multiple filesystems. These writes are synchronous and atomic. The usual configu-\nration choice is to write to local disk as well as a remote NFS mount.\n44 | Chapter 3: The Hadoop Distributed FilesystemIt is also possible to run a secondary namenode, which despite its name does not act as\na namenode. Its main role is to periodically merge the namespace image with the edit\nlog to prevent the edit log from becoming too large. The secondary namenode usually\nruns on a separate physical machine, since it requires plenty of CPU and as much\nmemory as the namenode to perform the merge. It keeps a copy of the merged name-\nspace image, which can be used in the event of the namenode failing. However, the\nstate of the secondary namenode lags that of the primary, so in the event of total failure\nof the primary data, loss is almost guaranteed. The usual course of action in this case\nis to copy the namenode’s metadata files that are on NFS to the secondary and run it\nas the new primary.\nSee “The filesystem image and edit log” on page 274 for more details.\nThe Command-Line Interface\nWe’re going to have a look at HDFS by interacting with it from the command line.\nThere are many other interfaces to HDFS, but the command line is one of the simplest,\nand to many developers the most familiar.\nWe are going to run HDFS on one machine, so first follow the instructions for setting\nup Hadoop in pseudo-distributed mode in Appendix A. Later you’ll see how to run on\na cluster of machines to give us scalability and fault tolerance.\nThere are two properties that we set in the pseudo-distributed configuration that de-\nserve further explanation. The first is fs.default.name, set to hdfs://localhost/, which is\nused to set a default filesystem for Hadoop. Filesystems are specified by a URI, and\nhere we have used a hdfs URI to configure Hadoop to use HDFS by default. The HDFS\ndaemons will use this property to determine the host and port for the HDFS namenode.\nWe’ll be running it on localhost, on the default HDFS port, 8020. And HDFS clients\nwill use this property to work out where the namenode is running so they can connect\nto it.\nWe set the second property, dfs.replication, to one so that HDFS doesn’t replicate\nfilesystem blocks by the usual default of three. When running with a single datanode,\nHDFS can’t replicate blocks to three datanodes, so it would perpetually warn about\nblocks being under-replicated. This setting solves that problem.\nBasic Filesystem Operations\nThe filesystem is ready to be used, and we can do all of the usual filesystem operations\nsuch as reading files, creating directories, moving files, deleting data, and listing direc-\ntories. You can type hadoop fs -help to get detailed help on every command.\nStart by copying a file from the local filesystem to HDFS:\n% hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/tom/quangle.txt\nThe Command-Line Interface | 45This command invokes Hadoop’s filesystem shell command fs, which supports a\nnumber of subcommands—in this case, we are running -copyFromLocal. The local file\nquangle.txt is copied to the file /user/tom/quangle.txt on the HDFS instance running on\nlocalhost. In fact, we could have omitted the scheme and host of the URI and picked\nup the default, hdfs://localhost, as specified in core-site.xml.\n% hadoop fs -copyFromLocal input/docs/quangle.txt /user/tom/quangle.txt\nWe could also have used a relative path, and copied the file to our home directory in\nHDFS, which in this case is /user/tom:\n% hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt\nLet’s copy the file back to the local filesystem and check whether it’s the same:\n% hadoop fs -copyToLocal quangle.txt quangle.copy.txt\n% md5 input/docs/quangle.txt quangle.copy.txt\nMD5 (input/docs/quangle.txt) = a16f231da6b05e2ba7a339320e7dacd9\nMD5 (quangle.copy.txt) = a16f231da6b05e2ba7a339320e7dacd9\nThe MD5 digests are the same, showing that the file survived its trip to HDFS and is\nback intact.\nFinally, let’s look at an HDFS file listing. We create a directory first just to see how it\nis displayed in the listing:\n% hadoop fs -mkdir books\n% hadoop fs -ls .\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\n-rw-r--r--\n1 tom supergroup\n0 2009-04-02 22:41 /user/tom/books\n118 2009-04-02 22:29 /user/tom/quangle.txt\nThe information returned is very similar to the Unix command ls -l, with a few minor\ndifferences. The first column shows the file mode. The second column is the replication\nfactor of the file (something a traditional Unix filesystems does not have). Remember\nwe set the default replication factor in the site-wide configuration to be 1, which is why\nwe see the same value here. The entry in this column is empty for directories since the\nconcept of replication does not apply to them—directories are treated as metadata and\nstored by the namenode, not the datanodes. The third and fourth columns show the\nfile owner and group. The fifth column is the size of the file in bytes, or zero for direc-\ntories. The six and seventh columns are the last modified date and time. Finally, the\neighth column is the absolute name of the file or directory.\n46 | Chapter 3: The Hadoop Distributed FilesystemFile Permissions in HDFS\nHDFS has a permissions model for files and directories that is much like POSIX.\nThere are three types of permission: the read permission (r), the write permission (w)\nand the execute permission (x). The read permission is required to read files or list the\ncontents of a directory. The write permission is required to write a file, or for a directory,\nto create or delete files or directories in it. The execute permission is ignored for a file\nsince you can’t execute a file on HDFS (unlike POSIX), and for a directory it is required\nto access its children.\nEach file and directory has an owner, a group, and a mode. The mode is made up of the\npermissions for the user who is the owner, the permissions for the users who are mem-\nbers of the group, and the permissions for users who are neither the owner nor members\nof the group.\nA client’s identity is determined by the username and groups of the process it is running\nin. Because clients are remote, this makes it possible to become an arbitrary user, simply\nby creating an account of that name on the remote system. Thus, permissions should\nbe used only in a cooperative community of users, as a mechanism for sharing filesystem\nresources and for avoiding accidental data loss, and not for securing resources in a\nhostile environment. However, despite these drawbacks, it is worthwhile having per-\nmissions enabled (as it is by default; see the dfs.permissions property), to avoid acci-\ndental modification or deletion of substantial parts of the filesystem, either by users or\nby automated tools or programs.\nWhen permissions checking is enabled, the owner permissions are checked if the cli-\nent’s username matches the owner, and the group permissions are checked if the client\nis a member of the group; otherwise, the other permissions are checked.\nThere is a concept of a super-user, which is the identity of the namenode process.\nPermissions checks are not performed for the super-user.\nHadoop Filesystems\nHadoop has an abstract notion of filesystem, of which HDFS is just one implementa-\ntion. The Java abstract class org.apache.hadoop.fs.FileSystem represents a filesystem\nin Hadoop, and there are several concrete implementations, which are described in\nTable 3-1.\nHadoop Filesystems | 47Table 3-1. Hadoop filesystems\nFilesystem URI scheme Java implementation (all under org.apache.hadoop) Description\nLocal file fs.LocalFileSystem A filesystem for a locally connec-\n                             ted disk with client-side check-\n                            sums. Use RawLocalFileSys\n                           tem for a local filesystem with no\n                          checksums. See “LocalFileSys-\n                           tem” on page 76.\nHDFS hdfs hdfs.DistributedFileSystem Hadoop’s distributed filesystem.\n                                      HDFS is designed to work effi-\n                                     ciently in conjunction with Map-\n                                    Reduce.\nHFTP hftp hdfs.HftpFileSystem A filesystem providing read-only\n                             access to HDFS over HTTP. (Despite\n                            its name, HFTP has no connection\n                           with FTP.) Often used with distcp\n                          (“Parallel Copying with\n                           distcp” on page 70) to copy data\n                            between HDFS clusters running\n                           different versions.\nHSFTP hsftp hdfs.HsftpFileSystem A filesystem providing read-only\n                                access to HDFS over HTTPS. (Again,\n                               this has no connection with FTP.)\nHAR har fs.HarFileSystem A filesystem layered on another\n                        filesystem for archiving files. Ha-\n                       doop Archives are typically used\n                      for archiving files in HDFS to reduce\n                     the namenode’s memory usage.\n                      See “Hadoop Ar-\n                       chives” on page 71.\nKFS (Cloud- kfs fs.kfs.KosmosFileSystem CloudStore (formerly Kosmos fil-\nStore) esystem) is a distributed filesys-\n      tem like HDFS or Google’s GFS,\n       written in C++. Find more infor-\n      mation about it at http://kosmosfs\n     .sourceforge.net/.\nFTP ftp fs.ftp.FTPFileSystem A filesystem backed by an FTP\n                            server.\nS3 (native) s3n fs.s3native.NativeS3FileSystem A filesystem backed by Amazon\n                                              S3. See http://wiki.apache.org/ha\n                                             doop/AmazonS3.\nS3 (block- s3 fs.s3.S3FileSystem A filesystem backed by Amazon\nbased) S3, which stores files in blocks\n      (much like HDFS) to overcome S3’s\n       5 GB file size limit.\n48 | Chapter 3: The Hadoop Distributed FilesystemHadoop provides many interfaces to its filesystems, and it generally uses the URI\nscheme to pick the correct filesystem instance to communicate with. For example, the\nfilesystem shell that we met in the previous section operates with all Hadoop filesys-\ntems. To list the files in the root directory of the local filesystem, type:\n% hadoop fs -ls file:///\nAlthough it is possible (and sometimes very convenient) to run MapReduce programs\nthat access any of these filesystems, when you are processing large volumes of data,\nyou should choose a distributed filesystem that has the data locality optimization, such\nas HDFS or KFS (see “Scaling Out” on page 27).\nInterfaces\nHadoop is written in Java, and all Hadoop filesystem interactions are mediated through\nthe Java API.‡ The filesystem shell, for example, is a Java application that uses the Java\nFileSystem class to provide filesystem operations. The other filesystem interfaces are\ndiscussed briefly in this section. These interfaces are most commonly used with HDFS,\nsince the other filesystems in Hadoop typically have existing tools to access the under-\nlying filesystem (FTP clients for FTP, S3 tools for S3, etc.), but many of them will work\nwith any Hadoop filesystem.\nThrift\nBy exposing its filesystem interface as a Java API, Hadoop makes it awkward for non-\nJava applications to access Hadoop filesystems. The Thrift API in the “thriftfs” contrib\nmodule remedies this deficiency by exposing Hadoop filesystems as an Apache Thrift\nservice, making it easy for any language that has Thrift bindings to interact with a\nHadoop filesystem, such as HDFS.\nTo use the Thrift API, run a Java server that exposes the Thrift service, and acts as a\nproxy to the Hadoop filesystem. Your application accesses the Thrift service, which is\ntypically running on the same machine as your application.\nThe Thrift API comes with a number of pregenerated stubs for a variety of languages,\nincluding C++, Perl, PHP, Python, and Ruby. Thrift has support for versioning, so it’s\na good choice if you want to access different versions of a Hadoop filesystem from the\nsame client code (you will need to run a proxy for each version of Hadoop to achieve\nthis, however).\nFor installation and usage instructions, please refer to the documentation in the\nsrc/contrib/thriftfs directory of the Hadoop distribution.\n‡ The RPC interfaces in Hadoop are based on Hadoop’s Writable interface, which is Java-centric. In the future,\nHadoop will adopt another, cross-language, RPC serialization format, which will allow native HDFS clients\nto be written in languages other than Java.\nHadoop Filesystems | 49C\nHadoop provides a C library called libhdfs that mirrors the Java FileSystem interface\n(it was written as a C library for accessing HDFS, but despite its name it can be used\nto access any Hadoop filesystem). It works using the Java Native Interface (JNI) to call\na Java filesystem client.\nThe C API is very similar to the Java one, but it typically lags the Java one, so newer\nfeatures may not be supported. You can find the generated documentation for the C\nAPI in the libhdfs/docs/api directory of the Hadoop distribution.\nHadoop comes with prebuilt libhdfs binaries for 32-bit Linux, but for other platforms,\nyou will need to build them yourself using the instructions at http://wiki.apache.org/\nhadoop/LibHDFS.\nFUSE\nFilesystem in Userspace (FUSE) allows filesystems that are implemented in user space\nto be integrated as a Unix filesystem. Hadoop’s Fuse-DFS contrib module allows any\nHadoop filesystem (but typically HDFS) to be mounted as a standard filesystem. You\ncan then use Unix utilities (such as ls and cat) to interact with the filesystem, as well\nas POSIX libraries to access the filesystem from any programming language.\nFuse-DFS is implemented in C using libhdfs as the interface to HDFS. Documentation\nfor compiling and running Fuse-DFS is located in the src/contrib/fuse-dfs directory of\nthe Hadoop distribution.\nWebDAV\nWebDAV is a set of extensions to HTTP to support editing and updating files. WebDAV\nshares can be mounted as filesystems on most operating systems, so by exposing HDFS\n(or other Hadoop filesystems) over WebDAV, it’s possible to access HDFS as a standard\nfilesystem.\nAt the time of this writing, WebDAV support in Hadoop (which is implemented by\ncalling the Java API to Hadoop) is still under development, and can be tracked at https:\n//issues.apache.org/jira/browse/HADOOP-496.\nOther HDFS Interfaces\nThere are two interfaces that are specific to HDFS:\nHTTP\nHDFS defines a read-only interface for retrieving directory listings and data over\nHTTP. Directory listings are served by the namenode’s embedded web server\n(which runs on port 50070) in XML format, while file data is streamed from\ndatanodes by their web servers (running on port 50075). This protocol is not tied\nto a specific HDFS version, making it possible to write clients that can use HTTP\n50 | Chapter 3: The Hadoop Distributed Filesystemto read data from HDFS clusters that run different versions of Hadoop. HftpFile\nSystem is a such a client: it is a Hadoop filesystem that talks to HDFS over HTTP\n(HsftpFileSystem is the HTTPS variant).\nFTP\nAlthough not complete at the time of this writing (https://issues.apache.org/jira/\nbrowse/HADOOP-3199), there is an FTP interface to HDFS, which permits the use\nof the FTP protocol to interact with HDFS. This interface is a convenient way to\ntransfer data into and out of HDFS using existing FTP clients.\nThe FTP interface to HDFS is not to be confused with FTPFileSystem, which ex-\nposes any FTP server as a Hadoop filesystem.\nThe Java Interface\nIn this section, we dig into the Hadoop’s FileSystem class: the API for interacting with\none of Hadoop’s filesystems.§ While we focus mainly on the HDFS implementation,\nDistributedFileSystem, in general you should strive to write your code against the\nFileSystem abstract class, to retain portability across filesystems. This is very useful\nwhen testing your program, for example, since you can rapidly run tests using data\nstored on the local filesystem.\nReading Data from a Hadoop URL\nOne of the simplest ways to read a file from a Hadoop filesystem is by using a\njava.net.URL object to open a stream to read the data from. The general idiom is:\nInputStream in = null;\ntry {\nin = new URL(""hdfs://host/path"").openStream();\n// process in\n} finally {\nIOUtils.closeStream(in);\n}\nThere’s a little bit more work required to make Java recognize Hadoop’s hdfs URL\nscheme. This is achieved by calling the setURLStreamHandlerFactory method on URL\nwith an instance of FsUrlStreamHandlerFactory. This method can only be called once\nper JVM, so it is typically executed in a static block. This limitation means that if some\nother part of your program—perhaps a third-party component outside your control—\nsets a URLStreamHandlerFactory, you won’t be able to use this approach for reading data\nfrom Hadoop. The next section discusses an alternative.\n§ There is a Java Specification Request (JSR 203, “NIO.2,” http://jcp.org/en/jsr/detail?id=203) for an improved\nfilesystem interface in Java, with the explicit goal of providing pluggable filesystem implementations. In the\nlong term, it is possible that this new interface (which is slated to appear in Java 7) will replace Hadoop’s\nFileSystem abstraction.\nThe Java Interface | 51Example 3-1 shows a program for displaying files from Hadoop filesystems on standard\noutput, like the Unix cat command.\nExample 3-1. Displaying files from a Hadoop filesystem on standard output using a\nURLStreamHandler\npublic class URLCat {\nstatic {\nURL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\n}\n}\npublic static void main(String[] args) throws Exception {\nInputStream in = null;\ntry {\nin = new URL(args[0]).openStream();\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n}\nWe make use of the handy IOUtils class that comes with Hadoop for closing the stream\nin the finally clause, and also for copying bytes between the input stream and the\noutput stream (System.out in this case). The last two arguments to the copyBytes\nmethod are the buffer size used for copying, and whether to close the streams when the\ncopy is complete. We close the input stream ourselves, and System.out doesn’t need to\nbe closed.\nHere’s a sample run:‖\n% hadoop URLCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nReading Data Using the FileSystem API\nAs the previous section explained, sometimes it is impossible to set a URLStreamHand\nlerFactory for your application. In this case, you will need to use the FileSystem API\nto open an input stream for a file.\nA file in a Hadoop filesystem is represented by a Hadoop Path object (and not a\njava.io.File object, since its semantics are too closely tied to the local filesystem). You\ncan think of a Path as a Hadoop filesystem URI, such as hdfs://localhost/user/tom/quan-\ngle.txt.\n‖ The text is from The Quangle Wangle’s Hat by Edward Lear.\n52 | Chapter 3: The Hadoop Distributed FilesystemFileSystem is a general filesystem API, so the first step is to retrieve an instance for the\nfilesystem we want to use—HDFS in this case. There are two static factory methods\nfor getting a FileSystem instance:\npublic static FileSystem get(Configuration conf) throws IOException\npublic static FileSystem get(URI uri, Configuration conf) throws IOException\nA Configuration object encapsulates a client or server’s configuration, which is set using\nconfiguration files read from the classpath, such as conf/core-site.xml. The first method\nreturns the default filesystem (as specified in the file conf/core-site.xml, or the default\nlocal filesystem if not specified there). The second uses the given URI’s scheme and\nauthority to determine the filesystem to use, falling back to the default filesystem if no\nscheme is specified in the given URI.\nWith a FileSystem instance in hand, we invoke an open() method to get the input stream\nfor a file:\npublic FSDataInputStream open(Path f) throws IOException\npublic abstract FSDataInputStream open(Path f, int bufferSize) throws IOException\nThe first method uses a default buffer size of 4 K.\nPutting this together, we can rewrite Example 3-1 as shown in Example 3-2.\nExample 3-2. Displaying files from a Hadoop filesystem on standard output by using the FileSystem\ndirectly\npublic class FileSystemCat {\n}\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nInputStream in = null;\ntry {\nin = fs.open(new Path(uri));\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n}\nThe program runs as follows:\n% hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nThe Java Interface | 53FSDataInputStream\nThe open() method on FileSystem actually returns a FSDataInputStream rather than a\nstandard java.io class. This class is a specialization of java.io.DataInputStream with\nsupport for random access, so you can read from any part of the stream.\npackage org.apache.hadoop.fs;\npublic class FSDataInputStream extends DataInputStream\nimplements Seekable, PositionedReadable {\n// implementation elided\n}\nThe Seekable interface permits seeking to a position in the file, and a query method for\nthe current offset from the start of the file (getPos()):\npublic interface Seekable {\nvoid seek(long pos) throws IOException;\nlong getPos() throws IOException;\nboolean seekToNewSource(long targetPos) throws IOException;\n}\nCalling seek() with a position that is greater than the length of the file will result in an\nIOException. Unlike the skip() method of java.io.InputStream which positions the\nstream at a point later than the current position, seek() can move to an arbitrary, ab-\nsolute position in the file.\nThe seekToNewSource() method is not normally used by application writers. It attempts\nto find another copy of the data and seek to the offset targetPos in the new copy. This\nis used internally in HDFS to provide a reliable input stream of data to the client in the\nface of datanode failure.\nExample 3-3 is a simple extension of Example 3-2 that writes a file to standard out\ntwice: after writing it once, it seeks to the start of the file and streams through it once\nagain.\nExample 3-3. Displaying files from a Hadoop filesystem on standard output twice, by using seek\npublic class FileSystemDoubleCat {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nFSDataInputStream in = null;\ntry {\nin = fs.open(new Path(uri));\nIOUtils.copyBytes(in, System.out, 4096, false);\nin.seek(0); // go back to the start of the file\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n54 | Chapter 3: The Hadoop Distributed Filesystem}\n}\nHere’s the result of running it on a small file:\n% hadoop FileSystemDoubleCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nFSDataInputStream also implements the PositionedReadable interface for reading parts\nof a file at a given offset:\npublic interface PositionedReadable {\npublic int read(long position, byte[] buffer, int offset, int length)\nthrows IOException;\npublic void readFully(long position, byte[] buffer, int offset, int length)\nthrows IOException;\n}\npublic void readFully(long position, byte[] buffer) throws IOException;\nThe read() method reads up to length bytes from the given position in the file into the\nbuffer at the given offset in the buffer. The return value is the number of bytes actually\nread: callers should check this value as it may be less than length. The readFully()\nmethods will read length bytes into the buffer (or buffer.length bytes for the version\nthat just takes a byte array buffer), unless the end of the file is reached, in which case\nan EOFException is thrown.\nAll of these methods preserve the current offset in the file and are thread-safe, so they\nprovide a convenient way to access another part of the file—metadata perhaps—while\nreading the main body of the file. In fact, they are just implemented using the\nSeekable interface using the following pattern:\nlong oldPos = getPos();\ntry {\nseek(position);\n// read data\n} finally {\nseek(oldPos);\n}\nFinally, bear in mind that calling seek() is a relatively expensive operation, and should\nbe used sparingly. You should structure your application access patterns to rely on\nstreaming data, (by using MapReduce, for example) rather than performing a large\nnumber of seeks.\nThe Java Interface | 55Writing Data\nThe FileSystem class has a number of methods for creating a file. The simplest is the\nmethod that takes a Path object for the file to be created and returns an output stream\nto write to:\npublic FSDataOutputStream create(Path f) throws IOException\nThere are overloaded versions of this method that allow you to specify whether to\nforcibly overwrite existing files, the replication factor of the file, the buffer size to use\nwhen writing the file, the block size for the file, and file permissions.\nThe create() methods create any parent directories of the file to be\nwritten that don’t already exist. Though convenient, this behavior may\nbe unexpected. If you want the write to fail if the parent directory doesn’t\nexist, then you should check for the existence of the parent directory\nfirst by calling the exists() method.\nThere’s also an overloaded method for passing a callback interface, Progressable, so\nyour application can be notified of the progress of the data being written to the\ndatanodes:\npackage org.apache.hadoop.util;\npublic interface Progressable {\npublic void progress();\n}\nAs an alternative to creating a new file, you can append to an existing file using the\nappend() method (there are also some other overloaded versions):\npublic FSDataOutputStream append(Path f) throws IOException\nThe append operation allows a single writer to modify an already written file by opening\nit and writing data from the final offset in the file. With this API, applications that\nproduce unbounded files, such as logfiles, can write to an existing file after a restart,\nfor example. The append operation is optional and not implemented by all Hadoop\nfilesystems. For example, HDFS supports append, but S3 filesystems don’t.\nExample 3-4 shows how to copy a local file to a Hadoop filesystem. We illustrate pro-\ngress by printing a period every time the progress() method is called by Hadoop, which\nis after each 64 K packet of data is written to the datanode pipeline. (Note that this\nparticular behavior is not specified by the API, so it is subject to change in later versions\nof Hadoop. The API merely allows you to infer that “something is happening.”)\nExample 3-4. Copying a local file to a Hadoop filesystem, and shows progress\npublic class FileCopyWithProgress {\npublic static void main(String[] args) throws Exception {\nString localSrc = args[0];\nString dst = args[1];\n56 | Chapter 3: The Hadoop Distributed FilesystemInputStream in = new BufferedInputStream(new FileInputStream(localSrc));\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(dst), conf);\nOutputStream out = fs.create(new Path(dst), new Progressable() {\npublic void progress() {\nSystem.out.print(""."");\n}\n});\n}\n}\nIOUtils.copyBytes(in, out, 4096, true);\nTypical usage:\n% hadoop FileCopyWithProgress input/docs/1400-8.txt hdfs://localhost/user/tom/1400-8.txt\n...............\nCurrently, none of the other Hadoop filesystems call progress() during writes. Progress\nis important in MapReduce applications, as you will see in later chapters.\nFSDataOutputStream\nThe create() method on FileSystem returns a FSDataOutputStream, which, like\nFSDataInputStream, has a method for querying the current position in the file:\npackage org.apache.hadoop.fs;\npublic class FSDataOutputStream extends DataOutputStream implements Syncable {\npublic long getPos() throws IOException {\n// implementation elided\n}\n// implementation elided\n}\nHowever, unlike FSDataInputStream, FSDataOutputStream does not permit seeking. This\nis because HDFS allows only sequential writes to an open file, or appends to an already\nwritten file. In other words, there is no support for writing to anywhere other than the\nend of the file, so there is no value in being able to seek while writing.\nDirectories\nFileSystem provides a method to create a directory:\npublic boolean mkdirs(Path f) throws IOException\nThis method creates all of the necessary parent directories if they don’t already exist,\njust like java.io.File’s mkdirs() method. It returns true if the directory (and all parent\ndirectories) was successfully created.\nThe Java Interface | 57Often, you don’t need to explicitly create a directory, since writing a file, by calling\ncreate(), will automatically create any parent directories.\nQuerying the Filesystem\nFile metadata: FileStatus\nAn important feature of any filesystem is the ability to navigate its directory structure\nand retrieve information about the files and directories that it stores. The FileStatus\nclass encapsulates filesystem metadata for files and directories, including file length,\nblock size, replication, modification time, ownership, and permission information.\nThe method getFileStatus() on FileSystem provides a way of getting a FileStatus\nobject for a single file or directory. Example 3-5 shows an example of its use.\nExample 3-5. Demonstrating file status information\npublic class ShowFileStatusTest {\nprivate MiniDFSCluster cluster; // use an in-process HDFS cluster for testing\nprivate FileSystem fs;\n@Before\npublic void setUp() throws IOException {\nConfiguration conf = new Configuration();\nif (System.getProperty(""test.build.data"") == null) {\nSystem.setProperty(""test.build.data"", ""/tmp"");\n}\ncluster = new MiniDFSCluster(conf, 1, true, null);\nfs = cluster.getFileSystem();\nOutputStream out = fs.create(new Path(""/dir/file""));\nout.write(""content"".getBytes(""UTF-8""));\nout.close();\n}\n@After\npublic void tearDown() throws IOException {\nif (fs != null) { fs.close(); }\nif (cluster != null) { cluster.shutdown(); }\n}\n@Test(expected = FileNotFoundException.class)\npublic void throwsFileNotFoundForNonExistentFile() throws IOException {\nfs.getFileStatus(new Path(""no-such-file""));\n}\n@Test\npublic void fileStatusForFile() throws IOException {\nPath file = new Path(""/dir/file"");\nFileStatus stat = fs.getFileStatus(file);\nassertThat(stat.getPath().toUri().getPath(), is(""/dir/file""));\nassertThat(stat.isDir(), is(false));\nassertThat(stat.getLen(), is(7L));\n58 | Chapter 3: The Hadoop Distributed Filesystem}\nassertThat(stat.getModificationTime(),\nis(lessThanOrEqualTo(System.currentTimeMillis())));\nassertThat(stat.getReplication(), is((short) 1));\nassertThat(stat.getBlockSize(), is(64 * 1024 * 1024L));\nassertThat(stat.getOwner(), is(""tom""));\nassertThat(stat.getGroup(), is(""supergroup""));\nassertThat(stat.getPermission().toString(), is(""rw-r--r--""));\n@Test\npublic void fileStatusForDirectory() throws IOException {\nPath dir = new Path(""/dir"");\nFileStatus stat = fs.getFileStatus(dir);\nassertThat(stat.getPath().toUri().getPath(), is(""/dir""));\nassertThat(stat.isDir(), is(true));\nassertThat(stat.getLen(), is(0L));\nassertThat(stat.getModificationTime(),\nis(lessThanOrEqualTo(System.currentTimeMillis())));\nassertThat(stat.getReplication(), is((short) 0));\nassertThat(stat.getBlockSize(), is(0L));\nassertThat(stat.getOwner(), is(""tom""));\nassertThat(stat.getGroup(), is(""supergroup""));\nassertThat(stat.getPermission().toString(), is(""rwxr-xr-x""));\n}\n}\nIf no file or directory exists, a FileNotFoundException is thrown. However, if you are\ninterested only in the existence of a file or directory, then the exists() method on\nFileSystem is more convenient:\npublic boolean exists(Path f) throws IOException\nListing files\nFinding information on a single file or directory is useful, but you also often need to be\nable to list the contents of a directory. That’s what FileSystem’s listStatus() methods\nare for:\npublic\npublic\npublic\npublic\nFileStatus[]\nFileStatus[]\nFileStatus[]\nFileStatus[]\nlistStatus(Path f) throws IOException\nlistStatus(Path f, PathFilter filter) throws IOException\nlistStatus(Path[] files) throws IOException\nlistStatus(Path[] files, PathFilter filter) throws IOException\nWhen the argument is a file, the simplest variant returns an array of FileStatus objects\nof length 1. When the argument is a directory it returns zero or more FileStatus objects\nrepresenting the files and directories contained in the directory.\nOverloaded variants allow a PathFilter to be supplied to restrict the files and directories\nto match—you will see an example in section “PathFilter” on page 61. Finally, if you\nspecify an array of paths the result is a shortcut for calling the equivalent single-path\nlistStatus method for each path in turn and accumulating the FileStatus object arrays\nin a single array. This can be useful for building up lists of input files to process from\nThe Java Interface | 59distinct parts of the filesystem tree. Example 3-6 is a simple demonstration of this idea.\nNote the use of stat2Paths() in FileUtil for turning an array of FileStatus objects to\nan array of Path objects.\nExample 3-6. Showing the file statuses for a collection of paths in a Hadoop filesystem\npublic class ListStatus {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath[] paths = new Path[args.length];\nfor (int i = 0; i < paths.length; i++) {\npaths[i] = new Path(args[i]);\n}\n}\n}\nFileStatus[] status = fs.listStatus(paths);\nPath[] listedPaths = FileUtil.stat2Paths(status);\nfor (Path p : listedPaths) {\nSystem.out.println(p);\n}\nWe can use this program to find the union of directory listings for a collection of paths:\n% hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tom\nhdfs://localhost/user\nhdfs://localhost/user/tom/books\nhdfs://localhost/user/tom/quangle.txt\nFile patterns\nIt is a common requirement to process sets of files in a single operation. For example,\na MapReduce job for log processing might analyze a month worth of files, contained\nin a number of directories. Rather than having to enumerate each file and directory to\nspecify the input, it is convenient to use wildcard characters to match multiple files\nwith a single expression, an operation that is known as globbing. Hadoop provides two\nFileSystem methods for processing globs:\npublic FileStatus[] globStatus(Path pathPattern) throws IOException\npublic FileStatus[] globStatus(Path pathPattern, PathFilter filter) throws IOException\nThe globStatus() methods returns an array of FileStatus objects whose paths match\nthe supplied pattern, sorted by path. An optional PathFilter can be specified to restrict\nthe matches further.\nHadoop supports the same set of glob characters as Unix bash (see Table 3-2).\n60 | Chapter 3: The Hadoop Distributed FilesystemTable 3-2. Glob characters and their meanings\nGlob Name Matches\n* asterisk Matches zero or more characters\n? question mark Matches a single character\n[ab] character class Matches a single character in the set {a, b}\n[^ab] negated character class Matches a single character that is not in the set {a, b}\n[a-b] character range Matches a single character in the (closed) range [a, b], where a is lexicographically\n                     less than or equal to b\n[^a-b] negated character range Matches a single character that is not in the (closed) range [a, b], where a is\n                              lexicographically less than or equal to b\n{a,b} alternation Matches either expression a or b\n\\c escaped character Matches character c when it is a metacharacter\nImagine that logfiles are stored in a directory structure organized hierarchically by date.\nSo, for example, logfiles for the last day of 2007 would go in a directory\nnamed /2007/12/31. Suppose that the full file listing is:\n•\n•\n•\n•\n/2007/12/30\n/2007/12/31\n/2008/01/01\n/2008/01/02\nHere are some file globs and their expansions:\nGlob Expansion\n/* /2007 /2008\n/*/* /2007/12 /2008/01\n/*/12/* /2007/12/30 /2007/12/31\n/200? /2007 /2008\n/200[78] /2007 /2008\n/200[7-8] /2007 /2008\n/200[^01234569] /2007 /2008\n/*/*/{31,01} /2007/12/31 /2008/01/01\n/*/*/3{0,1} /2007/12/30 /2007/12/31\n/*/{12/31,01/01} /2007/12/31 /2008/01/01\nPathFilter\nGlob patterns are not always powerful enough to describe a set of files you want to\naccess. For example, it is not generally possible to exclude a particular file using a glob\nThe Java Interface | 61pattern. The listStatus() and globStatus() methods of FileSystem take an optional\nPathFilter, which allows programmatic control over matching:\npackage org.apache.hadoop.fs;\npublic interface PathFilter {\nboolean accept(Path path);\n}\nPathFilter is the equivalent of java.io.FileFilter for Path objects rather than File\nobjects.\nExample 3-7 shows a PathFilter for excluding paths that match a regular expression.\nExample 3-7. A PathFilter for excluding paths that match a regular expression\npublic class RegexExcludePathFilter implements PathFilter {\nprivate final String regex;\npublic RegexExcludePathFilter(String regex) {\nthis.regex = regex;\n}\n}\npublic boolean accept(Path path) {\nreturn !path.toString().matches(regex);\n}\nThe filter passes only files that don’t match the regular expression. We use the filter in\nconjunction with a glob that picks out an initial set of files to include: the filter is used\nto refine the results. For example:\nfs.globStatus(new Path(""/2007/*/*""), new RegexExcludeFilter(""^.*/2007/12/31$""))\nwill expand to /2007/12/30.\nFilters can only act on a file’s name, as represented by a Path. They can’t use a file’s\nproperties, such as creation time, as the basis of the filter. Nevertheless, they can per-\nform matching that neither glob patterns nor regular expressions can achieve. For ex-\nample, if you store files in a directory structure that is laid out by date (like in the\nprevious section), then you can write a PathFilter to pick out files that fall in a given\ndate range.\nDeleting Data\nUse the delete() method on FileSystem to permanently remove files or directories:\npublic boolean delete(Path f, boolean recursive) throws IOException\nIf f is a file or an empty directory, then the value of recursive is ignored. A nonempty\ndirectory is only deleted, along with its contents, if recursive is true (otherwise an\nIOException is thrown).\n62 | Chapter 3: The Hadoop Distributed FilesystemData Flow\nAnatomy of a File Read\nTo get an idea of how data flows between the client interacting with HDFS, the name-\nnode and the datanode, consider Figure 3-1, which shows the main sequence of events\nwhen reading a file.\nFigure 3-1. A client reading data from HDFS\nThe client opens the file it wishes to read by calling open() on the FileSystem object,\nwhich for HDFS is an instance of DistributedFileSystem (step 1 in Figure 3-1).\nDistributedFileSystem calls the namenode, using RPC, to determine the locations of\nthe blocks for the first few blocks in the file (step 2). For each block, the namenode\nreturns the addresses of the datanodes that have a copy of that block. Furthermore, the\ndatanodes are sorted according to their proximity to the client (according to the top-\nology of the cluster’s network; see “Network Topology and Hadoop” on page 64). If\nthe client is itself a datanode (in the case of a MapReduce task, for instance), then it\nwill read from the local datanode.\nThe DistributedFileSystem returns a FSDataInputStream (an input stream that supports\nfile seeks) to the client for it to read data from. FSDataInputStream in turn wraps a\nDFSInputStream, which manages the datanode and namenode I/O.\nData Flow | 63The client then calls read() on the stream (step 3). DFSInputStream, which has stored\nthe datanode addresses for the first few blocks in the file, then connects to the first\n(closest) datanode for the first block in the file. Data is streamed from the datanode\nback to the client, which calls read() repeatedly on the stream (step 4). When the end\nof the block is reached, DFSInputStream will close the connection to the datanode, then\nfind the best datanode for the next block (step 5). This happens transparently to the\nclient, which from its point of view is just reading a continuous stream.\nBlocks are read in order with the DFSInputStream opening new connections to datanodes\nas the client reads through the stream. It will also call the namenode to retrieve the\ndatanode locations for the next batch of blocks as needed. When the client has finished\nreading, it calls close() on the FSDataInputStream (step 6).\nDuring reading, if the client encounters an error while communicating with a datanode,\nthen it will try the next closest one for that block. It will also remember datanodes that\nhave failed so that it doesn’t needlessly retry them for later blocks. The client also\nverifies checksums for the data transferred to it from the datanode. If a corrupted block\nis found, it is reported to the namenode, before the client attempts to read a replica of\nthe block from another datanode.\nOne important aspect of this design is that the client contacts datanodes directly to\nretrieve data, and is guided by the namenode to the best datanode for each block. This\ndesign allows HDFS to scale to large number of concurrent clients, since the data traffic\nis spread across all the datanodes in the cluster. The namenode meanwhile merely has\nto service block location requests (which it stores in memory, making them very effi-\ncient), and does not, for example, serve data, which would quickly become a bottleneck\nas the number of clients grew.\nNetwork Topology and Hadoop\nWhat does it mean for two nodes in a local network to be “close” to each other? In the\ncontext of high-volume data processing, the limiting factor is the rate at which we can\ntransfer data between nodes—bandwidth is a scarce commodity. The idea is to use the\nbandwidth between two nodes as a measure of distance.\nRather than measuring bandwidth between nodes, which can be difficult to do in prac-\ntice (it requires a quiet cluster, and the number of pairs of nodes in a cluster grows as\nthe square of the number of nodes), Hadoop takes a simple approach in which the\nnetwork is represented as a tree and the distance between two nodes is the sum of their\ndistances to their closest common ancestor. Levels in the tree are not predefined, but\nit is common to have levels that correspond to the data center, the rack, and the node\nthat a process is running on. The idea is that the bandwidth available for each of the\nfollowing scenarios becomes progressively less:\n64 | Chapter 3: The Hadoop Distributed Filesystem• Processes on the same node\n• Different nodes on the same rack\n• Nodes on different racks in the same data center\n• Nodes in different data centers#\nFor example, imagine a node n1 on rack r1 in data center d1. This can be represented\nas /d1/r1/n1. Using this notation, here are the distances for the four scenarios:\n• distance(/d1/r1/n1, /d1/r1/n1) = 0 (processes on the same node)\n• distance(/d1/r1/n1, /d1/r1/n2) = 2 (different nodes on the same rack)\n• distance(/d1/r1/n1, /d1/r2/n3) = 4 (nodes on different racks in the same data center)\n• distance(/d1/r1/n1, /d2/r3/n4) = 6 (nodes in different data centers)\nThis is illustrated schematically in Figure 3-2. (Mathematically inclined readers will\nnotice that this is an example of a distance metric.)\nFinally, it is important to realize that Hadoop cannot divine your network topology for\nyou. It needs some help; we’ll cover how to configure topology in “Network Topol-\nogy” on page 247. By default though, it assumes that the network is flat—a single-level\nhierarchy—or in other words, that all nodes are on a single rack in a single data center.\nFor small clusters, this may actually be the case, and no further configuration is\nrequired.\nFigure 3-2. Network distance in Hadoop\n# At the time of this writing, Hadoop is not suited for running across data centers.\nData Flow | 65Anatomy of a File Write\nNext we’ll look at how files are written to HDFS. Although quite detailed, it is instruc-\ntive to understand the data flow since it clarifies HDFS’s coherency model.\nThe case we’re going to consider is the case of creating a new file, writing data to it,\nthen closing the file. See Figure 3-3.\nFigure 3-3. A client writing data to HDFS\nThe client creates the file by calling create() on DistributedFileSystem (step 1 in\nFigure 3-3). DistributedFileSystem makes an RPC call to the namenode to create a new\nfile in the filesystem’s namespace, with no blocks associated with it (step 2). The name-\nnode performs various checks to make sure the file doesn’t already exist, and that the\nclient has the right permissions to create the file. If these checks pass, the namenode\nmakes a record of the new file; otherwise, file creation fails and the client is thrown an\nIOException. The DistributedFileSystem returns a FSDataOutputStream for the client to\nstart writing data to. Just as in the read case, FSDataOutputStream wraps a DFSOutput\nStream, which handles communication with the datanodes and namenode.\nAs the client writes data (step 3), DFSOutputStream splits it into packets, which it writes\nto an internal queue, called the data queue. The data queue is consumed by the Data\nStreamer, whose responsibility it is to ask the namenode to allocate new blocks by\npicking a list of suitable datanodes to store the replicas. The list of datanodes forms a\npipeline—we’ll assume the replication level is 3, so there are three nodes in the pipeline.\nThe DataStreamer streams the packets to the first datanode in the pipeline, which stores\n66 | Chapter 3: The Hadoop Distributed Filesystemthe packet and forwards it to the second datanode in the pipeline. Similarly, the second\ndatanode stores the packet and forwards it to the third (and last) datanode in the pipe-\nline (step 4).\nDFSOutputStream also maintains an internal queue of packets that are waiting to be\nacknowledged by datanodes, called the ack queue. A packet is removed from the ack\nqueue only when it has been acknowledged by all the datanodes in the pipeline (step 5).\nIf a datanode fails while data is being written to it, then the following actions are taken,\nwhich are transparent to the client writing the data. First the pipeline is closed, and any\npackets in the ack queue are added to the front of the data queue so that datanodes\nthat are downstream from the failed node will not miss any packets. The current block\non the good datanodes is given a new identity, which is communicated to the name-\nnode, so that the partial block on the failed datanode will be deleted if the failed data-\nnode recovers later on. The failed datanode is removed from the pipeline and the\nremainder of the block’s data is written to the two good datanodes in the pipeline. The\nnamenode notices that the block is under-replicated, and it arranges for a further replica\nto be created on another node. Subsequent blocks are then treated as normal.\nIt’s possible, but unlikely, that multiple datanodes fail while a block is being written.\nAs long as dfs.replication.min replicas (default one) are written the write will succeed,\nand the block will be asynchronously replicated across the cluster until its target rep-\nlication factor is reached (dfs.replication, which defaults to three).\nWhen the client has finished writing data it calls close() on the stream (step 6). This\naction flushes all the remaining packets to the datanode pipeline and waits for ac-\nknowledgments before contacting the namenode to signal that the file is complete (step\n7). The namenode already knows which blocks the file is made up of (via Data\nStreamer asking for block allocations), so it only has to wait for blocks to be minimally\nreplicated before returning successfully.\nReplica Placement\nHow does the namenode choose which datanodes to store replicas on? There’s a trade-\noff between reliability and write bandwidth and read bandwidth here. For example,\nplacing all replicas on a single node incurs the lowest write bandwidth penalty since\nthe replication pipeline runs on a single node, but this offers no real redundancy (if the\nnode fails, the data for that block is lost). Also, the read bandwidth is high for off-rack\nreads. At the other extreme, placing replicas in different data centers may maximize\nredundancy, but at the cost of bandwidth. Even in the same data center (which is what\nall Hadoop clusters to date have run in), there are a variety of placement strategies.\nIndeed, Hadoop changed its placement strategy in release 0.17.0 to one that helps keep\na fairly even distribution of blocks across the cluster. (See “balancer” on page 284 for\ndetails on keeping a cluster balanced.)\nHadoop’s strategy is to place the first replica on the same node as the client (for clients\nrunning outside the cluster, a node is chosen at random, although the system tries not\nto pick nodes that are too full or too busy). The second replica is placed on a different\nData Flow | 67rack from the first (off-rack), chosen at random. The third replica is placed on the same\nrack as the second, but on a different node chosen at random. Further replicas are\nplaced on random nodes on the cluster, although the system tries to avoid placing too\nmany replicas on the same rack.\nOnce the replica locations have been chosen, a pipeline is built, taking network topol-\nogy into account. For a replication factor of 3, the pipeline might look like Figure 3-4.\nOverall, this strategy gives a good balance between reliability (blocks are stored on two\nracks), write bandwidth (writes only have to traverse a single network switch), read\nperformance (there’s a choice of two racks to read from), and block distribution across\nthe cluster (clients only write a single block on the local rack).\nFigure 3-4. A typical replica pipeline\nCoherency Model\nA coherency model for a filesystem describes the data visibility of reads and writes for\na file. HDFS trades off some POSIX requirements for performance, so some operations\nmay behave differently than you expect them to.\nAfter creating a file, it is visible in the filesystem namespace, as expected:\nPath p = new Path(""p"");\nfs.create(p);\nassertThat(fs.exists(p), is(true));\nHowever, any content written to the file is not guaranteed to be visible, even if the\nstream is flushed. So the file appears to have a length of zero:\nPath p = new Path(""p"");\nOutputStream out = fs.create(p);\n68 | Chapter 3: The Hadoop Distributed Filesystemout.write(""content"".getBytes(""UTF-8""));\nout.flush();\nassertThat(fs.getFileStatus(p).getLen(), is(0L));\nOnce more than a block’s worth of data has been written, the first block will be visible\nto new readers. This is true of subsequent blocks, too: it is always the current block\nbeing written that is not visible to other readers.\nHDFS provides a method for forcing all buffers to be synchronized to the datanodes\nvia the sync() method on FSDataOutputStream. After a successful return from sync(),\nHDFS guarantees that the data written up to that point in the file is persisted and visible\nto all new readers.* In the event of a crash (of the client or HDFS), the data will not be\nlost:\nPath p = new Path(""p"");\nFSDataOutputStream out = fs.create(p);\nout.write(""content"".getBytes(""UTF-8""));\nout.flush();\nout.sync();\nassertThat(fs.getFileStatus(p).getLen(), is(((long) ""content"".length())));\nThis behavior is similar to the fsync system call in Unix that commits buffered data for\na file descriptor. For example, using the Java API to write a local file, we are guaranteed\nto see the content after flushing the stream and synchronizing:\nFileOutputStream out = new FileOutputStream(localFile);\nout.write(""content"".getBytes(""UTF-8""));\nout.flush(); // flush to operating system\nout.getFD().sync(); // sync to disk\nassertThat(localFile.length(), is(((long) ""content"".length())));\nClosing a file in HDFS performs an implicit sync(), too:\nPath p = new Path(""p"");\nOutputStream out = fs.create(p);\nout.write(""content"".getBytes(""UTF-8""));\nout.close();\nassertThat(fs.getFileStatus(p).getLen(), is(((long) ""content"".length())));\nConsequences for application design\nThis coherency model has implications for the way you design applications. With no\ncalls to sync(), you should be prepared to lose up to a block of data in the event of\nclient or system failure. For many applications, this is unacceptable, so you should call\nsync() at suitable points, such as after writing a certain number of records or number\nof bytes. Though the sync() operation is designed to not unduly tax HDFS, it does have\nsome overhead, so there is a trade-off between data robustness and throughput. What\nis an acceptable trade-off is application-dependent, and suitable values can be selected\nafter measuring your application’s performance with different sync() frequencies.\n* At the time of this writing, the visibility guarantee is not honored. See https://issues.apache.org/jira/browse/\nHADOOP-4379.\nData Flow | 69Parallel Copying with distcp\nThe HDFS access patterns that we have seen so far focus on single-threaded access. It’s\npossible to act on a collection of files, by specifying file globs, for example, but for\nefficient, parallel processing of these files you would have to write a program yourself.\nHadoop comes with a useful program called distcp for copying large amounts of data\nto and from Hadoop filesystems in parallel.\nThe canonical use case for distcp is for transferring data between two HDFS clusters.\nIf the clusters are running identical versions of Hadoop, the hdfs scheme is\nappropriate:\n% hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar\nThis will copy the /foo directory (and its contents) from the first cluster to the /bar\ndirectory on the second cluster, so the second cluster ends up with the directory struc-\nture /bar/foo. If /bar doesn’t exist, it will be created first. You can specify multiple source\npaths, and all will be copied to the destination. Source paths must be absolute.\nBy default, distcp will skip files that already exist in the destination, but they can be\noverwritten by supplying the -overwrite option. You can also update only files that\nhave changed using the -update option.\nUsing either (or both) of -overwrite or -update changes the how the\nsource and destination paths are interpreted. This is best shown with\nan example. If we changed a file in the /foo subtree on the first cluster\nfrom the previous example, then we could synchronize the change with\nthe second cluster by running:\n% hadoop distcp -update hdfs://namenode1/foo hdfs://namenode2/bar/foo\nThe extra trailing /foo subdirectory is needed on the destination, as now\nthe contents of the source directory are copied to the contents of the\ndestination directory. (If you are familiar with rsync, you can think of\nthe -overwrite or -update options as adding an implicit trailing slash to\nthe source.)\nIf you are unsure of the effect of a distcp operation, it is a good idea to\ntry it out on a small test directory tree first.\nThere are more options to control the behavior of distcp, including ones to preserve file\nattributes, ignore failures, and limit the number of files or total data copied. Run it with\nno options to see the usage instructions.\ndistcp is implemented as a MapReduce job where the work of copying is done by the\nmaps that run in parallel across the cluster. There are no reducers. Each file is copied\nby a single map, and distcp tries to give each map approximately the same amount of\ndata, by bucketing files into roughly equal allocations.\n70 | Chapter 3: The Hadoop Distributed FilesystemThe number of maps is decided as follows. Since it’s a good idea to get each map to\ncopy a reasonable amount of data to minimize overheads in task setup, each map copies\nat least 256 MB (unless the total size of the input is less, in which case one map handles\nit all). For example, 1 GB of files will be given four map tasks. When the data size is\nvery large, it becomes necessary to limit the number of maps in order to limit bandwidth\nand cluster utilization. By default, the maximum number of maps is 20 per (tasktracker)\ncluster node. For example, copying 1,000 GB of files to a 100-node cluster will allocate\n2,000 maps (20 per node), so each will copy 512 MB on average. This can be reduced\nby specifying the -m argument to distcp. For example, -m 1000 would allocate 1,000\nmaps, each copying 1 GB on average.\nIf you try to use distcp between two HDFS clusters that are running different versions,\nthe copy will fail if you use the hdfs protocol, since the RPC systems are incompatible.\nTo remedy this, you can use the HTTP-based HFTP filesystem to read from the source.\nThe job must run on the destination cluster so that the HDFS RPC versions are com-\npatible. To repeat the previous example using HFTP:\n% hadoop distcp hftp://namenode1:50070/foo hdfs://namenode2/bar\nNote that you need to specify the namenode’s web port in the source URI. This is\ndetermined by the dfs.http.address property, which defaults to 50070.\nKeeping an HDFS Cluster Balanced\nWhen copying data into HDFS, it’s important to consider cluster balance. HDFS works\nbest when the file blocks are evenly spread across the cluster, so you want to ensure\nthat distcp doesn’t disrupt this. Going back to the 1,000 GB example, by specifying -m\n1 a single map would do the copy, which—apart from being slow and not using the\ncluster resources efficiently—would mean that the first replica of each block would\nreside on the node running the map (until the disk filled up). The second and third\nreplicas would be spread across the cluster, but this one node would be unbalanced.\nBy having more maps than nodes in the cluster, this problem is avoided—for this rea-\nson, it’s best to start by running distcp with the default of 20 maps per node.\nHowever, it’s not always possible to prevent a cluster from becoming unbalanced. Per-\nhaps you want to limit the number of maps so that some of the nodes can be used by\nother jobs. In this case, you can use the balancer tool (see “balancer” on page 284) to\nsubsequently improve the block distribution across the cluster.\nHadoop Archives\nHDFS stores small files inefficiently, since each file is stored in a block, and block\nmetadata is held in memory by the namenode. Thus, a large number of small files can\neat up a lot of memory on the namenode. (Note, however, that small files do not take\nup any more disk space than is required to store the raw contents of the file. For\nHadoop Archives | 71example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not\n128 MB.)\nHadoop Archives, or HAR files, are a file archiving facility that packs files into HDFS\nblocks more efficiently, thereby reducing namenode memory usage while still allowing\ntransparent access to files. In particular, Hadoop Archives can be used as input to\nMapReduce.\nUsing Hadoop Archives\nA Hadoop Archive is created from a collection of files using the archive tool. The tool\nruns a MapReduce job to process the input files in parallel, so to run it, you need a\nMapReduce cluster running to use it. Here are some files in HDFS that we would like\nto archive:\n% hadoop fs -lsr /my/files\n-rw-r--r--\n1 tom supergroup\ndrwxr-xr-x\n- tom supergroup\n-rw-r--r--\n1 tom supergroup\n1 2009-04-09 19:13 /my/files/a\n0 2009-04-09 19:13 /my/files/dir\n1 2009-04-09 19:13 /my/files/dir/b\nNow we can run the archive command:\n% hadoop archive -archiveName files.har /my/files /my\nThe first option is the name of the archive, here files.har. HAR files always have\na .har extension, which is mandatory for reasons you shall see later. Next comes the\nfiles to put in the archive. Here we are archiving only one source tree, the files in /my/\nfiles in HDFS, but the tool accepts multiple source trees. The final argument is the\noutput directory for the HAR file. Let’s see what the archive has created:\n% hadoop fs -ls /my\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\ndrwxr-xr-x\n- tom supergroup\n% hadoop fs -ls /my/files.har\nFound 3 items\n-rw-r--r-- 10 tom supergroup\n-rw-r--r-- 10 tom supergroup\n-rw-r--r--\n1 tom supergroup\n0 2009-04-09 19:13 /my/files\n0 2009-04-09 19:13 /my/files.har\n165 2009-04-09 19:13 /my/files.har/_index\n23 2009-04-09 19:13 /my/files.har/_masterindex\n2 2009-04-09 19:13 /my/files.har/part-0\nThe directory listing shows what a HAR file is made of: two index files and a collection\nof part files—just one in this example. The part files contain the contents of a number\nof the original files concatenated together, and the indexes make it possible to look up\nthe part file that an archived file is contained in, and its offset and length. All these\ndetails are hidden from the application, however, which uses the har URI scheme to\ninteract with HAR files, using a HAR filesystem that is layered on top of the underlying\nfilesystem (HDFS in this case). The following command recursively lists the files in the\narchive:\n% hadoop fs -lsr har:///my/files.har\ndrw-r--r--\n- tom supergroup\ndrw-r--r--\n- tom supergroup\n72 | Chapter 3: The Hadoop Distributed Filesystem\n0 2009-04-09 19:13 /my/files.har/my\n0 2009-04-09 19:13 /my/files.har/my/files-rw-r--r--\ndrw-r--r--\n-rw-r--r--\n10 tom supergroup\n- tom supergroup\n10 tom supergroup\n1 2009-04-09 19:13 /my/files.har/my/files/a\n0 2009-04-09 19:13 /my/files.har/my/files/dir\n1 2009-04-09 19:13 /my/files.har/my/files/dir/b\nThis is quite straightforward if the filesystem that the HAR file is on is the default\nfilesystem. On the other hand, if you want to refer to a HAR file on a different filesystem,\nthen you need to use a different form of the path URI to normal. These two commands\nhave the same effect, for example:\n% hadoop fs -lsr har:///my/files.har/my/files/dir\n% hadoop fs -lsr har://hdfs-localhost:8020/my/files.har/my/files/dir\nNotice in the second form that the scheme is still har to signify a HAR filesystem, but\nthe authority is hdfs to specify the underlying filesystem’s scheme, followed by a dash\nand the HDFS host (localhost) and port (8020). We can now see why HAR files have\nto have a .har extension. The HAR filesystem translates the har URI into a URI for the\nunderlying filesystem, by looking at the authority and path up to and including the\ncomponent with the .har extension. In this case, it is hdfs://localhost:8020/user/tom/\nfiles.har. The remaining part of the path is the path of the file in the archive: /user/tom/\nfiles/dir.\nTo delete a HAR file, you need to use the recursive form of delete, since to the underlying\nfilesystem, the HAR file is a directory.\n% hadoop fs -rmr /my/files.har\nLimitations\nThere are a few limitations to be aware of with HAR files. Creating an archive creates\na copy of the original files, so you need as much disk space as the files you are archiving\nto create the archive (although you can delete the originals once you have created the\narchive). There is currently no support for archive compression, although the files that\ngo into the archive can be compressed (HAR files are like tar files in this respect).\nArchives are immutable once they have been created. To add or remove files, you must\nrecreate the archive. In practice, this is not a problem for files that don’t change after\nbeing written, since they can be archived in batches on a regular basis, such as daily or\nweekly.\nAs noted earlier, HAR files can be used as input to MapReduce. However, there is no\narchive-aware InputFormat that can pack multiple files into a single MapReduce split,\nso processing lots of small files, even in a HAR file, can still be inefficient. “Small files\nand CombineFileInputFormat” on page 190 discusses another approach to this\nproblem.\nHadoop Archives | 73CHAPTER 4\nHadoop I/O\nHadoop comes with a set of primitives for data I/O. Some of these are techniques that\nare more general than Hadoop, such as data integrity and compression, but deserve\nspecial consideration when dealing with multiterabyte datasets. Others are Hadoop\ntools or APIs that form the building blocks for developing distributed systems, such as\nserialization frameworks and on-disk data structures.\nData Integrity\nUsers of Hadoop rightly expect that no data will be lost or corrupted during storage or\nprocessing. However, since every I/O operation on the disk or network carries with it\na small chance of introducing errors into the data that it is reading or writing, when the\nvolumes of data flowing through the system are as large as the ones Hadoop is capable\nof handling, the chance of data corruption occurring is high.\nThe usual way of detecting corrupted data is by computing a checksum for the data\nwhen it first enters the system, and then whenever it is transmitted across a channel\nthat is unreliable and hence capable of corrupting the data. The data is deemed to be\ncorrupt if the newly generated checksum doesn’t exactly match the original. This tech-\nnique doesn’t offer any way to fix the data—merely error detection. (And this is a reason\nfor not using low-end hardware; in particular, be sure to use ECC memory.) Note that\nit is possible that it’s the checksum that is corrupt, not the data, but this is very unlikely,\nsince the checksum is much smaller than the data.\nA commonly used error-detecting code is CRC-32 (cyclic redundancy check), which\ncomputes a 32-bit integer checksum for input of any size.\nData Integrity in HDFS\nHDFS transparently checksums all data written to it and by default verifies checksums\nwhen reading data. A separate checksum is created for every io.bytes.per.checksum\nbytes of data. The default is 512 bytes, and since a CRC-32 checksum is 4 bytes long,\nthe storage overhead is less than 1%.\n75Datanodes are responsible for verifying the data they receive before storing the data\nand its checksum. This applies to data that they receive from clients and from other\ndatanodes during replication. A client writing data sends it to a pipeline of datanodes\n(as explained in Chapter 3), and the last datanode in the pipeline verifies the checksum.\nIf it detects an error, the client receives a ChecksumException, a subclass of IOException.\nWhen clients read data from datanodes, they verify checksums as well, comparing them\nwith the ones stored at the datanode. Each datanode keeps a persistent log of checksum\nverifications, so it knows the last time each of its blocks was verified. When a client\nsuccessfully verifies a block, it tells the datanode, which updates its log. Keeping sta-\ntistics such as these is valuable in detecting bad disks.\nAside from block verification on client reads, each datanode runs a DataBlockScanner\nin a background thread that periodically verifies all the blocks stored on the datanode.\nThis is to guard against corruption due to “bit rot” in the physical storage media. See\n“Datanode block scanner” on page 283 for details on how to access the scanner\nreports.\nSince HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of\nthe good replicas to produce a new, uncorrupt replica. The way this works is that if a\nclient detects an error when reading a block, it reports the bad block and the datanode\nit was trying to read from to the namenode before throwing a ChecksumException. The\nnamenode marks the block replica as corrupt, so it doesn’t direct clients to it, or try to\ncopy this replica to another datanode. It then schedules a copy of the block to be re-\nplicated on another datanode, so its replication factor is back at the expected level.\nOnce this has happened, the corrupt replica is deleted.\nIt is possible to disable verification of checksums by passing false to the setVerify\nChecksum() method on FileSystem, before using the open() method to read a file. The\nsame effect is possible from the shell by using the -ignoreCrc option with the -get or\nthe equivalent -copyToLocal command. This feature is useful if you have a corrupt file\nthat you want to inspect so you can decide what to do with it. For example, you might\nwant to see whether it can be salvaged before you delete it.\nLocalFileSystem\nThe Hadoop LocalFileSystem performs client-side checksumming. This means that\nwhen you write a file called filename, the filesystem client transparently creates a hidden\nfile, .filename.crc, in the same directory containing the checksums for each chunk of\nthe file. Like HDFS, the chunk size is controlled by the io.bytes.per.checksum property,\nwhich defaults to 512 bytes. The chunk size is stored as metadata in the .crc file, so the\nfile can be read back correctly even if the setting for the chunk size has changed.\nChecksums are verified when the file is read, and if an error is detected,\nLocalFileSystem throws a ChecksumException.\n76 | Chapter 4: Hadoop I/OChecksums are fairly cheap to compute (in Java, they are implemented in native code),\ntypically adding a few percent overhead to the time to read or write a file. For most\napplications, this is an acceptable price to pay for data integrity. It is, however, possible\nto disable checksums: the use case here is when the underlying filesystem support\nchecksums natively. This is accomplished by using RawLocalFileSystem in place of\nLocalFileSystem. To do this globally in an application, it suffices to remap the imple-\nmentation for file URIs by setting the property fs.file.impl to the value\norg.apache.hadoop.fs.RawLocalFileSystem. Alternatively, you can directly create a Raw\nLocalFileSystem instance, which may be useful if you want to disable checksum veri-\nfication for only some reads; for example:\nConfiguration conf = ...\nFileSystem fs = new RawLocalFileSystem();\nfs.initialize(null, conf);\nChecksumFileSystem\nLocalFileSystem uses ChecksumFileSystem to do its work, and this class makes it easy\nto add checksumming to other (nonchecksummed) filesystems, as ChecksumFileSys\ntem is just a wrapper around FileSystem. The general idiom is as follows:\nFileSystem rawFs = ...\nFileSystem checksummedFs = new ChecksumFileSystem(rawFs);\nThe underlying filesystem is called the raw filesystem, and may be retrieved using the\ngetRawFileSystem() method on ChecksumFileSystem. ChecksumFileSystem has a few\nmore useful methods for working with checksums, such as getChecksumFile() for get-\nting the path of a checksum file for any file. Check the documentation for the others.\nIf an error is detected by ChecksumFileSystem when reading a file, it will call its\nreportChecksumFailure() method. The default implementation does nothing, but\nLocalFileSystem moves the offending file and its checksum to a side directory on the\nsame device called bad_files. Administrators should periodically check for these bad\nfiles and take action on them.\nCompression\nFile compression brings two major benefits: it reduces the space needed to store files,\nand it speeds up data transfer across the network, or to or from disk. When dealing\nwith large volumes of data, both of these savings can be significant, so it pays to carefully\nconsider how to use compression in Hadoop.\nCompression | 77There are many different compression formats, tools and algorithms, each with differ-\nent characteristics. Table 4-1 lists some of the more common ones that can be used\nwith Hadoop.*\nTable 4-1. A summary of compression formats\nCompression format Tool Algorithm Filename extension Multiple files Splittable\nDEFLATEa N/A DEFLATE .deflate No No\ngzip gzip DEFLATE .gz No No\nZIP zip DEFLATE .zip Yes Yes, at file boundaries\nbzip2 bzip2 bzip2 .bz2 No Yes\nLZO lzop LZO .lzo No No\na DEFLATE is a compression algorithm whose standard implementation is zlib. There is no commonly\navailable command-line tool for producing files in DEFLATE format, as gzip is normally used. (Note that\nthe gzip file format is DEFLATE with extra headers and a footer.) The .deflate filename extension is a\nHadoop convention.\nAll compression algorithms exhibit a space/time trade-off: faster compression and de-\ncompression speeds usually come at the expense of smaller space savings. All of the\ntools listed in Table 4-1 give some control over this trade-off at compression time by\noffering nine different options: –1 means optimize for speed and -9 means optimize for\nspace. For example, the following command creates a compressed file file.gz using the\nfastest compression method:\ngzip -1 file\nThe different tools have very different compression characteristics. Both gzip and ZIP\nare general-purpose compressors, and sit in the middle of the space/time trade-off.\nBzip2 compresses more effectively than gzip or ZIP, but is slower. Bzip2’s decompres-\nsion speed is faster than its compression speed, but it is still slower than the other\nformats. LZO, on the other hand, optimizes for speed: it is faster than gzip or ZIP (or\nany other compression or decompression tool†), but compresses slightly less effectively.\nThe “Splittable” column in Table 4-1 indicates whether the compression format sup-\nports splitting; that is, whether you can seek to any point in the stream and start reading\nfrom some point further on. Splittable compression formats are especially suitable for\nMapReduce; see “Compression and Input Splits” on page 83 for further discussion.\n* At the time of this writing, Hadoop’s ZIP integration is incomplete. See https://issues.apache.org/jira/browse/\nHADOOP-1824.\n† Jeff Gilchrist’s Archive Comparison Test at http://compression.ca/act/act-summary.html contains benchmarks\nfor compression and decompression speed, and compression ratio for a wide range of tools.\n78 | Chapter 4: Hadoop I/OCodecs\nA codec is the implementation of a compression-decompression algorithm. In Hadoop,\na codec is represented by an implementation of the CompressionCodec interface. So, for\nexample, GzipCodec encapsulates the compression and decompression algorithm for\ngzip. Table 4-2 lists the codecs that are available for Hadoop.\nTable 4-2. Hadoop compression codecs\nCompression format Hadoop CompressionCodec\nDEFLATE org.apache.hadoop.io.compress.DefaultCodec\ngzip org.apache.hadoop.io.compress.GzipCodec\nbzip2 org.apache.hadoop.io.compress.BZip2Codec\nLZO com.hadoop.compression.lzo.LzopCodec\nThe LZO libraries are GPL-licensed and may not be included in Apache distributions,\nso for this reason the Hadoop codecs must be downloaded separately from http://code\n.google.com/p/hadoop-gpl-compression/. The LzopCodec is compatible with the lzop tool,\nwhich is essentially the LZO format with extra headers, and is the one you normally\nwant. There is also a LzoCodec for the pure LZO format, which uses the .lzo_deflate\nfilename extension (by analogy with DEFLATE, which is gzip without the headers).\nCompressing and decompressing streams with CompressionCodec\nCompressionCodec has two methods that allow you to easily compress or decompress\ndata. To compress data being written to an output stream, use the createOutput\nStream(OutputStream out) method to create a CompressionOutputStream to which you\nwrite your uncompressed data to have it written in compressed form to the underlying\nstream. Conversely, to decompress data being read from an input stream, call\ncreateInputStream(InputStream in) to obtain a CompressionInputStream, which allows\nyou to read uncompressed data from the underlying stream.\nCompressionOutputStream\nand\nCompressionInputStream\nare\nsimilar\nto\njava.util.zip.DeflaterOutputStream and java.util.zip.DeflaterInputStream, except\nthat both of the former provide the ability to reset their underlying compressor or de-\ncompressor, which is important for applications that compress sections of the data\nstream as separate blocks, such as SequenceFile, described in “Sequence-\nFile” on page 103.\nExample 4-1 illustrates how to use the API to compress data read from standard input\nand write it to standard output.\nExample 4-1. A program to compress data read from standard input and write it to standard output\npublic class StreamCompressor {\npublic static void main(String[] args) throws Exception {\nCompression | 79String codecClassname = args[0];\nClass<?> codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\n}\n}\nCompressionOutputStream out = codec.createOutputStream(System.out);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\nThe application expects the fully qualified name of the CompressionCodec implementa-\ntion as the first command-line argument. We use ReflectionUtils to construct a new\ninstance of the codec, then obtain a compression wrapper around System.out. Then we\ncall the utility method copyBytes() on IOUtils to copy the input to the output, which\nis compressed by the CompressionOutputStream. Finally, we call finish() on\nCompressionOutputStream, which tells the compressor to finish writing to the com-\npressed stream, but doesn’t close the stream. We can try it out with the following\ncommand line, which compresses the string “Text” using the StreamCompressor pro-\ngram with the GzipCodec, then decompresses it from standard input using gunzip:\n% echo ""Text"" | hadoop StreamCompressor org.apache.hadoop.io.compress.GzipCodec \\\n| gunzip -\nText\nInferring CompressionCodecs using CompressionCodecFactory\nIf you are reading a compressed file, you can normally infer the codec to use by looking\nat its filename extension. A file ending in .gz can be read with GzipCodec, and so on.\nThe extension for each compression format is listed in Table 4-1.\nCompressionCodecFactory provides a way of mapping a filename extension to a\nCompressionCodec using its getCodec() method, which takes a Path object for the file in\nquestion. Example 4-2 shows an application that uses this feature to decompress files.\nExample 4-2. A program to decompress a compressed file using a codec inferred from the file's\nextension\npublic class FileDecompressor {\npublic static void main(String[] args) throws Exception {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath inputPath = new Path(uri);\nCompressionCodecFactory factory = new CompressionCodecFactory(conf);\nCompressionCodec codec = factory.getCodec(inputPath);\nif (codec == null) {\nSystem.err.println(""No codec found for "" + uri);\nSystem.exit(1);\n80 | Chapter 4: Hadoop I/O}\nString outputUri =\nCompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension());\n}\n}\nInputStream in = null;\nOutputStream out = null;\ntry {\nin = codec.createInputStream(fs.open(inputPath));\nout = fs.create(new Path(outputUri));\nIOUtils.copyBytes(in, out, conf);\n} finally {\nIOUtils.closeStream(in);\nIOUtils.closeStream(out);\n}\nOnce the codec has been found, it is used to strip off the file suffix to form the output\nfilename (via the removeSuffix() static method of CompressionCodecFactory). In this\nway, a file named file.gz is decompressed to file by invoking the program as follows:\n% hadoop FileDecompressor file.gz\nCompressionCodecFactory\nfinds codecs from a list defined by the\nio.compression.codecs configuration property. By default, this lists all the codecs pro-\nvided by Hadoop (see Table 4-3), so you would need to alter it only if you have a custom\ncodec that you wish to register (such as the externally hosted LZO codecs). Each codec\nknows its default filename extension, thus permitting CompressionCodecFactory to\nsearch through the registered codecs to find a match for a given extension (if any).\nTable 4-3. Compression codec properties\nProperty name Type Default value Description\nio.compression.codecs comma-separated org.apache.hadoop.io. A list of the Compres\n                      Class names compress.DefaultCodec, sionCodec classes for\n                                  org.apache.hadoop.io. compression/\n                                  compress.GzipCodec, decompression.\n                                  org.apache.hadoop.io. \n                                  compress.Bzip2Codec \nNative libraries\nFor performance, it is preferable to use a native library for compression and\ndecompression. For example, in one test, using the native gzip libraries reduced de-\ncompression times by up to 50% and compression times by around 10% (compared to\nthe built-in Java implementation). Table 4-4 shows the availability of Java and native\nimplementations for each compression format. Not all formats have native implemen-\ntations (bzip2, for example), whereas others are only available as a native implemen-\ntation (LZO, for example).\nCompression | 81Table 4-4. Compression library implementations\nCompression format Java implementation Native implementation\nDEFLATE Yes Yes\ngzip Yes Yes\nbzip2 Yes No\nLZO No Yes\nHadoop comes with prebuilt native compression libraries for 32- and 64-bit Linux,\nwhich you can find in the lib/native directory. For other platforms, you will need to\ncompile the libraries yourself, following the instructions on the Hadoop wiki at http://\nwiki.apache.org/hadoop/NativeHadoop.\nThe native libraries are picked up using the Java system property java.library.path.\nThe hadoop script in the bin directory sets this property for you, but if you don’t use\nthis script, you will need to set the property in your application.\nBy default Hadoop looks for native libraries for the platform it is running on, and loads\nthem automatically if they are found. This means you don’t have to change any con-\nfiguration settings to use the native libraries. In some circumstances, however, you may\nwish to disable use of native libraries, such as when you are debugging a compression-\nrelated problem. You can achieve this by setting the property hadoop.native.lib to\nfalse, which ensures that the built-in Java equivalents will be used (if they are available).\nCodecPool. If you are using a native library and you are doing a lot of compression or\ndecompression in your application, consider using CodecPool, which allows you to re-\nuse compressors and decompressors, thereby amortizing the cost of creating these\nobjects.\nThe code in Example 4-3 shows the API, although in this program, which only creates\na single Compressor, there is really no need to use a pool.\nExample 4-3. A program to compress data read from standard input and write it to standard output\nusing a pooled compressor\npublic class PooledStreamCompressor {\npublic static void main(String[] args) throws Exception {\nString codecClassname = args[0];\nClass<?> codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\nCompressor compressor = null;\ntry {\ncompressor = CodecPool.getCompressor(codec);\nCompressionOutputStream out =\ncodec.createOutputStream(System.out, compressor);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\n82 | Chapter 4: Hadoop I/O}\n}\n} finally {\nCodecPool.returnCompressor(compressor);\n}\nWe retrieve a Compressor instance from the pool for a given CompressionCodec, which\nwe use in the codec’s overloaded createOutputStream() method. By using a finally\nblock, we ensure that the compressor is returned to the pool even if there is an\nIOException while copying the bytes between the streams.\nCompression and Input Splits\nWhen considering how to compress data that will be processed by MapReduce, it is\nimportant to understand whether the compression format supports splitting. Consider\nan uncompressed file stored in HDFS whose size is 1 GB. With a HDFS block size of\n64 MB, the file will be stored as 16 blocks, and a MapReduce job using this file as input\nwill create 16 input splits, each processed independently as input to a separate map task.\nImagine now the file is a gzip-compressed file whose compressed size is 1 GB. As before,\nHDFS will store the file as 16 blocks. However, creating a split for each block won’t\nwork since it is impossible to start reading at an arbitrary point in the gzip stream, and\ntherefore impossible for a map task to read its split independently of the others. The\ngzip format uses DEFLATE to store the compressed data, and DEFLATE stores data\nas a series of compressed blocks. The problem is that the start of each block is not\ndistinguished in any way that would allow a reader positioned at an arbitrary point in\nthe stream to advance to the beginning of the next block, thereby synchronizing itself\nwith the stream. For this reason, gzip does not support splitting.\nIn this case, MapReduce will do the right thing, and not try to split the gzipped file,\nsince it knows that the input is gzip-compressed (by looking at the filename extension)\nand that gzip does not support splitting. This will work, but at the expense of locality:\na single map will process the 16 HDFS blocks, most of which will not be local to the\nmap. Also, with fewer maps, the job is less granular, and so may take longer to run.\nIf the file in our hypothetical example were an LZO file, we would have the same\nproblem since the underlying compression format does not provide a way for a reader\nto synchronize itself with the stream. A bzip2 file, however, does provide a synchroni-\nzation marker between blocks (a 48-bit approximation of pi), so it does support split-\nting. (Table 4-1 lists whether each compression format supports splitting.)\nFor collections of files, the issues are slightly different. ZIP is an archive format, so it\ncan combine multiple files into a single ZIP archive. Each file is compressed separately,\nand the locations of all the files in the archive are stored in a central directory at the\nend of the ZIP file. This property means that ZIP files support splitting at file bounda-\nries, with each split containing one or more files from the ZIP archive. At the time of\nthis writing, however, Hadoop does not support ZIP files as an input format.\nCompression | 83Which Compression Format Should I Use?\nWhich compression format you should use depends on your application. Do you want\nto maximize the speed of your application or are you more concerned about keeping\nstorage costs down? In general, you should try different strategies for your application,\nand benchmark them with representative datasets to find the best approach.\nFor large, unbounded files, like logfiles, the options are:\n• Store the files uncompressed.\n• Use a compression format that supports splitting, like bzip2.\n• Split the file into chunks in the application and compress each chunk separately\nusing any supported compression format (it doesn’t matter whether it is splittable).\nIn this case, you should choose the chunk size so that the compressed chunks are\napproximately the size of an HDFS block.\n• Use Sequence File, which supports compression and splitting. See “Sequence-\nFile” on page 103.\nFor large files, you should not use a compression format that does not support splitting\non the whole file, since you lose locality and make MapReduce applications very\ninefficient.\nFor archival purposes, consider the Hadoop archive format (see “Hadoop Ar-\nchives” on page 71), although it does not support compression.\nUsing Compression in MapReduce\nAs described in “Inferring CompressionCodecs using CompressionCodecFac-\ntory” on page 80, if your input files are compressed, they will be automatically decom-\npressed as they are read by MapReduce, using the filename extension to determine the\ncodec to use.\nTo compress the output of a MapReduce job, in the job configuration, set the\nmapred.output.compress property to true, and the mapred.output.compression.codec\nproperty to the classname of the compression codec you want to use, as shown in\nExample 4-4.\nExample 4-4. Application to run the maximum temperature job producing compressed output\npublic class MaxTemperatureWithCompression {\npublic static void main(String[] args) throws IOException {\nif (args.length != 2) {\nSystem.err.println(""Usage: MaxTemperatureWithCompression <input path> "" +\n""<output path>"");\nSystem.exit(-1);\n}\nJobConf conf = new JobConf(MaxTemperatureWithCompression.class);\n84 | Chapter 4: Hadoop I/Oconf.setJobName(""Max temperature with output compression"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setBoolean(""mapred.output.compress"", true);\nconf.setClass(""mapred.output.compression.codec"", GzipCodec.class,\nCompressionCodec.class);\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\n}\nJobClient.runJob(conf);\nWe run the program over compressed input (which doesn’t have to use the same com-\npression format as the output, although it does in this example) as follows:\n% hadoop MaxTemperatureWithCompression input/ncdc/sample.txt.gz output\nEach part of the final output is compressed; in this case, there is a single part:\n% gunzip -c output/part-00000.gz\n1949\n111\n1950\n22\nIf you are emitting sequence files for your output, then you can set the mapred.out\nput.compression.type property to control the type of compression to use. The default\nis RECORD, which compresses individual records. Changing this to BLOCK, which\ncompresses groups of records, is recommended since it compresses better (see “The\nSequenceFile Format” on page 109).\nCompressing map output\nEven if your MapReduce application reads and writes uncompressed data, it may ben-\nefit from compressing the intermediate output of the map phase. Since the map output\nis written to disk and transferred across the network to the reducer nodes, by using a\nfast compressor such as LZO, you can get performance gains simply because the volume\nof data to transfer is reduced. The configuration properties to enable compression for\nmap outputs and to set the compression format are shown in Table 4-5.\nCompression | 85Table 4-5. Map output compression properties\nProperty name Type Default value Description\nmapred.compress.map.output boolean false Compress map outputs.\nmapred.map.output. Class org.apache.hadoop.io. The compression codec to use for\ncompression.codec compress.DefaultCodec map outputs.\nHere are the lines to add to enable gzip map output compression in your job:\nconf.setCompressMapOutput(true);\nconf.setMapOutputCompressorClass(GzipCodec.class);\nSerialization\nSerialization is the process of turning structured objects into a byte stream for trans-\nmission over a network or for writing to persistent storage. Deserialization is the\nprocess of turning a byte stream back into a series of structured objects.\nSerialization appears in two quite distinct areas of distributed data processing: for\ninterprocess communication and for persistent storage.\nIn Hadoop, interprocess communication between nodes in the system is implemented\nusing remote procedure calls (RPCs). The RPC protocol uses serialization to render the\nmessage into a binary stream to be sent to the remote node, which then deserializes the\nbinary stream into the original message. In general, it is desirable that an RPC seriali-\nzation format is:\nCompact\nA compact format makes the best use of network bandwidth, which is the most\nscarce resource in a data center.\nFast\nInterprocess communication forms the backbone for a distributed system, so it is\nessential that there is as little performance overhead as possible for the serialization\nand deserialization process.\nExtensible\nProtocols change over time to meet new requirements, so it should be\nstraightforward to evolve the protocol in a controlled manner for clients and serv-\ners. For example, it should be possible to add a new argument to a method call,\nand have the new servers accept messages in the old format (without the new ar-\ngument) from old clients.\nInteroperable\nFor some systems, it is desirable to be able to support clients that are written in\ndifferent languages to the server, so the format needs to be designed to make this\npossible.\n86 | Chapter 4: Hadoop I/OOn the face of it, the data format chosen for persistent storage would have different\nrequirements from a serialization framework. After all, the lifespan of an RPC is less\nthan a second, whereas persistent data may be read years after it was written. As it turns\nout, the four desirable properties of an RPC’s serialization format are also crucial for a\npersistent storage format. We want the storage format to be compact (to make efficient\nuse of storage space), fast (so the overhead in reading or writing terabytes of data is\nminimal), extensible (so we can transparently read data written in an older format),\nand interoperable (so we can read or write persistent data using different languages).\nHadoop uses its own serialization format, Writables, which is certainly compact and\nfast (but not so easy to extend, or use from languages other than Java). Since Writables\nare central to Hadoop (MapReduce programs use them for their key and value types),\nwe look at them in some depth in the next section, before briefly turning to other well-\nknown serialization frameworks, like Apache Thrift and Google Protocol Buffers.\nThe Writable Interface\nThe Writable interface defines two methods: one for writing its state to a DataOutput\nbinary stream, and one for reading its state from a DataInput binary stream:\npackage org.apache.hadoop.io;\nimport java.io.DataOutput;\nimport java.io.DataInput;\nimport java.io.IOException;\npublic interface Writable {\nvoid write(DataOutput out) throws IOException;\nvoid readFields(DataInput in) throws IOException;\n}\nLet’s look at a particular Writable to see what we can do with it. We will use\nIntWritable, a wrapper for a Java int. We can create one and set its value using the\nset() method:\nIntWritable writable = new IntWritable();\nwritable.set(163);\nEquivalently, we can use the constructor that takes the integer value:\nIntWritable writable = new IntWritable(163);\nTo examine the serialized form of the IntWritable, we write a small helper method that\nwraps a java.io.ByteArrayOutputStream in a java.io.DataOutputStream (an implemen-\ntation of java.io.DataOutput) to capture the bytes in the serialized stream:\npublic static byte[] serialize(Writable writable) throws IOException {\nByteArrayOutputStream out = new ByteArrayOutputStream();\nDataOutputStream dataOut = new DataOutputStream(out);\nwritable.write(dataOut);\ndataOut.close();\nSerialization | 87}\nreturn out.toByteArray();\nAn integer is written using four bytes (as we see using JUnit 4 assertions):\nbyte[] bytes = serialize(writable);\nassertThat(bytes.length, is(4));\nThe bytes are written in big-endian order (so the most significant byte is written to the\nstream first, this is dictated by the java.io.DataOutput interface), and we can see their\nhexadecimal representation by using a method on Hadoop’s StringUtils:\nassertThat(StringUtils.byteToHexString(bytes), is(""000000a3""));\nLet’s try deserialization. Again, we create a helper method to read a Writable object\nfrom a byte array:\npublic static byte[] deserialize(Writable writable, byte[] bytes)\nthrows IOException {\nByteArrayInputStream in = new ByteArrayInputStream(bytes);\nDataInputStream dataIn = new DataInputStream(in);\nwritable.readFields(dataIn);\ndataIn.close();\nreturn bytes;\n}\nWe construct a new, value-less, IntWritable, then call deserialize() to read from the\noutput data that we just wrote. Then we check that its value, retrieved using the\nget() method, is the original value, 163:\nIntWritable newWritable = new IntWritable();\ndeserialize(newWritable, bytes);\nassertThat(newWritable.get(), is(163));\nWritableComparable and comparators\nIntWritable implements the WritableComparable interface, which is just a subinterface\nof the Writable and java.lang.Comparable interfaces:\npackage org.apache.hadoop.io;\npublic interface WritableComparable<T> extends Writable, Comparable<T> {\n}\nComparison of types is crucial for MapReduce, where there is a sorting phase during\nwhich keys are compared with one another. One optimization that Hadoop provides\nis the RawComparator extension of Java’s Comparator:\npackage org.apache.hadoop.io;\nimport java.util.Comparator;\npublic interface RawComparator<T> extends Comparator<T> {\npublic int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);\n88 | Chapter 4: Hadoop I/O}\nThis interface permits implementors to compare records read from a stream without\ndeserializing them into objects, thereby avoiding any overhead of object creation. For\nexample, the comparator for IntWritables implements the raw compare() method by\nreading an integer from each of the byte arrays b1 and b2 and comparing them directly,\nfrom the given start positions (s1 and s2) and lengths (l1 and l2).\nWritableComparator is a general-purpose implementation of RawComparator for\nWritableComparable classes. It provides two main functions. First, it provides a default\nimplementation of the raw compare() method that deserializes the objects to be com-\npared from the stream and invokes the object compare() method. Second, it acts as a\nfactory for RawComparator instances (that Writable implementations have registered).\nFor example, to obtain a comparator for IntWritable, we just use:\nRawComparator<IntWritable> comparator = WritableComparator.get(IntWritable.class);\nThe comparator can be used to compare two IntWritable objects:\nIntWritable w1 = new IntWritable(163);\nIntWritable w2 = new IntWritable(67);\nassertThat(comparator.compare(w1, w2), greaterThan(0));\nor their serialized representations:\nbyte[] b1 = serialize(w1);\nbyte[] b2 = serialize(w2);\nassertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length),\ngreaterThan(0));\nWritable Classes\nHadoop comes with a large selection of Writable classes in the org.apache.hadoop.io\npackage. They form the class hierarchy shown in Figure 4-1.\nWritable wrappers for Java primitives\nThere are Writable wrappers for all the Java primitive types (see Table 4-6) except\nshort and char (both of which can be stored in an IntWritable). All have a get() and\na set() method for retrieving and storing the wrapped value.\nTable 4-6. Writable wrapper classes for Java primitives\nJava primitive Writable implementation Serialized size (bytes)\nboolean BooleanWritable 1\nbyte ByteWritable 1\nint IntWritable 4\nVIntWritable 1–5\nfloat FloatWritable 4\nSerialization | 89Java primitive Writable implementation Serialized size (bytes)\nlong LongWritable 8\nVLongWritable 1–9\nDoubleWritable 8\ndouble\nFigure 4-1. Writable class hierarchy\nWhen it comes to encoding integers there is a choice between the fixed-length formats\n(IntWritable and LongWritable) and the variable-length formats (VIntWritable and\nVLongWritable). The variable-length formats use only a single byte to encode the value\nif it is small enough (between –112 and 127, inclusive); otherwise, they use the first\nbyte to indicate whether the value is positive or negative, and how many bytes follow.\nFor example, 163 requires two bytes:\n90 | Chapter 4: Hadoop I/Obyte[] data = serialize(new VIntWritable(163));\nassertThat(StringUtils.byteToHexString(data), is(""8fa3""));\nHow do you choose between a fixed-length and a variable-length encoding? Fixed-\nlength encodings are good when the distribution of values is fairly uniform across the\nwhole value space, such as a (well-designed) hash function. Most numeric variables\ntend to have nonuniform distributions, and on average the variable-length encoding\nwill save space. Another advantage of variable-length encodings is that you can switch\nfrom VIntWritable to VLongWritable, since their encodings are actually the same. So by\nchoosing a variable-length representation, you have room to grow without committing\nto an 8-byte long representation from the beginning.\nText\nText is a Writable for UTF-8 sequences. It can be thought of as the Writable equivalent\nof java.lang.String. Text is a replacement for the UTF8 class, which was deprecated\nbecause it didn’t support strings whose encoding was over 32,767 bytes, and because\nit used Java’s modified UTF-8.\nThe Text class uses an int (with a variable-length encoding) to store the number of\nbytes in the string encoding, so the maximum value is 2 GB. Furthermore, Text uses\nstandard UTF-8, which makes it potentially easier to interoperate with other tools that\nunderstand UTF-8.\nIndexing. Because of its emphasis on using standard UTF-8, there are some differences\nbetween Text and the Java String class. Indexing for the Text class is in terms of position\nin the encoded byte sequence, not the Unicode character in the string, or the Java\nchar code unit (as it is for String). For ASCII strings, these three concepts of index\nposition coincide. Here is an example to demonstrate the use of the charAt() method:\nText t = new Text(""hadoop"");\nassertThat(t.getLength(), is(6));\nassertThat(t.getBytes().length, is(6));\nassertThat(t.charAt(2), is((int) 'd'));\nassertThat(""Out of bounds"", t.charAt(100), is(-1));\nNotice that charAt() returns an int representing a Unicode code point, unlike the\nString variant that returns a char. Text also has a find() method, which is analogous\nto String’s indexOf():\nText t = new Text(""hadoop"");\nassertThat(""Find a substring"", t.find(""do""), is(2));\nassertThat(""Finds first 'o'"", t.find(""o""), is(3));\nassertThat(""Finds 'o' from position 4 or later"", t.find(""o"", 4), is(4));\nassertThat(""No match"", t.find(""pig""), is(-1));\nSerialization | 91Unicode. When we start using characters that are encoded with more than a single byte,\nthe differences between Text and String become clear. Consider the Unicode characters\nshown in Table 4-7.‡\nTable 4-7. Unicode characters\nUnicode code point U+0041 U+00DF U+6771 U+10400\nName LATIN CAPITAL LATIN SMALL LET- N/A (a unified Han DESERET CAPITAL LETTER LONG I\n     LETTER A TER SHARP S ideograph) \nUTF-8 code units 41 c3 9f e6 9d b1 f0 90 90 80\nJava representation \\u0041 \\u00DF \\u6771 \\uuD801\\uDC00\nAll but the last character in the table, U+10400, can be expressed using a single Java\nchar. U+10400 is a supplementary character and is represented by two Java chars,\nknown as a surrogate pair. The tests in Example 4-5 show the differences between\nString and Text when processing a string of the four characters from Table 4-7.\nExample 4-5. Tests showing the differences between the String and Text classes\npublic class StringTextComparisonTest {\n@Test\npublic void string() throws UnsupportedEncodingException {\nString s = ""\\u0041\\u00DF\\u6771\\uD801\\uDC00"";\nassertThat(s.length(), is(5));\nassertThat(s.getBytes(""UTF-8"").length, is(10));\nassertThat(s.indexOf(""\\u0041""), is(0));\nassertThat(s.indexOf(""\\u00DF""), is(1));\nassertThat(s.indexOf(""\\u6771""), is(2));\nassertThat(s.indexOf(""\\uD801\\uDC00""), is(3));\nassertThat(s.charAt(0),\nassertThat(s.charAt(1),\nassertThat(s.charAt(2),\nassertThat(s.charAt(3),\nassertThat(s.charAt(4),\n}\nis('\\u0041'));\nis('\\u00DF'));\nis('\\u6771'));\nis('\\uD801'));\nis('\\uDC00'));\nassertThat(s.codePointAt(0),\nassertThat(s.codePointAt(1),\nassertThat(s.codePointAt(2),\nassertThat(s.codePointAt(3),\nis(0x0041));\nis(0x00DF));\nis(0x6771));\nis(0x10400));\n@Test\npublic void text() {\nText t = new Text(""\\u0041\\u00DF\\u6771\\uD801\\uDC00"");\n‡ This example is based on one from the article Supplementary Characters in the Java Platform.\n92 | Chapter 4: Hadoop I/OassertThat(t.getLength(), is(10));\nassertThat(t.find(""\\u0041""), is(0));\nassertThat(t.find(""\\u00DF""), is(1));\nassertThat(t.find(""\\u6771""), is(3));\nassertThat(t.find(""\\uD801\\uDC00""), is(6));\n}\n}\nassertThat(t.charAt(0),\nassertThat(t.charAt(1),\nassertThat(t.charAt(3),\nassertThat(t.charAt(6),\nis(0x0041));\nis(0x00DF));\nis(0x6771));\nis(0x10400));\nThe test confirms that the length of a String is the number of char code units it contains\n(5, one from each of the first three characters in the string, and a surrogate pair from\nthe last), whereas the length of a Text object is the number of bytes in its UTF-8 encoding\n(10 = 1+2+3+4). Similarly, the indexOf() method in String returns an index in char\ncode units, and find() for Text is a byte offset.\nThe charAt() method in String returns the char code unit for the given index, which\nin the case of a surrogate pair will not represent a whole Unicode character. The code\nPointAt() method, indexed by char code unit, is needed to retrieve a single Unicode\ncharacter represented as an int. In fact, the charAt() method in Text is more like the\ncodePointAt() method than its namesake in String. The only difference is that it is\nindexed by byte offset.\nIteration. Iterating over the Unicode characters in Text is complicated by the use of byte\noffsets for indexing, since you can’t just increment the index. The idiom for iteration\nis a little obscure (see Example 4-6): turn the Text object into a java.nio.ByteBuffer,\nthen repeatedly call the bytesToCodePoint() static method on Text with the buffer. This\nmethod extracts the next code point as an int and updates the position in the buffer.\nThe end of the string is detected when bytesToCodePoint() returns –1.\nExample 4-6. Iterating over the characters in a Text object\npublic class TextIterator {\npublic static void main(String[] args) {\nText t = new Text(""\\u0041\\u00DF\\u6771\\uD801\\uDC00"");\n}\n}\nByteBuffer buf = ByteBuffer.wrap(t.getBytes(), 0, t.getLength());\nint cp;\nwhile (buf.hasRemaining() && (cp = Text.bytesToCodePoint(buf)) != -1) {\nSystem.out.println(Integer.toHexString(cp));\n}\nRunning the program prints the code points for the four characters in the string:\nSerialization | 93% hadoop TextIterator\n41\ndf\n6771\n10400\nMutability. Another difference with String is that Text is mutable (like all Writable im-\nplementations in Hadoop, except NullWritable, which is a singleton). You can reuse a\nText instance by calling one of the set() methods on it. For example:\nText t = new Text(""hadoop"");\nt.set(""pig"");\nassertThat(t.getLength(), is(3));\nassertThat(t.getBytes().length, is(3));\nIn some situations, the byte array returned by the getBytes() method\nmay be longer than the length returned by getLength():\nText t = new Text(""hadoop"");\nt.set(new Text(""pig""));\nassertThat(t.getLength(), is(3));\nassertThat(""Byte length not shortened"", t.getBytes().length, is(6));\nThis shows why it is imperative that you always call getLength() when\ncalling getBytes(), so you know how much of the byte array is valid data.\nResorting to String. Text doesn’t have as rich an API for manipulating strings as\njava.lang.String, so in many cases, you need to convert the Text object to a String.\nThis is done in the usual way, using the toString() method.\nassertThat(new Text(""hadoop"").toString(), is(""hadoop""));\nBytesWritable\nBytesWritable is a wrapper for an array of binary data. Its serialized format is an integer\nfield (4 bytes) that specifies the number of bytes to follow, followed by the bytes them-\nselves. For example, the byte array of length two with values 3 and 5 is serialized as a\n4-byte integer (00000002) followed by the two bytes from the array (03 and 05):\nBytesWritable b = new BytesWritable(new byte[] { 3, 5 });\nbyte[] bytes = serialize(b);\nassertThat(StringUtils.byteToHexString(bytes), is(""000000020305""));\nBytesWritable is mutable, and its value may be changed by calling its set() method.\nAs with Text, the size of the byte array returned from the getBytes() method for Byte\nsWritable—the capacity—may not reflect the actual size of the data stored in the\nBytesWritable. You can determine the size of the BytesWritable by calling get\nLength(). To demonstrate:\nb.setCapacity(11);\nassertThat(b.getLength(), is(2));\nassertThat(b.getBytes().length, is(11));\n94 | Chapter 4: Hadoop I/ONullWritable\nNullWritable is a special type of Writable, as it has a zero-length serialization. No bytes\nare written to, or read from, the stream. It is used as a placeholder; for example, in\nMapReduce, a key or a value can be declared as a NullWritable when you don’t need\nto use that position—it effectively stores a constant empty value. NullWritable can also\nbe useful as a key in SequenceFile when you want to store a list of values, as opposed\nto key-value pairs. It is an immutable singleton: the instance can be retrieved by calling\nNullWritable.get().\nObjectWritable and GenericWritable\nObjectWritable is a general-purpose wrapper for the following: Java primitives, String,\nenum, Writable, null, or arrays of any of these types. It is used in Hadoop RPC to marshal\nand unmarshal method arguments and return types.\nObjectWritable is useful when a field can be of more than one type: for example, if the\nvalues in a SequenceFile have multiple types, then you can declare the value type as an\nObjectWritable and wrap each type in an ObjectWritable. Being a general-purpose\nmechanism, it’s fairly wasteful of space since it writes the classname of the wrapped\ntype every time it is serialized. In cases where the number of types is small and known\nahead of time, this can be improved by having a static array of types, and using the\nindex into the array as the serialized reference to the type. This is the approach that\nGenericWritable takes, and you have to subclass it to specify the types to support.\nWritable collections\nThere are four Writable collection types in the org.apache.hadoop.io package: Array\nWritable, TwoDArrayWritable, MapWritable, and SortedMapWritable.\nArrayWritable and TwoDArrayWritable are Writable implementations for arrays and\ntwo-dimensional arrays (array of arrays) of Writable instances. All the elements of an\nArrayWritable or a TwoDArrayWritable must be instances of the same class, which is\nspecified at construction, as follows:\nArrayWritable writable = new ArrayWritable(Text.class);\nIn contexts where the Writable is defined by type, such as in SequenceFile keys or\nvalues, or as input to MapReduce in general, you need to subclass ArrayWritable (or\nTwoDArrayWritable, as appropriate) to set the type statically. For example:\npublic class TextArrayWritable extends ArrayWritable {\npublic TextArrayWritable() {\nsuper(Text.class);\n}\n}\nArrayWritable and TwoDArrayWritable both have get() and set() methods, as well as a\ntoArray() method, which creates a shallow copy of the array (or 2D array).\nSerialization | 95MapWritable and SortedMapWritable are implementations of java.util.Map<Writable,\nWritable> and java.util.SortedMap<WritableComparable, Writable>, respectively. The\ntype of each key and value field is a part of the serialization format for that field. The\ntype is stored as a single byte that acts as an index into an array of types. The array is\npopulated with the standard types in the org.apache.hadoop.io package, but custom\nWritable types are accommodated, too, by writing a header that encodes the type array\nfor nonstandard types. As they are implemented, MapWritable and SortedMapWritable\nuse positive byte values for custom types, so a maximum of 127 distinct nonstandard\nWritable classes can be used in any particular MapWritable or SortedMapWritable in-\nstance. Here’s a demonstration of using a MapWritable with different types for keys and\nvalues:\nMapWritable src = new MapWritable();\nsrc.put(new IntWritable(1), new Text(""cat""));\nsrc.put(new VIntWritable(2), new LongWritable(163));\nMapWritable dest = new MapWritable();\nWritableUtils.cloneInto(dest, src);\nassertThat((Text) dest.get(new IntWritable(1)), is(new Text(""cat"")));\nassertThat((LongWritable) dest.get(new VIntWritable(2)), is(new LongWritable(163)));\nConspicuous by their absence are Writable collection implementations for sets and\nlists. A set can be emulated by using a MapWritable (or a SortedMapWritable for a sorted\nset), with NullWritable values. For lists of a single type of Writable, ArrayWritable is\nadequate, but to store different types of Writable in a single list, you can use\nGenericWritable to wrap the elements in an ArrayWritable. Alternatively, you could\nwrite a general ListWritable using the ideas from MapWritable.\nImplementing a Custom Writable\nHadoop comes with a useful set of Writable implementations that serve most purposes;\nhowever, on occasion, you may need to write your own custom implementation. With\na custom Writable, you have full control over the binary representation and the sort\norder. Because Writables are at the heart of the MapReduce data path, tuning the binary\nrepresentation can have a significant effect on performance. The stock Writable im-\nplementations that come with Hadoop are well-tuned, but for more elaborate struc-\ntures, it is often better to create a new Writable type, rather than compose the stock\ntypes.\nTo demonstrate how to create a custom Writable, we shall write an implementation\nthat represents a pair of strings, called TextPair. The basic implementation is shown\nin Example 4-7.\nExample 4-7. A Writable implementation that stores a pair of Text objects\nimport java.io.*;\nimport org.apache.hadoop.io.*;\n96 | Chapter 4: Hadoop I/Opublic class TextPair implements WritableComparable<TextPair> {\nprivate Text first;\nprivate Text second;\npublic TextPair() {\nset(new Text(), new Text());\n}\npublic TextPair(String first, String second) {\nset(new Text(first), new Text(second));\n}\npublic TextPair(Text first, Text second) {\nset(first, second);\n}\npublic void set(Text first, Text second) {\nthis.first = first;\nthis.second = second;\n}\npublic Text getFirst() {\nreturn first;\n}\npublic Text getSecond() {\nreturn second;\n}\n@Override\npublic void write(DataOutput out) throws IOException {\nfirst.write(out);\nsecond.write(out);\n}\n@Override\npublic void readFields(DataInput in) throws IOException {\nfirst.readFields(in);\nsecond.readFields(in);\n}\n@Override\npublic int hashCode() {\nreturn first.hashCode() * 163 + second.hashCode();\n}\n@Override\npublic boolean equals(Object o) {\nif (o instanceof TextPair) {\nTextPair tp = (TextPair) o;\nreturn first.equals(tp.first) && second.equals(tp.second);\n}\nreturn false;\n}\nSerialization | 97@Override\npublic String toString() {\nreturn first + ""\\t"" + second;\n}\n}\n@Override\npublic int compareTo(TextPair tp) {\nint cmp = first.compareTo(tp.first);\nif (cmp != 0) {\nreturn cmp;\n}\nreturn second.compareTo(tp.second);\n}\nThe first part of the implementation is straightforward: there are two Text instance\nvariables, first and second, and associated constructors, getters, and setters. All\nWritable implementations must have a default constructor so that the MapReduce\nframework can instantiate them, then populate their fields by calling readFields().\nWritable instances are mutable and often reused, so you should take care to avoid\nallocating objects in the write() or readFields() methods.\nTextPair’s write() method serializes each Text object in turn to the output stream, by\ndelegating to the Text objects themselves. Similarly, readFields() deserializes the bytes\nfrom the input stream by delegating to each Text object. The DataOutput and\nDataInput interfaces have a rich set of methods for serializing and deserializing Java\nprimitives, so, in general, you have complete control over the wire format of your\nWritable object.\nJust as you would for any value object you write in Java, you should override the\nhashCode(), equals(), and toString() methods from java.lang.Object. The hash\nCode() method is used by the HashPartitioner (the default partitioner in MapReduce)\nto choose a reduce partition, so you should make sure that you write a good hash\nfunction that mixes well to ensure reduce partitions are of a similar size.\nIf you ever plan to use your custom Writable with TextOutputFormat,\nthen you must implement its toString() method. TextOutputFormat calls\ntoString() on keys and values for their output representation. For Text\nPair, we write the underlying Text objects as strings separated by a tab\ncharacter.\nTextPair is an implementation of WritableComparable, so it provides an implementation\nof the compareTo() method that imposes the ordering you would expect: it sorts by the\nfirst string followed by the second. Notice that TextPair differs from TextArrayWrita\nble from the previous section (apart from the number of Text objects it can store), since\nTextArrayWritable is only a Writable, not a WritableComparable.\n98 | Chapter 4: Hadoop I/OImplementing a RawComparator for speed\nThe code for TextPair in Example 4-7 will work as it stands; however, there is a further\noptimization we can make. As explained in “WritableComparable and compara-\ntors” on page 88, when TextPair is being used as a key in MapReduce, it will have to\nbe deserialized into an object for the compareTo() method to be invoked. What if it were\npossible to compare two TextPair objects just by looking at their serialized\nrepresentations?\nIt turns out that we can do this, since TextPair is the concatenation of two Text objects,\nand the binary representation of a Text object is a variable-length integer containing\nthe number of bytes in the UTF-8 representation of the string, followed by the UTF-8\nbytes themselves. The trick is to read the initial length, so we know how long the first\nText object’s byte representation is; then we can delegate to Text’s RawComparator, and\ninvoke it with the appropriate offsets for the first or second string. Example 4-8 gives\nthe details (note that this code is nested in the TextPair class).\nExample 4-8. A RawComparator for comparing TextPair byte representations\npublic static class Comparator extends WritableComparator {\nprivate static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator();\npublic Comparator() {\nsuper(TextPair.class);\n}\n@Override\npublic int compare(byte[] b1, int s1, int l1,\nbyte[] b2, int s2, int l2) {\n}\n}\ntry {\nint firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1);\nint firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2);\nint cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2);\nif (cmp != 0) {\nreturn cmp;\n}\nreturn TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1,\nb2, s2 + firstL2, l2 - firstL2);\n} catch (IOException e) {\nthrow new IllegalArgumentException(e);\n}\nstatic {\nWritableComparator.define(TextPair.class, new Comparator());\n}\nWe actually subclass WritableComparator rather than implement RawComparator di-\nrectly, since it provides some convenience methods and default implementations. The\nSerialization | 99subtle part of this code is calculating firstL1 and firstL2, the lengths of the first\nText field in each byte stream. Each is made up of the length of the variable-length\ninteger (returned by decodeVIntSize() on WritableUtils), and the value it is encoding\n(returned by readVInt()).\nThe static block registers the raw comparator so that whenever MapReduce sees the\nTextPair class, it knows to use the raw comparator as its default comparator.\nCustom comparators\nAs we can see with TextPair, writing raw comparators takes some care, since you have\nto deal with details at the byte level. It is worth looking at some of the implementations\nof Writable in the org.apache.hadoop.io package for further ideas, if you need to write\nyour own. The utility methods on WritableUtils are very handy too.\nCustom comparators should also be written to be RawComparators, if possible. These\nare comparators that implement a different sort order to the natural sort order defined\nby the default comparator. Example 4-9 shows a comparator for TextPair, called First\nComparator, that considers only the first string of the pair. Note that we override the\ncompare() method that takes objects so both compare() methods have the same\nsemantics.\nWe will make use of this comparator in Chapter 8, when we look at joins and secondary\nsorting in MapReduce (see “Joins” on page 233).\nExample 4-9. A custom RawComparator for comparing the first field of TextPair byte representations\npublic static class FirstComparator extends WritableComparator {\nprivate static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator();\npublic FirstComparator() {\nsuper(TextPair.class);\n}\n@Override\npublic int compare(byte[] b1, int s1, int l1,\nbyte[] b2, int s2, int l2) {\n}\ntry {\nint firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1);\nint firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2);\nreturn TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2);\n} catch (IOException e) {\nthrow new IllegalArgumentException(e);\n}\n@Override\npublic int compare(WritableComparable a, WritableComparable b) {\nif (a instanceof TextPair && b instanceof TextPair) {\nreturn ((TextPair) a).first.compareTo(((TextPair) b).first);\n100 | Chapter 4: Hadoop I/O}\n}\n}\nreturn super.compare(a, b);\nSerialization Frameworks\nAlthough most MapReduce programs use Writable key and value types, this isn’t man-\ndated by the MapReduce API. In fact, any types can be used; the only requirement is\nthat there be a mechanism that translates to and from a binary representation of each\ntype.\nTo support this, Hadoop has an API for pluggable serialization frameworks. A seriali-\nzation framework is represented by an implementation of Serialization (in the\norg.apache.hadoop.io.serializer package). WritableSerialization, for example, is\nthe implementation of Serialization for Writable types.\nA Serialization defines a mapping from types to Serializer instances (for turning an\nobject into a byte stream) and Deserializer instances (for turning a byte stream into\nan object).\nSet the io.serializations property to a comma-separated list of classnames to register\nSerialization implementations. Its default value is org.apache.hadoop.io.serial\nizer.WritableSerialization, which means that only Writable objects can be serialized\nor deserialized out of the box.\nHadoop includes a class called JavaSerialization that uses Java Object Serialization.\nAlthough making it convenient to be able to use standard Java types in MapReduce\nprograms, like Integer or String, Java Object Serialization is not as efficient as Writa-\nbles, so it’s not worth making this trade-off (see the sidebar).\nWhy Not Use Java Object Serialization?\nJava comes with its own serialization mechanism, called Java Object Serialization (often\nreferred to simply as “Java Serialization”), that is tightly integrated with the language,\nso it’s natural to ask why this wasn’t used in Hadoop. Here’s what Doug Cutting said\nin response to that question:\nWhy didn’t I use Serialization when we first started Hadoop? Because it looked\nbig and hairy and I thought we needed something lean and mean, where we had\nprecise control over exactly how objects are written and read, since that is central\nto Hadoop. With Serialization you can get some control, but you have to fight for\nit.\nThe logic for not using RMI was similar. Effective, high-performance inter-process\ncommunications are critical to Hadoop. I felt like we’d need to precisely control\nhow things like connections, timeouts and buffers are handled, and RMI gives you\nlittle control over those.\nThe problem is that Java Serialization doesn’t meet the criteria for a serialization format\nlisted earlier: compact, fast, extensible, and interoperable.\nSerialization | 101Java Serialization is not compact: it writes the classname of each object being written\nto the stream—this is true of classes that implement java.io.Serializable or\njava.io.Externalizable. Subsequent instances of the same class write a reference han-\ndle to the first occurrence, which occupies only 5 bytes. However, reference handles\ndon’t work well with random access, since the referent class may occur at any point in\nthe preceding stream—that is, there is state stored in the stream. Even worse, reference\nhandles play havoc with sorting records in a serialized stream, since the first record of\na particular class is distinguished and must be treated as a special case.\nAll these problems are avoided by not writing the classname to the stream at all, which\nis the approach that Writable takes. This makes the assumption that the client knows\nthe expected type. The result is that the format is considerably more compact than Java\nSerialization, and random access and sorting work as expected since each record is\nindependent of the others (so there is no stream state).\nJava Serialization is a general-purpose mechanism for serializing graphs of objects, so\nit necessarily has some overhead for serialization and deserialization operations. What’s\nmore, the deserialization procedure creates a new instance for each object deserialized\nfrom the stream. Writable objects, on the other hand, can be (and often are) reused.\nFor example, for a MapReduce job, which at its core serializes and deserializes billions\nof records of just a handful of different types, the savings gained by not having to allocate\nnew objects are significant.\nIn terms of extensibility, Java Serialization has some support for evolving a type, but it\nis brittle and hard to use effectively (Writables have no support: the programmer has\nto manage them himself).\nIn principle, other languages could interpret the Java Serialization stream protocol (de-\nfined by the Java Object Serialization Specification), but in practice there are no widely\nused implementations in other languages, so it is a Java-only solution. The situation is\nthe same for Writables.\nSerialization IDL\nThere are a number of other serialization frameworks that approach the problem in a\ndifferent way: rather than defining types through code, you define them in a language-\nneutral, declarative fashion, using an interface description language (IDL). The system\ncan then generate types for different languages, which is good for interoperability. They\nalso typically define versioning schemes that make type evolution straightforward.\nHadoop’s own Record I/O (found in the org.apache.hadoop.record package) has an\nIDL that is compiled into Writable objects, which makes it convenient for generating\ntypes that are compatible with MapReduce. For whatever reason, however, Record\nI/O is not widely used.\nApache Thrift and Google Protocol Buffers are both popular serialization frameworks,\nand they are commonly used as a format for persistent binary data. There is limited\nsupport for these as MapReduce formats;§ however, Thrift is used in parts of Hadoop\n102 | Chapter 4: Hadoop I/Oto provide cross-language APIs, such as the “thriftfs” contrib module, where it is used\nto expose an API to Hadoop filesystems (see “Thrift” on page 49).\nFinally, Avro is a new (at the time of this writing) Hadoop subproject that defines a\nserialization format. The goal is to migrate Hadoop’s RPC mechanism to use Avro.\nAvro will also be suitable as a data format for large files.\nFile-Based Data Structures\nFor some applications, you need a specialized data structure to hold your data. For\ndoing MapReduce-based processing, putting each blob of binary data into its own file\ndoesn’t scale, so Hadoop developed a number of higher-level containers for these\nsituations.\nSequenceFile\nImagine a logfile, where each log record is a new line of text. If you want to log binary\ntypes, plain text isn’t a suitable format. Hadoop’s SequenceFile class fits the bill in this\nsituation, providing a persistent data structure for binary key-value pairs. To use it as\na logfile format, you would choose a key, such as timestamp represented by a LongWrit\nable, and the value is a Writable that represents the quantity being logged.\nSequenceFiles also work well as containers for smaller files. HDFS and MapReduce are\noptimized for large files, so packing files into a SequenceFile makes storing and pro-\ncessing the smaller files more efficient. (“Processing a whole file as a re-\ncord” on page 192 contains a program to pack files into a SequenceFile.‖)\nWriting a SequenceFile\nTo create a SequenceFile, use one of its createWriter() static methods, which returns\na SequenceFile.Writer instance. There are several overloaded versions, but they all\nrequire you to specify a stream to write to (either a FSDataOutputStream or a FileSys\ntem and Path pairing), a Configuration object, and the key and value types. Optional\narguments include the compression type and codec, a Progressable callback to be in-\nformed of write progress, and a Metadata instance to be stored in the SequenceFile\nheader.\nThe keys and values stored in a SequenceFile do not necessarily need to be Writable.\nAny types that can be serialized and deserialized by a Serialization may be used.\n§ You can find the latest status for a Thrift Serialization at https://issues.apache.org/jira/browse/HADOOP\n-3787, and a Protocol Buffers Serialization at https://issues.apache.org/jira/browse/HADOOP-3788.\n‖ In a similar vein, the blog post “A Million Little Files” by Stuart Sierra includes code for converting a tar file\ninto a SequenceFile http://stuartsierra.com/2008/04/24/a-million-little-files.\nFile-Based Data Structures | 103Once you have a SequenceFile.Writer, you then write key-value pairs, using the\nappend() method. Then when you’ve finished you call the close() method (Sequence\nFile.Writer implements java.io.Closeable).\nExample 4-10 shows a short program to write some key-value pairs to a Sequence\nFile, using the API just described.\nExample 4-10. Writing a SequenceFile\npublic class SequenceFileWriteDemo {\nprivate static final String[] DATA = {\n""One, two, buckle my shoe"",\n""Three, four, shut the door"",\n""Five, six, pick up sticks"",\n""Seven, eight, lay them straight"",\n""Nine, ten, a big fat hen""\n};\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath path = new Path(uri);\nIntWritable key = new IntWritable();\nText value = new Text();\nSequenceFile.Writer writer = null;\ntry {\nwriter = SequenceFile.createWriter(fs, conf, path,\nkey.getClass(), value.getClass());\n}\n}\nfor (int i = 0; i < 100; i++) {\nkey.set(100 - i);\nvalue.set(DATA[i % DATA.length]);\nSystem.out.printf(""[%s]\\t%s\\t%s\\n"", writer.getLength(), key, value);\nwriter.append(key, value);\n}\n} finally {\nIOUtils.closeStream(writer);\n}\nThe keys in the sequence file are integers counting down from 100 to 1, represented as\nIntWritable objects. The values are Text objects. Before each record is appended to the\nSequenceFile.Writer, we call the getLength() method to discover the current position\nin the file. (We will use this information about record boundaries in the next section\nwhen we read the file nonsequentially.) We write the position out to the console, along\nwith the key and value pairs. The result of running it is shown here:\n% hadoop SequenceFileWriteDemo numbers.seq\n[128]\n100\nOne, two, buckle my shoe\n[173]\n99\nThree, four, shut the door\n104 | Chapter 4: Hadoop I/O[220]\n[264]\n[314]\n[359]\n[404]\n[451]\n[495]\n[545]\n...\n[1976]\n[2021]\n[2088]\n[2132]\n[2182]\n...\n[4557]\n[4602]\n[4649]\n[4693]\n[4743]\n98 Five, six, pick up sticks\n97 Seven, eight, lay them straight\n96 Nine, ten, a big fat hen\n95 One, two, buckle my shoe\n94 Three, four, shut the door\n93 Five, six, pick up sticks\n92 Seven, eight, lay them straight\n91 Nine, ten, a big fat hen\n60 One, two, buckle my shoe\n59 Three, four, shut the door\n58 Five, six, pick up sticks\n57 Seven, eight, lay them straight\n56 Nine, ten, a big fat hen\n5 One, two, buckle my shoe\n4 Three, four, shut the door\n3 Five, six, pick up sticks\n2 Seven, eight, lay them straight\n1 Nine, ten, a big fat hen\nReading a SequenceFile\nReading sequence files from beginning to end is a matter of creating an instance of\nSequenceFile.Reader, and iterating over records by repeatedly invoking one of the\nnext() methods. Which one you use depends on the serialization framework you are\nusing. If you are using Writable types, you can use the next() method that takes a key\nand a value argument, and reads the next key and value in the stream into these\nvariables:\npublic boolean next(Writable key, Writable val)\nThe return value is true if a key-value pair was read and false if the end of the file has\nbeen reached.\nFor other, non-Writable serialization frameworks (such as Apache Thrift), you should\nuse these two methods:\npublic Object next(Object key) throws IOException\npublic Object getCurrentValue(Object val) throws IOException\nIn this case, you need to make sure that the serialization you want to use has been set\nin the io.serializations property; see “Serialization Frameworks” on page 101.\nIf the next() method returns a non-null object, a key-value pair was read from the\nstream and the value can be retrieved using the getCurrentValue() method. Otherwise,\nif next() returns null, the end of the file has been reached.\nThe program in Example 4-11 demonstrates how to read a sequence file that has\nWritable keys and values. Note how the types are discovered from the Sequence\nFile.Reader via calls to getKeyClass() and getValueClass(), then ReflectionUtils is\nused to create an instance for the key and an instance for the value. By using this tech-\nnique, the program can be used with any sequence file that has Writable keys and values.\nFile-Based Data Structures | 105Example 4-11. Reading a SequenceFile\npublic class SequenceFileReadDemo {\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nPath path = new Path(uri);\n}\n}\nSequenceFile.Reader reader = null;\ntry {\nreader = new SequenceFile.Reader(fs, path, conf);\nWritable key = (Writable)\nReflectionUtils.newInstance(reader.getKeyClass(), conf);\nWritable value = (Writable)\nReflectionUtils.newInstance(reader.getValueClass(), conf);\nlong position = reader.getPosition();\nwhile (reader.next(key, value)) {\nString syncSeen = reader.syncSeen() ? ""*"" : """";\nSystem.out.printf(""[%s%s]\\t%s\\t%s\\n"", position, syncSeen, key, value);\nposition = reader.getPosition(); // beginning of next record\n}\n} finally {\nIOUtils.closeStream(reader);\n}\nAnother feature of the program is that it displays the position of the sync points in the\nsequence file. A sync point is a point in the stream which can be used to resynchronize\nwith a record boundary if the reader is “lost”—for example, after seeking to an arbitrary\nposition in the stream. Sync points are recorded by SequenceFile.Writer, which inserts\na special entry to mark the sync point every few records as a sequence file is being\nwritten. Such entries are small enough to incur only a modest storage overhead—less\nthan 1%. Sync points always align with record boundaries.\nRunning the program in Example 4-11 shows the sync points in the sequence file as\nasterisks. The first one occurs at position 2021 (the second one occurs at position 4075,\nbut is not shown in the output):\n% hadoop SequenceFileReadDemo numbers.seq\n[128]\n100\nOne, two, buckle my shoe\n[173]\n99\nThree, four, shut the door\n[220]\n98\nFive, six, pick up sticks\n[264]\n97\nSeven, eight, lay them straight\n[314]\n96\nNine, ten, a big fat hen\n[359]\n95\nOne, two, buckle my shoe\n[404]\n94\nThree, four, shut the door\n[451]\n93\nFive, six, pick up sticks\n[495]\n92\nSeven, eight, lay them straight\n[545]\n91\nNine, ten, a big fat hen\n[590]\n90\nOne, two, buckle my shoe\n...\n106 | Chapter 4: Hadoop I/O[1976]\n[2021*]\n[2088]\n[2132]\n[2182]\n...\n[4557]\n[4602]\n[4649]\n[4693]\n[4743]\n60 One, two, buckle my shoe\n59 Three, four, shut the door\n58 Five, six, pick up sticks\n57 Seven, eight, lay them straight\n56 Nine, ten, a big fat hen\n5 One, two, buckle my shoe\n4 Three, four, shut the door\n3 Five, six, pick up sticks\n2 Seven, eight, lay them straight\n1 Nine, ten, a big fat hen\nThere are two ways to seek to a given position in a sequence file. The first is the\nseek() method, which positions the reader at the given point in the file. For example,\nseeking to a record boundary works as expected:\nreader.seek(359);\nassertThat(reader.next(key, value), is(true));\nassertThat(((IntWritable) key).get(), is(95));\nBut if the position in the file is not at a record boundary, the reader fails when the\nnext() method is called:\nreader.seek(360);\nreader.next(key, value); // fails with IOException\nThe second way to find a record boundary makes use of sync points. The sync(long\nposition) method on SequenceFile.Reader positions the reader at the next sync point\nafter position. (If there are no sync points in the file after this position, then the reader\nwill be positioned at the end of the file.) Thus, we can call sync() with any position in\nthe stream—a nonrecord boundary, for example—and the reader will be reestablish\nitself at the next sync point so reading can continue:\nreader.sync(360);\nassertThat(reader.getPosition(), is(2021L));\nassertThat(reader.next(key, value), is(true));\nassertThat(((IntWritable) key).get(), is(59));\nSequenceFile.Writer has a method called sync() for inserting a sync\npoint at the current position in the stream. This is not to be confused\nwith the identically named but otherwise unrelated sync() method de-\nfined by the Syncable interface for synchronizing buffers to the under-\nlying device.\nSync points come into their own when using sequence files as input to MapReduce,\nsince they permit the file to be split, so different portions of it can be processed inde-\npendently by separate map tasks. See “SequenceFileInputFormat” on page 199.\nDisplaying a SequenceFile with the command-line interface\nThe hadoop fs command has a -text option to display sequence files in textual form.\nIt looks at a file’s magic number so that it can attempt to detect the type of the file and\nFile-Based Data Structures | 107appropriately convert it to text. It can recognize gzipped files and sequence files; oth-\nerwise, it assumes the input is plain text.\nFor sequence files, this command is really useful only if the keys and values have a\nmeaningful string representation (as defined by the toString() method). Also, if you\nhave your own key or value classes, then you will need to make sure they are on Ha-\ndoop’s classpath.\nRunning it on the sequence file we created in the previous section gives the following\noutput:\n% hadoop fs -text numbers.seq | head\n100\nOne, two, buckle my shoe\n99\nThree, four, shut the door\n98\nFive, six, pick up sticks\n97\nSeven, eight, lay them straight\n96\nNine, ten, a big fat hen\n95\nOne, two, buckle my shoe\n94\nThree, four, shut the door\n93\nFive, six, pick up sticks\n92\nSeven, eight, lay them straight\n91\nNine, ten, a big fat hen\nSorting and merging SequenceFiles\nThe most powerful way of sorting (and merging) one or more sequence files is to use\nMapReduce. MapReduce is inherently parallel and will let you specify the number of\nreducers to use, which determines the number of output partitions. For example, by\nspecifying one reducer, you get a single output file. We can use the sort example that\ncomes with Hadoop by specifying that the input and output are sequence files, and by\nsetting the key and value types:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort -r 1 \\\n-inFormat org.apache.hadoop.mapred.SequenceFileInputFormat \\\n-outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat \\\n-outKey org.apache.hadoop.io.IntWritable \\\n-outValue org.apache.hadoop.io.Text \\\nnumbers.seq sorted\n% hadoop fs -text sorted/part-00000 | head\n1\nNine, ten, a big fat hen\n2\nSeven, eight, lay them straight\n3\nFive, six, pick up sticks\n4\nThree, four, shut the door\n5\nOne, two, buckle my shoe\n6\nNine, ten, a big fat hen\n7\nSeven, eight, lay them straight\n8\nFive, six, pick up sticks\n9\nThree, four, shut the door\n10\nOne, two, buckle my shoe\nSorting is covered in more detail in “Sorting” on page 218.\n108 | Chapter 4: Hadoop I/OAs an alternative to using MapReduce for sort/merge, there is a SequenceFile.Sorter\nclass that has a number of sort() and merge() methods. These functions predate Map-\nReduce, and are lower-level functions than MapReduce (for example, to get parallelism,\nyou need to partition your data manually), so in general MapReduce is the preferred\napproach to sort and merge sequence files.\nThe SequenceFile Format\nA sequence file consists of a header followed by one or more records (see Figure 4-2).\nThe first three bytes of a sequence file are the bytes SEQ, which acts a magic number,\nfollowed by a single byte representing the version number. The header contains other\nfields including the names of the key and value classes, compression details, user-\ndefined metadata, and the sync marker.# Recall that the sync marker is used to allow\na reader to synchronize to a record boundary from any position in the file. Each file has\na randomly generated sync marker, whose value is stored in the header. Sync markers\nappear between records in the sequence file. They are designed to incur less than a 1%\nstorage overhead, so they don’t necessarily appear between every pair of records (such\nis the case for short records).\nFigure 4-2. The internal structure of a sequence file with no compression and record compression\nThe internal format of the records depends on whether compression is enabled, and if\nit is, whether it is record compression or block compression.\n# Full details of the format of these fields may be found in SequenceFile’s documentation and source code.\nFile-Based Data Structures | 109If no compression is enabled (the default), then each record is made up of the record\nlength (in bytes), the key length, the key and then the value. The length fields are written\nas four-byte integers adhering to the contract of the writeInt() method of java.io.Data\nOutput. Keys and values are serialized using the Serialization defined for the class being\nwritten to the sequence file.\nThe format for record compression is almost identical to no compression, except the\nvalue bytes are compressed using the codec defined in the header. Note that keys are\nnot compressed.\nBlock compression compresses multiple records at once; it is therefore more compact\nthan and should generally be preferred over record compression because it has the\nopportunity to take advantage of similarities between records. (See Figure 4-3.) Records\nare added to a block until it reaches a minimum size in bytes, defined by the io.seq\nfile.compress.blocksize property: the default is 1,000,000 bytes. A sync marker is\nwritten before the start of every block. The format of a block is a field indicating the\nnumber of records in the block, followed by four compressed fields: the key lengths,\nthe keys, the value lengths, and the values.\nFigure 4-3. The internal structure of a sequence file with block compression\nMapFile\nA MapFile is a sorted SequenceFile with an index to permit lookups by key. MapFile can\nbe thought of as a persistent form of java.util.Map (although it doesn’t implement this\ninterface), which is able to grow beyond the size of a Map that is kept in memory.\nWriting a MapFile\nWriting a MapFile is similar to writing a SequenceFile: you create an instance of\nMapFile.Writer, then call the append() method to add entries in order. (Attempting to\nadd entries out of order will result in an IOException.) Keys must be instances of\nWritableComparable, and values must be Writable—contrast this to SequenceFile,\nwhich can use any serialization framework for its entries.\n110 | Chapter 4: Hadoop I/OThe program in Example 4-12 creates a MapFile, and writes some entries to it. It is very\nsimilar to the program in Example 4-10 for creating a SequenceFile.\nExample 4-12. Writing a MapFile\npublic class MapFileWriteDemo {\nprivate static final String[] DATA = {\n""One, two, buckle my shoe"",\n""Three, four, shut the door"",\n""Five, six, pick up sticks"",\n""Seven, eight, lay them straight"",\n""Nine, ten, a big fat hen""\n};\npublic static void main(String[] args) throws IOException {\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(uri), conf);\nIntWritable key = new IntWritable();\nText value = new Text();\nMapFile.Writer writer = null;\ntry {\nwriter = new MapFile.Writer(conf, fs, uri,\nkey.getClass(), value.getClass());\n}\n}\nfor (int i = 0; i < 1024; i++) {\nkey.set(i + 1);\nvalue.set(DATA[i % DATA.length]);\nwriter.append(key, value);\n}\n} finally {\nIOUtils.closeStream(writer);\n}\nLet’s use this program to build a MapFile:\n% hadoop MapFileWriteDemo numbers.map\nIf we look at the MapFile, we see it’s actually a directory containing two files called\ndata and index:\n% ls -l numbers.map\ntotal 104\n-rw-r--r--\n1 tom tom\n-rw-r--r--\n1 tom tom\n47898 Jul 29 22:06 data\n251 Jul 29 22:06 index\nBoth files are SequenceFiles. The data file contains all of the entries, in order:\n% hadoop fs -text numbers.map/data | head\n1\nOne, two, buckle my shoe\n2\nThree, four, shut the door\n3\nFive, six, pick up sticks\nFile-Based Data Structures | 1114\n5\n6\n7\n8\n9\n10\nSeven, eight, lay them straight\nNine, ten, a big fat hen\nOne, two, buckle my shoe\nThree, four, shut the door\nFive, six, pick up sticks\nSeven, eight, lay them straight\nNine, ten, a big fat hen\nThe index file contains a fraction of the keys, and contains a mapping from the key to\nthat key’s offset in the data file:\n% hadoop fs -text numbers.map/index\n1\n128\n129\n6079\n257\n12054\n385\n18030\n513\n24002\n641\n29976\n769\n35947\n897\n41922\nAs we can see from the output, by default only every 128th key is included in the index,\nalthough you can change this value either by setting the io.map.index.interval\nproperty or by calling the setIndexInterval() method on the MapFile.Writer instance.\nA reason to increase the index interval would be to decrease the amount of memory\nthat the MapFile needs to store the index. Conversely, you might decrease the interval\nto improve the time for random (since fewer records need to be skipped on average) at\nthe expense of memory usage.\nSince the index is only a partial index of keys, MapFile is not able to provide methods\nto enumerate, or even count, all the keys it contains. The only way to perform these\noperations is to read the whole file.\nReading a MapFile\nIterating through the entries in order in a MapFile is similar to the procedure for a\nSequenceFile: you create a MapFile.Reader, then call the next() method until it returns\nfalse, signifying that no entry was read because the end of the file was reached:\npublic boolean next(WritableComparable key, Writable val) throws IOException\nA random access lookup can be performed by calling the get() method:\npublic Writable get(WritableComparable key, Writable val) throws IOException\nThe return value is used to determine if an entry was found in the MapFile; if it’s null,\nthen no value exists for the given key. If key was found, then the value for that key is\nread into val, as well as being returned from the method call.\nIt might be helpful to understand how this is implemented. Here is a snippet of code\nthat retrieves an entry for the MapFile we created in the previous section:\n112 | Chapter 4: Hadoop I/OText value = new Text();\nreader.get(new IntWritable(496), value);\nassertThat(value.toString(), is(""One, two, buckle my shoe""));\nFor this operation, the MapFile.Reader reads the index file into memory (this is cached\nso that subsequent random access calls will use the same in-memory index). The reader\nthen performs a binary search on the in-memory index to find the key in the index that\nis less than or equal to the search key, 496. In this example, the index key found is 385,\nwith value 18030, which is the offset in the data file. Next the reader seeks to this offset\nin the data file and reads entries until the key is greater than or equal to the search key,\n496. In this case, a match is found and the value is read from the data file. Overall, a\nlookup takes a single disk seek and a scan through up to 128 entries on disk. For a\nrandom-access read, this is actually very efficient.\nThe getClosest() method is like get() except it returns the “closest” match to the\nspecified key, rather than returning null on no match. More precisely, if the MapFile\ncontains the specified key then that is the entry returned; otherwise, the key in the\nMapFile that is immediately after (or before, according to a boolean argument) the\nspecified key is returned.\nA very large MapFile’s index can take up a lot of memory. Rather than reindex to change\nthe index interval, it is possible to load only a fraction of the index keys into memory\nwhen reading the MapFile by setting the io.map.index.skip property. This property is\nnormally 0, which means no index keys are skipped; a value of 1 means skip one key\nfor every key in the index (so every other key ends up in the index), 2 means skip two\nkeys for every key in the index (so one third of the keys end up in the index), and so\non. Larger skip values save memory but at the expense of lookup time, since more\nentries have to be scanned on disk, on average.\nConverting a SequenceFile to a MapFile\nOne way of looking at a MapFile is as an indexed and sorted SequenceFile. So it’s quite\nnatural to want to be able to convert a SequenceFile into a MapFile. We covered how\nto sort a SequenceFile in “Sorting and merging SequenceFiles” on page 108, so here we\nlook at how to create an index for a SequenceFile. The program in Example 4-13 hinges\naround the static utility method fix() on MapFile, which recreates the index for a\nMapFile.\nExample 4-13. Recreating the index for a MapFile\npublic class MapFileFixer {\npublic static void main(String[] args) throws Exception {\nString mapUri = args[0];\nConfiguration conf = new Configuration();\nFileSystem fs = FileSystem.get(URI.create(mapUri), conf);\nPath map = new Path(mapUri);\nFile-Based Data Structures | 113Path mapData = new Path(map, MapFile.DATA_FILE_NAME);\n// Get key and value types from data sequence file\nSequenceFile.Reader reader = new SequenceFile.Reader(fs, mapData, conf);\nClass keyClass = reader.getKeyClass();\nClass valueClass = reader.getValueClass();\nreader.close();\n}\n}\n// Create the map file index file\nlong entries = MapFile.fix(fs, map, keyClass, valueClass, false, conf);\nSystem.out.printf(""Created MapFile %s with %d entries\\n"", map, entries);\nThe fix() method is usually used for recreating corrupted indexes, but since it creates\na new index from scratch, it’s exactly what we need here. The recipe is as follows:\n1. Sort the sequence file numbers.seq into a new directory called number.map that will\nbecome the MapFile. (If the sequence file is already sorted, then you can skip this\nstep. Instead, copy it to a file number.map/data, then go to step 3.)\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort -r 1 \\\n-inFormat org.apache.hadoop.mapred.SequenceFileInputFormat \\\n-outFormat org.apache.hadoop.mapred.SequenceFileOutputFormat \\\n-outKey org.apache.hadoop.io.IntWritable \\\n-outValue org.apache.hadoop.io.Text \\\nnumbers.seq numbers.map\n2. Rename the MapReduce output to be the data file:\n% hadoop fs -mv numbers.map/part-00000 numbers.map/data\n3. Create the index file:\n% hadoop MapFileFixer numbers.map\nCreated MapFile numbers.map with 100 entries\nThe MapFile numbers.map now exists and can be used.\n114 | Chapter 4: Hadoop I/OCHAPTER 5\nDeveloping a MapReduce Application\nIn Chapter 2, we introduced the MapReduce model. In this chapter, we look at the\npractical aspects of developing a MapReduce application in Hadoop.\nWriting a program in MapReduce has a certain flow to it. You start by writing your\nmap and reduce functions, ideally with unit tests to make sure they do what you expect.\nThen you write a driver program to run a job, which can run from your IDE using a\nsmall subset of the data to check that it is working. If it fails, then you can use your\nIDE’s debugger to find the source of the problem. With this information, you can\nexpand your unit tests to cover this case, and improve your mapper or reducer as ap-\npropriate to handle such input correctly.\nWhen the program runs as expected against the small dataset, you are ready to unleash\nit on a cluster. Running against the full dataset is likely to expose some more issues,\nwhich you can fix as before, by expanding your tests and mapper or reducer to handle\nthe new cases. Debugging failing programs in the cluster is a challenge, but Hadoop\nprovides some tools to help, such as an IsolationRunner, which allows you to run a\ntask over the same input on which it failed, with a debugger attached, if necessary.\nAfter the program is working, you may wish to do some tuning, first by running through\nsome standard checks for making MapReduce programs faster and then by doing task\nprofiling. Profiling distributed programs is not trivial, but Hadoop has hooks to aid the\nprocess.\nBefore we start writing a MapReduce program, we need to set up and configure the\ndevelopment environment. And to do that, we need to learn a bit about how Hadoop\ndoes configuration.\n115The Configuration API\nComponents in Hadoop are configured using Hadoop’s own configuration API. An\ninstance of the Configuration class (found in the org.apache.hadoop.conf package)\nrepresents a collection of configuration properties and their values. Each property is\nnamed by a String, and the type of a value may be one of several types, including Java\nprimitives such as boolean, int, long, float, and other useful types such as String, Class,\njava.io.File, and collections of Strings.\nConfigurations read their properties from resources—XML files with a simple structure\nfor defining name-value pairs. See Example 5-1.\nExample 5-1. A simple configuration file, configuration-1.xml\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>color</name>\n<value>yellow</value>\n<description>Color</description>\n</property>\n<property>\n<name>size</name>\n<value>10</value>\n<description>Size</description>\n</property>\n<property>\n<name>weight</name>\n<value>heavy</value>\n<final>true</final>\n<description>Weight</description>\n</property>\n<property>\n<name>size-weight</name>\n<value>${size},${weight}</value>\n<description>Size and weight</description>\n</property>\n</configuration>\nAssuming this configuration file is in a file called configuration-1.xml, we can access its\nproperties using a piece of code like this:\nConfiguration conf = new Configuration();\nconf.addResource(""configuration-1.xml"");\nassertThat(conf.get(""color""), is(""yellow""));\nassertThat(conf.getInt(""size"", 0), is(10));\nassertThat(conf.get(""breadth"", ""wide""), is(""wide""));\nThere are a couple of things to note: type information is not stored in the XML file;\ninstead, properties can be interpreted as a given type when they are read. Also, the\n116 | Chapter 5: Developing a MapReduce Applicationget() methods allow you to specify a default value, which is used if the property is not\ndefined in the XML file, as in the case of breadth here.\nCombining Resources\nThings get interesting when more than one resource is used to define a configuration.\nThis is used in Hadoop to separate out the default properties for the system, defined\ninternally in a file called core-default.xml, from the site-specific overrides, in core-\nsite.xml. The file in Example 5-2 defines the size and weight properties.\nExample 5-2. A second configuration file, configuration-2.xml\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>size</name>\n<value>12</value>\n</property>\n<property>\n<name>weight</name>\n<value>light</value>\n</property>\n</configuration>\nResources are added to a Configuration in order:\nConfiguration conf = new Configuration();\nconf.addResource(""configuration-1.xml"");\nconf.addResource(""configuration-2.xml"");\nProperties defined in resources that are added later override the earlier definitions. So\nthe size property takes its value from the second configuration file, configuration-2.xml:\nassertThat(conf.getInt(""size"", 0), is(12));\nHowever, properties that are marked as final cannot be overridden in later definitions.\nThe weight property is final in the first configuration file, so the attempt to override it\nin the second fails and it takes the value from the first:\nassertThat(conf.get(""weight""), is(""heavy""));\nAttempting to override final properties usually indicates a configuration error, so this\nresults in a warning message being logged to aid diagnosis. Administrators mark prop-\nerties as final in the daemon’s site files that they don’t want users to change in their\nclient-side configuration files, or job submission parameters.\nVariable Expansion\nConfiguration properties can be defined in terms of other properties, or system prop-\nerties. For example, the property size-weight in the first configuration file is defined\nThe Configuration API | 117as ${size},${weight}, and these properties are expanded using the values found in the\nconfiguration:\nassertThat(conf.get(""size-weight""), is(""12,heavy""));\nSystem properties take priority over properties defined in resource files:\nSystem.setProperty(""size"", ""14"");\nassertThat(conf.get(""size-weight""), is(""14,heavy""));\nThis feature is useful for overriding properties on the command line by using\n-Dproperty=value JVM arguments.\nNote that while configuration properties can be defined in terms of system properties,\nunless system properties are redefined using configuration properties, they are not ac-\ncessible through the configuration API. Hence:\nSystem.setProperty(""length"", ""2"");\nassertThat(conf.get(""length""), is((String) null));\nConfiguring the Development Environment\nThe first step is to download the version of Hadoop that you plan to use and unpack\nit on your development machine (this is described in Appendix A). Then, in your fa-\nvorite IDE, create a new project and add all the JAR files from the top level of the\nunpacked distribution and from the lib directory to the classpath. You will then be able\nto compile Java Hadoop programs, and run them in local (standalone) mode within\nthe IDE.\nFor Eclipse users, there is a plug-in available for browsing HDFS and\nlaunching MapReduce programs. Instructions are available on the Ha-\ndoop wiki at http://wiki.apache.org/hadoop/EclipsePlugIn.\nManaging Configuration\nWhen developing Hadoop applications, it is common to switch between running the\napplication locally and running it on a cluster. In fact, you may have several clusters\nyou work with, or you may have a local “pseudo-distributed” cluster that you like to\ntest on (a pseudo-distributed cluster is one whose daemons all run on the local machine;\nsetting up this mode is covered in Appendix A, too).\nOne way to accommodate these variations is to have Hadoop configuration files con-\ntaining the connection settings for each cluster you run against, and specify which one\nyou are using when you run Hadoop applications or tools. As a matter of best practice,\nit’s recommended to keep these files outside Hadoop’s installation directory tree, as\nthis makes it easy to switch between Hadoop versions without duplicating or losing\nsettings.\n118 | Chapter 5: Developing a MapReduce ApplicationFor the purposes of this book, we assume the existence of a directory called conf that\ncontains three configuration files: hadoop-local.xml, hadoop-localhost.xml, and\nhadoop-cluster.xml (these are available in the example code for this book). Note that\nthere is nothing special about the names of these files—they are just convenient ways\nto package up some configuration settings. (Compare this to Table A-1 in Appen-\ndix A, which sets out the equivalent server-side configurations.)\nThe hadoop-local.xml file contains the default Hadoop configuration for the default\nfilesystem and the jobtracker:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>file:///</value>\n</property>\n<property>\n<name>mapred.job.tracker</name>\n<value>local</value>\n</property>\n</configuration>\nThe settings in hadoop-localhost.xml point to a namenode and a jobtracker both run-\nning on localhost:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://localhost/</value>\n</property>\n<property>\n<name>mapred.job.tracker</name>\n<value>localhost:8021</value>\n</property>\n</configuration>\nFinally, hadoop-cluster.xml contains details of the cluster’s namenode and jobtracker\naddresses. In practice, you would name the file after the name of the cluster, rather\nthan “cluster” as we have here:\n<?xml version=""1.0""?>\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://namenode/</value>\n</property>\nConfiguring the Development Environment | 119<property>\n<name>mapred.job.tracker</name>\n<value>jobtracker:8021</value>\n</property>\n</configuration>\nYou can add other configuration properties to these files as needed. For example, if you\nwanted to set your Hadoop username for a particular cluster, you could do it in the\nappropriate file.\nSetting User Identity\nThe user identity that Hadoop uses for permissions in HDFS is determined by running\nthe whoami command on the client system. Similarly, the group names are derived from\nthe output of running groups.\nIf, however, your Hadoop user identity is different from the name of your user account\non your client machine, then you can explicitly set your Hadoop username and group\nnames by setting the hadoop.job.ugi property. The username and group names are\nspecified as a comma-separated list of strings; e.g., preston,director,inventor would\nset the username to preston and the group names to director and inventor.\nYou can set the user identity that the HDFS web interface runs as by setting\ndfs.web.ugi using the same syntax. By default it is webuser,webgroup, which is not a\nsuper user, so system files are not accessible through the web interface.\nNotice that there is no authentication with this system, a limitation that a future version\nof Hadoop will remedy.\nWith this setup, it is easy to use any configuration with the -conf command-line switch.\nFor example, the following command shows a directory listing on the HDFS server\nrunning in pseudo-distributed mode on localhost:\n% hadoop fs -conf conf/hadoop-localhost.xml -ls .\nFound 2 items\ndrwxr-xr-x\n- tom supergroup\n0 2009-04-08 10:32 /user/tom/input\ndrwxr-xr-x\n- tom supergroup\n0 2009-04-08 13:09 /user/tom/output\nIf you omit the -conf option, then you pick up the Hadoop configuration in the conf\nsubdirectory under $HADOOP_INSTALL. Depending on how you set this up, this may be\nfor a standalone setup or a pseudo-distributed cluster.\nTools that come with Hadoop support the -conf option, but it’s also straightforward\nto make your programs (such as programs that run MapReduce jobs) support it, too,\nusing the Tool interface.\n120 | Chapter 5: Developing a MapReduce ApplicationGenericOptionsParser, Tool, and ToolRunner\nHadoop comes with a few helper classes for making it easier to run jobs from the\ncommand line. GenericOptionsParser is a class that interprets common Hadoop\ncommand-line options and sets them on a Configuration object for your application to\nuse as desired. You don’t usually use GenericOptionsParser directly, as it’s more con-\nvenient to implement the Tool interface and run your application with the ToolRun\nner, which uses GenericOptionsParser internally:\npublic interface Tool extends Configurable {\nint run(String [] args) throws Exception;\n}\nExample 5-3 shows a very simple implementation of Tool, for printing the keys and\nvalues of all the properties in the Tool’s Configuration object.\nExample 5-3. An example Tool implementation for printing the properties in a Configuration\npublic class ConfigurationPrinter extends Configured implements Tool {\nstatic {\nConfiguration.addDefaultResource(""hdfs-default.xml"");\nConfiguration.addDefaultResource(""hdfs-site.xml"");\nConfiguration.addDefaultResource(""mapred-default.xml"");\nConfiguration.addDefaultResource(""mapred-site.xml"");\n}\n@Override\npublic int run(String[] args) throws Exception {\nConfiguration conf = getConf();\nfor (Entry<String, String> entry: conf) {\nSystem.out.printf(""%s=%s\\n"", entry.getKey(), entry.getValue());\n}\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new ConfigurationPrinter(), args);\nSystem.exit(exitCode);\n}\nWe make ConfigurationPrinter a subclass of Configured, which is an implementation\nof the Configurable interface. All implementations of Tool need to implement\nConfigurable (since Tool extends it), and subclassing Configured is often the easiest way\nto achieve this. The run() method obtains the Configuration using Configurable’s\ngetConf() method, and then iterates over it, printing each property to standard output.\nThe static block makes sure that the HDFS and MapReduce configurations are picked\nup in addition to the core ones (which Configuration knows about already).\nConfigurationPrinter’s main() method does not invoke its own run() method directly.\nInstead, we call ToolRunner’s static run() method, which takes care of creating a\nConfiguring the Development Environment | 121Configuration object for the Tool, before calling its run() method. ToolRunner also uses\na GenericOptionsParser to pick up any standard options specified on the command\nline, and set them on the Configuration instance. We can see the effect of picking up\nthe properties specified in conf/hadoop-localhost.xml by running the following\ncommand:\n% hadoop ConfigurationPrinter -conf conf/hadoop-localhost.xml \\\n| grep mapred.job.tracker=\nmapred.job.tracker=localhost:8021\nWhich Properties Can I Set?\nConfigurationPrinter is a useful tool for telling you what a property is set to in your\nenvironment.\nYou can also see the default settings for all the public properties in Hadoop by looking\nin the docs directory of your Hadoop installation for HTML files called\ncore-default.html, hdfs-default.html, and mapred-default.html. Each property has a de-\nscription which explains what it is for and what values it can be set to.\nBe aware that some properties have no effect when set in the client configuration. For\nexample, if in your job submission you set mapred.tasktracker.map.tasks.maximum with\nthe expectation that it would change the number of task slots for the tasktrackers run-\nning your job then you would be disappointed, since this property only is only honored\nif set in the tasktracker’s mapred-site.html file. In general, you can tell the component\nwhere a property should be set by its name, so the fact that mapred.task\ntracker.map.tasks.maximum starts with mapred.tasktracker gives you a clue that it can\nbe set only for the tasktracker daemon. This is not a hard and fast rule, however, so in\nsome cases you may need to resort to trial and error, or even reading the source.\nWe discuss many of Hadoop’s most important configuration properties throughout\nthis book. You can find a configuration property reference on the book’s website at\nhttp://www.hadoopbook.com.\nGenericOptionsParser also allows you to set individual properties. For example:\n% hadoop ConfigurationPrinter -D color=yellow | grep color\ncolor=yellow\nThe -D option is used to set the configuration property with key color to the value\nyellow. Options specified with -D take priority over properties from the configuration\nfiles. This is very useful: you can put defaults into configuration files, and then override\nthem with the -D option as needed. A common example of this is setting the number\nof reducers for a MapReduce job via -D mapred.reduce.tasks=n. This will override the\nnumber of reducers set on the cluster, or if set in any client-side configuration files.\nThe other options that GenericOptionsParser and ToolRunner support are listed in Ta-\nble 5-1. You can find more on Hadoop’s configuration API in “The Configuration\nAPI” on page 116.\n122 | Chapter 5: Developing a MapReduce ApplicationDo not confuse setting Hadoop properties using the -D\nproperty=value option to GenericOptionsParser (and ToolRunner) with\nsetting JVM system properties using the -Dproperty=value option to the\njava command. The syntax for JVM system properties does not allow\nany whitespace between the D and the property name, whereas\nGenericOptionsParser requires them to be separated by whitespace.\nJVM system properties are retrieved from the java.lang.System class,\nwhereas Hadoop properties are accessible only from a Configuration\nobject. So, the following command will print nothing, since the\nSystem class is not used by ConfigurationPrinter:\n% hadoop -Dcolor=yellow ConfigurationPrinter | grep color\nIf you want to be able to set configuration through system properties,\nthen you need to mirror the system properties of interest in the config-\nuration file. See “Variable Expansion” on page 117 for further\ndiscussion.\nTable 5-1. GenericOptionsParser and ToolRunner options\nOption Description\n-D property=value Sets the given Hadoop configuration property to the given value. Overrides any default\n                 or site properties in the configuration, and any properties set via the -conf option.\n-conf filename ... Adds the given files to the list of resources in the configuration. This is a convenient way\n                  to set site properties, or to set a number of properties at once.\n-fs uri Sets the default filesystem to the given URI. Shortcut for -D fs.default.name=uri\n-jt host:port Sets the jobtracker to the given host and port. Shortcut for -D\nmapred.job.tracker=host:port\n-files file1,file2,... Copies the specified files from the local filesystem (or any filesystem if a scheme is\n                      specified) to the shared filesystem used by the jobtracker (usually HDFS) and makes\n                     them available to MapReduce programs in the task’s working directory. (See “Distributed\n                        Cache” on page 239 for more on the distributed cache mechanism for copying files to\n                         tasktracker machines.)\n-archives Copies the specified archives from the local filesystem (or any filesystem if a scheme is\narchive1,archive2,... specified) to the shared filesystem used by the jobtracker (usually HDFS), unarchives\n                     them, and makes them available to MapReduce programs in the task’s working\n                      directory.\n-libjars jar1,jar2,... Copies the specified JAR files from the local filesystem (or any filesystem if a scheme is\n                      specified) to the shared filesystem used by the jobtracker (usually HDFS), and adds them\n                     to the MapReduce task’s classpath. This option is a useful way of shipping JAR files that\n                      a job is dependent on.\nWriting a Unit Test\nThe map and reduce functions in MapReduce are easy to test in isolation, which is a\nconsequence of their functional style. For known inputs, they produce known outputs.\nWriting a Unit Test | 123However, since outputs are written to an OutputCollector, rather than simply being\nreturned from the method call, the OutputCollector needs to be replaced with a mock\nso that its outputs can be verified. There are several Java mock object frameworks that\ncan help build mocks; here we use Mockito, which is noted for its clean syntax, although\nany mock framework should work just as well.*\nAll of the tests described here can be run from within an IDE.\nMapper\nThe test for the mapper is shown in Example 5-4.\nExample 5-4. Unit test for MaxTemperatureMapper\nimport static org.mockito.Matchers.anyObject;\nimport static org.mockito.Mockito.*;\nimport\nimport\nimport\nimport\njava.io.IOException;\norg.apache.hadoop.io.*;\norg.apache.hadoop.mapred.OutputCollector;\norg.junit.*;\npublic class MaxTemperatureMapperTest {\n@Test\npublic void processesValidRecord() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0043011990999991950051518004+68750+023550FM-12+0382"" +\n// Year ^^^^\n""99999V0203201N00261220001CN9999999N9-00111+99999999999"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nmapper.map(null, value, output, null);\n}\n}\nverify(output).collect(new Text(""1950""), new IntWritable(-11));\nThe test is very simple: it passes a weather record as input to the mapper, then checks\nthe output is the year and temperature reading. The input key and Reporter are both\nignored by the mapper, so we can pass in anything, including null as we do here. To\ncreate a mock OutputCollector we call Mockito’s mock() method (a static import),\npassing the class of the type we want to mock. Then we invoke the mapper’s map()\nmethod, which executes the code being tested. Finally, we verify that the mock object\nwas called with the correct method and arguments, using Mockito’s verify() method\n(again, statically imported). Here we verify that OutputCollector’s collect() method\n* See also the MRUnit contrib module, which aims to make unit testing MapReduce programs easier.\n124 | Chapter 5: Developing a MapReduce Applicationwas called with a Text object representing the year (1950) and an IntWritable repre-\nsenting the temperature (−1.1°C).\nProceeding in a test-driven fashion, we create a Mapper implementation that passes the\ntest (see Example 5-5). Since we will be evolving the classes in this chapter, each is put\nin a different package indicating its version for ease of exposition. For example, v1.Max\nTemperatureMapper is version 1 of MaxTemperatureMapper. In reality, of course, you would\nevolve classes without repackaging them.\nExample 5-5. First version of a Mapper that passes MaxTemperatureMapperTest\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nint airTemperature = Integer.parseInt(line.substring(87, 92));\noutput.collect(new Text(year), new IntWritable(airTemperature));\nThis is a very simple implementation, which pulls the year and temperature fields from\nthe line and emits them in the OutputCollector. Let’s add a test for missing values,\nwhich in the raw data are represented by a temperature of +9999:\n@Test\npublic void ignoresMissingTemperatureRecord() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0043011990999991950051518004+68750+023550FM-12+0382"" +\n// Year ^^^^\n""99999V0203201N00261220001CN9999999N9+99991+99999999999"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nmapper.map(null, value, output, null);\n}\nText outputKey = anyObject();\nIntWritable outputValue = anyObject();\nverify(output, never()).collect(outputKey, outputValue);\nSince records with missing temperatures should be filtered out, this test uses Mockito\nto verify that the collect method on the OutputCollector is never called for any Text key\nor IntWritable value.\nThe existing test fails with a NumberFormatException, as parseInt() cannot parse integers\nwith a leading plus sign, so we fix up the implementation (version 2) to handle missing\nvalues:\nWriting a Unit Test | 125public void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\nString line = value.toString();\nString year = line.substring(15, 19);\nString temp = line.substring(87, 92);\nif (!missing(temp)) {\nint airTemperature = Integer.parseInt(temp);\noutput.collect(new Text(year), new IntWritable(airTemperature));\n}\nprivate boolean missing(String temp) {\nreturn temp.equals(""+9999"");\n}\nWith the test passing, we move on to writing the reducer.\nReducer\nThe reducer has to find the maximum value for a given key. Here’s a simple test for\nthis feature:\n@Test\npublic void returnsMaximumIntegerInValues() throws IOException {\nMaxTemperatureReducer reducer = new MaxTemperatureReducer();\nText key = new Text(""1950"");\nIterator<IntWritable> values = Arrays.asList(\nnew IntWritable(10), new IntWritable(5)).iterator();\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nreducer.reduce(key, values, output, null);\n}\nverify(output).collect(key, new IntWritable(10));\nWe construct an iterator over some IntWritable values and then verify that\nMaxTemperatureReducer picks the largest. The code in Example 5-6 is for an implemen-\ntation of MaxTemperatureReducer that passes the test. Notice that we haven’t tested the\ncase of an empty values iterator, but arguably we don’t need to, since MapReduce\nwould never call the reducer in this case, as every key produced by a mapper has a value.\nExample 5-6. Reducer for maximum temperature example\npublic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nint maxValue = Integer.MIN_VALUE;\n126 | Chapter 5: Developing a MapReduce Application}\n}\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(key, new IntWritable(maxValue));\nRunning Locally on Test Data\nNow that we’ve got the mapper and reducer working on controlled inputs, the next\nstep is to write a job driver and run it on some test data on a development machine.\nRunning a Job in a Local Job Runner\nUsing the Tool interface introduced earlier in the chapter, it’s easy to write a driver to\nrun our MapReduce job for finding the maximum temperature by year (see\nMaxTemperatureDriver in Example 5-7).\nExample 5-7. Application to find the maximum temperature\npublic class MaxTemperatureDriver extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.printf(""Usage: %s [generic options] <input> <output>\\n"",\ngetClass().getSimpleName());\nToolRunner.printGenericCommandUsage(System.err);\nreturn -1;\n}\nJobConf conf = new JobConf(getConf(), getClass());\nconf.setJobName(""Max temperature"");\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureDriver(), args);\nSystem.exit(exitCode);\nRunning Locally on Test Data | 127}\n}\nMaxTemperatureDriver implements the Tool interface, so we get the benefit of being able\nto set the options that GenericOptionsParser supports. The run() method constructs\nand configures a JobConf object, before launching a job described by the JobConf.\nAmong the possible job configuration parameters, we set the input and output file\npaths, the mapper, reducer and combiner classes, and the output types (the input types\nare determined by the input format, which defaults to TextInputFormat and has Long\nWritable keys and Text values). It’s also a good idea to set a name for the job so that\nyou can pick it out in the job list during execution and after it has completed. By default,\nthe name is the name of the JAR file, which is normally not particularly descriptive.\nNow we can run this application against some local files. Hadoop comes with a local\njob runner, a cut-down version of the MapReduce execution engine for running Map-\nReduce jobs in a single JVM. It’s designed for testing, and is very convenient for use in\nan IDE, since you can run it in a debugger to step through the code in your mapper and\nreducer.\nThe local job runner is only designed for simple testing of MapReduce\nprograms, so inevitably it differs from the full MapReduce implemen-\ntation. The biggest difference is that it can’t run more than one reducer.\n(It can support the zero reducer case, too.) This is normally not a prob-\nlem, as most applications can work with one reducer, although on a\ncluster you would choose a larger number to take advantage of paral-\nlelism. The thing to watch out for is that even if you set the number of\nreducers to a value over one, the local runner will silently ignore the\nsetting and use a single reducer.\nThe local job runner also has no support for the DistributedCache fea-\nture (described in “Distributed Cache” on page 239).\nNeither of these limitations is inherent in the local job runner, and future\nversions of Hadoop may relax these restrictions.\nThe local job runner is enabled by a configuration setting. Normally\nmapred.job.tracker is a host:port pair to specify the address of the jobtracker, but when\nit has the special value of local, the job is run in-process without accessing an external\njobtracker.\nFrom the command line, we can run the driver by typing:\n% hadoop v2.MaxTemperatureDriver -conf conf/hadoop-local.xml \\\ninput/ncdc/micro max-temp\nEquivalently, we could use the -fs and -jt options provided by GenericOptionsParser:\n% hadoop v2.MaxTemperatureDriver -fs file:/// -jt local input/ncdc/micro max-temp\n128 | Chapter 5: Developing a MapReduce ApplicationThis command executes MaxTemperatureDriver using input from the local input/ncdc/\nmicro directory, producing output in the local max-temp directory. Note that although\nwe’ve set -fs so we use the local filesystem (file:///), the local job runner will actually\nwork fine against any filesystem, including HDFS (and it can be handy to do this if you\nhave a few files that are on HDFS).\nWhen we run the program it fails and prints the following exception:\njava.lang.NumberFormatException: For input string: ""+0000""\nFixing the mapper\nThis exception shows that the map method still can’t parse positive temperatures. (If\nthe stack trace hadn’t given us enough information to diagnose the fault, we could run\nthe test in a local debugger, since it runs in a single JVM.) Earlier, we made it handle\nthe special case of missing temperature, +9999, but not the general case of any positive\ntemperature. With more logic going into the mapper, it makes sense to factor out a\nparser class to encapsulate the parsing logic; see Example 5-8 (now on version 3).\nExample 5-8. A class for parsing weather records in NCDC format\npublic class NcdcRecordParser {\nprivate static final int MISSING_TEMPERATURE = 9999;\nprivate String year;\nprivate int airTemperature;\nprivate String quality;\npublic void parse(String record) {\nyear = record.substring(15, 19);\nString airTemperatureString;\n// Remove leading plus sign as parseInt doesn't like them\nif (record.charAt(87) == '+') {\nairTemperatureString = record.substring(88, 92);\n} else {\nairTemperatureString = record.substring(87, 92);\n}\nairTemperature = Integer.parseInt(airTemperatureString);\nquality = record.substring(92, 93);\n}\npublic void parse(Text record) {\nparse(record.toString());\n}\npublic boolean isValidTemperature() {\nreturn airTemperature != MISSING_TEMPERATURE && quality.matches(""[01459]"");\n}\npublic String getYear() {\nreturn year;\n}\nRunning Locally on Test Data | 129}\npublic int getAirTemperature() {\nreturn airTemperature;\n}\nThe resulting mapper is much simpler (see Example 5-9). It just calls the parser’s\nparse() method, which parses the fields of interest from a line of input, checks whether\na valid temperature was found using the isValidTemperature() query method, and if it\nwas retrieves the year and the temperature using the getter methods on the parser.\nNotice that we also check the quality status field as well as missing temperatures in\nisValidTemperature() to filter out poor temperature readings.\nAnother benefit of creating a parser class is that it makes it easy to write related mappers\nfor similar jobs without duplicating code. It also gives us the opportunity to write unit\ntests directly against the parser, for more targeted testing.\nExample 5-9. A Mapper that uses a utility class to parse records\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new Text(parser.getYear()),\nnew IntWritable(parser.getAirTemperature()));\n}\nWith these changes, the test passes.\nTesting the Driver\nApart from the flexible configuration options offered by making your application im-\nplement Tool, you also make it more testable because it allows you to inject an arbitrary\nConfiguration. You can take advantage of this to write a test that uses a local job runner\nto run a job against known input data, which checks that the output is as expected.\nThere are two approaches to doing this. The first is to use the local job runner and run\nthe job against a test file on the local filesystem. The code in Example 5-10 gives an\nidea of how to do this.\n130 | Chapter 5: Developing a MapReduce ApplicationExample 5-10. A test for MaxTemperatureDriver that uses a local, in-process job runner\n@Test\npublic void test() throws Exception {\nJobConf conf = new JobConf();\nconf.set(""fs.default.name"", ""file:///"");\nconf.set(""mapred.job.tracker"", ""local"");\nPath input = new Path(""input/ncdc/micro"");\nPath output = new Path(""output"");\nFileSystem fs = FileSystem.getLocal(conf);\nfs.delete(output, true); // delete old output\nMaxTemperatureDriver driver = new MaxTemperatureDriver();\ndriver.setConf(conf);\nint exitCode = driver.run(new String[] {\ninput.toString(), output.toString() });\nassertThat(exitCode, is(0));\n}\ncheckOutput(conf, output);\nThe test explicitly sets fs.default.name and mapred.job.tracker so it uses the local\nfilesystem and the local job runner. It then runs the MaxTemperatureDriver via its Tool\ninterface against a small amount of known data. At the end of the test, the checkOut\nput() method is called to compare the actual output with the expected output, line by\nline.\nThe second way of testing the driver is to run it using a “mini-” cluster. Hadoop has a\npair of testing classes, called MiniDFSCluster and MiniMRCluster, which provide a pro-\ngrammatic way of creating in-process clusters. Unlike the local job runner, these allow\ntesting against the full HDFS and MapReduce machinery. Bear in mind too that task-\ntrackers in a mini-cluster launch separate JVMs to run tasks in, which can make de-\nbugging more difficult.\nMini-clusters are used extensively in Hadoop’s own automated test suite, but they can\nbe used for testing user code too. Hadoop’s ClusterMapReduceTestCase abstract class\nprovides a useful base for writing such a test, handles the details of starting and stopping\nthe in-process HDFS and MapReduce clusters in its setUp() and tearDown() methods,\nand generating a suitable JobConf object that is configured to work with them. Sub-\nclasses need populate only data in HDFS (perhaps by copying from a local file), run a\nMapReduce job, then confirm the output is as expected. Refer to the MaxTemperature\nDriverMiniTest class in the example code that comes with this book for the listing.\nTests like this serve as regression tests, and are a useful repository of input edge cases\nand their expected results. As you encounter more test cases, you can simply add them\nto the input file and update the file of expected output accordingly.\nRunning Locally on Test Data | 131Running on a Cluster\nNow that we are happy with the program running on a small test dataset, we are ready\nto try it on the full dataset on a Hadoop cluster. Chapter 9 covers how to set up a fully\ndistributed cluster, although you can also work through this section on a pseudo-\ndistributed cluster.\nPackaging\nWe don’t need to make any modifications to the program to run on a cluster rather\nthan on a single machine, but we do need to package the program as a JAR file to send\nto the cluster. This is conveniently achieved using Ant, using a task such as this (you\ncan find the complete build file in the example code):\n<jar destfile=""job.jar"" basedir=""${classes.dir}""/>\nIf you have a single job per JAR, then you can specify the main class to run in the JAR\nfile’s manifest. If the main class is not in the manifest, then it must be specified on the\ncommand line (as you will see shortly). Also, any dependent JAR files should be pack-\naged in a lib subdirectory in the JAR file. (This is analogous to a Java Web application\narchive, or WAR file, except in that case the JAR files go in a WEB-INF/lib subdirectory\nin the WAR file.)\nLaunching a Job\nTo launch the job, we need to run the driver, specifying the cluster that we want to run\nthe job on with the -conf option (we could equally have used the -fs and -jt options):\n% hadoop jar job.jar v3.MaxTemperatureDriver -conf conf/hadoop-cluster.xml \\\ninput/ncdc/all max-temp\nThe runJob() method on JobClient launches the job and polls for progress, writing a\nline summarizing the map and reduce’s progress whenever either changes. Here’s the\noutput (some lines have been removed for clarity):\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n...\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n08:15:52 INFO mapred.FileInputFormat: Total input paths to process : 101\n08:15:53 INFO mapred.JobClient: Running job: job_200904110811_0002\n08:15:54 INFO mapred.JobClient: map 0% reduce 0%\n08:16:06 INFO mapred.JobClient: map 28% reduce 0%\n08:16:07 INFO mapred.JobClient: map 30% reduce 0%\n08:21:36 INFO mapred.JobClient: map 100% reduce 100%\n08:21:38 INFO mapred.JobClient: Job complete: job_200904110811_0002\n08:21:38 INFO mapred.JobClient: Counters: 19\n08:21:38 INFO mapred.JobClient: Job Counters\n08:21:38 INFO mapred.JobClient:\n08:21:38 INFO Launched reduce tasks=32\n08:21:38 INFO mapred.JobClient:\n08:21:38 INFO Rack-local map tasks=82\n08:21:38 INFO mapred.JobClient:\n             Launched map tasks=127\n            mapred.JobClient:\n           Data-local map tasks=45\n          mapred.JobClient: FileSystemCounters\n132 | Chapter 5: Developing a MapReduce Application09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n09/04/11\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\n08:21:38\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nFILE_BYTES_READ=12667214\nHDFS_BYTES_READ=33485841275\nFILE_BYTES_WRITTEN=989397\nHDFS_BYTES_WRITTEN=904\nMap-Reduce Framework\nReduce input groups=100\nCombine output records=4489\nMap input records=1209901509\nReduce shuffle bytes=19140\nReduce output records=100\nSpilled Records=9481\nMap output bytes=10282306995\nMap input bytes=274600205558\nCombine input records=1142482941\nMap output records=1142478555\nReduce input records=103\nThe output includes more useful information. Before the job starts, its ID is printed:\nthis is needed whenever you want to refer to the job, in logfiles for example, or when\ninterrogating it via the hadoop job command. When the job is complete, its statistics\n(known as counters) are printed out. These are very useful for confirming that the job\ndid what you expected. For example, for this job we can see that around 275 GB of\ninput data was analyzed (“Map input bytes”), read from around 34 GB of compressed\nfiles on HDFS (“HDFS_BYTES_READ”). The input was broken into 101 gzipped files\nof reasonable size, so there was no problem with not being able to split them.\nJob, Task, and Task Attempt IDs\nThe format of a job ID is composed of the time that the jobtracker (not the job) started,\nand an incrementing counter maintained by the jobtracker to uniquely identify the job\nto that instance of the jobtracker. So the job with this ID:\njob_200904110811_0002\nis the second (0002, job IDs are 1-based) job run by the jobtracker which started at\n08:11 on April 11, 2009. The counter is formatted with leading zeros to make job IDs\nsort nicely—in directory listings, for example. However, when the counter reaches\n10000 it is not reset, resulting in longer job IDs (which don’t sort so well).\nTasks belong to a job, and their IDs are formed by replacing the job prefix of a job ID\nwith a task prefix, and adding a suffix to identify the task within the job. For example,\ntask_200904110811_0002_m_000003\nis the fourth (000003, task IDs are 0-based) map (m) task of the job with ID\njob_200904110811_0002. The task IDs are created for a job when it is initialized, so they\ndo not necessarily dictate the order that the tasks will be executed in.\nTasks may be executed more than once, due to failure (see “Task Fail-\nure” on page 159) or speculative execution (see “Speculative Execu-\ntion” on page 169), so to identify different instances of a task execution, task attempts\nare given unique IDs on the jobtracker. For example,\nattempt_200904110811_0002_m_000003_0\nRunning on a Cluster | 133is\nthe first (0, attempt IDs are 0-based) attempt at running task\ntask_200904110811_0002_m_000003. Task attempts are allocated during the job run as\nneeded, so their ordering represents the order that they were created for tasktrackers\nto run.\nThe final count in the task attempt ID is incremented by one thousand if the job is\nrestarted after the jobtracker is restarted and recovers its running jobs.\nThe MapReduce Web UI\nHadoop comes with a web UI for viewing information about your jobs. It is useful for\nfollowing a job’s progress while it is running, as well as finding job statistics and logs\nafter the job has completed. You can find the UI at http://jobtracker-host:50030/.\nThe jobtracker page\nA screenshot of the home page is shown in Figure 5-1. The first section of the page gives\ndetails of the Hadoop installation, such as the version number and when it was com-\npiled, and the current state of the jobtracker (in this case, running), and when it was\nstarted.\nNext is a summary of the cluster, which has measures of cluster capacity and utilization.\nThis shows the number of maps and reduces currently running on the cluster, the total\nnumber of job submissions, the number of tasktracker nodes currently available, and\nthe cluster’s capacity: in terms of the number of map and reduce slots available across\nthe cluster (“Map Task Capacity” and “Reduce Task Capacity”), and the number of\navailable slots per node, on average. The number of tasktrackers that have been black-\nlisted by the jobtracker is listed as well (blacklisting is discussed in “Tasktracker Fail-\nure” on page 161).\nBelow the summary there is a section about the job scheduler that is running (here the\ndefault). You can click through to see job queues.\nFurther down we see sections for running, (successfully) completed, and failed jobs.\nEach of these sections has a table of jobs, with a row per job that shows the job’s ID,\nowner, name (as set using JobConf’s setJobName() method, which sets the\nmapred.job.name property) and progress information.\nFinally, at the foot of the page, there are links to the jobtracker’s logs, and the job-\ntracker’s history: information on all the jobs that the jobtracker has run. The main\ndisplay displays only 100 jobs (configurable via the mapred.jobtracker.completeuser\njobs.maximum property), before consigning them to the history page. Note also that the\njob history is persistent, so you can find jobs here from previous runs of the jobtracker.\n134 | Chapter 5: Developing a MapReduce ApplicationFigure 5-1. Screenshot of the jobtracker page\nJob History\nJob history refers to the events and configuration for a completed job. It is retained\nwhether the job was successful or not. Job history is used to support job recovery after\na jobtracker restart (see the mapred.jobtracker.restart.recover property), as well as\nproviding interesting information for the user running a job.\nJob history files are stored on the local filesystem of the jobtracker in a history subdir-\nectory of the logs directory. It is possible to set the location to an arbitrary Hadoop\nfilesystem via the hadoop.job.history.location property. The jobtracker’s history files\nare kept for 30 days before being deleted by the system.\nA second copy is also stored for the user, in the _logs/history subdirectory of the job’s\noutput\ndirectory.\nThis\nlocation\nmay\nbe\noverridden\nby\nsetting\nhadoop.job.history.user.location. By setting it to the special value none, no user job\nRunning on a Cluster | 135history is saved, although job history is still saved centrally. A user’s job history files\nare never deleted by the system.\nThe history log includes job, task and attempt events, all of which are stored in a plain-\ntext file. The history for a particular job may be viewed through the web UI, or via the\ncommand line, using hadoop job -history (which you point at the job’s output\ndirectory).\nThe job page\nClicking on a job ID brings you to a page for the job, illustrated in Figure 5-2. At the\ntop of the page is a summary of the job, with basic information such as job owner and\nname, and how long the job has been running for. The job file is the consolidated\nconfiguration file for the job, containing all the properties and their values that were in\neffect during the job run. If you are unsure of what a particular property was set to, you\ncan click through to inspect the file.\nWhile the job is running, you can monitor its progress on this page, which periodically\nupdates itself. Below the summary is a table that shows the map progress and the reduce\nprogress. “Num Tasks” shows the total number of map and reduce tasks for this job\n(a row for each). The other columns then show the state of these tasks: “Pending”\n(waiting to run), “Running,” “Complete” (successfully run), “Killed” (tasks that have\nfailed—this column would be more accurately labeled “Failed”). The final column\nshows the total number of failed and killed task attempts for all the map or reduce tasks\nfor the job (task attempts may be marked as killed if they are a speculative execution\nduplicate, if the tasktracker they are running on dies, or if they are killed by a user). See\n“Task Failure” on page 159 for background on task failure.\nFarther down the page, you can find completion graphs for each task that show their\nprogress graphically. The reduce completion graph is divided into the three phases of\nthe reduce task: copy (when the map outputs are being transferred to the reduce’s\ntasktracker), sort (when the reduce inputs are being merged), and reduce (when the\nreduce function is being run to produce the final output). The phases are described in\nmore detail in “Shuffle and Sort” on page 163.\nIn the middle of the page is a table of job counters. These are dynamically updated\nduring the job run, and provide another useful window into the job’s progress and\ngeneral health. There is more information about what these counters mean in “Built-\nin Counters” on page 211.\nRetrieving the Results\nOnce the job is finished, there are various ways to retrieve the results. Each reducer\nproduces one output file, so there are 30 part files named part-00000 to part-00029 in\nthe max-temp directory.\n136 | Chapter 5: Developing a MapReduce ApplicationFigure 5-2. Screenshot of the job page\nRunning on a Cluster | 137A good way to think of these “part” files is as parts of the max-temp\n“file.”\nIf the output is large (which it isn’t in this case), then it is important to\nhave multiple parts so that more than one reducer can work in parallel.\nUsually, if a file is in this partitioned form, it can still be used easily\nenough: as the input to another MapReduce job, for example. In some\ncases, you can exploit the structure of multiple partitions; to do a map-\nside join, for example, (“Map-Side Joins” on page 233) or a MapFile\nlookup (“An application: Partitioned MapFile lookups” on page 221).\nThis job produces a very small amount of output, so it is convenient to copy it from\nHDFS to our development machine. The -getmerge option to the hadoop fs command\nis useful here, as it gets all the files in the directory specified in the source pattern and\nmerges them into a single file on the local filesystem:\n% hadoop fs -getmerge max-temp max-temp-local\n% sort max-temp-local | tail\n1991\n607\n1992\n605\n1993\n567\n1994\n568\n1995\n567\n1996\n561\n1997\n565\n1998\n568\n1999\n568\n2000\n558\nWe sorted the output, as the reduce output partitions are unordered (owing to the hash\npartition function). Doing a bit of postprocessing of data from MapReduce is very\ncommon, as is feeding it into analysis tools, such as R, a spreadsheet, or even a relational\ndatabase.\nAnother way of retrieving the output if it is small is to use the -cat option to print the\noutput files to the console:\n% hadoop fs -cat max-temp/*\nOn closer inspection, we see that some of the results don’t look plausible. For instance,\nthe maximum temperature for 1951 (not shown here) is 590°C! How do we find out\nwhat’s causing this? Is it corrupt input data or a bug in the program?\nDebugging a Job\nThe time-honored way of debugging programs is via print statements, and this is cer-\ntainly possible in Hadoop. However, there are complications to consider: with pro-\ngrams running on tens, hundreds, or thousands of nodes, how do we find and examine\nthe output of the debug statements, which may be scattered across these nodes? For\nthis particular case, where we are looking for (what we think is) an unusual case, we\n138 | Chapter 5: Developing a MapReduce Applicationcan use a debug statement to log to standard error, in conjunction with a message to\nupdate the task’s status message to prompt us to look in the error log. The web UI\nmakes this easy, as you will see.\nWe also create a custom counter, to count the total number of records with implausible\ntemperatures in the whole dataset. This gives us valuable information about how to\ndeal with the condition—if it turns out to be a common occurrence, then we might\nneed to learn more about the condition and how to extract the temperature in these\ncases, rather than simply dropping the record. In fact, when trying to debug a job, you\nshould always ask yourself if you can use a counter to get the information you need to\nfind out what’s happening. Even if you need to use logging or a status message, it may\nbe useful to use a counter to gauge the extent of the problem. (There is more on counters\nin “Counters” on page 211.)\nIf the amount of log data you produce in the course of debugging is large, then you’ve\ngot a couple of options. The first is to write the information to the map’s output, rather\nthan to standard error, for analysis and aggregation by the reduce. This approach usu-\nally necessitates structural changes to your program, so start with the other techniques\nfirst. Alternatively, you can write a program (in MapReduce of course) to analyze the\nlogs produced by your job. There are tools to make this easier, such as Chukwa, a\nHadoop subproject.\nWe add our debugging to the mapper (version 4), as opposed to the reducer, as we\nwant to find out what the source data causing the anomalous output looks like:\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nOVER_100\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\nif (airTemperature > 1000) {\nSystem.err.println(""Temperature over 100 degrees for input: "" + value);\nreporter.setStatus(""Detected possibly corrupt record: see logs."");\nreporter.incrCounter(Temperature.OVER_100, 1);\n}\noutput.collect(new Text(parser.getYear()), new IntWritable(airTemperature));\n}\nRunning on a Cluster | 139If the temperature is over 100°C (represented by 1000, since temperatures are in tenths\nof a degree), we print a line to standard error with the suspect line, as well as updating\nthe map’s status message using the setStatus() method on Reporter directing us to\nlook in the log. We also increment a counter, which in Java is represented by a field of\nan enum type. In this program, we have defined a single field OVER_100 as a way to count\nthe number of records with a temperature of over 100°C.\nWith this modification, we recompile the code, recreate the JAR file, then rerun the\njob, and while it’s running go to the tasks page.\nThe tasks page\nThe job page has a number of links for look at the tasks in a job in more detail. For\nexample, by clicking on the “map” link, you are brought to a page which lists infor-\nmation for all of the map tasks on one page. You can also see just the completed tasks.\nThe screenshot in Figure 5-3 shows a portion of this page for the job run with our\ndebugging statements. Each row in the table is a task, and it provides such information\nas the start and end times for each task, any errors reported back from the tasktracker,\nand a link to view the counters for an individual task.\nFigure 5-3. Screenshot of the tasks page\nRelevant to debugging is the Status column, which shows a task’s latest status message.\nBefore a task starts, it shows its status as “initializing,” then once it starts reading re-\ncords it shows the split information for the split it is reading as a filename with a byte\noffset and length. You can see the status we set for debugging for task\ntask_200811201130_0054_m_000000, so let’s click through to the logs page to find the\nassociated debug message. (Notice too that there is an extra counter for this task, since\nour user counter has a nonzero count for this task.)\n140 | Chapter 5: Developing a MapReduce ApplicationThe task details page\nFrom the tasks page, you can click on any task to get more information about it. The\ntask details page, shown in Figure 5-4, shows each task attempt. In this case, there was\none task attempt, which completed successfully. The table provides further useful data,\nsuch as the node the task attempt ran on, and links to task logfiles and counters.\nFigure 5-4. Screenshot of the task details page\nThe “Actions” column contains links for killing a task attempt. By default, this is dis-\nabled, making the web UI a read-only interface. Set webinterface.private.actions to\ntrue to enable the actions links.\nBy setting webinterface.private.actions to true, you also allow anyone\nwith access to the HDFS web interface to delete files. The dfs.web.ugi\nproperty determines the user that the HDFS web UI runs as, thus con-\ntrolling which files may be viewed and deleted.\nFor map tasks, there is also a section showing which nodes the input split was located\non.\nBy following one of the links to the logfiles for the successful task attempt (you can see\nthe last 4 KB or 8 KB of each logfile, or the entire file), we can find the suspect input\nrecord that we logged (the line is wrapped and truncated to fit on the page):\nTemperature over 100 degrees for input:\n0335999999433181957042302005+37950+139117SAO +0004RJSN V020113590031500703569999994\n33201957010100005+35317+139650SAO +000899999V02002359002650076249N004000599+0067...\nThis record seems to be in a different format to the others. For one thing, there are\nspaces in the line, which are not described in the specification.\nRunning on a Cluster | 141When the job has finished, we can look at the value of the counter we defined to see\nhow many records over 100°C there are in the whole dataset. Counters are accessible\nvia the web UI, or the command line:\n% hadoop job -counter job_200904110811_0003 'v4.MaxTemperatureMapper$Temperature' \\\nOVER_100\n3\nThe -counter option takes the job ID, counter group name (which is the fully qualified\nclassname here), and the counter name (the enum name). There are only three mal-\nformed records in the entire dataset of over a billion records. Throwing out bad records\nis standard for many big data problems, although we need to be careful in this case,\nsince we are looking for an extreme value—the maximum temperature rather than an\naggregate measure. Still, throwing away three records is probably not going to change\nthe result.\nHadoop User Logs\nHadoop produces logs in various places, for various audiences. These are summarized\nin Table 5-2. Many of these files can be analyzed in aggregate using Chukwa, a Hadoop\nsubproject.\nAs you have seen in this section, MapReduce task logs are accessible through the web\nUI, which is the most convenient way to view them. You can also find the logfiles on\nthe local filesystem of the tasktracker that ran the task attempt, in a directory named\nby the task attempt. If task JVM reuse is enabled (“Task JVM Reuse” on page 170),\nthen each logfile accumulates the logs for the entire JVM run, so multiple task attempts\nwill be found in each logfile. The web UI hides this by showing only the portion that\nis relevant for the task attempt being viewed.\nIt is straightforward to write to these logfiles. Anything written to standard output, or\nstandard error, is directed to the relevant logfile. (Of course, in Streaming, standard\noutput is used for the map or reduce output, so it will not show up in the standard\noutput log.)\nIn Java, you can write to the task’s syslog file if you wish by using the Apache Commons\nLogging API. The actual logging is done by log4j in this case: the relevant log4j appender\nis called TLA (Task Log Appender) in the log4j.properties file in Hadoop’s configuration\ndirectory.\nThere are some controls for managing retention and size of task logs. By default, logs\nare deleted after a minimum of 24 hours (set using the mapred.userlog.retain.hours\nproperty). You can also set a cap on the maximum size of each logfile using the\nmapred.userlog.limit.kb property, which is 0 by default, meaning there is no cap.\n142 | Chapter 5: Developing a MapReduce ApplicationTable 5-2. Hadoop logs\nLogs Primary audience Description Further information\nSystem Administrators Each Hadoop daemon produces a logfile (using log4j) and “System log-\ndaemon another file that combines standard out and error. Written in files” on page 256.\nlogs the directory defined by the HADOOP_LOG_DIR environment \n     variable. \nHDFS audit Administrators A log of all HDFS requests, turned off by default. Written to “Audit Log-\nlogs the namenode’s log, although this is configurable. ging” on page 280.\nMapReduce Users A log of the events (such as task completion) that occur in the “Job His-\njob history course of running a job. Saved centrally on the jobtracker, and tory” on page 135.\nlogs in the job’s output directory in a _logs/history subdirectory. \nMapReduce Users Each tasktracker child process produces a logfile using log4j See next section.\ntask logs (called syslog), a file for data sent to standard out (stdout), and \n          a file for standard error (stderr). Written in the userlogs sub- \n          directory of the directory defined by the \n          HADOOP_LOG_DIR environment variable. \nHandling malformed data\nCapturing input data that causes a problem is valuable, as we can use it in a test to\ncheck that the mapper does the right thing:\n@Test\npublic void parsesMalformedTemperature() throws IOException {\nMaxTemperatureMapper mapper = new MaxTemperatureMapper();\nText value = new Text(""0335999999433181957042302005+37950+139117SAO +0004"" +\n// Year ^^^^\n""RJSN V02011359003150070356999999433201957010100005+353"");\n// Temperature ^^^^^\nOutputCollector<Text, IntWritable> output = mock(OutputCollector.class);\nReporter reporter = mock(Reporter.class);\nmapper.map(null, value, output, reporter);\n}\nText outputKey = anyObject();\nIntWritable outputValue = anyObject();\nverify(output, never()).collect(outputKey, outputValue);\nverify(reporter).incrCounter(MaxTemperatureMapper.Temperature.MALFORMED, 1);\nThe record that was causing the problem is of a different format to the other lines we’ve\nseen. Example 5-11 shows a modified program (version 5) using a parser that ignores\neach line with a temperature field that does not have a leading sign (plus or minus).\nWe’ve also introduced a counter to measure the number of records that we are ignoring\nfor this reason.\nRunning on a Cluster | 143Example 5-11. Mapper for maximum temperature example\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nMALFORMED\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\noutput.collect(new Text(parser.getYear()), new IntWritable(airTemperature));\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n}\nUsing a Remote Debugger\nWhen a task fails and there is not enough information logged to diagnose the error,\nyou may want to resort to running a debugger for that task. This is hard to arrange\nwhen running the job on a cluster, as you don’t know which node is going to process\nwhich part of the input, so you can’t set up your debugger ahead of the failure. Instead,\nyou run the job with a property set that instructs Hadoop to keep all the intermediate\ndata generated during the job run. This data can then be used to rerun the failing task\nin isolation with a debugger attached. Note that the task is run in situ, on the same\nnode that it failed on, which increases the chances of the error being reproducible.†\nFirst, set the configuration property keep.failed.task.files to true, so that when tasks\nfail, the tasktracker keeps enough information to allow the task to be rerun over the\nsame input data. Then run the job again and note which node the task fails on, and the\ntask attempt ID (it begins with the string attempt_) using the web UI.\nNext we need to run a special task runner called IsolationRunner with the retained files\nas input. Log into the node that the task failed on and look for the directory for that\ntask attempt. It will be under one of the local MapReduce directories, as set by the\nmapred.local.dir property (covered in more detail in “Important Hadoop Daemon\nProperties” on page 258). If this property is a comma-separated list of directories (to\n† This feature is currently broken in Hadoop 0.20.0. Track the fix under https://issues.apache.org/jira/browse/\nHADOOP-4041.\n144 | Chapter 5: Developing a MapReduce Applicationspread load across the physical disks on a machine), then you may need to look in all\nof the directories before you find the directory for that particular task attempt. The task\nattempt directory is in the following location:\nmapred.local.dir/taskTracker/jobcache/job-ID/task-attempt-ID\nThis directory contains various files and directories, including job.xml, which contains\nall of the job configuration properties in effect during the task attempt, and which\nIsolationRunner uses to create a JobConf instance. For map tasks, this directory also\ncontains a file containing a serialized representation of the input split, so the same input\ndata can be fetched for the task. For reduce tasks, a copy of the map output, which\nforms the reduce input is stored in a directory named output.\nThere is also a directory called work, which is the working directory for the task attempt.\nWe change into this directory to run the IsolationRunner. We need to set some options\nto allow the remote debugger to connect:‡\n% export HADOOP_OPTS=""-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,\\\naddress=8000""\nThe suspend=y option tells the JVM to wait until the debugger has attached before\nrunning code. The IsolationRunner is launched with the following command:\n% hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml\nNext, set breakpoints, attach your remote debugger (all the major Java IDEs support\nremote debugging—consult the documentation for instructions), and the task will be\nrun under your control. You can rerun the task any number of times like this. With any\nluck, you’ll be able to find and fix the error.\nDuring the process, you can use other, standard, Java debugging techniques, such as\nkill -QUIT pid or jstack to get thread dumps.\nMore generally, it’s worth knowing that this technique isn’t only useful for failing tasks.\nYou can keep the intermediate files for successful tasks too, which may be handy if you\nwant to examine a task that isn’t failing. In this case, set the property\nkeep.task.files.pattern to a regular expression that matches the IDs of the tasks you\nwant to keep.\nTuning a Job\nAfter a job is working the question many developers ask is, “Can I make it run faster?”\nThere are a few Hadoop-specific “usual suspects” that are worth checking to see if they\nare responsible for a performance problem. You should run through the checklist in\nTable 5-3 before you start trying to profile or optimize at the task level.\n‡ You can find details about debugging options on the Java Platform Debugger Architecture web page.\nTuning a Job | 145Table 5-3. Tuning checklist\nArea Best practice Further information\nNumber of map- How long are you mappers running for? If they are only running for a few seconds “Small files and Com-\npers on average, then you should see if there’s a way to have fewer mappers and bineFileInputFor-\n       make them all run longer, a minute or so, as a rule of thumb. The extent to mat” on page 190\n       which this is possible depends on the input format you are using. \nNumber of reducers For maximum performance, the number of reducers should be slightly less than “Choosing the Num-\n                   the number of reduce slots in the cluster. This allows the reducers to finish in ber of Reduc-\n                   one wave, and fully utilizes the cluster during the reduce phase. ers” on page 181\nCombiners Can your job take advantage of a combiner to reduce the amount of data in “Combiner Func-\n          passing through the shuffle? tions” on page 29\nIntermediate Job execution time can almost always benefit from enabling map output “Compressing map\ncompression compression. output” on page 85\nCustom If you are using your own custom Writable objects, or custom comparators, “Implementing a\nserialization then make sure you have implemented RawComparator. RawComparator for\n                                                                speed” on page 99\nShuffle tweaks The MapReduce shuffle exposes around a dozen tuning parameters for memory “Configuration Tun-\n               management, which may help you eke out the last bit of performance. ing” on page 166\nProfiling Tasks\nLike debugging, profiling a job running on a distributed system like MapReduce\npresents some challenges. Hadoop allows you to profile a fraction of the tasks in a job,\nand, as each task completes, pulls down the profile information to your machine for\nlater analysis with standard profiling tools.\nOf course, it’s possible, and somewhat easier, to profile a job running in the local job\nrunner. And provided you can run with enough input data to exercise the map and\nreduce tasks, this can be a valuable way of improving the performance of your mappers\nand reducers. There are a couple of caveats, however. The local job runner is a very\ndifferent environment from a cluster, and the data flow patterns are very different.\nOptimizing the CPU performance of your code may be pointless if your MapReduce\njob is I/O-bound (as many jobs are). To be sure that any tuning is effective, you should\ncompare the new execution time with the old running on a real cluster. Even this is\neasier said than done, since job execution times can vary due to resource contention\nwith other jobs and the decisions the scheduler makes to do with task placement. To\nget a good idea of job execution time under these circumstances, perform a series of\nruns (with and without the change) and check whether any improvement is statistically\nsignificant.\nIt’s unfortunately true that some problems (such as excessive memory use) can be re-\nproduced only on the cluster, and in these cases the ability to profile in situ is\nindispensable.\n146 | Chapter 5: Developing a MapReduce ApplicationThe HPROF profiler\nThere are a number of configuration properties to control profiling, which are also\nexposed via convenience methods on JobConf. The following modification to\nMaxTemperatureDriver (version 6) will enable remote HPROF profiling. HPROF is a\nprofiling tool that comes with the JDK that, although basic, can give valuable infor-\nmation about a program’s CPU and heap usage.§\nconf.setProfileEnabled(true);\nconf.setProfileParams(""-agentlib:hprof=cpu=samples,heap=sites,depth=6,"" +\n""force=n,thread=y,verbose=n,file=%s"");\nconf.setProfileTaskRange(true, ""0-2"");\nThe first line enables profiling, which by default is turned off. (This is equivalent to\nsetting the configuration property mapred.task.profile to true).\nNext we set the profile parameters, which are the extra command-line arguments to\npass to the task’s JVM. (When profiling is enabled, a new JVM is allocated for each\ntask, even if JVM reuse is turned on; see “Task JVM Reuse” on page 170.) The default\nparameters specify the HPROF profiler; here we set an extra HPROF option, depth=6,\nto give more stack trace depth than the HPROF default. The setProfileParams()\nmethod on JobConf is equivalent to setting the mapred.task.profile.params.\nFinally, we specify which tasks we want to profile. We normally only want profile\ninformation from a few tasks, so we use the setProfileTaskRange() method to specify\nthe range of task IDs that we want profile information for. We’ve set it to 0-2 (which\nis actually the default), which means tasks with IDs 0, 1, and 2 are profiled. The first\nargument to the setProfileTaskRange() method dictates whether the range is for map\nor reduce tasks: true is for maps, false is for reduces. A set of ranges is permitted, using\na notation that allows open ranges. For example, 0-1,4,6- would specify all tasks except\nthose with IDs 2, 3, and 5. The tasks to profile can also be controlled using the\nmapred.task.profile.maps property for map tasks, and mapred.task.profile.reduces\nfor reduce tasks.\nWhen we run a job with the modified driver, the profile output turns up at the end of\nthe job in the directory we launched the job from. Since we are only profiling a few\ntasks, we can run the job on a subset of the dataset.\nHere’s a snippet of one of the mapper’s profile files, which shows the CPU sampling\ninformation:\nCPU SAMPLES BEGIN (total =\nrank\nself accum\ncount\n1 3.49% 3.49%\n35\n2 3.39% 6.89%\n34\n3 3.19% 10.08%\n32\n1002) Sat Apr 11 11:17:52 2009\ntrace method\n307969 java.lang.Object.<init>\n307954 java.lang.Object.<init>\n307945 java.util.regex.Matcher.<init>\n§ HPROF uses byte code insertion to profile your code, so you do not need to recompile your application with\nspecial options to use it. For more information on HPROF, see “HPROF: A Heap/CPU Profiling Tool in J2SE\n5.0,” by Kelly O’Hair at http://java.sun.com/developer/technicalArticles/Programming/HPROF.html.\nTuning a Job | 1474\n5\n3.19% 13.27%\n3.19% 16.47%\n32 307963 java.lang.Object.<init>\n32 307973 java.lang.Object.<init>\nCross referencing the trace number 307973 gives us the stacktrace from the same file:\nTRACE 307973: (thread=200001)\njava.lang.Object.<init>(Object.java:20)\norg.apache.hadoop.io.IntWritable.<init>(IntWritable.java:29)\nv5.MaxTemperatureMapper.map(MaxTemperatureMapper.java:30)\nv5.MaxTemperatureMapper.map(MaxTemperatureMapper.java:14)\norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\norg.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:356)\nSo it looks like the mapper is spending 3% of its time constructing IntWritable objects.\nThis observation suggests that it might be worth reusing the Writable instances being\noutput (version 7):\nExample 5-12. Reusing the Text and IntWritable output objects\npublic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nenum Temperature {\nMALFORMED\n}\nprivate NcdcRecordParser parser = new NcdcRecordParser();\nprivate Text year = new Text();\nprivate IntWritable temp = new IntWritable();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\nyear.set(parser.getYear());\ntemp.set(parser.getAirTemperature());\noutput.collect(year, temp);\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n}\nHowever, we know if this is significant only if we can measure an improvement when\nrunning the job over the whole dataset. Running each variant five times on an otherwise\nquiet 11-node cluster showed no statistically significant difference in job execution\ntime. Of course, this result holds only for this particular combination of code, data,\nand hardware, so you should perform similar benchmarks to see whether such a change\nis significant for your setup.\n148 | Chapter 5: Developing a MapReduce ApplicationOther profilers\nAt the time of this writing, the mechanism for retrieving profile output is HPROF-\nspecific. Until this is fixed, it should be possible to use Hadoop’s profiling settings to\ntrigger profiling using any profiler (see the documentation for the particular profiler),\nalthough it may be necessary to manually retrieve the profiler’s output from tasktrack-\ners for analysis.\nIf the profiler is not installed on all the tasktracker machines, consider using the Dis-\ntributed Cache (“Distributed Cache” on page 239) for making the profiler binary\navailable on the required machines.\nMapReduce Workflows\nSo far in this chapter, you have seen the mechanics of writing a program using Map-\nReduce. We haven’t yet considered how to turn a data processing problem into the\nMapReduce model.\nThe data processing you have seen so far in this book is to solve a fairly simple problem\n(finding the maximum recorded temperature for given years). When the processing\ngets more complex, this complexity is generally manifested by having more MapReduce\njobs, rather than having more complex map and reduce functions. In other words, as\na rule of thumb, think about adding more jobs, rather than adding complexity to jobs.\nFor more complex problems, it is worth considering a higher-level language than Map-\nReduce, such as Pig, Hive, or Cascading. One immediate benefit is that it frees you up\nfrom having to do the translation into MapReduce jobs, allowing you to concentrate\non the analysis you are performing.\nDecomposing a Problem into MapReduce Jobs\nLet’s look at an example of a more complex problem that we want to translate into a\nMapReduce workflow.\nImagine that we want to find the mean maximum recorded temperature for every day\nof the year and every weather station. In concrete terms, to calculate the mean maxi-\nmum daily temperature recorded by station 029070-99999, say, on January 1, we take\nthe mean of the maximum daily temperatures for this station for January 1, 1901;\nJanuary 1, 1902; and so on up to January 1, 2000.\nHow can we compute this using MapReduce? The computation decomposes most\nnaturally into two stages:\n1. Compute the maximum daily temperature for every station-date pair.\nThe MapReduce program in this case is a variant of the maximum temperature\nprogram, except that the keys in this case are a composite station-date pair, rather\nthan just the year.\nMapReduce Workflows | 1492. Compute the mean of the maximum daily temperatures for every station-day-\nmonth key.\nThe mapper takes the output from the previous job (station-date, maximum tem-\nperature) records, and projects it into (station-day-month, maximum temperature)\nrecords by dropping the year component. The reduce function then takes the mean\nof the maximum temperatures, for each station-day-month key.\nThe output from first stage looks like this for the station we are interested in. (The\nmean_max_daily_temp.sh script in the examples provides an implementation in\nHadoop Streaming.)\n029070-99999\n029070-99999\n...\n19010101\n19020101\n0\n-94\nThe first two fields form the key, and the final column is the maximum temperature\nfrom all the readings for the given station and date. The second stage averages these\ndaily maxima over years to yield:\n029070-99999\n0101\n-68\nwhich is interpreted as saying the mean maximum daily temperature on January 1 for\nstation 029070-99999 over the century is −6.8°C.\nIt’s possible to do this computation in one MapReduce stage, but it takes more work\non the part of the programmer.‖\nThe arguments for having more (but simpler) MapReduce stages are that doing so leads\nto more composable and more maintainable mappers and reducers. The case studies\nin Chapter 14 cover a wide range of real-world problems that were solved using Map-\nReduce, and in each case, the data processing task is implemented using two or more\nMapReduce jobs. The details in that chapter are invaluable for getting a better idea of\nhow to decompose a processing problem into a MapReduce workflow.\nIt’s possible to make map and reduce functions even more composable than we have\ndone. A mapper commonly performs input format parsing, projection (selecting the\nrelevant fields), and filtering (removing records that are not of interest). In the mappers\nyou have seen so far, we have implemented all of these functions in a single mapper.\nHowever, there is a case for splitting these into distinct mappers and chaining them\ninto a single mapper using the ChainMapper library class that comes with Hadoop.\nCombined with a ChainReducer, you can run a chain of mappers, followed by a reducer\nand another chain of mappers in a single MapReduce job.\n‖ It’s an interesting exercise to do this. Hint: use “Secondary Sort” on page 227.\n150 | Chapter 5: Developing a MapReduce ApplicationRunning Dependent Jobs\nWhen there is more than one job in a MapReduce workflow, the question arises: how\ndo you manage the jobs so they are executed in order? There are several approaches,\nand the main consideration is whether you have a linear chain of jobs, or a more com-\nplex directed acyclic graph (DAG) of jobs.\nFor a linear chain, the simplest approach is to run each job one after another, waiting\nuntil a job completes successfully before running the next.\nJobClient.runJob(conf1);\nJobClient.runJob(conf2);\nIf a job fails, the runJob() method will throw an IOException, so later jobs in the pipeline\ndon’t get executed. Depending on your application, you might want to catch the ex-\nception and clean up any intermediate data that was produced by any previous jobs.\nFor anything more complex than a linear chain, there are libraries that can help or-\nchestrate your workflow (although they are suited to linear chains, or even one-off jobs,\ntoo). The simplest is in the org.apache.hadoop.mapred.jobcontrol package: the\nJobControl class. An instance of JobControl represents a graph of jobs to be run. You\nadd the job configurations, then tell the JobControl instance the dependencies between\njobs. You run the JobControl in a thread, and it runs the jobs in dependency order. You\ncan poll for progress, and when the jobs have finished, you can query for all the jobs’\nstatuses, and the associated errors for any failures. If a job fails, JobControl won’t run\nits dependencies.\nUnlike JobControl which runs on the client machine submitting the jobs, the Hadoop\nWorkflow Scheduler (HWS)# runs as a server, and a client submits a workflow to the\nscheduler. When the workflow completes the scheduler can make an HTTP callback\nto the client to inform it of the jobs’ statuses. HWS can run different types of jobs in\nthe same workflow: a Pig job followed by a Java MapReduce job, for example.\n# At the time of this writing, the Hadoop Workflow Scheduler is still under development. See https://issues\n.apache.org/jira/browse/HADOOP-5303.\nMapReduce Workflows | 151CHAPTER 6\nHow MapReduce Works\nIn this chapter, we look at how MapReduce in Hadoop works in detail. This knowledge\nprovides a good foundation for writing more advanced MapReduce programs, which\nwe will cover in the following two chapters.\nAnatomy of a MapReduce Job Run\nYou can run a MapReduce job with a single line of code: JobClient.runJob(conf). It’s\nvery short, but it conceals a great deal of processing behind the scenes. This section\nuncovers the steps Hadoop takes to run a job.\nThe whole process is illustrated in Figure 6-1. At the highest level, there are four inde-\npendent entities:\n• The client, which submits the MapReduce job.\n• The jobtracker, which coordinates the job run. The jobtracker is a Java application\nwhose main class is JobTracker.\n• The tasktrackers, which run the tasks that the job has been split into. Tasktrackers\nare Java applications whose main class is TaskTracker.\n• The distributed filesystem (normally HDFS, covered in Chapter 3), which is used\nfor sharing job files between the other entities.\nJob Submission\nThe runJob() method on JobClient is a convenience method that creates a new\nJobClient instance and calls submitJob() on it (step 1 in Figure 6-1). Having submitted\nthe job, runJob() polls the job’s progress once a second, and reports the progress to the\nconsole if it has changed since the last report. When the job is complete, if it was\nsuccessful, the job counters are displayed. Otherwise, the error that caused the job to\nfail is logged to the console.\n153Figure 6-1. How Hadoop runs a MapReduce job\nThe job submission process implemented by JobClient’s submitJob() method does the\nfollowing:\n• Asks the jobtracker for a new job ID (by calling getNewJobId() on JobTracker) (step\n2).\n• Checks the output specification of the job. For example, if the output directory has\nnot been specified or it already exists, the job is not submitted and an error is\nthrown to the MapReduce program.\n• Computes the input splits for the job. If the splits cannot be computed, because\nthe input paths don’t exist, for example, then the job is not submitted and an error\nis thrown to the MapReduce program.\n• Copies the resources needed to run the job, including the job JAR file, the config-\nuration file and the computed input splits, to the jobtracker’s filesystem in a di-\nrectory named after the job ID. The job JAR is copied with a high replication factor\n(controlled by the mapred.submit.replication property, which defaults to 10) so\nthat there are lots of copies across the cluster for the tasktrackers to access when\nthey run tasks for the job (step 3).\n154 | Chapter 6: How MapReduce Works• Tells the jobtracker that the job is ready for execution (by calling submitJob() on\nJobTracker) (step 4).\nJob Initialization\nWhen the JobTracker receives a call to its submitJob() method, it puts it into an internal\nqueue from where the job scheduler will pick it up and initialize it. Initialization involves\ncreating an object to represent the job being run, which encapsulates its tasks, and\nbookkeeping information to keep track of the tasks’ status and progress (step 5).\nTo create the list of tasks to run, the job scheduler first retrieves the input splits com-\nputed by the JobClient from the shared filesystem (step 6). It then creates one map task\nfor each split. The number of reduce tasks to create is determined by the\nmapred.reduce.tasks property in the JobConf, which is set by the setNumReduce\nTasks() method, and the scheduler simply creates this number of reduce tasks to be\nrun. Tasks are given IDs at this point.\nTask Assignment\nTasktrackers run a simple loop that periodically sends heartbeat method calls to the\njobtracker. Heartbeats tell the jobtracker that a tasktracker is alive, but they also double\nas a channel for messages. As a part of the heartbeat, a tasktracker will indicate whether\nit is ready to run a new task, and if it is, the jobtracker will allocate it a task, which it\ncommunicates to the tasktracker using the heartbeat return value (step 7).\nBefore it can choose a task for the tasktracker, the jobtracker must choose a job to select\nthe task from. There are various scheduling algorithms as explained later in this chapter\n(see “Job Scheduling” on page 161), but the default one simply maintains a priority\nlist of jobs. Having chosen a job, the jobtracker now chooses a task for the job.\nTasktrackers have a fixed number of slots for map tasks and for reduce tasks: for ex-\nample, a tasktracker may be able to run two map tasks and two reduce tasks simulta-\nneously. (The precise number depends on the number of cores and the amount of\nmemory on the tasktracker; see “Memory” on page 254.) The default scheduler fills\nempty map task slots before reduce task slots, so if the tasktracker has at least one\nempty map task slot, the jobtracker will select a map task; otherwise, it will select a\nreduce task.\nTo choose a reduce task the jobtracker simply takes the next in its list of yet-to-be-run\nreduce tasks, since there are no data locality considerations. For a map task, however,\nit takes account of the tasktracker’s network location and picks a task whose input split\nis as close as possible to the tasktracker. In the optimal case, the task is data-local, that\nis, running on the same node that the split resides on. Alternatively, the task may be\nrack-local: on the same rack, but not the same node, as the split. Some tasks are neither\ndata-local nor rack-local and retrieve their data from a different rack from the one they\nAnatomy of a MapReduce Job Run | 155are running on. You can tell the proportion of each type of task by looking at a job’s\ncounters (see “Built-in Counters” on page 211).\nTask Execution\nNow the tasktracker has been assigned a task, the next step is for it to run the task.\nFirst, it localizes the job JAR by copying it from the shared filesystem to the tasktracker’s\nfilesystem. It also copies any files needed from the distributed cache by the application\nto the local disk; see “Distributed Cache” on page 239 (step 8). Second, it creates a\nlocal working directory for the task, and un-jars the contents of the JAR into this di-\nrectory. Third, it creates an instance of TaskRunner to run the task.\nTaskRunner launches a new Java Virtual Machine (step 9) to run each task in (step 10),\nso that any bugs in the user-defined map and reduce functions don’t affect the task-\ntracker (by causing it to crash or hang, for example). It is however possible to reuse the\nJVM between tasks; see “Task JVM Reuse” on page 170.\nThe child process communicates with its parent through the umbilical interface. This\nway it informs the parent of the task’s progress every few seconds until the task is\ncomplete.\nStreaming and Pipes\nBoth Streaming and Pipes run special map and reduce tasks for the purpose of launching\nthe user-supplied executable and communicating with it (Figure 6-2).\nIn the case of Streaming, the Streaming task communicates with the process (which\nmay be written in any language) using standard input and output streams. The Pipes\ntask, on the other hand, listens on a socket and passes the C++ process a port number\nin its environment, so that on startup, the C++ process can establish a persistent socket\nconnection back to the parent Java Pipes task.\nIn both cases, during execution of the task, the Java process passes input key-value\npairs to the external process, which runs it through the user-defined map or reduce\nfunction and passes the output key-value pairs back to the Java process. From the\ntasktracker’s point of view, it is as if the tasktracker child process ran the map or reduce\ncode itself.\nProgress and Status Updates\nMapReduce jobs are long-running batch jobs, taking anything from minutes to hours\nto run. Because this is a significant length of time, it’s important for the user to get\nfeedback on how the job is progressing. A job and each of its tasks have a status, which\nincludes such things as the state of the job or task (e.g., running, successfully completed,\nfailed), the progress of maps and reduces, the values of the job’s counters, and a status\n156 | Chapter 6: How MapReduce WorksFigure 6-2. The relationship of the Streaming and Pipes executable to the tasktracker and its child\nmessage or description (which may be set by user code). These statuses change over\nthe course of the job, so how do they get communicated back to the client?\nWhen a task is running, it keeps track of its progress, that is, the proportion of the task\ncompleted. For map tasks, this is the proportion of the input that has been processed.\nFor reduce tasks, it’s a little more complex, but the system can still estimate the pro-\nportion of the reduce input processed. It does this by dividing the total progress into\nthree parts, corresponding to the three phases of the shuffle (see “Shuffle and\nSort” on page 163). For example, if the task has run the reducer on half its input, then\nthe task’s progress is 5⁄6, since it has completed the copy and sort phases (1⁄3 each) and\nis half way through the reduce phase (1⁄6).\nAnatomy of a MapReduce Job Run | 157What Constitutes Progress in MapReduce?\nProgress is not always measurable, but nevertheless it tells Hadoop that a task is doing\nsomething. For example, a task writing output records is making progress, even though\nit cannot be expressed as a percentage of the total number that will be written, since\nthe latter figure may not be known, even by the task producing the output.\nProgress reporting is important, as it means Hadoop will not fail a task that’s making\nprogress. All of the following operations constitute progress:\n• Reading an input record (in a mapper or reducer)\n• Writing an output record (in a mapper or reducer)\n• Setting the status description on a reporter (using Reporter’s setStatus() method)\n• Incrementing a counter (using Reporter’s incrCounter() method)\n• Calling Reporter’s progress() method\nTasks also have a set of counters that count various events as the task runs (we saw an\nexample in “A test run” on page 23), either those built into the framework, such as the\nnumber of map output records written, or ones defined by users.\nIf a task reports progress, it sets a flag to indicate that the status change should be sent\nto the tasktracker. The flag is checked in a separate thread every three seconds, and if\nset it notifies the tasktracker of the current task status. Meanwhile, the tasktracker is\nsending heartbeats to the jobtracker every five seconds (this is a minimum, as the\nheartbeat interval is actually dependent on the size of the cluster: for larger clusters,\nthe interval is longer), and the status of all the tasks being run by the tasktracker is sent\nin the call. Counters are sent less frequently than every five seconds, because they can\nbe relatively high-bandwidth.\nThe jobtracker combines these updates to produce a global view of the status of all the\njobs being run and their constituent tasks. Finally, as mentioned earlier, the\nJobClient receives the latest status by polling the jobtracker every second. Clients can\nalso use JobClient’s getJob() method to obtain a RunningJob instance, which contains\nall of the status information for the job.\nThe method calls are illustrated in Figure 6-3.\nJob Completion\nWhen the jobtracker receives a notification that the last task for a job is complete, it\nchanges the status for the job to “successful.” Then, when the JobClient polls for status,\nit learns that the job has completed successfully, so it prints a message to tell the user,\nand then returns from the runJob() method.\n158 | Chapter 6: How MapReduce WorksFigure 6-3. How status updates are propagated through the MapReduce system\nThe jobtracker also sends a HTTP job notification if it is configured to do so. This can\nbe configured by clients wishing to receive callbacks, via the job.end.notifica\ntion.url property.\nLast, the jobtracker cleans up its working state for the job, and instructs tasktrackers\nto do the same (so intermediate output is deleted, for example).\nFailures\nIn the real world, user code is buggy, processes crash, and machines fail. One of the\nmajor benefits of using Hadoop is its ability to handle such failures and allow your job\nto complete.\nTask Failure\nConsider first the case of the child task failing. The most common way that this happens\nis when user code in the map or reduce task throws a runtime exception. If this happens,\nFailures | 159the child JVM reports the error back to its parent tasktracker, before it exits. The error\nultimately makes it into the user logs. The tasktracker marks the task attempt as\nfailed, freeing up a slot to run another task.\nFor Streaming tasks, if the Streaming process exits with a nonzero exit code, it is marked\nas failed. This behavior is governed by the stream.non.zero.exit.is.failure property\n(the default is true).\nAnother failure mode is the sudden exit of the child JVM—perhaps there is a JVM bug\nthat causes the JVM to exit for a particular set of circumstances exposed by the Map-\nReduce user code. In this case, the tasktracker notices that the process has exited, and\nmarks the attempt as failed.\nHanging tasks are dealt with differently. The tasktracker notices that it hasn’t received\na progress update for a while, and proceeds to mark the task as failed. The child JVM\nprocess will be automatically killed after this period.* The timeout period after which\ntasks are considered failed is normally 10 minutes, and can be configured on a per-job\nbasis (or a cluster basis) by setting the mapred.task.timeout property to a value in\nmilliseconds.\nSetting the timeout to a value of zero disables the timeout, so long-running tasks are\nnever marked as failed. In this case, a hanging task will never free up its slot, and over\ntime there may be cluster slowdown as a result. This approach should therefore be\navoided, and making sure that a task is reporting progress periodically will suffice (see\n“What Constitutes Progress in MapReduce?” on page 158).\nWhen the jobtracker is notified of a task attempt that has failed (by the tasktracker’s\nheartbeat call) it will reschedule execution of the task. The jobtracker will try to avoid\nrescheduling the task on a tasktracker where it has previously failed. Furthermore, if a\ntask fails more than four times, it will not be retried further. This value is configurable:\nthe maximum number of attempts to run a task is controlled by the\nmapred.map.max.attempts property for map tasks, and mapred.reduce.max.attempts for\nreduce tasks. By default, if any task fails more than four times (or whatever the maxi-\nmum number of attempts is configured to), the whole job fails.\nFor some applications it is undesirable to abort the job if a few tasks fail, as it may be\npossible to use the results of the job despite some failures. In this case, the maximum\npercentage of tasks that are allowed to fail without triggering job failure can be set for\nthe job. Map tasks and reduce tasks are controlled independently, using the\nmapred.max.map.failures.percent\nand\nmapred.max.reduce.failures.percent\nproperties.\n* If a Streaming process hangs, the tasktracker does not try to kill it (although the JVM that launched it will\nbe killed), so you should take precautions to monitor for this scenario, and kill orphaned processes by some\nother means.\n160 | Chapter 6: How MapReduce WorksA task attempt may also be killed, which is different from it failing. A task attempt may\nbe killed because it is a speculative duplicate (for more, see “Speculative Execu-\ntion” on page 169), or because the tasktracker it was running on failed, and the job-\ntracker marked all the task attempts running on it as killed. Killed task attempts do not\ncount against the number of attempts to run the task (as set by\nmapred.map.max.attempts and mapred.reduce.max.attempts), since it wasn’t the task’s\nfault that an attempt was killed.\nUsers may also kill or fail task attempts using the web UI or the command line (type\nhadoop job to see the options). Jobs may also be killed by the same mechanisms.\nTasktracker Failure\nFailure of a tasktracker is another failure mode. If a tasktracker fails by crashing, or\nrunning very slowly, it will stop sending heartbeats to the jobtracker (or send them very\ninfrequently). The jobtracker will notice a tasktracker that has stopped sending heart-\nbeats (if it hasn’t received one for 10 minutes, configured via the mapred.task\ntracker.expiry.interval property, in milliseconds) and remove it from its pool of\ntasktrackers to schedule tasks on. The jobtracker arranges for map tasks that were run\nand completed successfully on that tasktracker to be rerun if they belong to incomplete\njobs, since their intermediate output residing on the failed tasktracker’s local filesystem\nmay not be accessible to the reduce task. Any tasks in progress are also rescheduled.\nA tasktracker can also be blacklisted by the jobtracker, even if the tasktracker has not\nfailed. A tasktracker is blacklisted if the number of tasks that have failed on it is sig-\nnificantly higher than the average task failure rate on the cluster. Blacklisted tasktrack-\ners can be restarted to remove them from the jobtracker’s blacklist.\nJobtracker Failure\nFailure of the jobtracker is the most serious failure mode. Currently, Hadoop has no\nmechanism for dealing with failure of the jobtracker—it is a single point of failure—\nso in this case the job fails. However, this failure mode has a low chance of occurring\nsince the chance of a particular machine failing is low. It is possible that a future release\nof Hadoop will remove this limitation by running multiple jobtrackers, only one of\nwhich is the primary jobtracker at any time (using ZooKeeper as a coordination mech-\nanism for the jobtrackers to decide who is the primary; see Chapter 13).\nJob Scheduling\nEarly versions of Hadoop had a very simple approach to scheduling users’ jobs: they\nran in order of submission, using a FIFO scheduler. Typically each job would use the\nwhole cluster, so jobs had to wait their turn. Although a shared cluster offers great\npotential for offering large resources to many users, the problem of sharing resources\nJob Scheduling | 161fairly between users requires a better scheduler. Production jobs need to complete in a\ntimely manner, while allowing users who are making smaller ad hoc queries to get\nresults back in a reasonable time.\nLater on, the ability to set a job’s priority was added, via the mapred.job.priority\nproperty or the setJobPriority() method on JobClient (both of which take one of the\nvalues VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW). When the job scheduler is choosing the\nnext job to run, it selects one with the highest priority. However, with the FIFO\nscheduler, priorities do not support preemption, so a high-priority job can still be\nblocked by a long-running low priority job that started before the high-priority job was\nscheduled.\nMapReduce in Hadoop now comes with a choice of schedulers. The default is the\noriginal FIFO queue-based scheduler, and there also a multi-user scheduler called the\nFair Scheduler.\nThe Fair Scheduler\nThe Fair Scheduler aims to give every user a fair share of the cluster capacity over time.\nIf a single job is running, it gets all of the cluster. As more jobs are submitted, free task\nslots are given to the jobs in such a way as to give each user a fair share of the cluster.\nA short job belonging to one user will complete in a reasonable time even while another\nuser’s long job is running, and the long job will still make progress.\nJobs are placed in pools, and by default, each user gets their own pool. A user who\nsubmits more jobs than a second user will not get any more cluster resources than the\nsecond, on average. It is also possible to define custom pools with guaranteed minimum\ncapacities defined in terms of the number of map and reduce slots, and to set weightings\nfor each pool.\nThe Fair Scheduler supports preemption, so if a pool has not received its fair share for\na certain period of time, then the scheduler will kill tasks in pools running over capacity\nin order to give the slots to the pool running under capacity.\nThe Fair Scheduler is a “contrib” module. To enable it, place its JAR file on Hadoop’s\nclasspath, by copying it from Hadoop’s contrib/fairscheduler directory to the lib direc-\ntory. Then set the mapred.jobtracker.taskScheduler property to:\norg.apache.hadoop.mapred.FairScheduler\nThe Fair Scheduler will work without further configuration, but to take full advantage\nof its features and how to configure it (including its web interface), refer to README\nin the src/contrib/fairscheduler directory of the distribution.\n162 | Chapter 6: How MapReduce WorksShuffle and Sort\nMapReduce makes the guarantee that the input to every reducer is sorted by key. The\nprocess by which the system performs the sort—and transfers the map outputs to the\nreducers as inputs—is known as the shuffle.† In this section, we look at how the shuffle\nworks, as a basic understanding would be helpful, should you need to optimize a Map-\nReduce program. The shuffle is an area of the codebase where refinements and\nimprovements are continually being made, so the following description necessarily\nconceals many details (and may change over time). In many ways, the shuffle is the\nheart of MapReduce, and is where the “magic” happens.\nThe Map Side\nWhen the map function starts producing output, it is not simply written to disk. The\nprocess is more involved, and takes advantage of buffering writes in memory and doing\nsome presorting for efficiency reasons. Figure 6-4 shows what happens.\nFigure 6-4. Shuffle and sort in MapReduce\nEach map task has a circular memory buffer that it writes the output to. The buffer is\n100 MB by default, a size which can be tuned by changing the io.sort.mb property.\nWhen the contents of the buffer reaches a certain threshold size (io.sort.spill.per\ncent, default 0.80, or 80%) a background thread will start to spill the contents to disk.\nMap outputs will continue to be written to the buffer while the spill takes place, but if\nthe buffer fills up during this time, the map will block until the spill is complete.\n† The term shuffle is actually imprecise, since in some contexts it refers to only the part of the process where\nmap outputs are fetched by reduce tasks. In this section, we take it to mean the whole process from the point\nwhere a map produces output to where a reduce consumes input.\nShuffle and Sort | 163Spills are written in round-robin fashion to the directories specified by the\nmapred.local.dir property, in a job-specific subdirectory.\nBefore it writes to disk, the thread first divides the data into partitions corresponding\nto the reducers that they will ultimately be sent to. Within each partition, the back-\nground thread performs an in-memory sort by key, and if there is a combiner function,\nit is run on the output of the sort.\nEach time the memory buffer reaches the spill threshold, a new spill file is created, so\nafter the map task has written its last output record there could be several spill files.\nBefore the task is finished, the spill files are merged into a single partitioned and sorted\noutput file. The configuration property io.sort.factor controls the maximum number\nof streams to merge at once; the default is 10.\nIf a combiner function has been specified, and the number of spills is at least three (the\nvalue of the min.num.spills.for.combine property), then the combiner is run before the\noutput file is written. Recall that combiners may be run repeatedly over the input with-\nout affecting the final result. The point is that running combiners makes for a more\ncompact map output, so there is less data to write to local disk and to transfer to the\nreducer.\nIt is often a good idea to compress the map output as it is written to disk, since doing\nso makes it faster to write to disk, saves disk space, and reduces the amount of data to\ntransfer to the reducer. By default the output is not compressed, but it is easy to enable\nby setting mapred.compress.map.output to true. The compression library to use is speci-\nfied by mapred.map.output.compression.codec; see “Compression” on page 77 for more\non compression formats.\nThe output file’s partitions are made available to the reducers over HTTP. The number\nof worker threads used to serve the file partitions is controlled by the task\ntracker.http.threads property—this setting is per tasktracker, not per map task slot.\nThe default of 40 may need increasing for large clusters running large jobs.\nThe Reduce Side\nLet’s turn now to the reduce part of the process. The map output file is sitting on the\nlocal disk of the tasktracker that ran the map task (note that although map outputs\nalways get written to the local disk of the map tasktracker, reduce outputs may not be),\nbut now it is needed by the tasktracker that is about to run the reduce task for the\npartition. Furthermore, the reduce task needs the map output for its particular partition\nfrom several map tasks across the cluster. The map tasks may finish at different times,\nso the reduce task starts copying their outputs as soon as each completes. This is known\nas the copy phase of the reduce task. The reduce task has a small number of copier\nthreads so that it can fetch map outputs in parallel. The default is five threads, but this\nnumber can be changed by setting the mapred.reduce.parallel.copies property.\n164 | Chapter 6: How MapReduce WorksHow do reducers know which tasktrackers to fetch map output from?\nAs map tasks complete successfully, they notify their parent tasktracker\nof the status update, which in turn notifies the jobtracker. These noti-\nfications are transmitted over the heartbeat communication mechanism\ndescribed earlier. Therefore, for a given job, the jobtracker knows the\nmapping between map outputs and tasktrackers. A thread in the reducer\nperiodically asks the jobtracker for map output locations until it has\nretrieved them all.\nTasktrackers do not delete map outputs from disk as soon as the first\nreducer has retrieved them, as the reducer may fail. Instead, they wait\nuntil they are told to delete them by the jobtracker, which is after the\njob has completed.\nThe map outputs are copied to the reduce tasktracker’s memory if they are small enough\n(the buffer’s size is controlled by mapred.job.shuffle.input.buffer.percent, which\nspecifies the proportion of the heap to use for this purpose); otherwise, they are copied\nto disk. When the in-memory buffer reaches a threshold size (controlled by\nmapred.job.shuffle.merge.percent), or reaches a threshold number of map outputs\n(mapred.inmem.merge.threshold), it is merged and spilled to disk.\nAs the copies accumulate on disk, a background thread merges them into larger, sorted\nfiles. This saves some time merging later on. Note that any map outputs that were\ncompressed (by the map task) have to be decompressed in memory in order to perform\na merge on them.\nWhen all the map outputs have been copied, the reduce task moves into the sort\nphase (which should properly be called the merge phase, as the sorting was carried out\non the map side), which merges the map outputs, maintaining their sort ordering. This\nis done in rounds. For example, if there were 50 map outputs, and the merge factor was\n10 (the default, controlled by the io.sort.factor property, just like in the map’s merge),\nthen there would be 5 rounds. Each round would merge 10 files into one, so at the end\nthere would be five intermediate files.\nRather than have a final round that merges these five files into a single sorted file, the\nmerge saves a trip to disk by directly feeding the reduce function in what is the last\nphase: the reduce phase. This final merge can come from a mixture of in-memory and\non-disk segments.\nShuffle and Sort | 165The number of files merged in each round is actually more subtle than\nthis example suggests. The goal is to merge the minimum number of\nfiles to get to the merge factor for the final round. So if there were 40\nfiles, the merge would not merge 10 files in each of the four rounds to\nget 4 files. Instead, the first round would merge only 4 files, and the\nsubsequent three rounds would merge the full 10 files. The 4 merged\nfiles, and the 6 (as yet unmerged) files make a total of 10 files for the\nfinal round.\nNote that this does not change the number of rounds, it’s just an opti-\nmization to minimize the amount of data that is written to disk, since\nthe final round always merges directly into the reduce.\nDuring the reduce phase the reduce function is invoked for each key in the sorted\noutput. The output of this phase is written directly to the output filesystem, typically\nHDFS. In the case of HDFS, since the tasktracker node is also running a datanode, the\nfirst block replica will be written to the local disk.\nConfiguration Tuning\nWe are now in a better position to understand how to tune the shuffle to improve\nMapReduce performance. The relevant settings, which can be used on a per-job basis\n(except where noted), are summarized in Tables 6-1 and 6-2, along with the defaults,\nwhich are good for general-purpose jobs.\nThe general principle is to give the shuffle as much memory as possible. However, there\nis a trade-off, in that you need to make sure that your map and reduce functions get\nenough memory to operate. This is why it is best to write your map and reduce functions\nto use as little memory as possible—certainly they should not use an unbounded\namount of memory (by avoiding accumulating values in a map, for example).\nThe amount of memory given to the JVMs in which the map and reduce tasks run is\nset by the mapred.child.java.opts property. You should try to make this as large as\npossible for the amount of memory on you task nodes; the discussion in “Mem-\nory” on page 254 goes through the constraints to consider.\nOn the map side, the best performance can be obtained by avoiding multiple spills to\ndisk; one is optimal. If you can estimate the size of your map outputs, then you can set\nthe io.sort.* properties appropriately to minimize the number of spills. In particular,\nyou should increase io.sort.mb if you can. There is a MapReduce counter (“Spilled\nrecords”; see “Counters” on page 211) that counts the total number of records that\nwere spilled to disk over the course of a job, which can be useful for tuning. Note that\nthe counter includes both map and reduce side spills.\nOn the reduce side, the best performance is obtained when the intermediate data can\nreside entirely in memory. By default, this does not happen, since for the general case\nall the memory is reserved for the reduce function. But if your reduce function has light\n166 | Chapter 6: How MapReduce Worksmemory requirements, then setting mapred.inmem.merge.threshold to 0 and\nmapred.job.reduce.input.buffer.percent to 1.0 (or a lower value; see Table 6-2) may\nbring a performance boost.\nMore generally, Hadoop’s uses a buffer size of 4 KB by default, which is low, so you\nshould increase this across the cluster (by setting io.file.buffer.size, see also “Other\nHadoop Properties” on page 264).\nIn April 2008, Hadoop won the general-purpose terabyte sort benchmark (described\nin “TeraByte Sort on Apache Hadoop” on page 461), and one of the optimizations\nused was this one of keeping the intermediate data in memory on the reduce side.\nTable 6-1. Map-side tuning properties\nProperty name Type Default value Description\nio.sort.mb int 100 The size, in megabytes, of the mem-\n                  ory buffer to use while sorting map\n                 output.\nio.sort.record.percent float 0.05 The proportion of io.sort.mb re-\n                                 served for storing record boundaries\n                                of the map outputs. The remaining\n                               space is used for the map output re-\n                              cords themselves.\nio.sort.spill.percent float 0.80 The threshold usage proportion for\n                                both the map output memory buffer\n                               and the record boundaries index to\n                              start the process of spilling to disk.\nio.sort.factor int 10 The maximum number of streams to\n                     merge at once when sorting files.\n                    This property is also used in the re-\n                   duce. It’s fairly common to increase\n                    this to 100.\nmin.num.spills.for. int 3 The minimum number of spill files\ncombine needed for the combiner to run (if a\n       combiner is specified).\nmapred.compress.map. boolean false Compress map outputs.\noutput \nmapred.map.output. Class name org.apache.hadoop.io. The compression codec to use for\ncompression.codec compress.DefaultCodec map outputs.\ntask int 40 The number of worker threads per\ntracker.http.threads tasktracker for serving the map out-\n                    puts to reducers. This is a cluster-\n                   wide setting and cannot be set by\n                  individual jobs.\nShuffle and Sort | 167Table 6-2. Reduce-side tuning properties\nProperty name Type Default value Description\nmapred.reduce.parallel. int 5 The number of threads used to copy map outputs\ncopies to the reducer.\nmapred.reduce.copy.backoff int 300 The maximum amount of time, in seconds, to spend\n                                  retrieving one map output for a reducer before de-\n                                 claring it as failed. The reducer may repeatedly re-\n                                attempt a transfer within this time if it fails (using\n                               exponential backoff).\nio.sort.factor int 10 The maximum number of streams to merge at once\n                     when sorting files. This property is also used in the\n                    map.\nmapred.job.shuffle.input. float 0.70 The proportion of total heap size to be allocated to\nbuffer.percent the map outputs buffer during the copy phase of the\n              shuffle.\nmapred.job.shuffle.merge. float 0.66 The threshold usage proportion for the map outputs\npercent buffer (defined by mapred.job.shuf\n       fle.input.buffer.percent) for starting\n      the process of merging the outputs and spilling to\n     disk.\nmapred.inmem.merge.threshold int 1000 The threshold number of map outputs for starting\n                                     the process of merging the outputs and spilling to\n                                    disk. A value of 0 or less means there is no threshold,\n                                   and the spill behavior is governed solely by\n                                  mapred.job.shuffle.merge.percent.\nmapred.job.reduce.input. float 0.0 The proportion of total heap size to be used for re-\nbuffer.percent taining map outputs in memory during the reduce.\n              For the reduce phase to begin, the size of map out-\n             puts in memory must be no more than this size. By\n            default, all map outputs are merged to disk before\n           the reduce begins, to give the reduces as much\n          memory as possible. However, if your reducers re-\n         quire less memory, this value may be increased to\n        minimize the number of trips to disk.\nTask Execution\nWe saw how the MapReduce system executes tasks in the context of the overall job at\nthe beginning of the chapter in “Anatomy of a MapReduce Job Run” on page 153. In\nthis section, we’ll look at some more controls that MapReduce users have over task\nexecution.\n168 | Chapter 6: How MapReduce WorksSpeculative Execution\nThe MapReduce model is to break jobs into tasks and run the tasks in parallel to make\nthe overall job execution time smaller than it would otherwise be if the tasks ran se-\nquentially. This makes job execution time sensitive to slow-running tasks, as it takes\nonly one slow task to make the whole job take significantly longer than it would have\ndone otherwise. When a job consists of hundreds or thousands of tasks, the possibility\nof a few straggling tasks is very real.\nTasks may be slow for various reasons, including hardware degradation, or software\nmis-configuration, but the causes may be hard to detect since the tasks still complete\nsuccessfully, albeit after a longer time than expected. Hadoop doesn’t try to diagnose\nand fix slow-running tasks; instead, it tries to detect when a task is running slower than\nexpected and launches another, equivalent, task as a backup. This is termed speculative\nexecution of tasks.\nIt’s important to understand that speculative execution does not work by launching\ntwo duplicate tasks at about the same time so they can race each other. This would be\nwasteful of cluster resources. Rather, a speculative task is launched only after all the\ntasks for a job have been launched, and then only for tasks that have been running for\nsome time (at least a minute), and have failed to make as much progress, on average,\nas the other tasks from the job. When a task completes successfully, any duplicate tasks\nthat are running are killed since they are no longer needed. So if the original task com-\npletes before the speculative task then the speculative task is killed; on the other hand,\nif the speculative task finishes first, then the original is killed.\nSpeculative execution is an optimization, not a feature to make jobs run more reliably.\nIf there are bugs that sometimes cause a task to hang or slow down, then relying on\nspeculative execution to avoid these problems is unwise, and won’t work reliably, since\nthe same bugs are likely to affect the speculative task. You should fix the bug so that\nthe task doesn’t hang or slow down.\nSpeculative execution is turned on by default. It can be enabled or disabled independ-\nently for map tasks and reduce tasks, on a cluster-wide basis, or on a per-job basis. The\nrelevant properties are shown in Table 6-3.\nTable 6-3. Speculative execution properties\nProperty name Type Default value Description\nmapred.map.tasks.speculative.execution boolean true Whether extra instances of map\n                                                   tasks may be launched if a task is\n                                                  making slow progress.\nmapred.reduce.tasks.speculative. boolean true Whether extra instances of re-\nexecution duce tasks may be launched if a\n         task is making slow progress.\nTask Execution | 169Why would you ever want to turn off speculative execution? The goal of speculative\nexecution is reducing job execution time, but this comes at the cost of cluster efficiency.\nOn a busy cluster speculative execution can reduce overall throughput, since redundant\ntasks are being executed in an attempt to bring down the execution time for a single\njob. For this reason, some cluster administrators prefer to turn it off on the cluster, and\nhave users explicitly turn it on for individual jobs. This was especially relevant for older\nversions of Hadoop, when speculative execution could be overly aggressive in sched-\nuling speculative tasks.\nTask JVM Reuse\nHadoop runs tasks in their own Java Virtual Machine to isolate them from other run-\nning tasks. The overhead of starting a new JVM for each task can take around a second,\nwhich for jobs that run for a minute or so is insignificant. However, jobs that have a\nlarge number of very short-lived tasks (these are usually map tasks) or that have lengthy\ninitialization, can see performance gains when the JVM is reused for subsequent tasks.\nWith task JVM reuse enabled, tasks do not run concurrently in a single JVM. The JVM\nruns tasks sequentially. Tasktrackers can however run more than one task at a time,\nbut this is always done in separate JVMs. The properties for controlling the tasktrackers\nnumber of map task slots and reduce task slots are discussed in “Memory”\non page 254.\nThe property for controlling task JVM reuse is mapred.job.reuse.jvm.num.tasks: it\nspecifies the maximum number of tasks to run for a given job for each JVM launched,\nthe default is 1 (see Table 6-4). Tasks from different jobs are always run in separate\nJVMs. If the property is set to –1, there is no limit to the number of tasks from the same\njob that may share a JVM. The method setNumTasksToExecutePerJvm() on JobConf can\nalso be used to configure this property.\nTable 6-4. Task JVM Reuse properties\nProperty name Type Default value Description\nmapred.job.reuse.jvm.num.tasks int 1 The maximum number of tasks to run for a given\n                                    job for each JVM on a tasktracker. A value of –1\n                                     indicates no limit: the same JVM may be used for\n                                    all tasks for a job.\nTasks that are CPU-bound may also benefit from task JVM reuse by taking advantage\nof runtime optimizations applied by the HotSpot JVM. After running for a while, the\nHotSpot JVM builds up enough information to detect performance-critical sections in\nthe code, and dynamically translates the Java byte codes of these hot spots into native\nmachine code. This works well for long-running processes, but JVMs that run for sec-\nonds or a few minutes may not gain the full benefit of HotSpot. In these cases, it is\nworth enabling task JVM reuse.\n170 | Chapter 6: How MapReduce WorksAnother place where a shared JVM is useful is for sharing state between the tasks of a\njob. By storing reference data in a static field, tasks get rapid access to the shared data.\nSkipping Bad Records\nLarge datasets are messy. They often have corrupt records. They often have records\nthat are in a different format. They often have missing fields. In an ideal world, your\ncode would cope gracefully with all of these conditions. In practice, it is often expedient\nto ignore the offending records. Depending on the analysis being performed, if only a\nsmall percentage of records are affected, then skipping them may not significantly affect\nthe result. However, if a task trips up when it encounters a bad record—by throwing\na runtime exception—then the task fails. Failing tasks are retried (since the failure may\nbe due to hardware failure or some other reason outside the task’s control), but if a\ntask fails four times, then the whole job is marked as failed (see “Task Fail-\nure” on page 159). If it is the data that is causing the task to throw an exception,\nrerunning the task won’t help, since it will fail in exactly the same way each time.\nIf you are using TextInputFormat (“TextInputFormat” on page 196),\nthen you can set a maximum expected line length to safeguard against\ncorrupted files. Corruption in a file can manifest itself as a very long line,\nwhich can cause out of memory errors and then task failure. By setting\nmapred.linerecordreader.maxlength to a value in bytes that fits in mem-\nory (and is comfortably greater than the length of lines in your input\ndata), the record reader will skip the (long) corrupt lines without the\ntask failing.\nThe best way to handle corrupt records is in your mapper or reducer code. You can\ndetect the bad record and ignore it, or you can abort the job by throwing an exception.\nYou can also count the total number of bad records in the job using counters to see\nhow widespread the problem is.\nIn rare cases, though, you can’t handle the problem because there is a bug in a third-\nparty library that you can’t work around in your mapper or reducer. In these cases, you\ncan use Hadoop’s optional skipping mode for automatically skipping bad records.\nWhen skipping mode is enabled, tasks report the records being processed back to the\ntasktracker. When the task fails, the tasktracker retries the task, skipping the records\nthat caused the failure. Because of the extra network traffic and bookkeeping to main-\ntain the failed record ranges, skipping mode is turned on for a task only after it has\nfailed twice.\nThus for a task consistently failing on a bad record, the tasktracker runs the following\ntask attempts with these outcomes:\nTask Execution | 1711.\n2.\n3.\n4.\nTask fails.\nTask fails.\nSkipping mode is enabled. Task fails but failed record is stored by the tasktracker.\nSkipping mode is still enabled. Task succeeds by skipping the bad record that failed\nin the previous attempt.\nSkipping mode is off by default; you enable it independently for map and reduce tasks\nusing the SkipBadRecords class. It’s important to note that skipping mode can detect\nonly one bad record per task attempt, so this mechanism is appropriate only for de-\ntecting occasional bad records (a few per task, say). You may need to increase the\nmaximum number of task attempts (via mapred.map.max.attempts and\nmapred.reduce.max.attempts) to give skipping mode enough attempts to detect and skip\nall the bad records in an input split.\nBad records that have been detected by Hadoop are saved as sequence files in the job’s\noutput directory under the _logs/skip subdirectory. These can be inspected for diag-\nnostic purposes after the job has completed (using hadoop fs -text, for example).\nThe Task Execution Environment\nHadoop provides information to a map or reduce task about the environment in which\nit is running. For example, a map task can discover the name of the file it is processing\n(see “File information in the mapper” on page 192), and a map or reduce task can find\nout the attempt number of the task. The properties in Table 6-5 can be accessed from\nthe job’s configuration, obtained by providing an implementation of the configure()\nmethod for Mapper or Reducer, where the configuration is passed in as an argument.\nTable 6-5. Task environment properties\nProperty name Type Description Example\nmapred.job.id String The job ID. (See “Job, job_200811201130_0004\n                       Task, and Task Attempt \n                       IDs” on page 133 for a \n                         description of the \n                         format.) \nmapred.tip.id String The task ID. task_200811201130_0004_m_000003\nmapred.task.id String The task attempt ID. attempt_200811201130_0004_m_000003_0\n                      (Not the task ID.) \nmapred.task. int The ID of the task within 3\npartition the job. \nmapred.task.is.map boolean Whether this task is a true\n                           map task. \n172 | Chapter 6: How MapReduce WorksStreaming environment variables\nHadoop sets job configuration parameters as environment variables for Streaming pro-\ngrams. However, it replaces nonalphanumeric characters with underscores to make\nsure they are valid names. The following Python expression illustrates how you can\nretrieve the value of the mapred.job.id property from within a Python Streaming script:\nos.environ[""mapred_job_id""]\nYou can also set environment variables for the Streaming processes launched by Map-\nReduce by supplying the -cmdenv option to the Streaming launcher program (once for\neach variable you wish to set). For example, the following sets the MAGIC_PARAMETER\nenvironment variable:\n-cmdenv MAGIC_PARAMETER=abracadabra\nTask side-effect files\nThe usual way of writing output from map and reduce tasks is by using the OutputCol\nlector to collect key-value pairs. Some applications need more flexibility than a single\nkey-value pair model, so these applications write output files directly from the map or\nreduce task to a distributed filesystem, like HDFS. (There are other ways to produce\nmultiple outputs, too, as described in “Multiple Outputs” on page 203.)\nCare needs to be taken to ensure that multiple instances of the same task don’t try to\nwrite to the same file. There are two problems to avoid: if a task failed and was retried,\nthen the old partial output would still be present when the second task ran, and it would\nhave to delete the old file first. Second, with speculative execution enabled, two in-\nstances of the same task could try to write to the same file simultaneously.\nHadoop solves this problem for the regular outputs from a task by writing outputs to\na temporary directory that is specific to that task attempt. The directory is ${mapred.out\nput.dir}/_temporary/${mapred.task.id}. On successful completion of the task, the\ncontents of the directory are copied to the job’s output directory (${mapred.out\nput.dir}). Thus if the task fails and is retried, the first attempt’s partial output will just\nbe cleaned up. A task and another speculative instance of the same task will get separate\nworking directories, and only the first to finish will have the content of its working\ndirectory promoted to the output directory—the other will be discarded.\nThe way that a task’s output is committed on completion is implemen-\nted by an OutputCommitter, which is associated with the OutputFormat.\nThe OutputCommitter for FileOutputFormat is a FileOutputCommitter,\nwhich implements the commit protocol described earlier. The getOut\nputCommitter() method on OutputFormat may be overridden to return a\ncustom OutputCommitter, in case you want to implement the commit\nprocess in a different way.\nTask Execution | 173Hadoop provides a mechanism for application writers to use this feature, too. A task\nmay find its working directory by retrieving the value of the mapred.work.output.dir\nproperty from its configuration file. Alternatively, a MapReduce program using the Java\nAPI may call the getWorkOutputPath() static method on FileOutputFormat to get the\nPath object representing the working directory. The framework creates the working\ndirectory before executing the task, so you don’t need to create it.\nTo take a simple example, imagine a program for converting image files from one format\nto another. One way to do this is to have a map-only job, where each map is given a\nset of images to convert (perhaps using NLineInputFormat; see “NLineInputFor-\nmat” on page 198). If a map task writes the converted images into its working directory,\nthen they will be promoted to the output directory when the task successfully finishes.\n174 | Chapter 6: How MapReduce WorksCHAPTER 7\nMapReduce Types and Formats\nMapReduce has a simple model of data processing: inputs and outputs for the map and\nreduce functions are key-value pairs. This chapter looks at the MapReduce model in\ndetail, and, in particular, how data in various formats, from simple text to structured\nbinary objects, can be used with this model.\nMapReduce Types\nThe map and reduce functions in Hadoop MapReduce have the following general form:\nmap: (K1, V1) → list(K2, V2)\nreduce: (K2, list(V2)) → list(K3, V3)\nIn general, the map input key and value types (K1 and V1) are different from the map\noutput types ( K2 and V2). However, the reduce input must have the same types as the\nmap output, although the reduce output types may be different again (K3 and V3). The\nJava interfaces mirror this form:\npublic interface Mapper<K1, V1, K2, V2> extends JobConfigurable, Closeable {\n}\nvoid map(K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter)\nthrows IOException;\npublic interface Reducer<K2, V2, K3, V3> extends JobConfigurable, Closeable {\n}\nvoid reduce(K2 key, Iterator<V2> values,\nOutputCollector<K3, V3> output, Reporter reporter) throws IOException;\nRecall that the OutputCollector is purely for emitting key-value pairs (and is hence\nparameterized with their types), while the Reporter is for updating counters and status.\n(In the new MapReduce API in release 0.20.0 and later, these two functions are com-\nbined in a single context object.)\nIf a combine function is used then it is the same form as the reduce function (and is an\nimplementation of Reducer), except its output types are the intermediate key and value\ntypes (K2 and V2), so they can feed the reduce function:\n175map: (K1, V1) → list(K2, V2)\ncombine: (K2, list(V2)) → list(K2, V2)\nreduce: (K2, list(V2)) → list(K3, V3)\nOften the combine and reduce functions are the same, in which case, K3 is the same as\nK2, and V3 is the same as V2.\nThe partition function operates on the intermediate key and value types (K2 and V2),\nand returns the partition index. In practice, the partition is determined solely by the\nkey (the value is ignored):\npartition: (K2, V2) → integer\nOr in Java:\npublic interface Partitioner<K2, V2> extends JobConfigurable {\n}\nint getPartition(K2 key, V2 value, int numPartitions);\nSo much for the theory, how does this help configure MapReduce jobs? Table 7-1\nsummarizes the configuration options. It is divided into the properties that determine\nthe types, and those that have to be compatible with the configured types.\n176 | Chapter 7: MapReduce Types and FormatsInput types are set by the input format. So, for instance, a TextInputFormat generates\nkeys of type LongWritable and values of type Text. The other types are set explicitly by\ncalling the methods on the JobConf. If not set explicitly, the intermediate types default\nto the (final) output types, which default to LongWritable and Text. So if K2 and K3 are\nthe same, you don’t need to call setMapOutputKeyClass(), since it falls back to the type\nMapReduce Types | 177\nsetOutputKeyComparatorClass() setOutputValueGroupingComparator() setReducerClass() setOutputFormat()\nOutput typesset by calling setOutputKeyClass(). Similarly, if V2 and V3 are the same, you only need\nto use setOutputValueClass().\nIt may seem strange that these methods for setting the intermediate and final output\ntypes exist at all. After all, why can’t the types be determined from a combination of\nthe mapper and the reducer? The answer is that it’s to do with a limitation in Java\ngenerics: type erasure means that the type information isn’t always present at runtime,\nso Hadoop has to be given it explicitly. This also means that it’s possible to configure\na MapReduce job with incompatible types, because the configuration isn’t checked at\ncompile time. The settings that have to be compatible with the MapReduce types are\nlisted in the lower part of Table 7-1. Type conflicts are detected at runtime during job\nexecution, and for this reason, it is wise to run a test job using a small amount of data\nto flush out and fix any type incompatibilities.\nThe Default MapReduce Job\nWhat happens when you run MapReduce without setting a mapper or a reducer? Let’s\ntry it by running this minimal MapReduce program:\npublic class MinimalMapReduce extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nSystem.err.printf(""Usage: %s [generic options] <input> <output>\\n"",\ngetClass().getSimpleName());\nToolRunner.printGenericCommandUsage(System.err);\nreturn -1;\n}\n}\n}\nJobConf conf = new JobConf(getConf(), getClass());\nFileInputFormat.addInputPath(conf, new Path(args[0]));\nFileOutputFormat.setOutputPath(conf, new Path(args[1]));\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MinimalMapReduce(), args);\nSystem.exit(exitCode);\n}\nThe only configuration that we set is an input path and an output path. We run it over\na subset of our weather data with the following:\n% hadoop MinimalMapReduce ""input/ncdc/all/190{1,2}.gz"" output\nWe do get some output: one file named part-00000 in the output directory. Here’s what\nthe first few lines look like (truncated to fit the page):\n178 | Chapter 7: MapReduce Types and Formats0→0029029070999991901010106004+64333+023450FM-12+000599999V0202701N01591...\n0→0035029070999991902010106004+64333+023450FM-12+000599999V0201401N01181...\n135→0029029070999991901010113004+64333+023450FM-12+000599999V0202901N00821...\n141→0035029070999991902010113004+64333+023450FM-12+000599999V0201401N01181...\n270→0029029070999991901010120004+64333+023450FM-12+000599999V0209991C00001...\n282→0035029070999991902010120004+64333+023450FM-12+000599999V0201401N01391...\nEach line is an integer followed by a tab character, followed by the original weather\ndata record. Admittedly, it’s not a very useful program, but understanding how it pro-\nduces its output does provide some insight into the defaults that Hadoop uses when\nrunning MapReduce jobs. Example 7-1 shows a program that has exactly the same\neffect as MinimalMapReduce, but explicitly sets the job settings to their defaults.\nExample 7-1. A minimal MapReduce driver, with the defaults explicitly set\npublic class MinimalMapReduceWithDefaults extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(TextInputFormat.class);\nconf.setNumMapTasks(1);\nconf.setMapperClass(IdentityMapper.class);\nconf.setMapRunnerClass(MapRunner.class);\nconf.setMapOutputKeyClass(LongWritable.class);\nconf.setMapOutputValueClass(Text.class);\nconf.setPartitionerClass(HashPartitioner.class);\nconf.setNumReduceTasks(1);\nconf.setReducerClass(IdentityReducer.class);\nconf.setOutputKeyClass(LongWritable.class);\nconf.setOutputValueClass(Text.class);\nconf.setOutputFormat(TextOutputFormat.class);\nJobClient.runJob(conf);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MinimalMapReduceWithDefaults(), args);\nSystem.exit(exitCode);\n}\nWe’ve simplified the first few lines of the run() method, by extracting the logic for\nprinting usage and setting the input and output paths into a helper method. Almost all\nMapReduce Types | 179MapReduce drivers take these two arguments (input and output), so reducing the\nboilerplate code here is a good thing. Here are the relevant methods in the Job\nBuilder class for reference:\npublic static JobConf parseInputAndOutput(Tool tool, Configuration conf,\nString[] args) {\n}\nif (args.length != 2) {\nprintUsage(tool, ""<input> <output>"");\nreturn null;\n}\nJobConf jobConf = new JobConf(conf, tool.getClass());\nFileInputFormat.addInputPath(jobConf, new Path(args[0]));\nFileOutputFormat.setOutputPath(jobConf, new Path(args[1]));\nreturn jobConf;\npublic static void printUsage(Tool tool, String extraArgsUsage) {\nSystem.err.printf(""Usage: %s [genericOptions] %s\\n\\n"",\ntool.getClass().getSimpleName(), extraArgsUsage);\nGenericOptionsParser.printGenericCommandUsage(System.err);\n}\nGoing back to MinimalMapReduceWithDefaults in Example 7-1, although there are many\nother default job settings, the ones highlighted are those most central to running a job.\nLet’s go through them in turn.\nThe default input format is TextInputFormat, which produces keys of type LongWrita\nble (the offset of the beginning of the line in the file) and values of type Text (the line\nof text). This explains where the integers in the final output come from: they are the\nline offsets.\nDespite appearances, the setNumMapTasks() call does not necessarily set the number of\nmap tasks to one, in fact. It is a hint, and the actual number of map tasks depends on\nthe size of the input, and the file’s block size (if the file is in HDFS). This is discussed\nfurther in “FileInputFormat input splits” on page 188.\nThe default mapper is IdentityMapper, which writes the input key and value unchanged\nto the output:\npublic class IdentityMapper<K, V>\nextends MapReduceBase implements Mapper<K, V, K, V> {\n}\npublic void map(K key, V val,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\noutput.collect(key, val);\n}\nIdentityMapper is a generic type, which allows it to work with any key or value types,\nwith the restriction that the map input and output keys are of the same type, and the\n180 | Chapter 7: MapReduce Types and Formatsmap input and output values are of the same type. In this case, the map output key is\nLongWritable and the map output value is Text.\nMap tasks are run by MapRunner, the default implementation of MapRunnable that calls\nthe Mapper’s map() method sequentially with each record.\nThe default partitioner is HashPartitioner, which hashes a record’s key to determine\nwhich partition the record belongs in. Each partition is processed by a reduce task, so\nthe number of partitions is equal to the number of reduce tasks for the job:\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\npublic void configure(JobConf job) {}\npublic int getPartition(K2 key, V2 value,\nint numPartitions) {\nreturn (key.hashCode() & Integer.MAX_VALUE) % numPartitions;\n}\n}\nThe key’s hash code is turned into a nonnegative integer by bitwise ANDing it with the\nlargest integer value. It is then reduced modulo the number of partitions to find the\nindex of the partition that the record belongs in.\nBy default, there is a single reducer, and therefore a single partition, so the action of\nthe partitioner is irrelevant in this case since everything goes into one partition. How-\never, it is important to understand the behavior of HashPartitioner when you have\nmore than one reduce task. Assuming the key’s hash function is a good one, the records\nwill be evenly allocated across reduce tasks, with all records sharing the same key being\nprocessed by the same reduce task.\nChoosing the Number of Reducers\nThe single reducer default is something of a gotcha for new users to Hadoop.Almost\nall real-world jobs should set this to a larger number; otherwise, the job will be very\nslow since all the intermediate data flows through a single reduce task. (Note that when\nrunning under the local job runner, only zero or one reducers are supported.)\nThe optimal number of reducers is related to the total number of available reducer slots\nin your cluster. The total number of slots is given by the product of the number of nodes\nin the cluster and the value of the mapred.tasktracker.reduce.tasks.maximum property\n(see “Environment Settings” on page 254).\nOne common setting is to have slightly fewer reducers than total slots, which gives one\nwave of reduce tasks (and tolerates a few failures, without extending job execution\ntime). If your reduce tasks are very big, then it makes sense to have a larger number of\nreducers (resulting in two waves, for example) so that the tasks are more fine-grained,\nand failure doesn’t affect job execution time significantly.\nMapReduce Types | 181The default reducer is IdentityReducer, again a generic type, which simply writes all\nits input to its output:\npublic class IdentityReducer<K, V>\nextends MapReduceBase implements Reducer<K, V, K, V> {\n}\npublic void reduce(K key, Iterator<V> values,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\nwhile (values.hasNext()) {\noutput.collect(key, values.next());\n}\n}\nFor this job the output key is LongWritable, and the output value is Text. In fact, all the\nkeys for this MapReduce program are LongWritable, and all the values are Text, since\nthese are the input keys and values, and the map and reduce functions are both identity\nfunctions which by definition preserve type. Most MapReduce programs, however,\ndon’t use the same key or value types throughout, so you need to configure the job to\ndeclare the types you are using, as described in the previous section.\nRecords are sorted by the MapReduce system before being presented to the reducer.\nIn this case the keys are sorted numerically, which has the effect of interleaving the lines\nfrom the input files into one combined output file.\nThe default output format is TextOutputFormat, which writes out records, one per line,\nby converting keys and values to strings and separating them with a tab character. This\nis why the output is tab-separated: it is a feature of TextOutputFormat.\nThe default Streaming job\nIn Streaming, the default job is similar, but not identical, to the Java equivalent. The\nminimal form is:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-mapper /bin/cat\nNotice that you have to supply a mapper: the default identity mapper will not work.\nThe reason has to do with the default input format, TextInputFormat, which generates\nLongWritable keys and Text values. However, Streaming output keys and values (in-\ncluding the map keys and values) are always both of type Text.* The identity mapper\ncannot change LongWritable keys to Text keys, so it fails.\nWhen we specify a non-Java mapper, and the input format is TextInputFormat, Stream-\ning does something special. It doesn’t pass the key to the mapper process, it just passes\n* Except when used in binary mode, from version 0.21.0 onward, via the -io rawbytes or -io typedbytes\noptions. Text mode (-io text) is the default.\n182 | Chapter 7: MapReduce Types and Formatsthe value. This is actually very useful, since the key is just the line offset in the file, and\nthe value is the line, which is all most applications are interested in. The overall effect\nof this job is to perform a sort of the input.\nWith more of the defaults spelled out, the command looks like this:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-input input/ncdc/sample.txt \\\n-output output \\\n-inputformat org.apache.hadoop.mapred.TextInputFormat \\\n-mapper /bin/cat \\\n-partitioner org.apache.hadoop.mapred.lib.HashPartitioner \\\n-numReduceTasks 1 \\\n-reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n-outputformat org.apache.hadoop.mapred.TextOutputFormat\nThe mapper and reducer arguments take a command or a Java class. A combiner may\noptionally be specified, using the -combiner argument.\nKeys and values in Streaming\nA Streaming application can control the separator that is used when a key-value pair is\nturned into a series of bytes and sent to the map or reduce process over standard input.\nThe default is a tab character, but it is useful to be able to change it in the case that the\nkeys or values themselves contain tab characters.\nSimilarly, when the map or reduce writes out key-value pairs, they may be separated\nby a configurable separator. Furthermore, the key from the output can be composed\nof more than the first field: it can be made up of the first n fields (defined by\nstream.num.map.output.key.fields or stream.num.reduce.output.key.fields), with\nthe value being the remaining fields. For example, if the output from a Streaming proc-\ness was a,b,c (and the separator is a comma), and n is two, then the key would be\nparsed as a,b and the value as c.\nSeparators may be configured independently for maps and reduces. The properties are\nlisted in Table 7-2 and shown in a diagram of the data flow path in Figure 7-1.\nThese settings do not have any bearing on the input and output formats. For example,\nif stream.reduce.output.field.separator were set to be a colon, say, and the reduce\nstream process wrote the line a:b to standard out, then the Streaming reducer would\nknow to extract the key as a and the value as b. With the standard TextOutputFormat,\nthis record would be written to the output file with a tab separating a and b. You can\nchange the separator that TextOutputFormat uses by setting mapred.textoutputfor\nmat.separator.\nMapReduce Types | 183Figure 7-1. Where separators are used in a Streaming MapReduce job\nTable 7-2. Streaming separator properties\nProperty name Type Default value Description\nstream.map.input.field. String \\t The separator to use when passing the input key\nseparator and value strings to the stream map process as a\n         stream of bytes.\nstream.map.output.field. String \\t The separator to use when splitting the output from\nseparator the stream map process into key and value strings\n         for the map output.\nstream.num.map. int 1 The number of fields separated by\noutput.key.fields \nstream.reduce.input.field. String \\t The separator to use when passing the input key\nseparator and value strings to the stream reduce process as a\n         stream of bytes.\nstream.reduce.output.field. String \\t The separator to use when splitting the output from\nseparator the stream reduce process into key and value strings\n         for the final reduce output.\nstream.num.reduce. int 1 The number of fields separated by\noutput.key.fields \nstream.map.output.field.separator\nto treat as the map output key.\nstream.reduce.output.field.separa\ntor to treat as the reduce output key.\nInput Formats\nHadoop can process many different types of data formats, from flat text files to data-\nbases. In this section, we explore the different formats available.\n184 | Chapter 7: MapReduce Types and FormatsInput Splits and Records\nAs you saw in Chapter 2, an input split is a chunk of the input that is processed by a\nsingle map. Each map processes a single split. Each split is divided into records, and\nthe map processes each record—a key-value pair—in turn. Splits and records are log-\nical: there is nothing that requires them to be tied to files, for example, although in their\nmost common incarnations, they are. In a database context, a split might correspond\nto a range of rows from a table, and a record to a row in that range (this is precisely\nwhat DBInputFormat does, an input format for reading data from a relational database).\nInput splits are represented by the Java interface, InputSplit (which, like all of the\nclasses mentioned in this section, is in the org.apache.hadoop.mapred package†):\npublic interface InputSplit extends Writable {\nlong getLength() throws IOException;\nString[] getLocations() throws IOException;\n}\nAn InputSplit has a length in bytes, and a set of storage locations, which are just host-\nname strings. Notice that a split doesn’t contain the input data; it is just a reference to\nthe data. The storage locations are used by the MapReduce system to place map tasks\nas close to the split’s data as possible, and the size is used to order the splits so that the\nlargest get processed first, in an attempt to minimize the job runtime (this is an instance\nof a greedy approximation algorithm).\nAs a MapReduce application writer, you don’t need to deal with InputSplits directly,\nas they are created by an InputFormat. An InputFormat is responsible for creating the\ninput splits, and dividing them into records. Before we see some concrete examples of\nInputFormat, let’s briefly examine how it is used in MapReduce. Here’s the interface:\npublic interface InputFormat<K, V> {\nInputSplit[] getSplits(JobConf job, int numSplits) throws IOException;\n}\nRecordReader<K, V> getRecordReader(InputSplit split,\nJobConf job,\nReporter reporter) throws IOException;\nThe JobClient calls the getSplits() method, passing the desired number of map tasks\nas the numSplits argument. This number is treated as a hint, as InputFormat imple-\nmentations are free to return a different number of splits to the number specified in\nnumSplits. Having calculated the splits, the client sends them to the jobtracker, which\nuses their storage locations to schedule map tasks to process them on the tasktrackers.\n† But see the new MapReduce classes in org.apache.hadoop.mapreduce, described in “The new Java MapReduce\nAPI” on page 25.\nInput Formats | 185On a tasktracker, the map task passes the split to the getRecordReader() method on\nInputFormat to obtain a RecordReader for that split. A RecordReader is little more than\nan iterator over records, and the map task uses one to generate record key-value pairs,\nwhich it passes to the map function. A code snippet (based on the code in MapRunner)\nillustrates the idea:\nK key = reader.createKey();\nV value = reader.createValue();\nwhile (reader.next(key, value)) {\nmapper.map(key, value, output, reporter);\n}\nHere the RecordReader’s next() method is called repeatedly to populate the key and\nvalue objects for the mapper. When the reader gets to the end of the stream, the\nnext() method returns false, and the map task completes.\nThis code snippet makes it clear that the same key and value objects are\nused on each invocation of the map() method—only their contents are\nchanged (by the reader’s next() method). This can be a surprise to users,\nwho might expect keys and values to be immutable. This causes prob-\nlems when a reference to a key or value object is retained outside the\nmap() method, as its value can change without warning. If you need to\ndo this, make a copy of the object you want to hold on to. For example,\nfor a Text object, you can use its copy constructor: new Text(value).\nThe situation is similar with reducers. In this case the value objects in\nthe reducer’s iterator are reused, so you need to copy any that you need\nto retain between calls to the iterator (see Example 8-14).\nFinally, note that MapRunner is only one way of running mappers. MultithreadedMapRun\nner is another implementation of the MapRunnable interface that runs mappers concur-\nrently in a configurable number of threads (set by mapred.map.multithreadedrun\nner.threads). For most data processing tasks, it confers no advantage over MapRunner.\nHowever, for mappers that spend a long time processing each record, because they\ncontact external servers, for example, it allows multiple mappers to run in one JVM\nwith little contention. See “Fetcher: A multi-threaded MapRunner in ac-\ntion” on page 435 for an example of an application that uses MultithreadedMapRunner.\nFileInputFormat\nFileInputFormat is the base class for all implementations of InputFormat that use files\nas their data source (see Figure 7-2). It provides two things: a place to define which files\nare included as the input to a job, and an implementation for generating splits for the\ninput files. The job of dividing splits into records is performed by subclasses.\n186 | Chapter 7: MapReduce Types and FormatsFigure 7-2. InputFormat class hierarchy\nFileInputFormat input paths\nThe input to a job is specified as a collection of paths, which offers great flexibility in\nconstraining the input to a job. FileInputFormat offers four static convenience methods\nfor setting a JobConf’s input paths:\npublic\npublic\npublic\npublic\nstatic\nstatic\nstatic\nstatic\nvoid\nvoid\nvoid\nvoid\naddInputPath(JobConf conf, Path path)\naddInputPaths(JobConf conf, String commaSeparatedPaths)\nsetInputPaths(JobConf conf, Path... inputPaths)\nsetInputPaths(JobConf conf, String commaSeparatedPaths)\nThe addInputPath() and addInputPaths() methods add a path or paths to the list of\ninputs. You can call these methods repeatedly to build the list of paths. The setInput\nPaths() methods set the entire list of paths in one go (replacing any paths set on the\nJobConf in previous calls).\nA path may represent a file, a directory, or, by using a glob, a collection of files and\ndirectories. A path representing a directory includes all the files in the directory as input\nto the job. See “File patterns” on page 60 for more on using globs.\nInput Formats | 187The contents of a directory specified as an input path are not processed\nrecursively. In fact, the directory should only contain files: if the direc-\ntory contains a subdirectory, it will be interpreted as a file, which will\ncause an error. The way to handle this case is to use a file glob or a filter\nto select only the files in the directory.\nThe add and set methods allow files to be specified by inclusion only. To exclude certain\nfiles from the input, you can set a filter using the setInputPathFilter() method on\nFileInputFormat:\npublic static void setInputPathFilter(JobConf conf,\nClass<? extends PathFilter> filter)\nFilters are discussed in more detail in “PathFilter” on page 61.\nEven if you don’t set a filter, FileInputFormat uses a default filter that excludes hidden\nfiles (those whose names begin with a dot or an underscore). If you set a filter by calling\nsetInputPathFilter(), it acts in addition to the default filter. In other words, only non-\nhidden files that are accepted by your filter get through.\nPaths and filters can be set through configuration properties too (Table 7-3), which can\nbe handy for Streaming and Pipes. Setting paths is done with the -input option for both\nStreaming and Pipes interfaces, so setting paths directly is not usually needed.\nTable 7-3. Input path and filter properties\nProperty name Type Default value Description\nmapred.input.dir comma-separated none The input files for a job. Paths that contain com-\n                 paths mas should have those commas escaped by a\n                      backslash character. For example, the glob\n                     {a,b} would be escaped as {a\\,b}.\nmapred.input.path PathFilter none The filter to apply to the input files for a job.\nFilter.class \nclassname\nFileInputFormat input splits\nGiven a set of files, how does FileInputFormat turn them into splits? FileInputFormat\nsplits only large files. Here “large” means larger than an HDFS block. The split size is\nnormally the size of an HDFS block, which is appropriate for most applications; how-\never, it is possible to control this value by setting various Hadoop properties, as shown\nin Table 7-4.\n188 | Chapter 7: MapReduce Types and FormatsTable 7-4. Properties for controlling split size\nProperty name Type Default value Description\nmapred.min.split.size int 1 The smallest valid size in\n                           bytes for a file split.\nmapred.max.split.sizea long Long.MAX_VALUE, that is The largest valid size in\n                                                   bytes for a file split.\ndfs.block.size long 64 MB, that is 67108864 The size of a block in HDFS\n                                           in bytes.\n9223372036854775807\na This property is not present in the old MapReduce API (with the exception of CombineFileInputFormat).\nInstead, it is calculated indirectly as the size of the total input for the job, divided by the guide number of\nmap tasks specified by mapred.map.tasks (or the setNumMapTasks() method on JobConf). Because\nmapred.map.tasks defaults to 1, this makes the maximum split size the size of the input.\nThe minimum split size is usually 1 byte, although some formats have a lower bound\non the split size. (For example, sequence files insert sync entries every so often in the\nstream, so the minimum split size has to be large enough to ensure that every split has\na sync point to allow the reader to resynchronize with a record boundary.)\nApplications may impose a minimum split size: by setting this to a value larger than\nthe block size, they can force splits to be larger than a block. There is no good reason\nfor doing this when using HDFS, since doing so will increase the number of blocks that\nare not local to a map task.\nThe maximum split size defaults to the maximum value that can be represented by a\nJava long type. It has an effect only when it is less than the block size, forcing splits to\nbe smaller than a block.\nThe split size is calculated by the formula (see the computeSplitSize() method in\nFileInputFormat):\nmax(minimumSize, min(maximumSize, blockSize))\nby default:\nminimumSize < blockSize < maximumSize\nso the split size is blockSize. Various settings for these parameters and how they affect\nthe final split size are illustrated in Table 7-5.\nInput Formats | 189Table 7-5. Examples of how to control the split size\nMinimum split size Maximum split size Block size Split size Comment\n1 (default) Long.MAX_VALUE 64 MB (default) 64 MB By default split size is the same as the\n                                                default block size.\nLong.MAX_VALUE 128 MB 128 MB The most natural way to increase the\n                            split size is to have larger blocks in\n                           HDFS, by setting dfs.block.size,\n                          or on a per-file basis at file construction\n                         time.\nLong.MAX_VALUE 64 MB (default) 128 MB Making the minimum split size greater\n                                     than the block size increases the split\n                                    size, but at the cost of locality.\n32 MB 64 MB (default) 32 MB Making the maximum split size less\n                           than the block size decreases the split\n                          size.\n(default)\n1 (default)\n(default)\n128 MB\n(default)\n1 (default)\nSmall files and CombineFileInputFormat\nHadoop works better with a small number of large files than a large number of small\nfiles. One reason for this is that FileInputFormat generates splits in such a way that each\nsplit is all or part of a single file. If the file is very small (“small” means significantly\nsmaller than an HDFS block) and there are a lot of them, then each map task will process\nvery little input, and there will be a lot of them (one per file), each of which imposes\nextra bookkeeping overhead. Compare a 1 GB file broken into sixteen 64 MB blocks,\nand 10,000 or so 100 KB files. The 10,000 files use one map each, and the job time can\nbe tens or hundreds of times slower than the equivalent one with a single input file and\n16 map tasks.\nThe situation is alleviated somewhat by CombineFileInputFormat, which was designed\nto work well with small files. Where FileInputFormat creates a split per file, CombineFi\nleInputFormat packs many files into each split so that each mapper has more to process.\nCrucially, CombineFileInputFormat takes node and rack locality into account when de-\nciding which blocks to place in the same split, so it does not compromise the speed at\nwhich it can process the input in a typical MapReduce job.\nOf course, if possible, it is still a good idea to avoid the many small files case since\nMapReduce works best when it can operate at the transfer rate of the disks in the cluster,\nand processing many small files increases the number of seeks that are needed to run\na job. Also, storing large numbers of small files in HDFS is wasteful of the namenode’s\nmemory. One technique for avoiding the many small files case is to merge small files\ninto larger files by using a SequenceFile: the keys can act as filenames (or a constant\nsuch as NullWritable, if not needed) and the values as file contents. See Example 7-4.\nBut if you already have a large number of small files in HDFS, then CombineFileInput\nFormat is worth trying.\n190 | Chapter 7: MapReduce Types and FormatsCombineFileInputFormat isn’t just good for small files—it can bring ben-\nefits when processing large files too. Essentially, CombineFileInputFor\nmat decouples the amount of data that a mapper consumes from the\nblock size of the files in HDFS.\nIf your mappers can process each block in a matter of seconds, then you\ncould use CombineFileInputFormat with the maximum split size set to a\nsmall multiple of the number of blocks (by setting the\nmapred.max.split.size property in bytes) so that each mapper processes\nmore than one block. In return, the overall processing time falls, since\nproportionally fewer mappers run, which reduces the overhead in task\nbookkeeping and startup time associated with a large number of short-\nlived mappers.\nSince CombineFileInputFormat is an abstract class without any concrete classes (unlike\nFileInputFormat), you need to do a bit more work to use it. (Hopefully, common im-\nplementations will be added to the library over time.) For example, to have the\nCombineFileInputFormat equivalent of TextInputFormat, you would create a concrete\nsubclass of CombineFileInputFormat and implement the getRecordReader() method.\nPreventing splitting\nSome applications don’t want files to be split so that a single mapper can process each\ninput file in its entirety. For example, a simple way to check if all the records in a file\nare sorted is to go through the records in order, checking whether each record is not\nless than the preceding one. Implemented as a map task, this algorithm will work only\nif one map processes the whole file.‡\nThere are a couple of ways to ensure that an existing file is not split. The first (quick\nand dirty) way is to increase the minimum split size to be larger than the largest file in\nyour system. Setting it to its maximum value, Long.MAX_VALUE, has this effect. The sec-\nond is to subclass the concrete subclass of FileInputFormat that you want to use, to\noverride the isSplitable() method§ to return false. For example, here’s a nonsplit-\ntable TextInputFormat:\nimport org.apache.hadoop.fs.*;\nimport org.apache.hadoop.mapred.TextInputFormat;\npublic class NonSplittableTextInputFormat extends TextInputFormat {\n@Override\nprotected boolean isSplitable(FileSystem fs, Path file) {\nreturn false;\n}\n}\n‡ This is how the mapper in SortValidator.RecordStatsChecker is implemented.\n§ In the method name isSplitable(), “splitable” has a single “t.” It is usually spelled “splittable,” which is the\nspelling I have used in this book.\nInput Formats | 191File information in the mapper\nA mapper processing a file input split can find information about the split by reading\nsome special properties from its job configuration object, which may be obtained by\nimplementing configure() in your Mapper implementation to get access to the\nJobConf object. Table 7-6 lists the properties available. These are in addition to the ones\navailable to all mappers and reducers, listed in “The Task Execution Environ-\nment” on page 172.\nTable 7-6. File split properties\nProperty name Type Description\nmap.input.file String The path of the input file being processed\nmap.input.start long The byte offset of the start of the split\nmap.input.length long The length of the split in bytes\nIn the next section, you shall see how to use this when we need to access the split’s\nfilename.\nProcessing a whole file as a record\nA related requirement that sometimes crops up is for mappers to have access to the full\ncontents of a file. Not splitting the file gets you part of the way there, but you also need\nto have a RecordReader that delivers the file contents as the value of the record. The\nlisting for WholeFileInputFormat in Example 7-2 shows a way of doing this.\nExample 7-2. An InputFormat for reading a whole file as a record\npublic class WholeFileInputFormat\nextends FileInputFormat<NullWritable, BytesWritable> {\n@Override\nprotected boolean isSplitable(FileSystem fs, Path filename) {\nreturn false;\n}\n@Override\npublic RecordReader<NullWritable, BytesWritable> getRecordReader(\nInputSplit split, JobConf job, Reporter reporter) throws IOException {\n}\n}\nreturn new WholeFileRecordReader((FileSplit) split, job);\nWholeFileInputFormat defines a format where the keys are not used, represented by\nNullWritable, and the values are the file contents, represented by BytesWritable in-\nstances. It defines two methods. First, the format is careful to specify that input files\nshould never be split, by overriding isSplitable() to return false. Second, we\n192 | Chapter 7: MapReduce Types and Formatsimplement getRecordReader() to return a custom implementation of RecordReader,\nwhich appears in Example 7-3.\nExample 7-3. The RecordReader used by WholeFileInputFormat for reading a whole file as a record\nclass WholeFileRecordReader implements RecordReader<NullWritable, BytesWritable> {\nprivate FileSplit fileSplit;\nprivate Configuration conf;\nprivate boolean processed = false;\npublic WholeFileRecordReader(FileSplit fileSplit, Configuration conf)\nthrows IOException {\nthis.fileSplit = fileSplit;\nthis.conf = conf;\n}\n@Override\npublic NullWritable createKey() {\nreturn NullWritable.get();\n}\n@Override\npublic BytesWritable createValue() {\nreturn new BytesWritable();\n}\n@Override\npublic long getPos() throws IOException {\nreturn processed ? fileSplit.getLength() : 0;\n}\n@Override\npublic float getProgress() throws IOException {\nreturn processed ? 1.0f : 0.0f;\n}\n@Override\npublic boolean next(NullWritable key, BytesWritable value) throws IOException {\nif (!processed) {\nbyte[] contents = new byte[(int) fileSplit.getLength()];\nPath file = fileSplit.getPath();\nFileSystem fs = file.getFileSystem(conf);\nFSDataInputStream in = null;\ntry {\nin = fs.open(file);\nIOUtils.readFully(in, contents, 0, contents.length);\nvalue.set(contents, 0, contents.length);\n} finally {\nIOUtils.closeStream(in);\n}\nprocessed = true;\nreturn true;\n}\nreturn false;\nInput Formats | 193}\n}\n@Override\npublic void close() throws IOException {\n// do nothing\n}\nWholeFileRecordReader is responsible for taking a FileSplit and converting it into a\nsingle record, with a null key and a value containing the bytes of the file. Because there\nis only a single record, WholeFileRecordReader has either processed it or not, so it main-\ntains a boolean called processed. If, when the next() method is called, the file has not\nbeen processed, then we open the file, create a byte array whose length is the length of\nthe file, and use the Hadoop IOUtils class to slurp the file into the byte array. Then we\nset the array on the BytesWritable instance that was passed into the next() method,\nand return true to signal that a record has been read.\nThe other methods are straightforward bookkeeping methods for creating the correct\nkey and value types, getting the position and progress of the reader, and a close()\nmethod, which is invoked by the MapReduce framework when the reader is done with.\nTo demonstrate how WholeFileInputFormat can be used, consider a MapReduce job for\npackaging small files into sequence files, where the key is the original filename, and the\nvalue is the content of the file. The listing is in Example 7-4.\nExample 7-4. A MapReduce program for packaging a collection of small files as a single SequenceFile\npublic class SmallFilesToSequenceFileConverter extends Configured\nimplements Tool {\nstatic class SequenceFileMapper extends MapReduceBase\nimplements Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\nprivate JobConf conf;\n@Override\npublic void configure(JobConf conf) {\nthis.conf = conf;\n}\n@Override\npublic void map(NullWritable key, BytesWritable value,\nOutputCollector<Text, BytesWritable> output, Reporter reporter)\nthrows IOException {\n}\nString filename = conf.get(""map.input.file"");\noutput.collect(new Text(filename), value);\n}\n@Override\npublic int run(String[] args) throws IOException {\n194 | Chapter 7: MapReduce Types and FormatsJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(WholeFileInputFormat.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(BytesWritable.class);\nconf.setMapperClass(SequenceFileMapper.class);\nconf.setReducerClass(IdentityReducer.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(), args);\nSystem.exit(exitCode);\n}\nSince the input format is a WholeFileInputFormat, the mapper has to find only the\nfilename for the input file split. It does this by retrieving the map.input.file property\nfrom the JobConf, which is set to the split’s filename by the MapReduce framework,\nbut only for splits that are FileSplit instances (this includes most subclasses of FileIn\nputFormat). The reducer is the IdentityReducer, and the output format is a SequenceFi\nleOutputFormat.\nHere’s a run on a few small files. We’ve chosen to use two reducers, so we get two\noutput sequence files:\n% hadoop jar job.jar SmallFilesToSequenceFileConverter \\\n-conf conf/hadoop-localhost.xml -D mapred.reduce.tasks=2 input/smallfiles output\nTwo part files are created, each of which is a sequence file, which we can inspect with\nthe -text option to the filesystem shell:\n% hadoop fs -conf conf/hadoop-localhost.xml -text output/part-00000\nhdfs://localhost/user/tom/input/smallfiles/a\n61 61 61 61 61 61 61\nhdfs://localhost/user/tom/input/smallfiles/c\n63 63 63 63 63 63 63\nhdfs://localhost/user/tom/input/smallfiles/e\n% hadoop fs -conf conf/hadoop-localhost.xml -text output/part-00001\nhdfs://localhost/user/tom/input/smallfiles/b\n62 62 62 62 62 62 62\nhdfs://localhost/user/tom/input/smallfiles/d\n64 64 64 64 64 64 64\nhdfs://localhost/user/tom/input/smallfiles/f\n66 66 66 66 66 66 66\n61 61 61\n63 63 63\n62 62 62\n64 64 64\n66 66 66\nThe input files were named a, b, c, d, e, and f, and each contained 10 characters of the\ncorresponding letter (so, for example, a contained 10 “a” characters), except e, which\nwas empty. We can see this in the textual rendering of the sequence files, which prints\nthe filename followed by the hex representation of the file.\nInput Formats | 195There’s at least one way we could improve this program. As mentioned earlier, having\none mapper per file is inefficient, so subclassing CombineFileInputFormat instead of\nFileInputFormat is the better approach (there’s code for this in the accompanying ex-\nample code). Also, for a related technique of packing files into a Hadoop Archive, rather\nthan a sequence file, see the section “Hadoop Archives” on page 71.\nText Input\nHadoop excels at processing unstructured text. In this section, we discuss the different\nInputFormats that Hadoop provides to process text.\nTextInputFormat\nTextInputFormat is the default InputFormat. Each record is a line of input. The key, a\nLongWritable, is the byte offset within the file of the beginning of the line. The value is\nthe contents of the line, excluding any line terminators (newline, carriage return), and\nis packaged as a Text object. So a file containing the following text:\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nis divided into one split of four records. The records are interpreted as the following\nkey-value pairs:\n(0, On the top of the Crumpetty Tree)\n(33, The Quangle Wangle sat,)\n(57, But his face you could not see,)\n(89, On account of his Beaver Hat.)\nClearly, the keys are not line numbers. This would be impossible to implement in gen-\neral, in that a file is broken into splits, at byte, not line, boundaries. Splits are processed\nindependently. Line numbers are really a sequential notion: you have to keep a count\nof lines as you consume them, so knowing the line number within a split would be\npossible, but not within the file.\nHowever, the offset within the file of each line is known by each split independently of\nthe other splits, since each split knows the size of the preceding splits and just adds this\non to the offsets within the split to produce a global file offset. The offset is usually\nsufficient for applications that need a unique identifier for each line. Combined with\nthe file’s name, it is unique within the filesystem. Of course, if all the lines are a fixed\nwidth, then calculating the line number is simply a matter of dividing the offset by the\nwidth.\n196 | Chapter 7: MapReduce Types and FormatsThe Relationship Between Input Splits and HDFS Blocks\nThe logical records that FileInputFormats define do not usually fit neatly into HDFS\nblocks. For example, a TextInputFormat’s logical records are lines, which will cross\nHDFS boundaries more often than not. This has no bearing on the functioning of your\nprogram—lines are not missed or broken, for example—but it’s worth knowing about,\nas it does mean that data-local maps (that is, maps that are running on the same host\nas their input data) will perform some remote reads. The slight overhead this causes is\nnot normally significant.\nFigure 7-3 shows an example. A single file is broken into lines, and the line boundaries\ndo not correspond with the HDFS block boundaries. Splits honor logical record boun-\ndaries, in this case lines, so we see that the first split contains line 5, even though it\nspans the first and second block. The second split starts at line 6.\nFigure 7-3. Logical records and HDFS blocks for TextInputFormat\nKeyValueTextInputFormat\nTextInputFormat’s keys, being simply the offset within the file, are not normally very\nuseful. It is common for each line in a file to be a key-value pair, separated by a delimiter\nsuch as a tab character. For example, this is the output produced by TextOutputFor\nmat, Hadoop’s default OutputFormat. To interpret such files correctly, KeyValueTextIn\nputFormat is appropriate.\nYou can specify the separator via the key.value.separator.in.input.line property. It\nis a tab character by default. Consider the following input file, where → represents a\n(horizontal) tab character:\nline1→On the top of the Crumpetty Tree\nline2→The Quangle Wangle sat,\nline3→But his face you could not see,\nline4→On account of his Beaver Hat.\nLike in the TextInputFormat case, the input is in a single split comprising four records,\nalthough this time the keys are the Text sequences before the tab in each line:\n(line1,\n(line2,\n(line3,\n(line4,\nOn the top of the Crumpetty Tree)\nThe Quangle Wangle sat,)\nBut his face you could not see,)\nOn account of his Beaver Hat.)\nInput Formats | 197NLineInputFormat\nWith TextInputFormat and KeyValueTextInputFormat, each mapper receives a variable\nnumber of lines of input. The number depends on the size of the split and the length\nof the lines. If you want your mappers to receive a fixed number of lines of input, then\nNLineInputFormat is the InputFormat to use. Like TextInputFormat, the keys are the byte\noffsets within the file and the values are the lines themselves.\nN refers to the number of lines of input that each mapper receives. With N set to one\n(the default), each mapper receives exactly one line of input. The\nmapred.line.input.format.linespermap property controls the value of N. By way of\nexample, consider these four lines again:\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\nIf, for example, N is two, then each split contains two lines. One mapper will receive\nthe first two key-value pairs:\n(0, On the top of the Crumpetty Tree)\n(33, The Quangle Wangle sat,)\nAnd another mapper will receive the second two key-value pairs:\n(57, But his face you could not see,)\n(89, On account of his Beaver Hat.)\nThe keys and values are the same as TextInputFormat produces. What is different is the\nway the splits are constructed.\nUsually having a map task for a small number of lines of input is inefficient (due to the\noverhead in task setup), but there are applications that take a small amount of input\ndata and run an extensive (that is, CPU-intensive) computation for it, then emit their\noutput. Simulations are a good example. By creating an input file that specifies input\nparameters, one per line, you can perform a parameter sweep: run a set of simulations\nin parallel to find how a model varies as the parameter changes.\nIf you have long-running simulations, you may fall afoul of task time-\nouts. When a task doesn’t report progress for more than 10 minutes,\nthen the tasktracker assumes it has failed and aborts the process (see\n“Task Failure” on page 159).\nThe best way to guard against this is to report progress periodically, by\nwriting a status message, or incrementing a counter, for example. See\n“What Constitutes Progress in MapReduce?” on page 158.\nAnother example is using Hadoop to bootstrap data loading from multiple data sour-\nces, such as databases. You create a “seed” input file which lists the data sources, one\nper line. Then each mapper is allocated a single data source, and it loads the data from\n198 | Chapter 7: MapReduce Types and Formatsthat source into HDFS. The job doesn’t need the reduce phase, so the number of re-\nducers should be set to zero (by calling setNumReduceTasks() on Job). Furthermore,\nMapReduce jobs can be run to process the data loaded into HDFS. See Appendix C for\nan example.\nXML\nMost XML parsers operate on whole XML documents, so if a large XML document is\nmade up of multiple input splits, then it is a challenge to parse these individually. Of\ncourse, you can process the entire XML document in one mapper (if it is not too large)\nusing the technique in “Processing a whole file as a record” on page 192.\nLarge XML documents that are composed of a series of “records” (XML document\nfragments) can be broken into these records using simple string or regular-expression\nmatching to find start and end tags of records. This alleviates the problem when the\ndocument is split by the framework, since the next start tag of a record is easy to find\nby simply scanning from the start of the split, just like TextInputFormat finds newline\nboundaries.\nHadoop comes with a class for this purpose called StreamXmlRecordReader (which is in\nthe org.apache.hadoop.streaming package, although it can be used outside of Stream-\ning). You can use it by setting your input format to StreamInputFormat, and setting the\nstream.recordreader.class property to org.apache.hadoop.streaming.StreamXmlRecor\ndReader. The reader is configured by setting job configuration properties to tell it the\npatterns for the start and end tags (see the class documentation for details).‖\nTo take an example, Wikipedia provides dumps of its content in XML form, which are\nappropriate for processing in parallel using MapReduce using this approach. The data\nis contained in one large XML wrapper document, which contains a series of elements,\nsuch as page elements that contain a page’s content and associated metadata. Using\nStreamXmlRecordReader, the page elements can be interpreted as records for processing\nby a mapper.\nBinary Input\nHadoop MapReduce is not just restricted to processing textual data—it has support\nfor binary formats, too.\nSequenceFileInputFormat\nHadoop’s sequence file format stores sequences of binary key-value pairs. They are well\nsuited as a format for MapReduce data since they are splittable (they have sync points\nso that readers can synchronize with record boundaries from an arbitrary point in the\nfile, such as the start of a split), they support compression as a part of the format, and\n‖ See https://issues.apache.org/jira/browse/HADOOP-2439 for an improved XML input format.\nInput Formats | 199they can store arbitrary types using a variety of serialization frameworks. (These topics\nare covered in “SequenceFile” on page 103.)\nTo use data from sequence files as the input to MapReduce, you use SequenceFileIn\nputFormat. The keys and values are determined by the sequence file, and you need to\nmake sure that your map input types correspond. For example, if your sequence file\nhas IntWritable keys and Text values, like the one created in Chapter 4, then the map\nsignature would be Mapper<IntWritable, Text, K, V>, where K and V are the types of\nthe map’s output keys and values.\nAlthough its name doesn’t give it away, SequenceFileInputFormat can\nread MapFiles as well as sequence files. If it finds a directory where it\nwas expecting a sequence file, SequenceFileInputFormat assumes that it\nis reading a MapFile and uses its data file. This is why there is no\nMapFileInputFormat class.\nSequenceFileAsTextInputFormat\nSequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that converts\nthe sequence file’s keys and values to Text objects. The conversion is performed by\ncalling toString() on the keys and values. This format makes sequence files suitable\ninput for Streaming.\nSequenceFileAsBinaryInputFormat\nSequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves\nthe sequence file’s keys and values as opaque binary objects. They are encapsulated as\nBytesWritable objects, and the application is free to interpret the underlying byte array\nas it pleases. Combined with SequenceFile.Reader’s appendRaw() method, this provides\na way to use any binary data types with MapReduce (packaged as a sequence file),\nalthough plugging into Hadoop’s serialization mechanism is normally a cleaner alter-\nnative (see “Serialization Frameworks” on page 101).\nMultiple Inputs\nAlthough the input to a MapReduce job may consist of multiple input files (constructed\nby a combination of file globs, filters and plain paths), all of the input is interpreted by\na single InputFormat and a single Mapper. What often happens, however, is that over\ntime, the data format evolves, so you have to write your mapper to cope with all of your\nlegacy formats. Or, you have data sources that provide the same type of data but in\ndifferent formats. This arises in the case of performing joins of different datasets; see\n“Reduce-Side Joins” on page 235. For instance, one might be tab-separated plain text,\nthe other a binary sequence file. Even if they are in the same format, they may have\ndifferent representations, and therefore need to be parsed differently.\n200 | Chapter 7: MapReduce Types and FormatsThese cases are handled elegantly by using the MultipleInputs class, which allows you\nto specify the InputFormat and Mapper to use on a per-path basis. For example, if we\nhad weather data from the U.K. Met Office# that we wanted to combine with the\nNCDC data for our maximum temperature analysis, then we might set up the input as\nfollows:\nMultipleInputs.addInputPath(conf, ncdcInputPath,\nTextInputFormat.class, MaxTemperatureMapper.class)\nMultipleInputs.addInputPath(conf, metOfficeInputPath,\nTextInputFormat.class, MetOfficeMaxTemperatureMapper.class);\nThis code replaces the usual calls to FileInputFormat.addInputPath() and conf.setMap\nperClass(). Both Met Office and NCDC data is text-based, so we use TextInputFor\nmat for each. But the line format of the two data sources is different, so we use two\ndifferent mappers. The MaxTemperatureMapper reads NCDC input data and extracts the\nyear and temperature fields. The MetOfficeMaxTemperatureMapper reads Met Office in-\nput data and extracts the year and temperature fields. The important thing is that the\nmap outputs have the same types, since the reducers (which are all of the same type)\nsee the aggregated map outputs and are not aware of the different mappers used to\nproduce them.\nThe MultipleInputs class has an overloaded version of addInputPath() that doesn’t take\na mapper:\npublic static void addInputPath(JobConf conf, Path path,\nClass<? extends InputFormat> inputFormatClass)\nThis is useful when you only have one mapper (set using the JobConf’s setMapper\nClass() method), but multiple input formats.\nDatabase Input (and Output)\nDBInputFormat is an input format for reading data from a relational database, using\nJDBC. Because it doesn’t have any sharding capabilities, you need to be careful not to\noverwhelm the database you are reading from by running too many mappers. For this\nreason, it is best used for loading relatively small datasets, perhaps for joining with\nlarger datasets from HDFS, using MultipleInputs. The corresponding output format is\nDBOutputFormat, which is useful for dumping job outputs (of modest size) into a\ndatabase.*\n# Met Office data is generally available only to the research and academic community. However, there is a\nsmall amount of monthly weather station data available at http://www.metoffice.gov.uk/climate/uk/\nstationdata/.\n* Instructions for how to use these formats are provided in “Database Access with Hadoop,” http://www\n.cloudera.com/blog/2009/03/06/database-access-with-hadoop/, by Aaron Kimball.\nInput Formats | 201HBase’s TableInputFormat is designed to allow a MapReduce program to operate on\ndata stored in an HBase table. TableOutputFormat is for writing MapReduce outputs\ninto an HBase table.\nOutput Formats\nHadoop has output data formats that correspond to the input formats covered in the\nprevious section. The OutputFormat class hierarchy appears in Figure 7-4.\nFigure 7-4. OutputFormat class hierarchy\nText Output\nThe default output format, TextOutputFormat, writes records as lines of text. Its keys\nand values may be of any type, since TextOutputFormat turns them to strings by calling\ntoString() on them. Each key-value pair is separated by a tab character, although that\nmay be changed using the mapred.textoutputformat.separator property. The counter-\npart to TextOuputFormat for reading in this case is KeyValueTextInputFormat, since it\nbreaks lines into key-value pairs based on a configurable separator (see “KeyValue-\nTextInputFormat” on page 197).\nYou can suppress the key or the value (or both, making this output format equivalent\nto NullOutputFormat, which emits nothing) from the output using a NullWritable type.\n202 | Chapter 7: MapReduce Types and FormatsThis also causes no separator to be written, which makes the output suitable for reading\nin using TextInputFormat.\nBinary Output\nSequenceFileOutputFormat\nAs the name indicates, SequenceFileOutputFormat writes sequence files for its output.\nThis is a good choice of output if it forms the input to a further MapReduce job, since\nit is compact, and is readily compressed. Compression is controlled via the static\nmethods on SequenceFileOutputFormat, as described in “Using Compression in Map-\nReduce” on page 84. For an example of how to use SequenceFileOutputFormat, see\n“Sorting” on page 218.\nSequenceFileAsBinaryOutputFormat\nSequenceFileAsBinaryOutputFormat is the counterpart to SequenceFileAsBinaryInput\nFormat, and it writes keys and values in raw binary format into a SequenceFile container.\nMapFileOutputFormat\nMapFileOutputFormat writes MapFiles as output. The keys in a MapFile must be added\nin order, so you need to ensure that your reducers emit keys in sorted order.\nThe reduce input keys are guaranteed to be sorted, but the output keys\nare under the control of the reduce function, and there is nothing in the\ngeneral MapReduce contract that states that the reduce output keys have\nto be ordered in any way. The extra constraint of sorted reduce output\nkeys is just needed for MapFileOutputFormat.\nMultiple Outputs\nFileOutputFormat and its subclasses generate a set of files in the output directory. There\nis one file per reducer and files are named by the partition number: part-00000,\npart-00001, etc. There is sometimes a need to have more control over the naming of\nthe files, or to produce multiple files per reducer. MapReduce comes with two libraries\nto help you do this: MultipleOutputFormat and MultipleOutputs.\nAn example: Partitioning data\nConsider the problem of partitioning the weather dataset by weather station. We would\nlike to run a job whose output is a file per station, with each file containing all the\nrecords for that station.\nOne way of doing this is to have a reducer for each weather station. To arrange this,\nwe need to do two things. First, write a partitioner that puts records from the same\nOutput Formats | 203weather station into the same partition. Second, set the number of reducers on the job\nto be the number of weather stations. The partitioner would look like this:\npublic class StationPartitioner implements Partitioner<LongWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n@Override\npublic int getPartition(LongWritable key, Text value, int numPartitions) {\nparser.parse(value);\nreturn getPartition(parser.getStationId());\n}\nprivate int getPartition(String stationId) {\n...\n}\n}\n@Override\npublic void configure(JobConf conf) { }\nThe getPartition(String) method, whose implementation is not shown, turns the\nstation ID into a partition index. To do this, it needs a list of all the station IDs and\nthen just returns the index of the station ID in the list.\nThere are two drawbacks to this approach. The first is that since the number of parti-\ntions needs to be known before the job is run, so does the number of weather stations.\nAlthough the NCDC provides metadata about its stations, there is no guarantee that\nthe IDs encountered in the data match those in the metadata. A station that appears in\nthe metadata but not in the data wastes a reducer slot. Worse, a station that appears\nin the data but not in the metadata doesn’t get a reducer slot—it has to be thrown away.\nOne way of mitigating this problem would be to write a job to extract the unique station\nIDs, but it’s a shame that we need an extra job to do this.\nThe second drawback is more subtle. It is generally a bad idea to allow the number of\npartitions to be rigidly fixed by the application, since it can lead to small or uneven-\nsized partitions. Having many reducers doing a small amount of work isn’t an efficient\nway of organizing a job: it’s much better to get reducers to do more work and have\nfewer of them, as the overhead in running a task is then reduced. Uneven-sized parti-\ntions can be difficult to avoid too. Different weather stations will have gathered a widely\nvarying amount of data: compare a station that opened one year ago to one that has\nbeen gathering data for one century. If a few reduce tasks take significantly longer than\nthe others, they will dominate the job execution time and cause it to be longer than it\nneeds to be.\n204 | Chapter 7: MapReduce Types and FormatsThere are two special cases when it does make sense to allow the ap-\nplication to set the number of partitions (or equivalently, the number\nof reducers):\nZero reducers\nThis is a vacuous case: there are no partitions, as the application\nneeds to run only map tasks.\nOne reducer\nIt can be convenient to run small jobs to combine the output of\nprevious jobs into a single file. This should only be attempted when\nthe amount of data is small enough to be processed comfortably\nby one reducer.\nIt is much better to let the cluster drive the number of partitions for a job; the idea being\nthat the more cluster reduce slots are available the faster the job can complete. This is\nwhy the default HashPartitioner works so well, as it works with any number of parti-\ntions and ensures each partition has a good mix of keys leading to more even-sized\npartitions.\nIf we go back to using HashPartitioner, each partition will contain multiple stations,\nso to create a file per station, we need to arrange for each reducer to write multiple files,\nwhich is where MultipleOutputFormat comes in.\nMultipleOutputFormat\nMultipleOutputFormat allows you to write data to multiple files whose names are de-\nrived from the output keys and values. MultipleOutputFormat is an abstract class with\ntwo concrete subclasses, MultipleTextOutputFormat and MultipleSequenceFileOutput\nFormat, which are the multiple file equivalents of TextOutputFormat and SequenceFi\nleOutputFormat. MultipleOutputFormat provides a few protected methods that sub-\nclasses can override to control the output filename. In Example 7-5, we create a subclass\nof MultipleTextOutputFormat to override the generateFileNameForKeyValue() method\nto return the station ID, which we extracted from the record value.\nExample 7-5. Partitioning whole dataset into files named by the station ID using\nMultipleOutputFormat\npublic class PartitionByStationUsingMultipleOutputFormat extends Configured\nimplements Tool {\nstatic class StationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\nOutput Formats | 205}\n}\nparser.parse(value);\noutput.collect(new Text(parser.getStationId()), value);\nstatic class StationReducer extends MapReduceBase\nimplements Reducer<Text, Text, NullWritable, Text> {\n}\n@Override\npublic void reduce(Text key, Iterator<Text> values,\nOutputCollector<NullWritable, Text> output, Reporter reporter)\nthrows IOException {\nwhile (values.hasNext()) {\noutput.collect(NullWritable.get(), values.next());\n}\n}\nstatic class StationNameMultipleTextOutputFormat\nextends MultipleTextOutputFormat<NullWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n}\nprotected String generateFileNameForKeyValue(NullWritable key, Text value,\nString name) {\nparser.parse(value);\nreturn parser.getStationId();\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(StationMapper.class);\nconf.setMapOutputKeyClass(Text.class);\nconf.setReducerClass(StationReducer.class);\nconf.setOutputKeyClass(NullWritable.class);\nconf.setOutputFormat(StationNameMultipleTextOutputFormat.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew PartitionByStationUsingMultipleOutputFormat(), args);\nSystem.exit(exitCode);\n}\n206 | Chapter 7: MapReduce Types and FormatsStationMapper pulls the station ID from the record and uses it as the key. This causes\nrecords from the same station to go into the same partition. StationReducer replaces\nthe key with a NullWritable so that when the final output is written using StationName\nMultipleTextOutputFormat (which like TextOutputFormat drops NullWritable keys), it\nconsists solely of weather records (and not the station ID key).\nThe overall effect is to place all the records for one station in a file named by the station\nID. Here is a few lines of output after running the program over a subset of the total\ndataset:\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n-rw-r--r--\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nroot\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\nsupergroup\n2887145\n1395129\n2054455\n1422448\n1419378\n1384421\n1480077\n1400448\n307141\n1433994\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n2009-04-17\n10:34\n10:33\n10:33\n10:34\n10:34\n10:33\n10:33\n10:33\n10:34\n10:33\n/output/010010-99999\n/output/010050-99999\n/output/010100-99999\n/output/010280-99999\n/output/010550-99999\n/output/010980-99999\n/output/011060-99999\n/output/012030-99999\n/output/012350-99999\n/output/012620-99999\nThe filename returned by generateFileNameForKeyValue() is actually a path that is in-\nterpreted relative to the output directory. It’s possible to create subdirectories of arbi-\ntrary depth. For example, the following modification partitions the data by station and\nyear so that each year’s data is contained in a directory named by the station ID:\nprotected String generateFileNameForKeyValue(NullWritable key, Text value,\nString name) {\nparser.parse(value);\nreturn parser.getStationId() + ""/"" + parser.getYear();\n}\nMultipleOutputFormat has more features that are not discussed here, such as the ability\nto copy the input directory structure and file naming for a map-only job. Please consult\nthe Java documentation for details.\nMultipleOutputs\nThere’s a second library in Hadoop for generating multiple outputs, provided by the\nMultipleOutputs class. Unlike MultipleOutputFormat, MultipleOutputs can emit differ-\nent types for each output. On the other hand, there is less control over the naming of\noutputs. The program in Example 7-6 shows how to use MultipleOutputs to partition\nthe dataset by station.\nExample 7-6. Partitioning whole dataset into files named by the station ID using MultipleOutputs\npublic class PartitionByStationUsingMultipleOutputs extends Configured\nimplements Tool {\nstatic class StationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, Text> {\nOutput Formats | 207private NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\noutput.collect(new Text(parser.getStationId()), value);\nstatic class MultipleOutputsReducer extends MapReduceBase\nimplements Reducer<Text, Text, NullWritable, Text> {\nprivate MultipleOutputs multipleOutputs;\n@Override\npublic void configure(JobConf conf) {\nmultipleOutputs = new MultipleOutputs(conf);\n}\npublic void reduce(Text key, Iterator<Text> values,\nOutputCollector<NullWritable, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nOutputCollector collector = multipleOutputs.getCollector(""station"",\nkey.toString().replace(""-"", """"), reporter);\nwhile (values.hasNext()) {\ncollector.collect(NullWritable.get(), values.next());\n}\n@Override\npublic void close() throws IOException {\nmultipleOutputs.close();\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(StationMapper.class);\nconf.setMapOutputKeyClass(Text.class);\nconf.setReducerClass(MultipleOutputsReducer.class);\nconf.setOutputKeyClass(NullWritable.class);\nconf.setOutputFormat(NullOutputFormat.class); // suppress empty part file\nMultipleOutputs.addMultiNamedOutput(conf, ""station"", TextOutputFormat.class,\nNullWritable.class, Text.class);\nJobClient.runJob(conf);\n208 | Chapter 7: MapReduce Types and Formats}\nreturn 0;\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new PartitionByStationUsingMultipleOutputs(),\nargs);\nSystem.exit(exitCode);\n}\nThe MultipleOutputs class is used to generate additional outputs to the usual output.\nOutputs are given names, and may be written to a single file (called single named out-\nput), or to multiple files (called multi named output). In this case, we want multiple files,\none for each station, so we use a multi named output, which we initialize in the driver\nby calling the addMultiNamedOutput() method of MultipleOutputs to specify the name\nof the output (here ""station""), the output format, and the output types. In addition,\nwe set the regular output format to be NullOutputFormat in order to suppress the usual\noutput.\nIn the reducer, where we generate the output, we construct an instance of MultipleOut\nputs in the configure() method, and assign it to an instance variable. We use the\nMultipleOutputs instance in the reduce() method to retrieve an OutputCollector for the\nmulti named output. The getCollector() method takes the name of the output\n(""station"" again) as well as a string identifying the part within the multi named output.\nHere we use the station identifier, with the “-” separator in the key removed, since only\nalphanumeric characters are allowed by MultipleOutputs.\nThe overall effect is to produce output files with the naming scheme station_<station\nidentifier>-r-<part_number>. The r appears in the name because the output is pro-\nduced by the reducer, and the part number is appended to be sure that there are no\ncollisions resulting from different partitions (reducers) writing output for the same\nstation. Since we partition by station, it cannot happen in this case (but it can in the\ngeneral case).\nIn one run, the first few output files were named as follows (other columns from the\ndirectory listing have been dropped):\n/output/station_01001099999-r-00027\n/output/station_01005099999-r-00013\n/output/station_01010099999-r-00015\n/output/station_01028099999-r-00014\n/output/station_01055099999-r-00000\n/output/station_01098099999-r-00011\n/output/station_01106099999-r-00025\n/output/station_01203099999-r-00029\n/output/station_01235099999-r-00018\n/output/station_01262099999-r-00004\nOutput Formats | 209What’s the Difference Between MultipleOutputFormat and\nMultipleOutputs?\nIt’s unfortunate (although not necessarily unusual in an open source project) to have\ntwo libraries that do almost the same thing, since it is confusing for users. To help you\nchoose which to use, here is a brief comparison.\nFeature MultipleOutputFormat MultipleOutputs\nComplete control over names of files and directories Yes No\nDifferent key and value types for different outputs No Yes\nUse from map and reduce in the same job No Yes\nMultiple outputs per record No Yes\nUse with any OutputFormat No, need to subclass Yes\nSo in summary, MultipleOutputs is more fully featured, but MultipleOutputFormat has\nmore control over the output directory structure and file naming.\nLazy Output\nFileOutputFormat subclasses will create output (part-nnnnn) files, even if they are empty.\nSome applications prefer that empty files not be created, which is where LazyOutput\nFormat helps.† It is a wrapper output format that ensures that the output file is created\nonly when the first record is emitted for a given partition. To use it, call its setOutput\nFormatClass() method with the JobConf and the underlying output format.\nStreaming and Pipes support a -lazyOutput option to enable LazyOutputFormat.\nDatabase Output\nThe output formats for writing to relational databases and to HBase are mentioned in\n“Database Input (and Output)” on page 201.\n† LazyOutputFormat is available from release 0.21.0 of Hadoop.\n210 | Chapter 7: MapReduce Types and FormatsCHAPTER 8\nMapReduce Features\nThis chapter looks at some of the more advanced features of MapReduce, including\ncounters and sorting and joining datasets.\nCounters\nThere are often things you would like to know about the data you are analyzing but\nwhich are peripheral to the analysis you are performing. For example, if you were\ncounting invalid records, and discovered that the proportion of invalid records in the\nwhole dataset was very high, you might be prompted to check why so many records\nwere being marked as invalid—perhaps there is a bug in the part of the program that\ndetects invalid records? Or if the data were of poor quality and genuinely did have very\nmany invalid records, after discovering this, you might decide to increase the size of\nthe dataset so that the number of good records was large enough for meaningful\nanalysis.\nCounters are a useful channel for gathering statistics about the job: for quality control,\nor for application level-statistics. They are also useful for problem diagnosis. If you are\ntempted to put a log message into your map or reduce task, then it is often better to\nsee whether you can use a counter instead to record that a particular condition occurred.\nIn addition to counter values being much easier to retrieve than log output for large\ndistributed jobs, you get a record of the number of times that condition occurred, which\nis more work to obtain from a set of logfiles.\nBuilt-in Counters\nHadoop maintains some built-in counters for every job (Table 8-1), which report var-\nious metrics for your job. For example, there are counters for the number of bytes and\nrecords processed, which allows you to confirm that the expected amount of input was\nconsumed and the expected amount of output was produced.\n211Table 8-1. Built-in counters\nGroup Counter Description\nMap-Reduce Map input records The number of input records consumed by all the maps in the job. Incremented\nFramework every time a record is read from a RecordReader and passed to the map’s\n           map() method by the framework.\nMap skipped records The number of input records skipped by all the maps in the job. See “Skipping\n                     Bad Records” on page 171.\nMap input bytes The number of bytes of uncompressed input consumed by all the maps in the\n               job. Incremented every time a record is read from a RecordReader and passed\n              to the map’s map() method by the framework.\nMap output records The number of map output records produced by all the maps in the job. Incre-\n                  mented every time the collect() method is called on a map’s OutputCol\n                   lector.\nMap output bytes The number of bytes of uncompressed output produced by all the maps in the\n                job. Incremented every time the collect() method is called on a map’s\n                 OutputCollector.\nCombine input records The number of input records consumed by all the combiners (if any) in the job.\n                     Incremented every time a value is read from the combiner’s iterator over values.\n                      Note that this count is the number of values consumed by the combiner, not the\n                     number of distinct key groups (which would not be a useful metric, since there\n                    is not necessarily one group per key for a combiner; see “Combiner Func-\n                     tions” on page 29, and also “Shuffle and Sort” on page 163).\nCombine output records The number of output records produced by all the combiners (if any) in the job.\n                      Incremented every time the collect() method is called on a combiner’s\n                       OutputCollector.\nReduce input groups The number of distinct key groups consumed by all the reducers in the job.\n                   Incremented every time the reducer’s reduce() method is called by the\n                    framework.\nReduce input records The number of input records consumed by all the reducers in the job. Incremented\n                    every time a value is read from the reducer’s iterator over values. If reducers\n                     consume all of their inputs this count should be the same as the count for Map\n                    output records.\nReduce output records The number of reduce output records produced by all the maps in the job.\n                     Incremented every time the collect() method is called on a reducer’s\n                      OutputCollector.\nReduce skipped groups The number of distinct key groups skipped by all the reducers in the job. See\n                     “Skipping Bad Records” on page 171.\nReduce skipped The number of input records skipped by all the reducers in the job.\nßrecords \nSpilled records The number of records spilled to disk in all map and reduce tasks in the job.\nFilesystem bytes read The number of bytes read by each filesystem by map and reduce tasks. There is\n                     a counter for each filesystem: Filesystem may be Local, HDFS, S3, KFS, etc.\nFilesystem bytes written The number of bytes written by each filesystem by map and reduce tasks.\nFile Systems\n212 | Chapter 8: MapReduce FeaturesGroup Counter Description\nJob Counters Launched map tasks The number of map tasks that were launched. Includes tasks that were started\n                               speculatively.\nLaunched reduce tasks The number of reduce tasks that were launched. Includes tasks that were started\n                     speculatively.\nFailed map tasks The number of map tasks that failed. See “Task Failure” on page 159 for potential\n                    causes.\nFailed reduce tasks The number of reduce tasks that failed.\nData-local map tasks The number of map tasks that ran on the same node as their input data.\nRack-local map tasks The number of map tasks that ran on a node in the same rack as their input data.\nOther local map tasks The number of map tasks that ran on a node in a different rack to their input\n                     data. Inter-rack bandwidth is scarce, and Hadoop tries to place map tasks close\n                    to their input data, so this count should be low.\nCounters are maintained by the task with which they are associated, and periodically\nsent to the tasktracker and then to the jobtracker, so they can be globally aggregated.\n(This is described in “Progress and Status Updates” on page 156.) The built-in Job\nCounters are actually maintained by the jobtracker, so they don’t need to be sent across\nthe network, unlike all other counters, including user-defined ones.\nA task’s counters are sent in full every time, rather than sending the counts since the\nlast transmission, since this guards against errors due to lost messages. Furthermore,\nduring a job run, counters may go down if a task fails. Counter values are definitive\nonly once a job has successfully completed.\nUser-Defined Java Counters\nMapReduce allows user code to define a set of counters, which are then incremented\nas desired in the mapper or reducer. Counters are defined by a Java enum, which serves\nto group related counters. A job may define an arbitrary number of enums, each with\nan arbitrary number of fields. The name of the enum is the group name, and the enum’s\nfields are the counter names. Counters are global: the MapReduce framework aggre-\ngates them across all maps and reduces to produce a grand total at the end of the job.\nWe created some counters in Chapter 5 for counting malformed records in the weather\ndataset. The program in Example 8-1 extends that example to count the number of\nmissing records and the distribution of temperature quality codes.\nCounters | 213Example 8-1. Application to run the maximum temperature job, including counting missing and\nmalformed fields and quality codes\npublic class MaxTemperatureWithCounters extends Configured implements Tool {\nenum Temperature {\nMISSING,\nMALFORMED\n}\nstatic class MaxTemperatureMapperWithCounters extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nparser.parse(value);\nif (parser.isValidTemperature()) {\nint airTemperature = parser.getAirTemperature();\noutput.collect(new Text(parser.getYear()),\nnew IntWritable(airTemperature));\n} else if (parser.isMalformedTemperature()) {\nSystem.err.println(""Ignoring possibly corrupt input: "" + value);\nreporter.incrCounter(Temperature.MALFORMED, 1);\n} else if (parser.isMissingTemperature()) {\nreporter.incrCounter(Temperature.MISSING, 1);\n}\n// dynamic counter\nreporter.incrCounter(""TemperatureQuality"", parser.getQuality(), 1);\n}\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(MaxTemperatureMapperWithCounters.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\n}\nJobClient.runJob(conf);\nreturn 0;\n214 | Chapter 8: MapReduce Features}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureWithCounters(), args);\nSystem.exit(exitCode);\n}\nThe best way to see what this program does is run it over the complete dataset:\n% hadoop jar job.jar MaxTemperatureWithCounters input/ncdc/all output-counters\nWhen the job has successfully completed, it prints out the counters at the end (this is\ndone by JobClient’s runJob() method). Here are the ones we are interested in:\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n09/04/20\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\n06:33:36\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nINFO\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nmapred.JobClient:\nTemperatureQuality\n2=1246032\n1=973422173\n0=1\n6=40066\n5=158291879\n4=10764500\n9=66136858\nAir Temperature Records\nMalformed=3\nMissing=66136856\nDynamic counters\nThe code makes use of a dynamic counter—one that isn’t defined by a Java enum. Since\na Java enum’s fields are defined at compile time, you can’t create new counters on the\nfly using enums. Here we want to count the distribution of temperature quality codes,\nand though the format specification defines the values that it can take, it is more con-\nvenient to use a dynamic counter to emit the values that it actually takes. The method\nwe use on the Reporter object takes a group and counter name using String names:\npublic void incrCounter(String group, String counter, long amount)\nThe two ways of creating and accessing counters—using enums and using Strings—\nare actually equivalent since Hadoop turns enums into Strings to send counters over\nRPC. Enums are slightly easier to work with, provide type safety, and are suitable for\nmost jobs. For the odd occasion when you need to create counters dynamically, you\ncan use the String interface.\nReadable counter names\nBy default, a counter’s name is the enum’s fully qualified Java classname. These names\nare not very readable when they appear on the web UI, or in the console, so Hadoop\nprovides a way to change the display names using resource bundles. We’ve done this\nhere, so we see “Air Temperature Records” instead of “Temperature$MISSING.” For\ndynamic counters, the group and counter names are used for the display names, so this\nis not normally an issue.\nCounters | 215The recipe to provide readable names is as follows. Create a properties file named after\nthe enum, using an underscore as a separator for nested classes. The properties file\nshould be in the same directory as the top-level class containing the enum. The file is\nnamed MaxTemperatureWithCounters_Temperature.properties for the counters in Ex-\nample 8-1.\nThe properties file should contain a single property named CounterGroupName, whose\nvalue is the display name for the whole group. Then each field in the enum should have\na corresponding property defined for it, whose name is the name of the field suffixed\nwith .name, and whose value is the display name for the counter. Here are the contents\nof MaxTemperatureWithCounters_Temperature.properties:\nCounterGroupName=Air Temperature Records\nMISSING.name=Missing\nMALFORMED.name=Malformed\nHadoop uses the standard Java localization mechanisms to load the correct properties\nfor the locale you are running in, so, for example, you can create a Chinese version of\nthe properties in a file named MaxTemperatureWithCounters_Tempera\nture_zh_CN.properties and they will be used when running in the zh_CN locale. Refer\nto the documentation for java.util.PropertyResourceBundle for more information.\nRetrieving counters\nIn addition to being available via the web UI and the command line (using hadoop job\n-counter), you can retrieve counter values using the Java API. You can do this while\nthe job is running, although it is more usual to get counters at the end of a job run,\nwhen they are stable. Example 8-2 shows a program that calculates the proportion of\nrecords that have missing temperature fields.\nExample 8-2. Application to calculate the proportion of records with missing temperature fields\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.mapred.*;\nimport org.apache.hadoop.util.*;\npublic class MissingTemperatureFields extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 1) {\nJobBuilder.printUsage(this, ""<job ID>"");\nreturn -1;\n}\nJobClient jobClient = new JobClient(new JobConf(getConf()));\nString jobID = args[0];\nRunningJob job = jobClient.getJob(JobID.forName(jobID));\nif (job == null) {\nSystem.err.printf(""No job with ID %s found.\\n"", jobID);\nreturn -1;\n}\n216 | Chapter 8: MapReduce Featuresif (!job.isComplete()) {\nSystem.err.printf(""Job %s is not complete.\\n"", jobID);\nreturn -1;\n}\nCounters counters = job.getCounters();\nlong missing = counters.getCounter(\nMaxTemperatureWithCounters.Temperature.MISSING);\nlong total = counters.findCounter(""org.apache.hadoop.mapred.Task$Counter"",\n""MAP_INPUT_RECORDS"").getCounter();\nSystem.out.printf(""Records with missing temperature fields: %.2f%%\\n"",\n100.0 * missing / total);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MissingTemperatureFields(), args);\nSystem.exit(exitCode);\n}\nFirst we retrieve a RunningJob object from a JobClient, by calling the getJob() method\nwith the job ID. We check whether there is actually a job with the given ID. There may\nnot be, either because the ID was incorrectly specified or because the jobtracker no\nlonger has a reference to the job (only the last 100 jobs are kept in memory, and all are\ncleared out if the jobtracker is restarted).\nAfter confirming that the job has completed, we call the RunningJob’s getCounters()\nmethod, which returns a Counters object, encapsulating all the counters for a job. The\nCounters class provides various methods for finding the names and values of counters.\nWe use the getCounter() method, which takes an enum to find the number of records\nthat had a missing temperature field.\nThere are also findCounter() methods, all of which return a Counter object. We use\nthis form to retrieve the built-in counter for map input records. To do this, we refer to\nthe counter by its group name—the fully qualified Java classname for the enum—and\ncounter name (both strings).*\nFinally, we print the proportion of records that had a missing temperature field. Here’s\nwhat we get for the whole weather dataset:\n% hadoop jar job.jar MissingTemperatureFields job_200904200610_0003\nRecords with missing temperature fields: 5.47%\n* The built-in counter’s enums are not currently a part of the public API, so this is the only way to retrieve\nthem. https://issues.apache.org/jira/browse/HADOOP-4043 will remedy this deficiency.\nCounters | 217User-Defined Streaming Counters\nA Streaming MapReduce program can increment counters by sending a specially for-\nmatted line to the standard error stream, which is co-opted as a control channel in this\ncase. The line must have the following format:\nreporter:counter:group,counter,amount\nThis snippet in Python shows how to increment the “Missing” counter in the “Tem-\nperature” group by one:\nsys.stderr.write(""reporter:counter:Temperature,Missing,1\\n"")\nIn a similar way, a status message may be sent with a line formatted like this:\nreporter:status:message\nSorting\nThe ability to sort data is at the heart of MapReduce. Even if your application isn’t\nconcerned with sorting per se, it may be able to use the sorting stage that MapReduce\nprovides to organize its data. In this section, we will examine different ways of sorting\ndatasets, and how you can control the sort order in MapReduce.\nPreparation\nWe are going to sort the weather dataset by temperature. Storing temperatures as\nText objects doesn’t work for sorting purposes, since signed integers don’t sort lexico-\ngraphically.† Instead, we are going to store the data using sequence files whose IntWrit\nable keys represent the temperature (and sort correctly), and whose Text values are the\nlines of data.\nThe MapReduce job in Example 8-3 is a map-only job that also filters the input to\nremove records that don’t have a valid temperature reading. Each map creates a single\nblock-compressed sequence file as output. It is invoked with the following command:\n% hadoop jar job.jar SortDataPreprocessor input/ncdc/all input/ncdc/all-seq\nExample 8-3. A MapReduce program for transforming the weather data into SequenceFile format\npublic class SortDataPreprocessor extends Configured implements Tool {\nstatic class CleanerMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, IntWritable, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\n† One commonly used workaround for this problem—particularly in text-based Streaming applications—is\nto add an offset to eliminate all negative numbers, and left pad with zeros, so all numbers are the same number\nof characters. However, see “Streaming” on page 231 for another approach.\n218 | Chapter 8: MapReduce Featurespublic void map(LongWritable key, Text value,\nOutputCollector<IntWritable, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new IntWritable(parser.getAirTemperature()), value);\n}\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(CleanerMapper.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputValueClass(Text.class);\nconf.setNumReduceTasks(0);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\nJobClient.runJob(conf);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortDataPreprocessor(), args);\nSystem.exit(exitCode);\n}\nPartial Sort\nIn “The Default MapReduce Job” on page 178, we saw that, by default, MapReduce\nwill sort input records by their keys. Example 8-4 is a variation for sorting sequence\nfiles with IntWritable keys.\nExample 8-4. A MapReduce program for sorting a SequenceFile with IntWritable keys using the\ndefault HashPartitioner\npublic class SortByTemperatureUsingHashPartitioner extends Configured\nimplements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\nSorting | 219}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortByTemperatureUsingHashPartitioner(),\nargs);\nSystem.exit(exitCode);\n}\nControlling Sort Order\nThe sort order for keys is controlled by a RawComparator, which is found as follows:\n1. If the property mapred.output.key.comparator.class is set, an instance of that class\nis used. (The setOutputKeyComparatorClass() method on JobConf is a convenient\nway to set this property.)\n2. Otherwise, keys must be a subclass of WritableComparable and the registered com-\nparator for the key class is used.\n3. If there is no registered comparator, then a RawComparator is used that deserializes\nthe byte streams being compared into objects, and delegates to the WritableCom\nparable’s compareTo() method.\nThese rules reinforce why it’s important to register optimized versions of RawCompara\ntors for your own custom Writable classes (which is covered in “Implementing a Raw-\nComparator for speed” on page 99), and also that it’s straightforward to override the\nsort order by setting your own comparator (we do this in “Secondary\nSort” on page 227).\nSuppose we run this program using 30 reducers:‡\n% hadoop jar job.jar SortByTemperatureUsingHashPartitioner \\\n-D mapred.reduce.tasks=30 input/ncdc/all-seq output-hashsort\nThis command produces 30 output files, each of which is sorted. However, there is no\neasy way to combine the files (by concatenation, for example, in the case of plain-text\n‡ See “Sorting and merging SequenceFiles” on page 108 for how to do the same thing using the sort program\nexample that comes with Hadoop.\n220 | Chapter 8: MapReduce Featuresfiles) to produce a globally sorted file. For many applications, this doesn’t matter. For\nexample, having a partially sorted set of files is fine if you want to do lookups.\nAn application: Partitioned MapFile lookups\nTo perform lookups by key, for instance, having multiple files works well. If we change\nthe output format to be a MapFileOutputFormat, as shown in Example 8-5, then the\noutput is 30 map files, which we can perform lookups against.\nExample 8-5. A MapReduce program for sorting a SequenceFile and producing MapFiles as output\npublic class SortByTemperatureToMapFile extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(MapFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new SortByTemperatureToMapFile(), args);\nSystem.exit(exitCode);\n}\nMapFileOutputFormat provides a pair of convenience static methods for performing\nlookups against MapReduce output; their use is shown in Example 8-6.\nExample 8-6. Retrieve the first entry with a given key from a collection of MapFiles\npublic class LookupRecordByTemperature extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nJobBuilder.printUsage(this, ""<path> <key>"");\nreturn -1;\n}\nPath path = new Path(args[0]);\nIntWritable key = new IntWritable(Integer.parseInt(args[1]));\nFileSystem fs = path.getFileSystem(getConf());\nSorting | 221}\n}\nReader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf());\nPartitioner<IntWritable, Text> partitioner =\nnew HashPartitioner<IntWritable, Text>();\nText val = new Text();\nWritable entry =\nMapFileOutputFormat.getEntry(readers, partitioner, key, val);\nif (entry == null) {\nSystem.err.println(""Key not found: "" + key);\nreturn -1;\n}\nNcdcRecordParser parser = new NcdcRecordParser();\nparser.parse(val.toString());\nSystem.out.printf(""%s\\t%s\\n"", parser.getStationId(), parser.getYear());\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new LookupRecordByTemperature(), args);\nSystem.exit(exitCode);\n}\nThe getReaders() method opens a MapFile.Reader for each of the output files created\nby the MapReduce job. The getEntry() method then uses the partitioner to choose the\nreader for the key, and finds the value for that key by calling Reader’s get() method. If\ngetEntry() returns null, it means no matching key was found. Otherwise, it returns\nthe value, which we translate into a station ID and year.\nTo see this in action, let’s find the first entry for a temperature of –10°C (remember\nthat temperatures are stored as integers representing tenths of a degree, which is why\nwe ask for a temperature of –100):\n% hadoop jar job.jar LookupRecordByTemperature output-hashmapsort -100\n357460-99999\n1956\nWe can also use the readers directly, in order to get all the records for a given key. The\narray of readers that is returned is ordered by partition, so that the reader for a given\nkey may be found using the same partitioner that was used in the MapReduce job:\nReader reader = readers[partitioner.getPartition(key, val, readers.length)];\nThen once we have the reader, we get the first key using MapFile’s get() method, then\nrepeatedly call next() to retrieve the next key and value, until the key changes. A pro-\ngram to do this is shown in Example 8-7.\nExample 8-7. Retrieve all entries with a given key from a collection of MapFiles\npublic class LookupRecordsByTemperature extends Configured implements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 2) {\nJobBuilder.printUsage(this, ""<path> <key>"");\n222 | Chapter 8: MapReduce Featuresreturn -1;\n}\nPath path = new Path(args[0]);\nIntWritable key = new IntWritable(Integer.parseInt(args[1]));\nFileSystem fs = path.getFileSystem(getConf());\nReader[] readers = MapFileOutputFormat.getReaders(fs, path, getConf());\nPartitioner<IntWritable, Text> partitioner =\nnew HashPartitioner<IntWritable, Text>();\nText val = new Text();\n}\n}\nReader reader = readers[partitioner.getPartition(key, val, readers.length)];\nWritable entry = reader.get(key, val);\nif (entry == null) {\nSystem.err.println(""Key not found: "" + key);\nreturn -1;\n}\nNcdcRecordParser parser = new NcdcRecordParser();\nIntWritable nextKey = new IntWritable();\ndo {\nparser.parse(val.toString());\nSystem.out.printf(""%s\\t%s\\n"", parser.getStationId(), parser.getYear());\n} while(reader.next(nextKey, val) && key.equals(nextKey));\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new LookupRecordsByTemperature(), args);\nSystem.exit(exitCode);\n}\nAnd here is a sample run to retrieve all readings of –10°C and count them:\n% hadoop jar job.jar LookupRecordsByTemperature output-hashmapsort -100 \\\n2> /dev/null | wc -l\n1489272\nTotal Sort\nHow can you produce a globally sorted file using Hadoop? The naive answer is to use\na single partition.§ But this is incredibly inefficient for large files, since one machine\nhas to process all of the output, so you are throwing away the benefits of the parallel\narchitecture that MapReduce provides.\nInstead it is possible to produce a set of sorted files that, if concatenated, would form\na globally sorted file. The secret to doing this is to use a partitioner that respects the\ntotal order of the output. For example, if we had four partitions, we could put keys for\ntemperatures less than –10°C in the first partition, those between –10°C and 0°C in the\nsecond, those between 0°C and 10°C in the third, and those over 10°C in the fourth.\n§ A better answer is to use Pig, which can sort with a single command. See “Sorting Data” on page 338.\nSorting | 223Although this approach works, you have to choose your partition sizes carefully to\nensure that they are fairly even so that job times aren’t dominated by a single reducer.\nFor the partitioning scheme just described, the relative sizes of the partitions are as\nfollows:\nTemperature range < –10°C [–10°C, 0°C) [0°C, 10°C) >= 10°C\nProportion of records 11% 13% 17% 59%\nThese partitions are not very even. To construct more even partitions, we need to have\na better understanding of the temperature distribution for the whole dataset. It’s fairly\neasy to write a MapReduce job to count the number of records that fall into a collection\nof temperature buckets. For example, Figure 8-1 shows the distribution for buckets of\nsize 1°C, where each point on the plot corresponds to one bucket.\nFigure 8-1. Temperature distribution for the weather dataset\nWhile we could use this information to construct a very even set of partitions, the fact\nthat we needed to run a job that used the entire dataset to construct them is not ideal.\nIt’s possible to get a fairly even set of partitions, by sampling the key space. The idea\nbehind sampling is that you look at a small subset of the keys to approximate the key\ndistribution, which is then used to construct partitions. Luckily, we don’t have to write\nthe code to do this ourselves, as Hadoop comes with a selection of samplers.\n224 | Chapter 8: MapReduce FeaturesThe InputSampler class defines a nested Sampler interface whose implementations re-\nturn a sample of keys given an InputFormat and JobConf:\npublic interface Sampler<K,V> {\nK[] getSample(InputFormat<K,V> inf, JobConf job) throws IOException;\n}\nThis interface is not usually called directly by clients. Instead, the writePartition\nFile() static method on InputSampler is used, which creates a sequence file to store the\nkeys that define the partitions:\npublic static <K,V> void writePartitionFile(JobConf job,\nSampler<K,V> sampler) throws IOException\nThe sequence file is used by TotalOrderPartitioner to create partitions for the sort job.\nExample 8-8 puts it all together.\nExample 8-8. A MapReduce program for sorting a SequenceFile with IntWritable keys using the\nTotalOrderPartitioner to globally sort the data\npublic class SortByTemperatureUsingTotalOrderPartitioner extends Configured\nimplements Tool {\n@Override\npublic int run(String[] args) throws Exception {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setInputFormat(SequenceFileInputFormat.class);\nconf.setOutputKeyClass(IntWritable.class);\nconf.setOutputFormat(SequenceFileOutputFormat.class);\nSequenceFileOutputFormat.setCompressOutput(conf, true);\nSequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\nSequenceFileOutputFormat.setOutputCompressionType(conf,\nCompressionType.BLOCK);\nconf.setPartitionerClass(TotalOrderPartitioner.class);\nInputSampler.Sampler<IntWritable, Text> sampler =\nnew InputSampler.RandomSampler<IntWritable, Text>(0.1, 10000, 10);\nPath input = FileInputFormat.getInputPaths(conf)[0];\ninput = input.makeQualified(input.getFileSystem(conf));\nPath partitionFile = new Path(input, ""_partitions"");\nTotalOrderPartitioner.setPartitionFile(conf, partitionFile);\nInputSampler.writePartitionFile(conf, sampler);\n// Add to DistributedCache\nURI partitionUri = new URI(partitionFile.toString() + ""#_partitions"");\nDistributedCache.addCacheFile(partitionUri, conf);\nDistributedCache.createSymlink(conf);\nSorting | 225}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew SortByTemperatureUsingTotalOrderPartitioner(), args);\nSystem.exit(exitCode);\n}\nWe use a RandomSampler, which chooses keys with a uniform probability—here, 0.1.\nThere are also parameters for the maximum number of samples to take, and the max-\nimum number of splits to sample (here, 10,000 and 10, respectively; these settings are\nthe defaults when InputSampler is run as an application), and the sampler stops when\nthe first of these limits is met. Samplers run on the client, making it important to limit\nthe number of splits that are downloaded, so the sampler runs quickly. In practice, the\ntime taken to run the sampler is a small fraction of the overall job time.\nThe partition file that InputSampler writes is called _partitions, which we have set to be\nin the input directory (it will not be picked up as an input file since it starts with an\nunderscore). To share the partition file with the tasks running on the cluster, we add\nit to the distributed cache (see “Distributed Cache” on page 239).\nOn one run, the sampler chose –5.6°C, 13.9°C, and 22.0°C as partition boundaries (for\nfour partitions), which translates into more even partition sizes that the earlier choice\nof partitions:\nTemperature range < –5.6°C [–5.6°C, 13.9°C) [13.9°C, 22.0°C) >= 22.0°C\nProportion of records 29% 24% 23% 24%\nYour input data determines the best sampler for you to use. For example, SplitSam\npler, which samples only the first n records in a split, is not so good for sorted data‖\nbecause it doesn’t select keys from throughout the split.\nOn the other hand, IntervalSampler chooses keys at regular intervals through the split,\nand makes a better choice for sorted data. RandomSampler is a good general-purpose\nsampler. If none of these suits your application (and remember that the point of sam-\npling is to produce partitions that are approximately equal in size), you can write your\nown implementation of the Sampler interface.\nOne of the nice properties of InputSampler and TotalOrderPartitioner is that you are\nfree to choose the number of partitions. This choice is normally driven by the number\nof reducer slots in you cluster (choose a number slightly fewer than the total, to allow\n‖ In some applications, it’s common for some of the input to already be sorted, or at least partially sorted. For\nexample, the weather dataset is ordered by time, which may introduce certain biases, making the\nRandomSampler a safe choice.\n226 | Chapter 8: MapReduce Featuresfor failures). However, TotalOrderPartitioner will work only if the partition\nboundaries are distinct: one problem with choosing a high number is that you may get\ncollisions if you have a small key space.\nHere’s how we run it:\n% hadoop jar job.jar SortByTemperatureUsingTotalOrderPartitioner \\\n-D mapred.reduce.tasks=30 input/ncdc/all-seq output-totalsort\nThe program produces 30 output partitions, each of which is internally sorted; in ad-\ndition, for these partitions, all the keys in partition i are less than the keys in partition\ni + 1.\nSecondary Sort\nThe MapReduce framework sorts the records by key before they reach the reducers.\nFor any particular key, however, the values are not sorted. The order that the values\nappear is not even stable from one run to the next, since they come from different map\ntasks, which may finish at different times from run to run. Generally speaking, most\nMapReduce programs are written so as not to depend on the order that the values\nappear to the reduce function. However, it is possible to impose an order on the values\nby sorting and grouping the keys in a particular way.\nTo illustrate the idea, consider the MapReduce program for calculating the maximum\ntemperature for each year. If we arranged for the values (temperatures) to be sorted in\ndescending order, we wouldn’t have to iterate through them to find the maximum—\nwe could take the first for each year and ignore the rest. (This approach isn’t the most\nefficient way to solve this particular problem, but it illustrates how secondary sort works\nin general.)\nTo achieve this, we change our keys to be composite: a combination of year and\ntemperature. We want the sort order for keys to be by year (ascending) and then by\ntemperature (descending):\n1900\n1900\n1900\n...\n1901\n1901\n35°C\n34°C\n34°C\n36°C\n35°C\nIf all we did was change the key, then this wouldn’t help since now records for the same\nyear would not (in general) go to the same reducer since they have different keys. For\nexample, (1900, 35°C) and (1900, 34°C) could go to different reducers. By setting a\npartitioner to partition by the year part of the key, we can guarantee that records for\nthe same year go to the same reducer. This still isn’t enough to achieve our goal, how-\never. A partitioner ensures only that one reducer receives all the records for a year; it\ndoesn’t change the fact that the reducer groups by key within the partition:\nSorting | 227The final piece of the puzzle is the setting to control the grouping. If we group values\nin the reducer by the year part of the key, then we will see all the records for the same\nyear in one reduce group. And since they are sorted by temperature in descending order,\nthe first is the maximum temperature:\nTo summarize, there is a recipe here to get the effect of sorting by value:\n• Make the key a composite of the natural key and the natural value.\n• The key comparator should order by the composite key, that is, the natural key\nand natural value.\n• The partitioner and grouping comparator for the composite key should consider\nonly the natural key for partitioning and grouping.\nJava code\nPutting this all together results in the code in Example 8-9. This program uses the plain-\ntext input again.\nExample 8-9. Application to find the maximum temperature by sorting temperatures in the key\npublic class MaxTemperatureUsingSecondarySort\nextends Configured implements Tool {\nstatic class MaxTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, IntPair, NullWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<IntPair, NullWritable> output, Reporter reporter)\nthrows IOException {\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new IntPair(parser.getYearInt(),\n228 | Chapter 8: MapReduce Features}\n}\n}\n+ parser.getAirTemperature()), NullWritable.get());\nstatic class MaxTemperatureReducer extends MapReduceBase\nimplements Reducer<IntPair, NullWritable, IntPair, NullWritable> {\npublic void reduce(IntPair key, Iterator<NullWritable> values,\nOutputCollector<IntPair, NullWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\noutput.collect(key, NullWritable.get());\npublic static class FirstPartitioner\nimplements Partitioner<IntPair, NullWritable> {\n@Override\npublic void configure(JobConf job) {}\n}\n@Override\npublic int getPartition(IntPair key, NullWritable value, int numPartitions) {\nreturn Math.abs(key.getFirst() * 127) % numPartitions;\n}\npublic static class KeyComparator extends WritableComparator {\nprotected KeyComparator() {\nsuper(IntPair.class, true);\n}\n@Override\npublic int compare(WritableComparable w1, WritableComparable w2) {\nIntPair ip1 = (IntPair) w1;\nIntPair ip2 = (IntPair) w2;\nint cmp = IntPair.compare(ip1.getFirst(), ip2.getFirst());\nif (cmp != 0) {\nreturn cmp;\n}\nreturn -IntPair.compare(ip1.getSecond(), ip2.getSecond()); //reverse\n}\n}\npublic static class GroupComparator extends WritableComparator {\nprotected GroupComparator() {\nsuper(IntPair.class, true);\n}\n@Override\npublic int compare(WritableComparable w1, WritableComparable w2) {\nIntPair ip1 = (IntPair) w1;\nIntPair ip2 = (IntPair) w2;\nreturn IntPair.compare(ip1.getFirst(), ip2.getFirst());\n}\n}\nSorting | 229@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setMapperClass(MaxTemperatureMapper.class);\nconf.setPartitionerClass(FirstPartitioner.class);\nconf.setOutputKeyComparatorClass(KeyComparator.class);\nconf.setOutputValueGroupingComparator(GroupComparator.class);\nconf.setReducerClass(MaxTemperatureReducer.class);\nconf.setOutputKeyClass(IntPair.class);\nconf.setOutputValueClass(NullWritable.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new MaxTemperatureUsingSecondarySort(), args);\nSystem.exit(exitCode);\n}\nIn the mapper, we create a key representing the year and temperature, using an IntPair\nWritable implementation. (IntPair is like the TextPair class we developed in “Imple-\nmenting a Custom Writable” on page 96.) We don’t need to carry any information in\nthe value, since we can get the first (maximum) temperature in the reducer from the\nkey, so we use a NullWritable. The reducer emits the first key, which due to the sec-\nondary sorting, is an IntPair for the year and its maximum temperature. IntPair’s\ntoString() method creates a tab-separated string, so the output is a set of tab-separated\nyear-temperature pairs.\nMany applications need to access all the sorted values, not just the first\nvalue as we have provided here. To do this, you need to populate the\nvalue fields since in the reducer you can retrieve only the first key. This\nnecessitates some unavoidable duplication of information between key\nand value.\n230 | Chapter 8: MapReduce FeaturesWe set the partitioner to partition by the first field of the key (the year), using a custom\npartitioner. To sort keys by year (ascending) and temperature (descending), we use a\ncustom key comparator that extracts the fields and performs the appropriate compar-\nisons. Similarly, to group keys by year, we set a custom comparator, using setOutput\nValueGroupingComparator(), to extract the first field of the key for comparison.#\nRunning this program gives the maximum temperatures for each year:\n% hadoop jar job.jar MaxTemperatureUsingSecondarySort input/ncdc/all output-secondarysort\n% hadoop fs -cat output-secondarysort/part-* | sort | head\n1901\n317\n1902\n244\n1903\n289\n1904\n256\n1905\n283\n1906\n294\n1907\n283\n1908\n289\n1909\n278\n1910\n294\nStreaming\nTo do a secondary sort in Streaming, we can take advantage of a couple of library classes\nthat Hadoop provides. Here’s the driver that we can use to do a secondary sort:\nhadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-D stream.num.map.output.key.fields=2 \\\n-D mapred.text.key.partitioner.options=-k1,1 \\\n-D mapred.output.key.comparator.class=\\\norg.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n-D mapred.text.key.comparator.options=""-k1n -k2nr"" \\\n-input input/ncdc/all \\\n-output output_secondarysort_streaming \\\n-mapper src/main/ch08/python/secondary_sort_map.py \\\n-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n-reducer src/main/ch08/python/secondary_sort_reduce.py \\\n-file src/main/ch08/python/secondary_sort_map.py \\\n-file src/main/ch08/python/secondary_sort_reduce.py\nOur map function (Example 8-10) emits records with year and temperature fields. We\nwant to treat the combination of both of these fields as the key, so we set\nstream.num.map.output.key.fields to 2. This means that values will be empty, just like\nin the Java case.\n# For simplicity, these custom comparators as shown are not optimized; see “Implementing a RawComparator\nfor speed” on page 99 for the steps we would need to take to make them faster.\nSorting | 231Example 8-10. Map function for secondary sort in Python\n#!/usr/bin/env python\nimport re\nimport sys\nfor line in sys.stdin:\nval = line.strip()\n(year, temp, q) = (val[15:19], int(val[87:92]), val[92:93])\nif temp == 9999:\nsys.stderr.write(""reporter:counter:Temperature,Missing,1\\n"")\nelif re.match(""[01459]"", q):\nprint ""%s\\t%s"" % (year, temp)\nHowever, we don’t want to partition by the entire key, so we use the KeyFieldBased\nPartitioner partitioner, which allows us to partition by a part of the key. The specifi-\ncation mapred.text.key.partitioner.options configures the partitioner. The value\n-k1,1 instructs the partitioner to use only the first field of the key, where fields are\nassumed to be separated by a string defined by the map.output.key.field.separator\nproperty (a tab character by default).\nNext, we want a comparator that sorts the year field in ascending order and the tem-\nperature field in descending order, so that the reduce function can simply return the\nfirst record in each group. Hadoop provides KeyFieldBasedComparator, which is ideal\nfor this purpose. The comparison order is defined by a specification that is like the one\nused for GNU sort. It is set using the mapred.text.key.comparator.options property.\nThe value -k1n -k2nr used in this example means “sort by the first field in numerical\norder, then by the second field in reverse numerical order.” Like its partitioner cousin,\nKeyFieldBasedPartitioner, it uses the separator defined by the map.out\nput.key.field.separator to split a key into fields.\nIn the Java version, we had to set the grouping comparator; however, in Streaming\ngroups are not demarcated in any way, so in the reduce function we have to detect the\ngroup boundaries ourselves by looking for when the year changes (Example 8-11).\nExample 8-11. Reducer function for secondary sort in Python\n#!/usr/bin/env python\nimport sys\nlast_group = None\nfor line in sys.stdin:\nval = line.strip()\n(year, temp) = val.split(""\\t"")\ngroup = year\nif last_group != group:\nprint val\nlast_group = group\nWhen we run the streaming program, we get the same output as the Java version.\n232 | Chapter 8: MapReduce FeaturesFinally, note that KeyFieldBasedPartitioner and KeyFieldBasedComparator are not con-\nfined to use in Streaming programs—they are applicable to Java MapReduce programs,\ntoo.\nJoins\nMapReduce can perform joins between large datasets, but writing the code to do joins\nfrom scratch is fairly involved. Rather than writing MapReduce programs, you might\nconsider using a higher-level framework such as Pig, Hive, or Cascading, in which join\noperations are a core part of the implementation.\nLet’s briefly consider the problem we are trying to solve. We have two datasets; for\nexample, the weather stations database, and the weather records—and we want to\nreconcile the two. For example, we want to see each station’s history, with the station’s\nmetadata inlined in each output row. This is illustrated in Figure 8-2.\nHow we implement the join depends on how large the datasets are and how they are\npartitioned. If one dataset is large (the weather records) but the other one is small\nenough to be distributed to each node in the cluster (as the station metadata is), then\nthe join can be effected by a MapReduce job that brings the records for each station\ntogether (a partial sort on station ID, for example). The mapper or reducer uses the\nsmaller dataset to look up the station metadata for a station ID, so it can be written out\nwith each record. See “Side Data Distribution” on page 238 for a discussion of this\napproach, where we focus on the mechanics of distributing the data to tasktrackers.\nIf both datasets are too large for either to be copied to each node in the cluster, then\nwe can join them using MapReduce, using either a map-side join or a reduce-side join.\nOne common example of this case is a user database, and a log of some user activity\n(such as access logs). For a popular service, it is not feasible to distribute the user\ndatabase (or the logs) to all the MapReduce nodes.\nMap-Side Joins\nA map-side join works by performing the join before the data reaches the map function.\nFor this to work, though, the inputs to each map must be partitioned and sorted in a\nparticular way. Each input dataset must be divided into the same number of partitions,\nand it must be sorted by the same key (the join key) in each source. All the records for\na particular key must reside in the same partition. This may sound like a strict require-\nment (and it is), but it actually fits the description of the output of a MapReduce job.\nA map-side join can be used to join the outputs of several jobs that had the same number\nof reducers, the same keys, and output files that are not splittable (by being smaller\nthan an HDFS block, or by virtue of being gzip compressed, for example). In the context\nof the weather example, if we ran a partial sort on the stations file by station ID, and\nanother, identical sort on the records, again by station ID, and with the same number\nJoins | 233Figure 8-2. Inner join of two datasets\nof reducers, then the two outputs would satisfy the conditions for running a map-side\njoin.\nUse a CompositeInputFormat from the org.apache.hadoop.mapred.join package to run\na map-side join. The input sources and join type (inner or outer) for CompositeInput\nFormat are configured through a join expression that is written according to a simple\ngrammar. The package documentation has details and examples.\nThe org.apache.hadoop.examples.Join example is a general-purpose command-line\nprogram for running a map-side join, since it allows you to run a MapReduce job for\nany specified mapper and reducer, over multiple inputs that are joined with a given join\noperation.\n234 | Chapter 8: MapReduce FeaturesReduce-Side Joins\nA reduce-side join is more general than a map-side join, in that the input datasets don’t\nhave to be structured in any particular way, but it is less efficient as both datasets have\nto go through the MapReduce shuffle. The basic idea is that the mapper tags each record\nwith its source, and uses the join key as the map output key so that the records with\nthe same key are brought together in the reducer. We use several ingredients to make\nthis work in practice:\nMultiple inputs\nThe input sources for the datasets have different formats, in general, so it is very\nconvenient to use the MultipleInputs class (see “Multiple Inputs” on page 200) to\nseparate the logic for parsing and tagging each source.\nSecondary sort\nAs described, the reducer will see the records from both sources that have same\nkey, but they are not guaranteed to be in any particular order. However, to perform\nthe join, it is important to have the data from one source before another. For the\nweather data join, the station record must be the first of the values seen for each\nkey, so the reducer can fill in the weather records with the station name and emit\nthem straightaway. Of course, it would be possible to receive the records in any\norder if we buffered them in memory, but this should be avoided, since the number\nof records in any group may be very large and exceed the amount of memory avail-\nable to the reducer.*\nWe saw in “Secondary Sort” on page 227 how to impose an order on the values\nfor each key that the reducers see, so we use this technique here.\nTo tag each record, we use TextPair from Chapter 4 for the keys, to store the station\nID, and the tag. The only requirement for the tag values is that they sort in such a way\nthat the station records come before the weather records. This can be achieved by\ntagging station records as 0 and weather records as 1. The mapper classes to do this are\nshown in Examples 8-12 and 8-13.\nExample 8-12. Mapper for tagging station records for a reduce-side join\npublic class JoinStationMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, TextPair, Text> {\nprivate NcdcStationMetadataParser parser = new NcdcStationMetadataParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<TextPair, Text> output, Reporter reporter)\nthrows IOException {\nif (parser.parse(value)) {\noutput.collect(new TextPair(parser.getStationId(), ""0""),\n* The data_join package in the contrib directory implements reduce-side joins by buffering records in memory,\nso it suffers from this limitation.\nJoins | 235}\n}\n}\nnew Text(parser.getStationName()));\nExample 8-13. Mapper for tagging weather records for a reduce-side join\npublic class JoinRecordMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, TextPair, Text> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<TextPair, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\noutput.collect(new TextPair(parser.getStationId(), ""1""), value);\nThe reducer knows that it will receive the station record first, so it extracts its name\nfrom the value and writes it out as a part of every output record (Example 8-14).\nExample 8-14. Reducer for joining tagged station records with tagged weather records\npublic class JoinReducer extends MapReduceBase implements\nReducer<TextPair, Text, Text, Text> {\npublic void reduce(TextPair key, Iterator<Text> values,\nOutputCollector<Text, Text> output, Reporter reporter)\nthrows IOException {\n}\n}\nText stationName = new Text(values.next());\nwhile (values.hasNext()) {\nText record = values.next();\nText outValue = new Text(stationName.toString() + ""\\t"" + record.toString());\noutput.collect(key.getFirst(), outValue);\n}\nThe code assumes that every station ID in the weather records has exactly one matching\nrecord in the station dataset. If this were not the case, we would need to generalize the\ncode to put the tag into the value objects, by using another TextPair. The reduce()\nmethod would then be able to tell which entries were station names, and detect (and\nhandle) missing or duplicate entries, before processing the weather records.\n236 | Chapter 8: MapReduce FeaturesBecause objects in the reducer’s values iterator are re-used (for efficiency\npurposes), it is vital that the code makes a copy of the first Text object\nfrom the values iterator:\nText stationName = new Text(values.next());\nIf the copy is not made, then the stationName reference will refer to the\nvalue just read when it is turned into a string, which is a bug.\nTying the job together is the driver class, shown in Example 8-15. The essential point\nis that we partition and group on the first part of the key, the station ID, which we do\nwith a custom Partitioner (KeyPartitioner), and a custom comparator, FirstCompara\ntor (from TextPair).\nExample 8-15. Application to join weather records with station names\npublic class JoinRecordWithStationName extends Configured implements Tool {\npublic static class KeyPartitioner implements Partitioner<TextPair, Text> {\n@Override\npublic void configure(JobConf job) {}\n}\n@Override\npublic int getPartition(TextPair key, Text value, int numPartitions) {\nreturn (key.getFirst().hashCode() & Integer.MAX_VALUE) % numPartitions;\n}\n@Override\npublic int run(String[] args) throws Exception {\nif (args.length != 3) {\nJobBuilder.printUsage(this, ""<ncdc input> <station input> <output>"");\nreturn -1;\n}\nJobConf conf = new JobConf(getConf(), getClass());\nconf.setJobName(""Join record with station name"");\nPath ncdcInputPath = new Path(args[0]);\nPath stationInputPath = new Path(args[1]);\nPath outputPath = new Path(args[2]);\nMultipleInputs.addInputPath(conf, ncdcInputPath,\nTextInputFormat.class, JoinRecordMapper.class);\nMultipleInputs.addInputPath(conf, stationInputPath,\nTextInputFormat.class, JoinStationMapper.class);\nFileOutputFormat.setOutputPath(conf, outputPath);\nconf.setPartitionerClass(KeyPartitioner.class);\nconf.setOutputValueGroupingComparator(TextPair.FirstComparator.class);\nconf.setMapOutputKeyClass(TextPair.class);\nconf.setReducerClass(JoinReducer.class);\nJoins | 237conf.setOutputKeyClass(Text.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new JoinRecordWithStationName(), args);\nSystem.exit(exitCode);\n}\nRunning the program on the sample data yields the following output:\n011990-99999\n011990-99999\n011990-99999\n012650-99999\n012650-99999\nSIHCCAJAVRI\nSIHCCAJAVRI\nSIHCCAJAVRI\nTYNSET-HANSMOEN\nTYNSET-HANSMOEN\n0067011990999991950051507004+68750...\n0043011990999991950051512004+68750...\n0043011990999991950051518004+68750...\n0043012650999991949032412004+62300...\n0043012650999991949032418004+62300...\nSide Data Distribution\nSide data can be defined as extra read-only data needed by a job to process the main\ndataset. The challenge is to make side data available to all the map or reduce tasks\n(which are spread across the cluster) in a convenient and efficient fashion.\nIn addition to the distribution mechanisms described in this section, it is possible to\ncache side-data in memory in a static field, so that tasks of the same job that run in\nsuccession on the same tasktracker can share the data. “Task JVM Re-\nuse” on page 170 describes how to enable this feature. If you take this approach, be\naware of the amount of memory that you are using, as it might affect the memory needed\nby the shuffle (see “Shuffle and Sort” on page 163).\nUsing the Job Configuration\nYou can set arbitrary key-value pairs in the job configuration using the various setter\nmethods on JobConf (inherited from Configuration). This is very useful if you need to\npass a small piece of metadata to your tasks. To retrieve the values in the task, override\nthe configure() method in the Mapper or Reducer and use a getter method on the\nJobConf object passed in.\nUsually a primitive type is sufficient to encode your metadata, but for arbitrary objects\nyou can either handle the serialization yourself (if you have an existing mechanism for\nturning objects to strings and back), or you can use Hadoop’s Stringifier class.\nDefaultStringifier uses Hadoop’s serialization framework to serialize objects (see\n“Serialization” on page 86).\n238 | Chapter 8: MapReduce FeaturesYou shouldn’t use this mechanism for transferring more than a few kilobytes of data\nbecause it can put pressure on the memory usage in the Hadoop daemons, particularly\nin a system running hundreds of jobs. The job configuration is read by the jobtracker,\nthe tasktracker, and the child JVM, and each time the configuration is read, all of its\nentries are read into memory, even if they are not used. User properties are not read on\nthe jobtracker or the tasktracker, so they just waste time and memory.\nDistributed Cache\nRather than serializing side data in the job configuration, it is preferable to distribute\ndatasets using Hadoop’s distributed cache mechanism. This provides a service for\ncopying files and archives to the task nodes in time for the tasks to use them when they\nrun. To save network bandwidth, files are normally copied to any particular node once\nper job.\nUsage\nFor tools that use GenericOptionsParser (this includes many of the programs in this\nbook—see “GenericOptionsParser, Tool, and ToolRunner” on page 121), you can\nspecify the files to be distributed as a comma-separated list of URIs as the argument to\nthe -files option. Files can be on the local filesystem, on HDFS, or on another Hadoop\nreadable filesystem (such as S3). If no scheme is supplied, then the files are assumed to\nbe local. (This is true even if the default filesystem is not the local filesystem.)\nYou can also copy archive files (JAR files, ZIP files, tar files, and gzipped tar files) to\nyour tasks, using the -archives option; these are unarchived on the task node. The\n-libjars option will add JAR files to the classpath of the mapper and reducer tasks.\nThis is useful if you haven’t bundled library JAR files in your job JAR file.\nStreaming doesn’t use the distributed cache for copying the streaming\nscripts across the cluster. You specify a file to be copied using the\n-file option (note the singular), which should be repeated for each file\nto be copied. Furthermore, files specified using the -file option must\nbe file paths only, not URIs, so they must be accessible from the local\nfilesystem of the client launching the Streaming job.\nStreaming also accepts the -files and -archives options for copying\nfiles into the distributed cache for use by your Streaming scripts.\nLet’s see how to use the distributed cache to share a metadata file for station names.\nThe command we will run is:\n% hadoop jar job.jar MaxTemperatureByStationNameUsingDistributedCacheFile \\\n-files input/ncdc/metadata/stations-fixed-width.txt input/ncdc/all output\nThis command will copy the local file stations-fixed-width.txt (no scheme is supplied,\nso the path is automatically interpreted as a local file) to the task nodes, so we can use\nSide Data Distribution | 239it to look up station names. The listing for MaxTemperatureByStationNameUsingDistri\nbutedCacheFile appears in Example 8-16.\nExample 8-16. Application to find the maximum temperature by station, showing station names from\na lookup table passed as a distributed cache file\npublic class MaxTemperatureByStationNameUsingDistributedCacheFile\nextends Configured implements Tool {\nstatic class StationTemperatureMapper extends MapReduceBase\nimplements Mapper<LongWritable, Text, Text, IntWritable> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\npublic void map(LongWritable key, Text value,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\n}\nparser.parse(value);\nif (parser.isValidTemperature()) {\noutput.collect(new Text(parser.getStationId()),\nnew IntWritable(parser.getAirTemperature()));\n}\nstatic class MaxTemperatureReducerWithStationLookup extends MapReduceBase\nimplements Reducer<Text, IntWritable, Text, IntWritable> {\nprivate NcdcStationMetadata metadata;\n@Override\npublic void configure(JobConf conf) {\nmetadata = new NcdcStationMetadata();\ntry {\nmetadata.initialize(new File(""stations-fixed-width.txt""));\n} catch (IOException e) {\nthrow new RuntimeException(e);\n}\n}\npublic void reduce(Text key, Iterator<IntWritable> values,\nOutputCollector<Text, IntWritable> output, Reporter reporter)\nthrows IOException {\nString stationName = metadata.getStationName(key.toString());\n}\n}\nint maxValue = Integer.MIN_VALUE;\nwhile (values.hasNext()) {\nmaxValue = Math.max(maxValue, values.next().get());\n}\noutput.collect(new Text(stationName), new IntWritable(maxValue));\n240 | Chapter 8: MapReduce Features@Override\npublic int run(String[] args) throws IOException {\nJobConf conf = JobBuilder.parseInputAndOutput(this, getConf(), args);\nif (conf == null) {\nreturn -1;\n}\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\nconf.setMapperClass(StationTemperatureMapper.class);\nconf.setCombinerClass(MaxTemperatureReducer.class);\nconf.setReducerClass(MaxTemperatureReducerWithStationLookup.class);\n}\n}\nJobClient.runJob(conf);\nreturn 0;\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(\nnew MaxTemperatureByStationNameUsingDistributedCacheFile(), args);\nSystem.exit(exitCode);\n}\nThe program finds the maximum temperature by weather station, so the mapper\n(StationTemperatureMapper) simply emits (station ID, temperature) pairs. For the\ncombiner we reuse MaxTemperatureReducer (from Chapters 2 and 5) to pick the\nmaximum temperature for any given group of map outputs on the map side. The re-\nducer (MaxTemperatureReducerWithStationLookup) is different from the combiner, since\nin addition to finding the maximum temperature, it uses the cache file to look up the\nstation name.\nWe use the reducer’s configure() method to retrieve the cache file using its original\nname, relative to the working directory of the task.\nYou can use the distributed cache for copying files that do not fit in\nmemory. MapFiles are very useful in this regard, since they serve as an\non-disk lookup format (see “MapFile” on page 110). Because MapFiles\nare a collection of files with a defined directory structure, you should\nput them into an archive format (JAR, ZIP, tar, or gzipped tar) and add\nthem to the cache using the -archives option.\nHere’s a snippet of the output, showing some maximum temperatures for a few weather\nstations:\nPEATS RIDGE WARATAH\nSTRATHALBYN RACECOU\nSHEOAKS AWS\nWANGARATTA AERO\n372\n410\n399\n409\nSide Data Distribution | 241MOOGARA\nMACKAY AERO\n334\n331\nHow it works\nWhen you launch a job, Hadoop copies the files specified by the -files and\n-archives options to the jobtracker’s filesystem (normally HDFS). Then, before a task\nis run, the tasktracker copies the files from the jobtracker’s filesystem to a local disk—\nthe cache—so the task can access the files. From the task’s point of view, the files are\njust there (and it doesn’t care that they came from HDFS).\nThe tasktracker also maintains a reference count for the number of tasks using each\nfile in the cache. After the task has run, the file’s reference count is decreased by one,\nand when it reaches zero it is eligible for deletion. Files are deleted to make room for a\nnew file when the cache exceeds a certain size—10 GB by default. The cache size may\nbe changed by setting the configuration property local.cache.size, which is measured\nin bytes.\nAlthough this design doesn’t guarantee that subsequent tasks from the same job run-\nning on the same tasktracker will find the file in the cache, it is very likely that they will,\nsince tasks from a job are usually scheduled to run at around the same time, so there\nisn’t the opportunity for enough other jobs to run and cause the original task’s file to\nbe deleted from the cache.\nFiles are localized under the ${mapred.local.dir}/taskTracker/archive directory on\nthe tasktrackers. Applications don’t have to know this, however, since the files are\nsymbolically linked from the task’s working directory.\nThe DistributedCache API\nMost applications don’t need to use the DistributedCache API because they can use the\ndistributed cache indirectly via GenericOptionsParser. GenericOptionsParser makes it\nmuch more convenient to use the distributed cache: for example, it copies local files\ninto HDFS and then the JobClient informs the DistributedCache of their locations in\nHDFS using the addCacheFile() and addCacheArchive() methods. The JobClient also\ngets DistributedCache to create symbolic links when the files are localized, by adding\nfragment identifiers to the files’ URIs. For example, the file specified by the URI hdfs://\nnamenode/foo/bar#myfile is symlinked as myfile in the task’s working directory.\nOn the task node, it is most convenient to access the localized file directly; however,\nsometimes you may need to get a list of all the available cache files. JobConf has two\nmethods for this purpose: getLocalCacheFiles() and getLocalCacheArchives(), which\nboth return an array of Path objects pointing to local files.\n242 | Chapter 8: MapReduce FeaturesMapReduce Library Classes\nHadoop comes with a library of mappers and reducers for commonly used functions.\nThey are listed with brief descriptions in Table 8-2. For further information on how to\nuse them, please consult their Java documentation.\nTable 8-2. MapReduce library classes\nClasses Description\nChainMapper, ChainReducer Run a chain of mappers in a single mapper, and a reducer followed by a chain of mappers\n                         in a single reducer. (Symbolically: M+RM*, where M is a mapper and R is a reducer.) This\n                        can substantially reduce the amount of disk I/O incurred compared to running multiple\n                       MapReduce jobs.\nFieldSelectionMapReduce A mapper and a reducer that can select fields (like the Unix cut command) from the\n                       input keys and values and emit them as output keys and values.\nIntSumReducer, Reducers that sum integer values to produce a total for every key.\nLongSumReducer \nInverseMapper A mapper that swaps keys and values.\nTokenCounterMapper A mapper that tokenizes the input value into words (using Java’s StringToken\n                    izer) and emits each word along with a count of one.\nRegexMapper A mapper that finds matches of a regular expression in the input value, and emits the\n           matches along with a count of one.\nMapReduce Library Classes | 243CHAPTER 9\nSetting Up a Hadoop Cluster\nThis chapter explains how to set up Hadoop to run on a cluster of machines. Running\nHDFS and MapReduce on a single machine is great for learning about these systems,\nbut to do useful work they need to run on multiple nodes.\nThere are a few options when it comes to getting a Hadoop cluster, from building your\nown to running on rented hardware, or using an offering that provides Hadoop as a\nservice in the cloud. This chapter and the next give you enough information to set up\nand operate your own cluster, but even if you are using a Hadoop service in which a\nlot of the routine maintenance is done for you, these chapters still offer valuable infor-\nmation about how Hadoop works from an operations point of view.\nCluster Specification\nHadoop is designed to run on commodity hardware. That means that you are not tied\nto expensive, proprietary offerings from a single vendor; rather, you can choose stand-\nardized, commonly available hardware from any of a large range of vendors to build\nyour cluster.\n“Commodity” does not mean “low-end.” Low-end machines often have cheap com-\nponents, which have higher failure rates than more expensive (but still commodity-\nclass) machines. When you are operating tens, hundreds, or thousands of machines,\ncheap components turn out to be a false economy, as the higher failure rate incurs a\ngreater maintenance cost. On the other hand, large database class machines are not\nrecommended either, since they don’t score well on the price/performance curve. And\neven though you would need fewer of them to build a cluster of comparable perform-\nance to one built of mid-range commodity hardware, when one did fail it would have\na bigger impact on the cluster, since a larger proportion of the cluster hardware would\nbe unavailable.\nHardware specifications rapidly become obsolete, but for the sake of illustration, a\ntypical choice of machine for running a Hadoop datanode and tasktracker in late 2008\nwould have the following specifications:\n245Processor\n2 quad-core Intel Xeon 2.0GHz CPUs\nMemory\n8 GB ECC RAM*\nStorage\n41 TB SATA disks\nNetwork\nGigabit Ethernet\nWhile the hardware specification for your cluster will assuredly be different, Hadoop\nis designed to use multiple cores and disks, so it will be able to take full advantage of\nmore powerful hardware.\nWhy Not Use RAID?\nHDFS clusters do not benefit from using RAID (Redundant Array of Independent\nDisks) for datanode storage (although RAID is used for the namenode’s disks, to protect\nagainst corruption of its metadata). The redundancy that RAID provides is not needed,\nsince HDFS handles it by replication between nodes.\nFurthermore, RAID striping (RAID 0) which is commonly used to increase perform-\nance, turns out to be slower than the JBOD (Just a Bunch Of Disks) configuration used\nby HDFS, which round-robins HDFS blocks between all disks. The reason for this is\nthat RAID 0 read and write operations are limited by the speed of the slowest disk in\nthe RAID array. In JBOD, disk operations are independent, so the average speed of\noperations is greater than that of the slowest disk. Disk performance often shows con-\nsiderable variation in practice, even for disks of the same model. In some benchmarking\ncarried out on a Yahoo! cluster (http://markmail.org/message/xmzc45zi25htr7ry),\nJBOD performed 10% faster than RAID 0 in one test (Gridmix), and 30% better in\nanother (HDFS write throughput).\nFinally, if a disk fails in a JBOD configuration, HDFS can continue to operate without\nthe failed disk, whereas with RAID, failure of a single disk causes the whole array (and\nhence the node) to become unavailable.\nThe bulk of Hadoop is written in Java, and can therefore run on any platform with a\nJVM, although there are enough parts that harbor Unix assumptions (the control\nscripts, for example) to make it unwise to run on a non-Unix platform in production.\nIn fact, Windows operating systems are not supported production platforms (although\nthey can be used with Cygwin as a development platform; see Appendix A).\nHow large should your cluster be? There isn’t an exact answer to this question, but the\nbeauty of Hadoop is that you can start with a small cluster (say, 10 nodes) and grow it\n* ECC memory is strongly recommended, as several Hadoop users have reported seeing many checksum errors\nwhen using non-ECC memory on Hadoop clusters.\n246 | Chapter 9: Setting Up a Hadoop Clusteras your storage and computational needs grow. In many ways, a better question is this:\nhow fast does my cluster need to grow? You can get a good feel for this by considering\nstorage capacity.\nFor example, if your data grows by 1 TB a week, and you have three-way HDFS repli-\ncation, then you need an additional 3 TB of raw storage per week. Allow some room\nfor intermediate files and logfiles (around 30%, say), and this works out at about one\nmachine (2008 vintage) per week, on average. In practice, you wouldn’t buy a new\nmachine each week and add it to the cluster. The value of doing a back-of-the-envelope\ncalculation like this is that it gives you a feel for how big your cluster should be: in this\nexample, a cluster that holds two years of data needs 100 machines.\nFor a small cluster (on the order of 10 nodes), it is usually acceptable to run the name-\nnode and the jobtracker on a single master machine (as long as at least one copy of the\nnamenode’s metadata is stored on a remote filesystem). As the cluster and the number\nof files stored in HDFS grow, the namenode needs more memory, so the namenode\nand jobtracker should be moved onto separate machines.\nThe secondary namenode can be run on the same machine as the namenode, but again\nfor reasons of memory usage (the secondary has the same memory requirements as the\nprimary), it is best to run it on a separate piece of hardware, especially for larger clusters.\n(This topic is discussed in more detail in “Master node scenarios” on page 254.) Ma-\nchines running the namenodes should typically run on 64-bit hardware to avoid the 3\nGB limit on Java heap size in 32-bit architectures.†\nNetwork Topology\nA common Hadoop cluster architecture consists of a two-level network topology, as\nillustrated in Figure 9-1. Typically there are 30 to 40 servers per rack, with a 1 GB switch\nfor the rack (only three are shown in the diagram), and an uplink to a core switch or\nrouter (which is normally 1 GB or better). The salient point is that the aggregate band-\nwidth between nodes on the same rack is much greater than that between nodes on\ndifferent racks.\nRack awareness\nTo get maximum performance out of Hadoop, it is important to configure Hadoop so\nthat it knows the topology of your network. If your cluster runs on a single rack, then\nthere is nothing more to do, since this is the default. However, for multirack clusters,\nyou need to map nodes to racks. By doing this, Hadoop will prefer within-rack transfers\n(where there is more bandwidth available) to off-rack transfers when placing\n† The traditional advice says other machines in the cluster (jobtracker, datanodes/tasktrackers) should be 32-\nbit to avoid the memory overhead of larger pointers. Sun’s Java 6 update 14 features “compressed ordinary\nobject pointers,” which eliminates much of this overhead, so there’s now no real downside to running on 64-\nbit hardware.\nCluster Specification | 247Figure 9-1. Typical two-level network architecture for a Hadoop cluster\nMapReduce tasks on nodes. HDFS will be able to place replicas more intelligently to\ntrade-off performance and resilience.\nNetwork locations such as nodes and racks are represented in a tree, which reflects the\nnetwork “distance” between locations. The namenode uses the network location when\ndetermining where to place block replicas (see “Network Topology and Ha-\ndoop” on page 64); the jobtracker uses network location to determine where the closest\nreplica is as input for a map task that is scheduled to run on a tasktracker.\nFor the network in Figure 9-1, the rack topology is described by two network locations,\nsay, /switch1/rack1 and /switch1/rack2. Since there is only one top-level switch in this\ncluster, the locations can be simplified to /rack1 and /rack2.\nThe Hadoop configuration must specify a map between node addresses and network\nlocations. The map is described by a Java interface, DNSToSwitchMapping, whose signa-\nture is:\npublic interface DNSToSwitchMapping {\npublic List<String> resolve(List<String> names);\n}\nThe names parameter is a list of IP addresses, and the return value is a list of corre-\nsponding network location strings. The topology.node.switch.mapping.impl configu-\nration property defines an implementation of the DNSToSwitchMapping interface that the\nnamenode and the jobtracker use to resolve worker node network locations.\nFor the network in our example, we would map node1, node2, and node3 to /rack1,\nand node4, node5, and node6 to /rack2.\nMost installations don’t need to implement the interface themselves, however, since\nthe default implementation is ScriptBasedMapping, which runs a user-defined script to\n248 | Chapter 9: Setting Up a Hadoop Clusterdetermine the mapping. The script’s location is controlled by the property\ntopology.script.file.name. The script must accept a variable number of arguments\nthat are the hostnames or IP addresses to be mapped, and it must emit the correspond-\ning network locations to standard output, separated by whitespace. The example code\nincludes a script for this purpose.\nIf no script location is specified, the default behavior is to map all nodes to a single\nnetwork location, called /default-rack.\nCluster Setup and Installation\nYour hardware has arrived. The next steps are to get it racked up and install the software\nneeded to run Hadoop.\nThere are various ways to install and configure Hadoop. This chapter describes how\nto do it from scratch using the Apache Hadoop distribution, and will give you the\nbackground to cover the things you need to think about when setting up Hadoop.\nAlternatively, if you would like to use RPMs or Debian packages for managing your\nHadoop installation, then you might want to start with Cloudera’s Distribution, de-\nscribed in Appendix B.\nTo ease the burden of installing and maintaining the same software on each node, it is\nnormal to use an automated installation method like Red Hat Linux’s Kickstart or\nDebian’s Fully Automatic Installation. These tools allow you to automate the operating\nsystem installation by recording the answers to questions that are asked during the\ninstallation process (such as the disk partition layout), as well as which packages to\ninstall. Crucially, they also provide hooks to run scripts at the end of the process, which\nare invaluable for doing final system tweaks and customization that is not covered by\nthe standard installer.\nThe following sections describe the customizations that are needed to run Hadoop.\nThese should all be added to the installation script.\nInstalling Java\nJava 6 or later is required to run Hadoop. The latest stable Sun JDK is the preferred\noption, although Java distributions from other vendors may work too. The following\ncommand confirms that Java was installed correctly:\n% java -version\njava version ""1.6.0_12""\nJava(TM) SE Runtime Environment (build 1.6.0_12-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 11.2-b01, mixed mode)\nCluster Setup and Installation | 249Creating a Hadoop User\nIt’s good practice to create a dedicated Hadoop user account to separate the Hadoop\ninstallation from other services running on the same machine.\nSome cluster administrators choose to make this user’s home directory an NFS-\nmounted drive, to aid with SSH key distribution (see the following discussion). The\nNFS server is typically outside the Hadoop cluster. If you use NFS, it is worth consid-\nering autofs, which allows you to mount the NFS filesystem on demand, when the\nsystem accesses it. Autofs provides some protection against the NFS server failing, and\nalso allows you to use replicated filesystems for failover. There are other NFS gotchas\nto watch out for, such as synchronizing UIDs and GIDs. For help setting up NFS on\nLinux, refer to the HOWTO at http://nfs.sourceforge.net/nfs-howto/index.html.\nInstalling Hadoop\nDownload Hadoop from the Apache Hadoop releases page (http://hadoop.apache.org/\ncore/releases.html), and unpack the contents of the distribution in a sensible location,\nsuch as /usr/local (/opt is another standard choice). Note that Hadoop is not installed\nin the hadoop user’s home directory, as that may be an NFS-mounted directory.\n% cd /usr/local\n% sudo tar xzf hadoop-x.y.z.tar.gz\nWe also need to change the owner of the Hadoop files to be the hadoop user and group:\n% sudo chown -R hadoop:hadoop hadoop-x.y.z\nSome administrators like to install HDFS and MapReduce in separate\nlocations on the same system. At the time of this writing, only HDFS\nand MapReduce from the same Hadoop release are compatible with one\nanother; however, in future releases, the compatibility requirements will\nbe loosened. When this happens, having independent installations\nmakes sense, as it gives more upgrade options (for more, see “Up-\ngrades” on page 296). For example, it is convenient to be able to up-\ngrade MapReduce—perhaps to patch a bug—while leaving HDFS\nrunning.\nNote that separate installations of HDFS and MapReduce can still share\nconfiguration by using the --config option (when starting daemons) to\nrefer to a common configuration directory. They can also log to the same\ndirectory, as the logfiles they produce are named in such a way as to\navoid clashes.\nTesting the Installation\nOnce you’ve created the installation file, you are ready to test it by installing it on the\nmachines in your cluster. This will probably take a few iterations as you discover kinks\n250 | Chapter 9: Setting Up a Hadoop Clusterin the install. When it’s working, you can proceed to configure Hadoop and give it a\ntest run. This process is documented in the following sections.\nSSH Configuration\nThe Hadoop control scripts rely on SSH to perform cluster-wide operations. For ex-\nample, there is a script for stopping and starting all the daemons in the cluster. Note\nthat the control scripts are optional—cluster-wide operations can be performed by\nother mechanisms, too (such as a distributed shell).\nTo work seamlessly, SSH needs to be set up to allow password-less login for the\nhadoop user from machines in the cluster. The simplest way to achieve this is to generate\na public/private key pair, and it will be shared across the cluster using NFS.\nFirst, generate an RSA key pair by typing the following in the hadoop user account:\n% ssh-keygen -t rsa -f ~/.ssh/id_rsa\nEven though we want password-less logins, keys without passphrases are not consid-\nered good practice (it’s OK to have an empty passphrase when running a local pseudo-\ndistributed cluster, as described in Appendix A), so we specify a passphrase when\nprompted for one. We shall use ssh-agent to avoid the need to enter a password for\neach connection.\nThe private key is in the file specified by the -f option, ~/.ssh/id_rsa, and the public key\nis stored in a file with the same name with .pub appended, ~/.ssh/id_rsa.pub.\nNext we need to make sure that the public key is in the ~/.ssh/authorized_keys file on\nall the machines in the cluster that we want to connect to. If the hadoop user’s home\ndirectory is an NFS filesystem, as described earlier, then the keys can be shared across\nthe cluster by typing:\n% cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nIf the home directory is not shared using NFS, then the public keys will need to be\nshared by some other means.\nTest that you can SSH from the master to a worker machine by making sure ssh-\nagent is running,‡ and then run ssh-add to store your passphrase. You should be able\nto ssh to a worker without entering the passphrase again.\nHadoop Configuration\nThere are a handful of files for controlling the configuration of a Hadoop installation;\nthe most important ones are listed in Table 9-1.\n‡ See its man page for instructions on how to start ssh-agent.\nHadoop Configuration | 251Table 9-1. Hadoop configuration files\nFilename Format Description\nhadoop-env.sh Bash script Environment variables that are used in the scripts to run Hadoop.\ncore-site.xml Hadoop configura- Configuration settings for Hadoop Core, such as I/O settings that are common to\n              tion XML HDFS and MapReduce.\nhdfs-site.xml Hadoop configura- Configuration settings for HDFS daemons: the namenode, the secondary name-\n              tion XML node, and the datanodes.\nmapred-site.xml Hadoop configura- Configuration settings for MapReduce daemons: the jobtracker, and the\n                tion XML tasktrackers.\nmasters Plain text A list of machines (one per line) that each run a secondary namenode.\nslaves Plain text A list of machines (one per line) that each run a datanode and a tasktracker.\nhadoop-met Java Properties Properties for controlling how metrics are published in Hadoop (see “Met-\nrics.properties rics” on page 286).\nlog4j.properties Java Properties Properties for system logfiles, the namenode audit log, and the task log for the\n                                tasktracker child process (“Hadoop User Logs” on page 142).\nThese files are all found in the conf directory of the Hadoop distribution. The config-\nuration directory can be relocated to another part of the filesystem (outside the Hadoop\ninstallation, which makes upgrades marginally easier) as long as daemons are started\nwith the --config option specifying the location of this directory on the local filesystem.\nConfiguration Management\nHadoop does not have a single, global location for configuration information. Instead,\neach Hadoop node in the cluster has its own set of configuration files, and it is up to\nadministrators to ensure that they are kept in sync across the system. Hadoop provides\na rudimentary facility for synchronizing configuration using rsync (see upcoming dis-\ncussion), alternatively there are parallel shell tools that can help do this, like dsh or pdsh.\nHadoop is designed so that it is possible to have a single set of configuration files that\nare used for all master and worker machines. The great advantage of this is simplicity,\nboth conceptually (since there is only one configuration to deal with) and operationally\n(as the Hadoop scripts are sufficient to manage a single configuration setup).\nFor some clusters, the one-size-fits-all configuration model breaks down. For example,\nif you expand the cluster with new machines that have a different hardware specifica-\ntion to the existing ones, then you need a different configuration for the new machines\nto take advantage of their extra resources.\nIn these cases, you need to have the concept of a class of machine, and maintain a\nseparate configuration for each class. Hadoop doesn’t provide tools to do this, but there\nare several excellent tools for doing precisely this type of configuration management,\nsuch as Puppet, cfengine, and bcfg2.\n252 | Chapter 9: Setting Up a Hadoop ClusterFor a cluster of any size, it can be a challenge to keep all of the machines in sync: consider\nwhat happens if the machine is unavailable when you push out an update—who en-\nsures it gets the update when it becomes available? This is a big problem and can lead\nto divergent installations, so even if you use the Hadoop control scripts for managing\nHadoop, it may be a good idea to use configuration management tools for maintaining\nthe cluster. These tools are also excellent for doing regular maintenance, such as patch-\ning security holes and updating system packages.\nControl scripts\nHadoop comes with scripts for running commands, and starting and stopping daemons\nacross the whole cluster. To use these scripts (which can be found in the bin directory),\nyou need to tell Hadoop which machines are in the cluster. There are two files for this\npurpose, called masters and slaves, each of which contains a list of the machine host-\nnames or IP addresses, one per line. The masters file is actually a misleading name, in\nthat it determines which machine or machines should run a secondary namenode. The\nslaves file lists the machines that the datanodes and tasktrackers should run on. Both\nmasters and slaves files reside in the configuration directory, although the slaves file\nmay be placed elsewhere (and given another name) by changing the HADOOP_SLAVES\nsetting in hadoop-env.sh. Also, these files do not need to be distributed to worker nodes,\nsince they are used only by the control scripts running on the namenode or jobtracker.\nYou don’t need to specify which machine (or machines) the namenode and jobtracker\nruns on in the masters file, as this is determined by the machine the scripts are run on.\n(In fact, specifying these in the masters file would cause a secondary namenode to run\nthere, which isn’t always what you want.) For example, the start-dfs.sh script, which\nstarts all the HDFS daemons in the cluster, runs the namenode on the machine the\nscript is run on. In slightly more detail, it:\n1. Starts a namenode on the local machine (the machine that the script is run on)\n2. Starts a datanode on each machine listed in the slaves file\n3. Starts a secondary namenode on each machine listed in the masters file\nThere is a similar script called start-mapred.sh, which starts all the MapReduce dae-\nmons in the cluster. More specifically, it:\n1. Starts a jobtracker on the local machine\n2. Starts a tasktracker on each machine listed in the slaves file\nNote that masters is not used by the MapReduce control scripts.\nAlso provided are stop-dfs.sh and stop-mapred.sh scripts to stop the daemons started\nby the corresponding start script.\nThese scripts start and stop Hadoop daemons using the hadoop-daemon.sh script. If\nyou use the aforementioned scripts, you shouldn’t call hadoop-daemon.sh directly. But\nif you need to control Hadoop daemons from another system or from your own scripts,\nHadoop Configuration | 253then the hadoop-daemon.sh script is a good integration point. Likewise, hadoop-dae\nmons.sh (with an “s”) is handy for starting the same daemon on a set of hosts.\nMaster node scenarios\nDepending on the size of the cluster, there are various configurations for running the\nmaster daemons: the namenode, secondary namenode, and jobtracker. On a small\ncluster (a few tens of nodes), it is convenient to put them on a single machine; however,\nas the cluster gets larger, there are good reasons to separate them.\nThe namenode has high memory requirements, as it holds file and block metadata for\nthe entire namespace in memory. The secondary namenode, while idle most of the time,\nhas a comparable memory footprint to the primary when it creates a checkpoint. (This\nis explained in detail in “The filesystem image and edit log” on page 274.) For filesys-\ntems with a large number of files, there may not be enough physical memory on one\nmachine to run both the primary and secondary namenode.\nThe secondary namenode keeps a copy of the latest checkpoint of the filesystem met-\nadata that it creates. Keeping this (stale) backup on a different node to the namenode\nallows recovery in the event of loss (or corruption) of all the namenode’s metadata files.\n(This is discussed further in Chapter 10.)\nOn a busy cluster running lots of MapReduce jobs, the jobtracker uses considerable\nmemory and CPU resources, so it should run on a dedicated node.\nWhether the master daemons run on one or more nodes, the following instructions\napply:\n• Run the HDFS control scripts from the namenode machine. The masters file should\ncontain the address of the secondary namenode.\n• Run the MapReduce control scripts from the jobtracker machine.\nWhen the namenode and jobtracker are on separate nodes, their slaves files need to be\nkept in sync, since each node in the cluster should run a datanode and a tasktracker.\nEnvironment Settings\nIn this section, we consider how to set the variables in hadoop-env.sh.\nMemory\nBy default, Hadoop allocates 1000 MB (1 GB) of memory to each daemon it runs. This\nis controlled by the HADOOP_HEAPSIZE setting in hadoop-env.sh. In addition, the task\ntracker launches separate child JVMs to run map and reduce tasks in, so we need to\nfactor these into the total memory footprint of a worker machine.\nThe maximum number of map tasks that will be run on a tasktracker at one time is\ncontrolled by the mapred.tasktracker.map.tasks.maximum property, which defaults to\n254 | Chapter 9: Setting Up a Hadoop Clustertwo tasks. There is a corresponding property for reduce tasks, mapred.task\ntracker.reduce.tasks.maximum, which also defaults to two tasks. The memory given to\neach of these child JVMs can be changed by setting the mapred.child.java.opts prop-\nerty. The default setting is -Xmx200m, which gives each task 200 MB of memory. (Inci-\ndentally, you can provide extra JVM options here, too. For example, you might enable\nverbose GC logging to debug GC.) The default configuration therefore uses 2800 MB\nof memory for a worker machine (see Table 9-2).\nTable 9-2. Worker node memory calculation\nJVM Default memory used (MB) Memory used for 8 processors, 400 MB per child (MB)\nDatanode 1000 1000\nTasktracker 1000 1000\nTasktracker child map task 2 × 200 7 × 400\nTasktracker child reduce task 2 × 200 7 × 400\nTotal 2800 7600\nThe number of tasks that can be run simultaneously on a tasktracker is governed by\nthe number of processors available on the machine. Because MapReduce jobs are nor-\nmally I/O-bound, it makes sense to have more tasks than processors to get better uti-\nlization. The amount of oversubscription depends on the CPU utilization of jobs you\nrun, but a good rule of thumb is to have a factor of between one and two more tasks\n(counting both map and reduce tasks) than processors.\nFor example, if you had 8 processors and you wanted to run 2 processes on each pro-\ncessor, then you could set mapred.tasktracker.map.tasks.maximum and mapred.task\ntracker.map.tasks.maximum to both be 7 (not 8, since the datanode and the tasktracker\neach take one slot). If you also increased the memory available to each child task to 400\nMB, then the total memory usage would be 7,600 MB (see Table 9-2).\nWhether this Java memory allocation will fit into 8 GB of physical memory depends\non the other processes that are running on the machine. If you are running Streaming\nor Pipes programs, this allocation will probably be inappropriate (and the memory\nallocated to the child should be dialled down), since it doesn’t allow enough memory\nfor users’ (Streaming or Pipes) processes to run. The thing to avoid is processes being\nswapped out, as this it leads to severe performance degradation. The precise memory\nsettings are necessarily very cluster-dependent, and can be optimized over time with\nexperience gained from monitoring the memory usage across the cluster. Tools like\nGanglia (“GangliaContext” on page 288) are good for gathering this information.\nHadoop also provides settings to control how much memory is used for MapReduce\noperations. These can be set on a per-job basis, and are covered in the section on\n“Shuffle and Sort” on page 163.\nHadoop Configuration | 255For the master node, each of the namenode, secondary namenode, and jobtracker dae-\nmons uses 1,000 MB by default, a total of 3,000 MB.\nA namenode can eat up memory, since a reference to every block of every\nfile is maintained in memory. For example, 1,000 MB is enough for a\nfew million files. You can increase the namenode’s memory without\nchanging the memory allocated to other Hadoop daemons by setting\nHADOOP_NAMENODE_OPTS in hadoop-env.sh to include a JVM option for set-\nting the memory size. HADOOP_NAMENODE_OPTS allows you to pass extra\noptions to the namenode’s JVM. So, for example, if using a Sun JVM,\n-Xmx2000m would specify that 2000 MB of memory should be allocated\nto the namenode.\nIf you change the namenode’s memory allocation, don’t forget to do the\nsame for the secondary namenode (using the HADOOP_SECONDARYNAME\nNODE_OPTS variable), since its memory requirements are comparable to\nthe primary namenode’s. You will probably also want to run the sec-\nondary namenode on a different machine, in this case.\nThere are corresponding environment variables for the other Hadoop\ndaemons, so you can customize their memory allocations, if desired. See\nhadoop-env.sh for details.\nJava\nThe location of the Java implementation to use is determined by the JAVA_HOME setting\nin hadoop-env.sh, or from the JAVA_HOME shell environment variable, if not set in hadoop-\nenv.sh. It’s a good idea to set the value in hadoop-env.sh, so that it is clearly defined in\none place, and to ensure that the whole cluster is using the same version of Java.\nSystem logfiles\nSystem logfiles produced by Hadoop are stored in $HADOOP_INSTALL/logs by default.\nThis can be changed using the HADOOP_LOG_DIR setting in hadoop-env.sh. It’s a good idea\nto change this so that logfiles are kept out of the directory that Hadoop is installed in,\nsince this keeps logfiles in one place even after the installation directory changes after\nan upgrade. A common choice is /var/log/hadoop, set by including the following line in\nhadoop-env.sh:\nexport HADOOP_LOG_DIR=/var/log/hadoop\nThe log directory will be created if it doesn’t already exist (if not, confirm that the\nHadoop user has permission to create it). Each Hadoop daemon running on a machine\nproduces two logfiles. The first is the log output written via log4j. This file, which ends\nin .log, should be the first port of call when diagnosing problems, since most application\nlog messages are written here. The standard Hadoop log4j configuration uses a Daily\nRolling File Appender to rotate logfiles. Old logfiles are never deleted, so you should\n256 | Chapter 9: Setting Up a Hadoop Clusterarrange for them to be periodically deleted or archived, so as to not run out of disk\nspace on the local node.\nThe second logfile is the combined standard output and standard error log. This logfile,\nwhich ends in .out, usually contains little or no output, since Hadoop uses log4j for\nlogging. It is only rotated when the daemon is restarted, and only the last five logs are\nretained. Old logfiles are suffixed with a number between 1 and 5, with 5 being the\noldest file.\nLogfile names (of both types) are a combination of the name of the user running the\ndaemon, the daemon name, and the machine hostname. For example, hadoop-tom-\ndatanode-sturges.local.log.2008-07-04 is the name of a logfile after it has been rotated.\nThis naming structure makes it possible to archive logs from all machines in the cluster\nin a single directory, if needed, since the filenames are unique.\nThe username in the logfile name is actually the default for the HADOOP_IDENT_STRING\nsetting in hadoop-env.sh. If you wish to give the Hadoop instance a different identity\nfor the purposes of naming the logfiles, change HADOOP_IDENT_STRING to be the identifier\nyou want.\nSSH settings\nThe control scripts allow you to run commands on (remote) worker nodes from the\nmaster node using SSH. It can be useful to customize the SSH settings, for various\nreasons. For example, you may want to reduce the connection timeout (using the\nConnectTimeout option) so the control scripts don’t hang around waiting to see whether\na dead node is going to respond. Obviously, this can be taken too far. If the timeout is\ntoo low, then busy nodes will be skipped, which is bad.\nAnother useful SSH setting is StrictHostKeyChecking, which can be set to no to auto-\nmatically add new host keys to the known hosts files. The default, ask, is to prompt\nthe user to confirm they have verified the key fingerprint, which is not a suitable setting\nin a large cluster environment.§\nTo pass extra options to SSH, define the HADOOP_SSH_OPTS environment variable in\nhadoop-env.sh. See the ssh and ssh_config manual pages for more SSH settings.\nThe Hadoop control scripts can distribute configuration files to all nodes of the cluster\nusing rsync. This is not enabled by default, but by defining the HADOOP_MASTER setting\nin hadoop-env.sh, worker daemons will rsync the tree rooted at HADOOP_MASTER to the\nlocal node’s HADOOP_INSTALL whenever the daemon starts up.\nWhat if you have two masters—a namenode and a jobtracker on separate machines?\nYou can pick one as the source, and the other can rsync from it, along with all the\n§ For more discussion on the security implications of SSH Host Keys, consult the article “SSH Host Key\nProtection” by Brian Hatch at http://www.securityfocus.com/infocus/1806.\nHadoop Configuration | 257workers. In fact, you could use any machine, even one outside the Hadoop cluster, to\nrsync from.\nBecause HADOOP_MASTER is unset by default, there is a bootstrapping problem: how do\nwe make sure hadoop-env.sh with HADOOP_MASTER set is present on worker nodes? For\nsmall clusters, it is easy to write a small script to copy hadoop-env.sh from the master\nto all of the worker nodes. For larger clusters, tools like dsh can do the copies in parallel.\nAlternatively, a suitable hadoop-env.sh can be created as a part of the automated in-\nstallation script (such as Kickstart).\nWhen starting a large cluster with rsyncing enabled, the worker nodes can overwhelm\nthe master node with rsync requests since the workers start at around the same time.\nTo avoid this, set the HADOOP_SLAVE_SLEEP setting to a small number of seconds, such\nas 0.1, for one-tenth of a second. When running commands on all nodes of the cluster,\nthe master will sleep for this period between invoking the command on each worker\nmachine in turn.\nImportant Hadoop Daemon Properties\nHadoop has a bewildering number of configuration properties. In this section, we ad-\ndress the ones that you need to define (or at least understand why the default is ap-\npropriate) for any real-world working cluster. These properties are set in the Hadoop\nsite files: core-site.xml, hdfs-site.xml, and mapred-site.xml. Example 9-1 shows a typical\nexample set of files. Notice that most are marked as final, in order to prevent them from\nbeing overridden by job configurations. You can learn more about how to write Ha-\ndoop’s configuration files in “The Configuration API” on page 116.\nExample 9-1. A typical set of site configuration files\n<?xml version=""1.0""?>\n<!-- core-site.xml -->\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://namenode/</value>\n<final>true</final>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- hdfs-site.xml -->\n<configuration>\n<property>\n<name>dfs.name.dir</name>\n<value>/disk1/hdfs/name,/remote/hdfs/name</value>\n<final>true</final>\n</property>\n<property>\n<name>dfs.data.dir</name>\n258 | Chapter 9: Setting Up a Hadoop Cluster<value>/disk1/hdfs/data,/disk2/hdfs/data</value>\n<final>true</final>\n</property>\n<property>\n<name>fs.checkpoint.dir</name>\n<value>/disk1/hdfs/namesecondary,/disk2/hdfs/namesecondary</value>\n<final>true</final>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- mapred-site.xml -->\n<configuration>\n<property>\n<name>mapred.job.tracker</name>\n<value>jobtracker:8021</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.local.dir</name>\n<value>/disk1/mapred/local,/disk2/mapred/local</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.system.dir</name>\n<value>/tmp/hadoop/mapred/system</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.tasktracker.map.tasks.maximum</name>\n<value>7</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.tasktracker.reduce.tasks.maximum</name>\n<value>7</value>\n<final>true</final>\n</property>\n<property>\n<name>mapred.child.java.opts</name>\n<value>-Xmx400m</value>\n<!-- Not marked as final so jobs can include JVM debugging options -->\n</property>\n</configuration>\nHDFS\nTo run HDFS, you need to designate one machine as a namenode. In this case, the\nproperty fs.default.name is a HDFS filesystem URI, whose host is the namenode’s\nHadoop Configuration | 259hostname or IP address, and port is the port that the namenode will listen on for RPCs.\nIf no port is specified, the default of 8020 is used.\nThe masters file that is used by the control scripts is not used by the\nHDFS (or MapReduce) daemons to determine hostnames. In fact, be-\ncause the masters file is only used by the scripts, you can ignore it if you\ndon’t use them.\nThe fs.default.name property also doubles as specifying the default filesystem. The\ndefault filesystem is used to resolve relative paths, which are handy to use since they\nsave typing (and avoid hardcoding knowledge of a particular namenode’s address). For\nexample, with the default filesystem defined in Example 9-1, the relative URI /a/b is\nresolved to hdfs://namenode/a/b.\nIf you are running HDFS, the fact that fs.default.name is used to specify\nboth the HDFS namenode and the default filesystem means HDFS has\nto be the default filesystem in the server configuration. Bear in mind,\nhowever, that it is possible to specify a different filesystem as the default\nin the client configuration, for convenience.\nFor example, if you use both HDFS and S3 filesystems, then you have\na choice of specifying either as the default in the client configuration,\nwhich allows you to refer to the default with a relative URI, and the\nother with an absolute URI.\nThere are a few other configuration properties you should set for HDFS: those that set\nthe storage directories for the namenode and for datanodes. The property\ndfs.name.dir specifies a list of directories where the namenode stores persistent file-\nsystem metadata (the edit log, and the filesystem image). A copy of each of the metadata\nfiles is stored in each directory for redundancy. It’s common to configure\ndfs.name.dir so that the namenode metadata is written to one or two local disks, and\na remote disk, such as a NFS-mounted directory. Such a setup guards against failure\nof a local disk, and failure of the entire namenode, since in both cases the files can be\nrecovered and used to start a new namenode. (The secondary namenode takes only\nperiodic checkpoints of the namenode, so it does not provide an up-to-date backup of\nthe namenode.)\nYou should also set the dfs.data.dir property, which specifies a list of directories for\na datanode to store its blocks. Unlike the namenode, which uses multiple directories\nfor redundancy, a datanode round-robins writes between its storage directories, so for\nperformance you should specify a storage directory for each local disk. Read perform-\nance also benefits from having multiple disks for storage, because blocks will be spread\nacross them, and concurrent reads for distinct blocks will be correspondingly spread\nacross disks.\n260 | Chapter 9: Setting Up a Hadoop ClusterFor maximum performance, you should mount storage disks with the\nnoatime option. This setting means that last accessed time information\nis not written on file reads, which gives significant performance gains.\nFinally, you should configure where the secondary namenode stores its checkpoints of\nthe filesystem. The fs.checkpoint.dir property specifies a list of directories where the\ncheckpoints are kept. Like the storage directories for the namenode, which keep re-\ndundant copies of the namenode metadata, the checkpointed filesystem image is stored\nin each checkpoint directory for redundancy.\nTable 9-3 summarizes the important configuration properties for HDFS.\nTable 9-3. Important HDFS daemon properties\nProperty name Type Default value Description\nfs.default.name URI file:/// The default filesystem. The URI defines\n                            the hostname and port that the job-\n                           tracker’s RPC server runs on. The default\n                            port is 8020. This property should be set\n                           in core-site.xml.\ndfs.name.dir comma-separated di- ${hadoop.tmp.dir}/ The list of directories where the name-\n             rectory names dfs/name node stores its persistent metadata.\n                                   The namenode stores a copy of the\n                                  metadata in each directory in the list.\ndfs.data.dir comma-separated di- ${hadoop.tmp.dir}/ A list of directories where the datanode\n             rectory names dfs/data stores blocks.\nfs.check comma-separated di- ${hadoop.tmp.dir}/ A list of directories where the\npoint.dir rectory names dfs/namesecondary secondary namenode stores check-\n                                         points. It stores a copy of the checkpoint\n                                        in each directory in the list.\nNote that the storage directories for HDFS are under Hadoop’s tempo-\nrary directory by default (the hadoop.tmp.dir property, whose default\nis /tmp/hadoop-${user.name}). Therefore it is critical that these proper-\nties are set so that data is not lost by the system clearing out temporary\ndirectories.\nMapReduce\nTo run MapReduce, you need to designate one machine as a jobtracker, which on small\nclusters may be the same machine as the namenode. To do this, set the\nmapred.job.tracker property to the hostname or IP address and port that the jobtracker\nwill listen on. Note that this property is not a URI, but a host-port pair, separated by\na colon. The port number 8021 is a common choice.\nHadoop Configuration | 261During a MapReduce job, intermediate data and working files are written to temporary\nlocal files. Since this data includes the potentially very large output of map tasks, you\nneed to ensure that the mapred.local.dir property, which controls the location of local\ntemporary storage, is configured to use disk partitions that are large enough. The\nmapred.local.dir property takes a comma-separated list of directory names, and you\nshould use all available local disks to spread disk I/O. Typically, you will use the same\ndisks and partitions (but different directories) for MapReduce temporary data as you\nuse for datanode block storage, as governed by the dfs.data.dir property, discussed\nearlier.\nMapReduce uses a distributed filesystem to share files (such as the job JAR file) with\nthe tasktrackers that run the MapReduce tasks. The mapred.system.dir property is used\nto specify a directory where these files can be stored. This directory is resolved relative\nto the default filesystem (configured in fs.default.name), which is usually HDFS.\nFinally, you should set the mapred.tasktracker.map.tasks.maximum and mapred.task\ntracker.reduce.tasks.maximum properties to reflect the number of available cores on\nthe tasktracker machines and mapred.child.java.opts to reflect the amount of memory\navailable for the tasktracker child JVMs. See the discussion in “Memory”\non page 254.\nTable 9-4 summarizes the important configuration properties for HDFS.\nTable 9-4. Important MapReduce daemon properties\nProperty name Type Default value Description\nmapred.job.tracker hostname and port local The hostname and port that the\n                                          jobtracker’s RPC server runs on. If\n                                           set to the default value of\n                                          local, then the jobtracker is run\n                                         in-process on demand when you\n                                        run a MapReduce job (you don’t\n                                         need to start the MapReduce dae-\n                                        mons in this case).\nmapred.local.dir comma-separated di- ${hadoop.tmp.dir} A list of directories where the\n                 rectory names /mapred/local MapReduce stores intermediate\n                                            data for jobs. The data is cleared\n                                           out when the job ends.\nmapred.system.dir URI ${hadoop.tmp.dir} The directory relative to\n                      /mapred/system fs.default.name where\n                                    shared files are stored, during a\n                                   job run.\nmapred.task int 2 The number of map tasks that\ntracker.map.tasks. may be run on a tasktracker at any\nmaximum one time.\n262 | Chapter 9: Setting Up a Hadoop ClusterProperty name Type Default value Description\nmapred.task int 2 The number of reduce tasks that\ntracker.reduce.tasks. may be run on a tasktracker at any\nmaximum one time.\nmapred.child.java.opts String -Xmx200m The JVM options used to launch\n                                      the tasktracker child process that\n                                     runs map and reduce tasks. This\n                                    property can be set on a per-job\n                                   basis, which can be useful for set-\n                                  ting JVM properties for debug-\n                                 ging, for example.\nHadoop Daemon Addresses and Ports\nHadoop daemons generally run both an RPC server (Table 9-5) for communication\nbetween daemons, and a HTTP server to provide web pages for human consumption\n(Table 9-6). Each server is configured by setting the network address and port number\nto listen on. By specifying the network address as 0.0.0.0, Hadoop will bind to all\naddresses on the machine. Alternatively, you can specify a single address to bind to. A\nport number of 0 instructs the server to start on a free port: this is generally discouraged,\nsince it is incompatible with setting cluster-wide firewall policies.\nTable 9-5. RPC server properties\nProperty name Default value Description\nfs.default.name file:/// When set to an HDFS URI, this property determines\n                        the namenode’s RPC server address and port. The\n                         default port is 8020 if not specified.\ndfs.datanode.ipc.address 0.0.0.0:50020 The datanode’s RPC server address and port.\nmapred.job.tracker local When set to a hostname and port, this property\n                        specifies the jobtracker’s RPC server address and\n                         port. A commonly used port is 8021.\nmapred.task.tracker.report.address 127.0.0.1:0 The tasktracker’s RPC server address and port. This\n                                                is used by the tasktracker’s child JVM to commu-\n                                                 nicate with the tasktracker. Using any free port is\n                                                acceptable in this case, as the server only binds to\n                                               the loopback address. You should change this set-\n                                              ting only if the machine has no loopback\n                                             address.\nIn addition to an RPC server, datanodes run a TCP/IP server for block transfers. The\nserver address and port is set by the dfs.datanode.address property, and has a default\nvalue of 0.0.0.0:50010.\nHadoop Configuration | 263Table 9-6. HTTP server properties\nProperty name Default value Description\nmapred.job.tracker.http.address 0.0.0.0:50030 The jobtracker’s HTTP server address and port.\nmapred.task.tracker.http.address 0.0.0.0:50060 The tasktracker’s HTTP server address and port.\ndfs.http.address 0.0.0.0:50070 The namenode’s HTTP server address and port.\ndfs.datanode.http.address 0.0.0.0:50075 The datanode’s HTTP server address and port.\ndfs.secondary.http.address 0.0.0.0:50090 The secondary namenode’s HTTP server address and\n                                          port.\nThere are also settings for controlling which network interfaces the datanodes and\ntasktrackers report as their IP addresses (for HTTP and RPC servers). The relevant\nproperties are dfs.datanode.dns.interface and mapred.tasktracker.dns.interface,\nboth of which are set to default, which will use the default network interface. You can\nset this explicitly to report the address of a particular interface; eth0, for example.\nOther Hadoop Properties\nThis section discusses some other properties that you might consider setting.\nCluster membership\nTo aid the addition and removal of nodes in the future, you can specify a list of au-\nthorized machines that may join the cluster as datanodes or tasktrackers. The list is\nspecified using the dfs.hosts (for datanodes) and mapred.hosts (for tasktrackers) prop-\nerties, as well as the corresponding dfs.hosts.exclude and mapred.hosts.exclude files\nused for decommissioning. See “Commissioning and Decommissioning No-\ndes” on page 293 for further discussion.\nService-level authorization\nYou can define ACLs to control which users and groups have permission to connect to\neach Hadoop service. Services are defined at the protocol level, so there are ones for\nMapReduce job submission, namenode communication, and so on. By default, au-\nthorization is turned off (see the hadoop.security.authorization property) so all users\nmay access all services. See the hadoop-policy.xml configuration file for more\ninformation.\nBuffer size\nHadoop uses a buffer size of 4 KB (4096 bytes) for its I/O operations. This is a conser-\nvative setting, and with modern hardware and operating systems, you will likely see\nperformance benefits by increasing it. 64 KB (65536 bytes) or 128 KB (131072 bytes)\nare common choices. Set this using the io.file.buffer.size property in core-site.xml.\n264 | Chapter 9: Setting Up a Hadoop ClusterHDFS block size\nThe HDFS block size is 64 MB by default, but many clusters use 128 MB (134,217,728\nbytes) or even 256 MB (268,435,456 bytes) to give mappers more data to work on. Set\nthis using the dfs.block.size property in hdfs-site.xml.\nReserved storage space\nBy default, datanodes will try to use all of the space available in their storage directories.\nIf you want to reserve some space on the storage volumes for non-HDFS use, then you\ncan set dfs.datanode.du.reserved to the amount, in bytes, of space to reserve.\nTrash\nHadoop filesystems have a trash facility, in which deleted files are not actually deleted,\nbut rather are moved to a trash folder, where they remain for a minimum period before\nbeing permanently deleted by the system. The minimum period in minutes that a file\nwill remain in the trash is set using the fs.trash.interval configuration property in\ncore-site.xml. By default the trash interval is zero, which disables trash.\nLike in many operating systems, Hadoop’s trash facility is a user-level feature, meaning\nthat only files that are deleted using the filesystem shell are put in the trash. Files deleted\nprogrammatically are deleted immediately. It is possible to use the trash programmat-\nically, however, by constructing a Trash instance, then calling its moveToTrash() method\nwith the Path of the file intended for deletion. The method returns a value indicating\nsuccess; a value of false means either that trash is not enabled or that the file is already\nin the trash.\nWhen trash is enabled, each user has her own trash directory called .Trash in her home\ndirectory. File recovery is simple: you look for the file in a subdirectory of .Trash and\nmove it out of the trash subtree.\nHDFS will automatically delete files in trash folders, but other filesystems will not, so\nyou have to arrange for this to be done periodically. You can expunge the trash, which\nwill delete files that have been in the trash longer than their minimum period, using\nthe filesystem shell:\n% hadoop fs -expunge\nThe Trash class exposes an expunge() method that has the same effect.\nTask memory limits\nOn a shared cluster, it shouldn’t be possible for one user’s errant MapReduce program\nto bring down nodes in the cluster. This can happen if the map or reduce task has a\nmemory leak, for example, because the machine on which the tasktracker is running\nwill run out of memory and may affect the other running processes. To prevent this\nsituation, you can set mapred.child.ulimit, which sets a maximum limit on the virtual\nmemory of the child process launched by the tasktracker. It is set in kilobytes, and\nHadoop Configuration | 265should\nbe\ncomfortably\nlarger\nthan\nthe\nmemory\nof\nthe\nJVM\nset\nby\nmapred.child.java.opts; otherwise, the child JVM might not start.\nAs an alternative, you can use limits.conf to set process limits at the operating system\nlevel.\nJob scheduler\nParticularly in a multiuser MapReduce setting, consider changing the default FIFO job\nscheduler to one of the more fully featured alternatives. See “Job Schedul-\ning” on page 161.\nPost Install\nOnce you have a Hadoop cluster up and running, you need to give users access to it.\nThis involves creating a home directory for each user, and setting ownership permis-\nsions on it:\n% hadoop fs -mkdir /user/username\n% hadoop fs -chown username:username /user/username\nThis is a good time to set space limits on the directory. The following sets a 1 TB limit\non the given user directory:\n% hadoop dfsadmin -setSpaceQuota 1t /user/username\nBenchmarking a Hadoop Cluster\nIs the cluster set up correctly? The best way to answer this question is empirically: run\nsome jobs and confirm that you get the expected results. Benchmarks make good tests,\nas you also get numbers that you can compare with other clusters as a sanity check on\nwhether your new cluster is performing roughly as expected. And you can tune a cluster\nusing benchmark results to squeeze the best performance out of it. This is often done\nwith monitoring systems in place (“Monitoring” on page 285), so you can see how\nresources are being used across the cluster.\nTo get the best results, you should run benchmarks on a cluster that is not being used\nby others. In practice, this is just before it is put into service, and users start relying on\nit. Once users have periodically scheduled jobs on a cluster it is generally impossible\nto find a time when the cluster is not being used (unless you arrange downtime with\nusers), so you should run benchmarks to your satisfaction before this happens.\nExperience has shown that most hardware failures for new systems are hard drive fail-\nures. By running I/O intensive benchmarks—such as the ones described next—you\ncan “burn in” the cluster before it goes live.\n266 | Chapter 9: Setting Up a Hadoop ClusterHadoop Benchmarks\nHadoop comes with several benchmarks that you can run very easily with minimal\nsetup cost. Benchmarks are packaged in the test JAR file, and you can get a list of them,\nwith descriptions, by invoking the JAR file with no arguments:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar\nMost of the benchmarks show usage instructions when invoked with no arguments.\nFor example:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO\nTestFDSIO.0.0.4\nUsage: TestFDSIO -read | -write | -clean [-nrFiles N] [-fileSize MB] [-resFile\nresultFileName] [-bufferSize Bytes]\nBenchmarking HDFS with TestDFSIO\nTestDFSIO tests the I/O performance of HDFS. It does this by using a MapReduce job\nas a convenient way to read or write files in parallel. Each file is read or written in a\nseparate map task, and the output of the map is used for collecting statistics relating\nto the file just processed. The statistics are accumulated in the reduce, to produce a\nsummary.\nThe following command writes 10 files of 1,000 MB each:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -write -nrFiles 10\n-fileSize 1000\nAt the end of the run, the results are written to the console and also recorded in a local\nfile (which is appended to, so you can rerun the benchmark and not lose old results):\n% cat TestDFSIO_results.log\n----- TestDFSIO ----- : write\nDate & time: Sun Apr 12 07:14:09 EDT 2009\nNumber of files: 10\nTotal MBytes processed: 10000\nThroughput mb/sec: 7.796340865378244\nAverage IO rate mb/sec: 7.8862199783325195\nIO rate std deviation: 0.9101254683525547\nTest exec time sec: 163.387\nThe files are written under the /benchmarks/TestDFSIO directory by default (this can\nbe changed by setting the test.build.data system property), in a directory called\nio_data.\nTo run a read benchmark, use the -read argument. Note that these files must already\nexist (having been written by TestDFSIO -write):\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -read -nrFiles 10\n-fileSize 1000\nHere are the results for a real run:\nBenchmarking a Hadoop Cluster | 267----- TestDFSIO ----- :\nDate & time:\nNumber of files:\nTotal MBytes processed:\nThroughput mb/sec:\nAverage IO rate mb/sec:\nIO rate std deviation:\nTest exec time sec:\nread\nSun Apr 12 07:24:28 EDT 2009\n10\n10000\n80.25553361904304\n98.6801528930664\n36.63507598174921\n47.624\nWhen you’ve finished benchmarking, you can delete all the generated files from HDFS\nusing the -clean argument:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar TestDFSIO -clean\nBenchmarking MapReduce with Sort\nHadoop comes with a MapReduce program that does a partial sort of its input. It is\nvery useful for benchmarking the whole MapReduce system, as the full input dataset\nis transferred through the shuffle. The three steps are: generate some random data,\nperform the sort, then validate the results.\nFirst we generate some random data using RandomWriter. It runs a MapReduce job with\n10 maps per node, and each map generates (approximately) 10 GB of random binary\ndata, with key and values of various sizes. You can change these values if you like by\nsetting the properties test.randomwriter.maps_per_host and test.random\nwrite.bytes_per_map. There are also settings for the size ranges of the keys and values;\nsee RandomWriter for details.\nHere’s how to invoke RandomWriter (found in the example JAR file, not the test one) to\nwrite its output to a directory called random-data:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar randomwriter random-data\nNext we can run the Sort program:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-examples.jar sort random-data sorted-data\nThe overall execution time of the sort is the metric we are interested in, but it’s in-\nstructive to watch the job’s progress via the web UI (http://jobtracker-host:50030/),\nwhere you can get a feel for how long each phase of the job takes. Adjusting the pa-\nrameters mentioned in “Tuning a Job” on page 145 is a useful exercise, too.\nAs a final sanity check, we validate the data in sorted-data is, in fact, correctly sorted:\n% hadoop jar $HADOOP_INSTALL/hadoop-*-test.jar testmapredsort -sortInput random-data \\\n-sortOutput sorted-data\nThis command runs the SortValidator program, which performs a series of checks on\nthe unsorted and sorted data to check whether the sort is accurate. It reports the out-\ncome to the console at the end of its run:\nSUCCESS! Validated the MapReduce framework's 'sort' successfully.\n268 | Chapter 9: Setting Up a Hadoop ClusterOther benchmarks\nThere are many more Hadoop benchmarks, but the following are widely used:\n• MRBench (invoked with mrbench) runs a small job a number of times. It acts as a good\ncounterpoint to sort, as it checks whether small job runs are responsive.\n• NNBench (invoked with nnbench) is useful for load testing namenode hardware.\n• Gridmix is a suite of benchmarks designed to model a realistic cluster workload,\nby mimicking a variety of data-access patterns seen in practice. See src/benchmarks/\ngridmix2 in the distribution for further details.‖\nUser Jobs\nFor tuning, it is best to include a few jobs that are representative of the jobs that your\nusers run, so your cluster is tuned for these and not just for the standard benchmarks.\nIf this is your first Hadoop cluster and you don’t have any user jobs yet, then Gridmix\nis a good substitute.\nWhen running your own jobs as benchmarks you should select a dataset for your user\njobs that you use each time you run the benchmarks to allow comparisons between\nruns. When you set up a new cluster, or upgrade a cluster, you will be able to use the\nsame dataset to compare the performance with previous runs.\nHadoop in the Cloud\nAlthough many organizations choose to run Hadoop in-house, it is also popular to run\nHadoop in the cloud on rented hardware or as a service. For instance, Cloudera offers\ntools for running Hadoop (see Appendix B) in a public or private cloud.\nIn this section, we look at running Hadoop on Amazon EC2, which is a great way to\ntry out your own Hadoop cluster on a low-commitment, trial basis.\nHadoop on Amazon EC2\nAmazon Elastic Compute Cloud (EC2) is a computing service that allows customers\nto rent computers (instances) on which they can run their own applications. A customer\ncan launch and terminate instances on demand, paying by the hour for active instances.\nHadoop comes with a set of scripts that make is easy to run Hadoop on EC2. The scripts\nallow you to perform such operations as launching or terminating a cluster, or adding\ninstances to an existing cluster.\nRunning Hadoop on EC2 is especially appropriate for certain workflows. For example,\nif you store data on Amazon S3, then you can run a cluster on EC2 and run MapReduce\n‖ In a similar vein, PigMix is a set of benchmarks for Pig available from http://wiki.apache.org/pig/PigMix.\nHadoop in the Cloud | 269jobs that read the S3 data, and write output back to S3, before shutting down the cluster.\nIf you’re working with longer-lived clusters, you might copy S3 data onto HDFS run-\nning on EC2 for more efficient processing, as HDFS can take advantage of data locality,\nbut S3 cannot (since S3 storage is not colocated with EC2 nodes).\nEC2 Network Topology\nThe EC2 APIs offer no control over network topology to clients. At the time of this\nwriting, it is not possible to give hints to direct placement of instances—to request that\nthey are split between two racks, say. Nor is it possible to discover in which rack an\ninstance is located after it has booted, which means that Hadoop considers the whole\ncluster to be situated in one rack (“Rack awareness” on page 247), even though in reality\ninstances are likely to be spread across racks. Hadoop will still work, but not as effi-\nciently as it might otherwise.\nSetup\nBefore you can run Hadoop on EC2, you need to work through Amazon’s Getting\nStarted Guide (linked from the EC2 website http://aws.amazon.com/ec2/), which goes\nthrough setting up an account, installing the EC2 command-line tools, and launching\nan instance.\nNext, get a copy of the Hadoop EC2 launch scripts. If you have already installed Apache\nHadoop on your workstation (Appendix A), you will find them in the src/contrib/ec2\nsubdirectory of the installation. If you haven’t installed Hadoop, then do so now. (Note\nthat the same scripts can be used to launch a cluster running Apache Hadoop, or Clou-\ndera’s Distribution for Hadoop, described in Appendix B.)\nYou need to configure the scripts to set your Amazon Web Service credentials, security\nkey details, the version of Hadoop to run, and the EC2 instance type to use (the extra-\nlarge instances are most suitable for running Hadoop). Do this by editing the file src/\ncontrib/ec2/bin/hadoop-ec2-env.sh: the comments in the file document how to set each\nvariable.\nLaunching a cluster\nWe are now ready to launch a cluster. To launch a cluster named test-hadoop-cluster\nwith one master node (running the namenode and jobtracker) and five worker nodes\n(running the datanodes and tasktackers), type:\n% bin/hadoop-ec2 launch-cluster test-hadoop-cluster 5\nThis will create EC2 security groups for the cluster, if they don’t already exist, and give\nthe master and worker nodes unfettered access to one another. It will also enable SSH\naccess from anywhere. Once the security groups have been set up, the master instance\nwill be launched, then once it has started, the five worker instances will be launched.\nThe reason that the worker nodes are launched separately is so that the master’s\n270 | Chapter 9: Setting Up a Hadoop Clusterhostname can be passed to the worker instances, and allow the datanodes and task-\ntrackers to connect to the master when they start up.\nRunning a MapReduce job\nJobs need to be run from within EC2 (so EC2 hostnames resolve correctly), and to do\nthis we need to transfer the job JAR file to the cluster. The scripts provide a convenient\nway to do this. The following command copies the JAR file to the master node:\n% bin/hadoop-ec2 push test-hadoop-cluster /Users/tom/htdg-examples/job.jar\nThere is also a shortcut for logging into the cluster. This command logs into the master\n(using SSH), but it is also possible to specify the EC2 instance ID instead of the cluster\nname to log into any node in the cluster. (There is also a shortcut for the screen com-\nmand that you may prefer.)\n% bin/hadoop-ec2 login test-hadoop-cluster\nThe cluster’s filesystem is empty, so before we run a job, we need to populate it with\ndata. Doing a parallel copy from S3 (see “Hadoop Filesystems” on page 47 for more on\nthe S3 filesystems in Hadoop) using Hadoop’s distcp tool is an efficient way to transfer\ndata into HDFS:\n# hadoop distcp s3n://hadoopbook/ncdc/all input/ncdc/all\nAfter the data has been copied, we can run a job in the usual way:\n# hadoop jar job.jar MaxTemperatureWithCombiner input/ncdc/all output\nAlternatively, we could have specified the input to be S3, which would have the same\neffect. When running multiple jobs over the same input data, it’s best to copy the data\nto HDFS first to save bandwidth:\n# hadoop jar job.jar MaxTemperatureWithCombiner s3n://hadoopbook/ncdc/all output\nWe can track the progress of the job using the jobtracker’s web UI, found at http://\nmaster_host:50030/.\nTerminating a cluster\nTo shut down the cluster, first log out from the EC2 node and issue the terminate-\ncluster command from your workstation:\n# exit\n% bin/hadoop-ec2 terminate-cluster test-hadoop-cluster\nYou will be asked to confirm that you want to terminate all the instances in the cluster.\nHadoop in the Cloud | 271CHAPTER 10\nAdministering Hadoop\nThe previous chapter was devoted to setting up a Hadoop cluster. In this chapter, we\nlook at the procedures to keep a cluster running smoothly.\nHDFS\nPersistent Data Structures\nAs an administrator, it is invaluable to have a basic understanding of how the compo-\nnents of HDFS—the namenode, the secondary namenode, and the datanodes—\norganize their persistent data on disk. Knowing which files are which can help you\ndiagnose problems, or spot that something is awry.\nNamenode directory structure\nA newly formatted namenode creates the following directory structure:\n${dfs.name.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\nRecall from Chapter 9 that the dfs.name.dir property is a list of directories, with the\nsame contents mirrored in each directory. This mechanism provides resilience, partic-\nularly if one of the directories is an NFS mount, as is recommended.\nThe VERSION file is a Java properties file that contains information about the version\nof HDFS that is running. Here are the contents of a typical file:\n#Tue Mar 10 19:21:36 GMT 2009\nnamespaceID=134368441\ncTime=0\nstorageType=NAME_NODE\nlayoutVersion=-18\n273The layoutVersion is a negative integer that defines the version of HDFS’s persistent\ndata structures. This version number has no relation to the release number of the Ha-\ndoop distribution. Whenever the layout changes the version number is decremented\n(for example, the version after −18 is −19). When this happens, HDFS needs to be\nupgraded, since a newer namenode (or datanode) will not operate if its storage layout\nis an older version. Upgrading HDFS is covered in “Upgrades” on page 296.\nThe namespaceID is a unique identifier for the filesystem, which is created when the\nfilesystem is first formatted. The namenode uses it to identify new datanodes, since\nthey will not know the namespaceID until they have registered with the namenode.\nThe cTime property marks the creation time of the namenode’s storage. For newly for-\nmatted storage, the value is always zero, but it is updated to a timestamp whenever the\nfilesystem is upgraded.\nThe storageType indicates that this storage directory contains data structures for a\nnamenode.\nThe other files in the namenode’s storage directory are edits, fsimage, and fstime. These\nare all binary files, which use Hadoop Writable objects as their serialization format (see\n“Serialization” on page 86). To understand what these files are for, we need to dig into\nthe workings of the namenode a little more.\nThe filesystem image and edit log\nWhen a filesystem client performs a write operation (such as creating or moving a file),\nit is first recorded in the edit log. The namenode also has an in-memory representation\nof the filesystem metadata, which it updates after the edit log has been modified. The\nin-memory metadata is used to serve read requests.\nThe edit log is flushed and synced after every write before a success code is returned to\nthe client. For namenodes that write to multiple directories, the write must be flushed\nand synced to every copy before returning successfully. This ensures that no operation\nis lost due to machine failure.\nThe fsimage file is a persistent checkpoint of the filesystem metadata. However, it is\nnot updated for every filesystem write operation, since writing out the fsimage file,\nwhich can grow to be gigabytes in size, would be very slow. This does not compromise\nresilience, however, because if the namenode fails, then the latest state of its metadata\ncan be reconstructed by loading the fsimage from disk into memory, then applying each\nof the operations in the edit log. In fact, this is precisely what the namenode does when\nit starts up (see “Safe Mode” on page 278).\n274 | Chapter 10: Administering HadoopThe fsimage file contains a serialized form of all the directory and file\ninodes in the filesystem. Each inode is an internal representation of a\nfile or directory’s metadata, and contains such information as the file’s\nreplication level, modification and access times, access permissions,\nblock size, and the blocks a file is made up of. For directories, the mod-\nification time, permissions, and quota metadata is stored.\nThe fsimage file does not record the datanodes on which the blocks are\nstored. Instead the namenode keeps this mapping in memory, which it\nconstructs by asking the datanodes for their block lists when they join\nthe cluster, and periodically afterward to ensure the namenode’s block\nmapping is up-to-date.\nAs described, the edits file would grow without bound. Though this state of affairs\nwould have no impact on the system while the namenode is running, if the namenode\nwere restarted, it would take a long time to apply each of the operations in its (very\nlong) edit log. During this time the filesystem would be offline, which is generally\nundesirable.\nThe solution is to run the secondary namenode, whose purpose is to produce check-\npoints of the primary’s in-memory filesystem metadata.* The checkpointing process\nproceeds as follows (and is shown schematically in Figure 10-1):\n1. The secondary asks the primary to roll its edits file, so new edits go to a new file.\n2. The secondary retrieves fsimage and edits from the primary (using HTTP GET).\n3. The secondary loads fsimage into memory, applies each operation from edits, then\ncreates a new consolidated fsimage file.\n4. The secondary sends the new fsimage back to the primary (using HTTP POST).\n5. The primary replaces the old fsimage with the new one from the secondary, and\nthe old edits file with the new one it started in step 1. It also updates the fstime file\nto record the time that the checkpoint was taken.\nAt the end of the process, the primary has an up-to-date fsimage file, and a shorter\nedits file (it is not necessarily empty, as it may have received some edits while the\ncheckpoint was being taken). It is possible for an administrator to run this process\nmanually while the namenode is in safe mode, using the hadoop dfsadmin\n-saveNamespace command.\n* From Hadoop version 0.21.0 onward (which will be released after this book is due to be published), the\nsecondary namenode will be replaced by a checkpoint node, which has the same functionality. At the same\ntime, a new type of namenode, called a backup node, will be introduced whose purpose is to maintain an up-\nto-date copy of the namenode metadata, and will act as a replacement for storing a copy of the metadata on\nNFS.\nHDFS | 275Figure 10-1. The checkpointing process\nThis procedure makes it clear why the secondary has similar memory requirements to\nthe primary (since it loads the fsimage into memory), which is the reason that the sec-\nondary needs a dedicated machine on large clusters.\nThe schedule for checkpointing is controlled by two configuration parameters. The\nsecondary namenode checkpoints every hour (fs.checkpoint.period in seconds) or\nsooner if the edit log has reached 64 MB (fs.checkpoint.size in bytes), which it checks\nevery five minutes.\nSecondary namenode directory structure\nA useful side effect of the checkpointing process is that the secondary has a checkpoint\nat the end of the process, which can be found in a subdirectory called previous.check\npoint. This can be used as a source for making (stale) backups of the namenode’s\nmetadata:\n276 | Chapter 10: Administering Hadoop${fs.checkpoint.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\n/previous.checkpoint/VERSION\n/edits\n/fsimage\n/fstime\nThe layout of this directory and of the secondary’s current directory is identical to the\nnamenode’s. This is by design, since in the event of total namenode failure (when there\nare no recoverable backups, even from NFS), it allows recovery from a secondary\nnamenode. This can be achieved either by copying the relevant storage directory to a\nnew namenode, or, if the secondary is taking over as the new primary namenode, by\nusing the -importCheckpoint option when starting the namenode daemon. The\n-importCheckpoint option will load the namenode metadata from the latest checkpoint\nin the directory defined by the fs.checkpoint.dir property, but only if there is no\nmetadata in the dfs.name.dir directory, so there is no risk of overwriting precious\nmetadata.\nDatanode directory structure\nUnlike namenodes, datanodes do not need to be explicitly formatted, since they create\ntheir storage directories automatically on startup. Here are the key files and directories:\n${dfs.data.dir}/current/VERSION\n/blk_<id_1>\n/blk_<id_1>.meta\n/blk_<id_2>\n/blk_<id_2>.meta\n/...\n/blk_<id_64>\n/blk_<id_64>.meta\n/subdir0/\n/subdir1/\n/...\n/subdir63/\nA datanode’s VERSION file is very similar to the namenode’s:\n#Tue Mar 10 21:32:31 GMT 2009\nnamespaceID=134368441\nstorageID=DS-547717739-172.16.85.1-50010-1236720751627\ncTime=0\nstorageType=DATA_NODE\nlayoutVersion=-18\nThe namespaceID, cTime, and layoutVersion are all the same as the values in the name-\nnode (in fact, the namespaceID is retrieved from the namenode when the datanode first\nconnects). The storageID is unique to the datanode (it is the same across all storage\ndirectories), and is used by the namenode to uniquely identify the datanode. The\nstorageType identifies this directory as a datanode storage directory.\nHDFS | 277The other files in the datanode’s current storage directory are the files with the blk_\nprefix. There are two types: the HDFS blocks themselves (which just consist of the file’s\nraw bytes) and the metadata for a block (with a .meta suffix). A block file just consists\nof the raw bytes of a portion of the file being stored; the metadata file is made up of a\nheader with version and type information, followed by a series of checksums for sec-\ntions of the block.\nWhen the number of blocks in a directory grows to a certain size, the datanode creates\na new subdirectory in which to place new blocks and their accompanying metadata. It\ncreates a new subdirectory every time the number of blocks in a directory reaches 64\n(set by the dfs.datanode.numblocks configuration property). The effect is to have a tree\nwith high fan-out, so even for systems with a very large number of blocks, the directories\nwill only be a few levels deep. By taking this measure, the datanode ensures that there\nis a manageable number of files per directory, which avoids the problems that most\noperating systems encounter when there are a large number of files (tens or hundreds\nof thousands) in a single directory.\nIf the configuration property dfs.data.dir specifies multiple directories (on different\ndrives), blocks are written to each in a round-robin fashion. Note that blocks are not\nreplicated on each drive on a single datanode: block replication is across distinct\ndatanodes.\nSafe Mode\nWhen the namenode starts, the first thing it does is load its image file (fsimage) into\nmemory and apply the edits from the edit log (edits). Once it has reconstructed a con-\nsistent in-memory image of the filesystem metadata, it creates a new fsimage file (ef-\nfectively doing the checkpoint itself, without recourse to the secondary namenode) and\nan empty edit log. Only at this point does the namenode start listening for RPC and\nHTTP requests. However, the namenode is running in safe mode, which means that it\noffers only a read-only view of the filesystem to clients.\nStrictly speaking, in safe mode only filesystem operations that access the\nfilesystem metadata (like producing a directory listing) are guaranteed\nto work. Reading a file will work only if the blocks are available on the\ncurrent set of datanodes in the cluster; and file modifications (writes,\ndeletes, or renames) will always fail.\nRecall that the locations of blocks in the system are not persisted by the namenode—\nthis information resides with the datanodes, in the form of a list of the blocks it is\nstoring. During normal operation of the system, the namenode has a map of block\nlocations stored in memory. Safe mode is needed to give the datanodes time to check\nin to the namenode with their block lists, so the namenode can be informed of enough\nblock locations to run the filesystem effectively. If the namenode didn’t wait for enough\ndatanodes to check in, then it would start the process of replicating blocks to new\n278 | Chapter 10: Administering Hadoopdatanodes, which would be unnecessary in most cases (since it only needed to wait for\nthe extra datanodes to check in), and would put a great strain on the cluster’s resources.\nIndeed, while in safe mode, the namenode does not issue any block replication or\ndeletion instructions to datanodes.\nSafe mode is exited when the minimal replication condition is reached, plus an extension\ntime of 30 seconds. The minimal replication condition is when 99.9% of the blocks in\nthe whole filesystem meet their minimum replication level (which defaults to one, and\nis set by dfs.replication.min).\nWhen you are starting a newly formatted HDFS cluster, the namenode does not go into\nsafe mode since there are no blocks in the system.\nTable 10-1. Safe mode properties\nProperty name Type Default value Description\ndfs.replication.min int 1 The minimum number of replicas that have to be writ-\n                         ten for a write to be successful.\ndfs.safemode.threshold.pct float 0.999 The proportion of blocks in the system that must meet\n                                      the minimum replication level defined by dfs.rep\n                                     lication.min before the namenode will exit safe\n                                    mode. Setting this value to 0 or less forces the name-\n                                   node not to start in safe mode. Setting this value to\n                                  more than 1 means the namenode never exits safe\n                                 mode.\ndfs.safemode.extension int 30000 The time, in milliseconds, to extend safe mode by after\n                                the minimum replication condition defined by\n                               dfs.safemode.threshold.pct has been satis-\n                              fied. For small clusters (tens of nodes), it can be set to\n                             0.\nEntering and leaving safe mode\nTo see whether the namenode is in safe mode, you can use the dfsadmin command:\n% hadoop dfsadmin -safemode get\nSafe mode is ON\nThe front page of the HDFS web UI provides another indication of whether the name-\nnode is in safe mode.\nSometimes you want to wait for the namenode to exit safe mode before carrying out a\ncommand, particularly in scripts. The wait option achieves this:\nhadoop dfsadmin -safemode wait\n# command to read or write a file\nAn administrator has the ability to make the namenode enter or leave safe mode at any\ntime. It is sometimes necessary to do this when carrying out maintenance on the cluster,\nHDFS | 279or after upgrading a cluster to confirm that data is still readable. To enter safe mode,\nuse the following command:\n% hadoop dfsadmin -safemode enter\nSafe mode is ON\nYou can use this command when the namenode is still in safe mode while starting up\nto ensure that it never leaves safe mode. Another way of making sure that the namenode\nstays in safe mode indefinitely is to set the property dfs.safemode.threshold.pct to a\nvalue over one.\nYou can make the namenode leave safe mode by using:\n% hadoop dfsadmin -safemode leave\nSafe mode is OFF\nAudit Logging\nHDFS has the ability to log all filesystem access requests, a feature that some organi-\nzations require for auditing purposes. Audit logging is implemented using log4j logging\nat the INFO level, and in the default configuration it is disabled, as the log threshold is\nset to WARN in log4j.properties:\nlog4j.logger.org.apache.hadoop.fs.FSNamesystem.audit=WARN\nYou can enable audit logging by replacing WARN with INFO, and the result will be a log\nline written to the namenode’s log for every HDFS event. Here’s an example for a list\nstatus request on /user/tom:\n2009-03-13 07:11:22,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit:\nugi=tom,staff,admin\nip=/127.0.0.1\ncmd=listStatus src=/user/tom dst=null\nperm=null\nIt is a good idea to configure log4j so that the audit log is written to a separate file and\nisn’t mixed up with the namenode’s other log entries. An example of how to do this\ncan be found on the Hadoop wiki at http://wiki.apache.org/hadoop/HowToConfigure.\nTools\ndfsadmin\nThe dfsadmin tool is a multi-purpose tool for finding information about the state of\nHDFS, as well as performing administration operations on HDFS. It is invoked as\nhadoop dfsadmin. Commands that alter HDFS state typically require superuser\nprivileges.\nThe available commands to dfsadmin are described in Table 10-2.\n280 | Chapter 10: Administering HadoopTable 10-2. dfsadmin commands\nCommand Description\n-help Shows help for a given command, or all commands if no command is specified.\n-report Shows filesystem statistics (similar to that shown in the web UI) and information on connected\n       datanodes.\n-metasave Dumps information to a file in Hadoop’s log directory about blocks that are being replicated or\n           deleted, and a list of connected datanodes.\n-safemode Changes or query the state of safe mode. See “Safe Mode” on page 278.\n-saveNamespace Saves the current in-memory filesystem image to a new fsimage file, and resets the edits file. This\n              operation may be performed only in safe mode.\n-refreshNodes Updates the set of datanodes that are permitted to connect to the namenode. See “Commissioning\n               and Decommissioning Nodes” on page 293.\n-upgradeProgress Gets information on the progress of a HDFS upgrade, or force an upgrade to proceed. See\n                “Upgrades” on page 296.\n-finalizeUpgrade Removes the previous version of the datanodes’ and namenode’s storage directories. Used after\n                    an upgrade has been applied and the cluster is running successfully on the new version. See\n                   “Upgrades” on page 296.\n-setQuota Sets directory quotas. Directory quotas set a limit on the number of names (files or directories) in\n         the directory tree. Directory quotas are useful for preventing users from creating large numbers\n        of small files, a measure that helps preserve the namenode’s memory (recall that accounting\n         information for every file, directory, and block in the filesystem is stored in memory).\n-clrQuota Clears specified directory quotas.\n-setSpaceQuota Sets space quotas on directories. Space quotas set a limit on the size of files that may be stored in\n              a directory tree. They are useful for giving users a limited amount of storage.\n-clrSpaceQuota Clears specified space quotas.\n-refreshServiceAcl Refreshes the namenode’s service-level authorization policy file.\nFilesystem check (fsck)\nHadoop provides an fsck utility for checking the health of files in HDFS. The tool looks\nfor blocks that are missing from all datanodes, as well as under- or over-replicated\nblocks. Here is an example of checking the whole filesystem for a small cluster:\n% hadoop fsck /\n......................Status: HEALTHY\nTotal size:\n511799225 B\nTotal dirs:\n10\nTotal files:\n22\nTotal blocks (validated):\n22 (avg. block size 23263601 B)\nMinimally replicated blocks:\n22 (100.0 %)\nOver-replicated blocks:\n0 (0.0 %)\nUnder-replicated blocks:\n0 (0.0 %)\nMis-replicated blocks:\n0 (0.0 %)\nDefault replication factor:\n3\nAverage block replication:\n3.0\nHDFS | 281Corrupt blocks:\nMissing replicas:\nNumber of data-nodes:\nNumber of racks:\n0\n0 (0.0 %)\n4\n1\nThe filesystem under path '/' is HEALTHY\nfsck recursively walks the filesystem namespace, starting at the given path (here the\nfilesystem root), and checks the files it finds. It prints a dot for every file it checks. To\ncheck a file, fsck retrieves the metadata for the file’s blocks, and looks for problems or\ninconsistencies. Note that fsck retrieves all of its information from the namenode; it\ndoes not communicate with any datanodes to actually retrieve any block data.\nMost of the output from fsck is self-explanatory, but here are some of the conditions it\nlooks for:\nOver-replicated blocks\nThese are blocks that exceed their target replication for the file they belong to.\nOver-replication is not normally a problem, and HDFS will automatically delete\nexcess replicas.\nUnder-replicated blocks\nThese are blocks that do not meet their target replication for the file they belong\nto. HDFS will automatically create new replicas of under-replicated blocks until\nthey meet the target replication. You can get information about the blocks being\nreplicated (or waiting to be replicated) using hadoop dfsadmin -metasave.\nMisreplicated blocks\nThese are blocks that do not satisfy the block replica placement policy (see “Replica\nPlacement” on page 67). For example, for a replication level of three in a multirack\ncluster, if all three replicas of a block are on the same rack, then the block is mis-\nreplicated since the replicas should be spread across at least two racks for resilience.\nA misreplicated block is not fixed automatically by HDFS (at the time of this writ-\ning). As a workaround, you can fix the problem manually by increasing the repli-\ncation of the file the block belongs to (using hadoop fs -setrep), waiting until the\nblock gets replicated, then decreasing the replication of the file back to its original\nvalue.\nCorrupt blocks\nThese are blocks whose replicas are all corrupt. Blocks with at least one noncorrupt\nreplica are not reported as corrupt; the namenode will replicate the noncorrupt\nreplica until the target replication is met.\nMissing replicas\nThese are blocks with no replicas anywhere in the cluster.\nCorrupt or missing blocks are the biggest cause for concern, as it means data has been\nlost. By default, fsck leaves files with corrupt or missing blocks, but you can tell it to\nperform one of the following actions on them:\n282 | Chapter 10: Administering Hadoop• Move the affected files to the /lost+found directory in HDFS, using the -move option.\nFiles are broken into chains of contiguous blocks to aid any salvaging efforts you\nmay attempt.\n• Delete the affected files, using the -delete option. Files cannot be recovered after\nbeing deleted.\nFinding the blocks for a file. The fsck tool provides an easy way to find out which blocks are\nin any particular file. For example:\n% hadoop fsck /user/tom/part-00007 -files -blocks -racks\n/user/tom/part-00007 25582428 bytes, 1 block(s): OK\n0. blk_-3724870485760122836_1035 len=25582428 repl=3 [/default-rack/10.251.43.2:50010,\n/default-rack/10.251.27.178:50010, /default-rack/10.251.123.163:50010]\nThis says that the file /user/tom/part-00007 is made up of one block, and shows the\ndatanodes where the blocks are located. The fsck options used are as follows:\n• The -files option shows the line with the filename, size, number of blocks, and\nits health (whether there are any missing blocks).\n• The -blocks option shows information about each block in the file, one line per\nblock.\n• The -racks option displays the rack location and the datanode addresses for each\nblock.\nRunning hadoop fsck without any arguments displays full usage instructions.\nDatanode block scanner\nEvery datanode runs a block scanner, which periodically verifies all the blocks stored\non the datanode. This allows bad blocks to be detected and fixed before they are read\nby clients. The DataBlockScanner maintains a list of blocks to verify, and scans them\none by one for checksum errors. The scanner employs a throttling mechanism to pre-\nserve disk bandwidth on the datanode.\nBlocks are periodically verified every three weeks to guard against disk errors over time\n(this is controlled by the dfs.datanode.scan.period.hours property, which defaults to\n504 hours). Corrupt blocks are reported to the namenode to be fixed.\nYou can get a block verification report for a datanode by visiting the datanode’s web\ninterface at http://datanode:50075/blockScannerReport. Here’s an example of a report,\nwhich should be self-explanatory:\nTotal Blocks\nVerified in last hour\nVerified in last day\nVerified in last week\nVerified in last four weeks\nVerified in SCAN_PERIOD\nNot yet verified\nVerified since restart\n:\n:\n:\n:\n:\n:\n:\n:\n21131\n70\n1767\n7360\n20057\n20057\n1074\n35912\nHDFS | 283Scans since restart\nScan errors since restart\nTransient scan errors\nCurrent scan rate limit KBps\nProgress this period\nTime left in cur period\n:\n:\n:\n:\n:\n:\n6541\n0\n0\n1024\n109%\n53.08%\nBy specifying the listblocks parameter, http://datanode:50075/blockScannerReport?\nlistblocks, the report is preceded by a list of all the blocks on the datanode along with\ntheir latest verification status. Here is a snippet of the block list (lines are split to fit the\npage):\nblk_6035596358209321442\n: status : ok\nnot yet verified\nblk_3065580480714947643\n: status : ok\n2008-07-11 05:48:26,400\nblk_8729669677359108508\n: status : ok\n2008-07-11 05:55:27,345\ntype : none\nscan time : 0\ntype : remote scan time : 1215755306400\ntype : local scan time : 1215755727345\nThe first column is the block ID, followed by some key-value pairs. The status can be\none of failed or ok according to whether the last scan of the block detected a checksum\nerror. The type of scan is local if it was performed by the background thread, remote\nif it was performed by a client or a remote datanode, or none if a scan of this block has\nyet to be made. The last piece of information is the scan time, which is displayed as the\nnumber of milliseconds since midnight 1 January 1970, and also as a more readable\nvalue.\nbalancer\nOver time the distribution of blocks across datanodes can become unbalanced. An\nunbalanced cluster can affect locality for MapReduce, and it puts a greater strain on\nthe highly utilized datanodes, so it’s best avoided.\nThe balancer program is a Hadoop daemon that re-distributes blocks by moving them\nfrom over-utilized datanodes to under-utilized datanodes, while adhering to the block\nreplica placement policy that makes data loss unlikely by placing block replicas on\ndifferent racks (see “Replica Placement” on page 67). It moves blocks until the cluster\nis deemed to be balanced, which means that the utilization of every datanode (ratio of\nused space on the node to total capacity of the node) differs from the utilization of the\ncluster (ratio of used space on the cluster to total capacity of the cluster) by no more\nthan a given threshold percentage. You can start the balancer with:\n% start-balancer.sh\nThe -threshold argument specifies the threshold percentage that defines what it means\nfor the cluster to be balanced. The flag is optional, in which case the threshold is 10%.\nAt any one time, only one balancer may be running on the cluster.\nThe balancer runs until the cluster is balanced, it cannot move any more blocks, or it\nloses contact with the namenode. It produces a logfile in the standard log directory,\n284 | Chapter 10: Administering Hadoopwhere it writes a line for every iteration of redistribution that it carries out. Here is the\noutput from a short run on a small cluster:\nTime Stamp\nIteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved\nMar 18, 2009 5:23:42 PM 0\n0 KB\n219.21 MB\n150.29 MB\nMar 18, 2009 5:27:14 PM 1\n195.24 MB\n22.45 MB\n150.29 MB\nThe cluster is balanced. Exiting...\nBalancing took 6.072933333333333 minutes\nThe balancer is designed to run in the background without unduly taxing the cluster,\nor interfering with other clients using the cluster. It limits the bandwidth that it uses to\ncopy a block from one node to another. The default is a modest 1 MB/s, but this can\nbe changed by setting the dfs.balance.bandwidthPerSec property in hdfs-site.xml, speci-\nfied in bytes.\nMonitoring\nMonitoring is an important part of system administration. In this section, we look at\nthe monitoring facilities in Hadoop, and how they can hook into external monitoring\nsystems.\nThe purpose of monitoring is to detect when the cluster is not providing the expected\nlevel of service. The master daemons are the most important to monitor: the namenodes\n(primary and secondary), and the jobtracker. Failure of datanodes and tasktrackers is\nto be expected, particularly on larger clusters, so you should provide extra capacity so\nthat the cluster can tolerate having a small percentage of dead nodes at any time.\nIn addition to the facilities described next, some administrators run test jobs on a pe-\nriodic basis as a test of the cluster’s health.\nThere is lot of work going on to add more monitoring capabilities to Hadoop, which\nis not covered here. For example, Chukwa† is a data collection and monitoring system\nbuilt on HDFS and MapReduce, and excels at mining log data for finding large-scale\ntrends. Another example is the work to define a service lifecycle for Hadoop daemons,\nwhich includes adding a “ping” method to get a brief summary of a daemon’s health.‡\nLogging\nAll Hadoop daemons produce logfiles that can be very useful for finding out what is\nhappening in the system. “System logfiles” on page 256 explains how to configure these\nfiles.\n† http://hadoop.apache.org/chukwa\n‡ https://issues.apache.org/jira/browse/HADOOP-3628\nMonitoring | 285Setting log levels\nWhen debugging a problem, it is very convenient to be able to change the log level\ntemporarily for a particular component in the system.\nHadoop daemons have a web page for changing the log level for any log4j log name,\nwhich can be found at /logLevel in the daemon’s web UI. By convention, log names in\nHadoop correspond to the classname doing the logging, although there are exceptions\nto this rule, so you should consult the source code to find log names.\nFor example, to enable debug logging for the JobTracker class, we would visit the job-\ntracker’s web UI at http://jobtracker-host:50030/logLevel and set the log name\norg.apache.hadoop.mapred.JobTracker to level DEBUG.\nThe same thing can be achieved from the command line as follows:\n% hadoop daemonlog -setlevel jobtracker-host:50030 \\\norg.apache.hadoop.mapred.JobTracker DEBUG\nLog levels changed in this way are reset when the daemon restarts, which is usually\nwhat you want. However, to make a persistent change to a log level, simply change the\nlog4j.properties file in the configuration directory. In this case, the line to add is:\nlog4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\nGetting stack traces\nHadoop daemons expose a web page (/stacks in the web UI) that produces a thread\ndump for all running threads in the daemon’s JVM. For example, you can get a thread\ndump for a jobtracker from http://jobtracker-host:50030/stacks.\nMetrics\nThe HDFS and MapReduce daemons collect information about events and measure-\nments that are collectively known as metrics. For example, datanodes collect the fol-\nlowing metrics (and many more): the number of bytes written, the number of blocks\nreplicated, and the number of read requests from clients (both local and remote).\nMetrics belong to a context, and Hadoop currently uses “dfs”, “mapred”, “rpc”, and\n“jvm” contexts. Hadoop daemons usually collect metrics under several contexts. For\nexample, datanodes collect metrics for the “dfs”, “rpc”, and “jvm” contexts.\nHow Do Metrics Differ from Counters?\nThe main difference is their scope: metrics are collected by Hadoop daemons, whereas\ncounters (see “Counters” on page 211) are collected for MapReduce tasks, and aggre-\ngated for the whole job. They have different audiences, too: broadly speaking, metrics\nare for administrators, and counters are for MapReduce users.\nThe way they are collected and aggregated is also different. Counters are a MapReduce\nfeature, and the MapReduce system ensures that counter values are propagated from\n286 | Chapter 10: Administering Hadoopthe tasktrackers where they are produced, back to the jobtracker, and finally back to\nthe client running the MapReduce job. (Counters are propagated via RPC heartbeats;\nsee “Progress and Status Updates” on page 156.) Both the tasktrackers and the job-\ntracker perform aggregation.\nThe collection mechanism for metrics is decoupled from the component that receives\nthe updates, and there are various pluggable outputs, including local files, Ganglia, and\nJMX. The daemon collecting the metrics performs aggregation on them before they are\nsent to the output.\nA context defines the unit of publication, you can choose to publish the “dfs” context,\nbut not the “jvm” context, for instance. Metrics are configured in the conf/hadoop-\nmetrics.properties file, and, by default, all contexts are configured so they do not publish\ntheir metrics. This is the contents of the default configuration file (minus the\ncomments):\ndfs.class=org.apache.hadoop.metrics.spi.NullContext\nmapred.class=org.apache.hadoop.metrics.spi.NullContext\njvm.class=org.apache.hadoop.metrics.spi.NullContext\nrpc.class=org.apache.hadoop.metrics.spi.NullContext\nEach line in this file configures a different context, and specifies the class that handles\nthe metrics for that context. The class must be an implementation of the MetricsCon\ntext interface; and, as the name suggests, the NullContext class neither publishes nor\nupdates metrics.§\nThe other implementations of MetricsContext are covered in the following sections.\nFileContext\nFileContext writes metrics to a local file. It exposes two configuration properties:\nfileName, which specifies the absolute name of the file to write to, and period, for the\ntime interval (in seconds) between file updates. Both properties are optional; if not set,\nthe metrics will be written to standard output every five seconds.\nConfiguration properties apply to a context name, and are specified by appending the\nproperty name to the context name (separated by a dot). For example, to dump the\n“jvm” context to a file, we alter its configuration to be the following:\njvm.class=org.apache.hadoop.metrics.file.FileContext\njvm.fileName=/tmp/jvm_metrics.log\nIn the first line, we have changed the “jvm” context to use a FileContext, and in the\nsecond, we have set the “jvm” context’s fileName property to be a temporary file. Here\nare two lines of output from the logfile, split over several lines to fit the page:\n§ The term “context” is (perhaps unfortunately) overloaded here, since it can refer to either a collection of\nmetrics (the “dfs” context, for example) or the class that publishes metrics (the NullContext, for example).\nMonitoring | 287jvm.metrics: hostName=ip-10-250-59-159, processName=NameNode, sessionId=, gcCount=46, ↵\ngcTimeMillis=394, logError=0, logFatal=0, logInfo=59, logWarn=1, ↵\nmemHeapCommittedM=4.9375, memHeapUsedM=2.5322647, memNonHeapCommittedM=18.25, ↵\nmemNonHeapUsedM=11.330269, threadsBlocked=0, threadsNew=0, threadsRunnable=6, ↵\nthreadsTerminated=0, threadsTimedWaiting=8, threadsWaiting=13\njvm.metrics: hostName=ip-10-250-59-159, processName=SecondaryNameNode, sessionId=, ↵\ngcCount=36, gcTimeMillis=261, logError=0, logFatal=0, logInfo=18, logWarn=4, ↵\nmemHeapCommittedM=5.4414062, memHeapUsedM=4.46756, memNonHeapCommittedM=18.25, ↵\nmemNonHeapUsedM=10.624519, threadsBlocked=0, threadsNew=0, threadsRunnable=5, ↵\nthreadsTerminated=0, threadsTimedWaiting=4, threadsWaiting=2\nFileContext can be useful on a local system for debugging purposes, but is unsuitable\non a larger cluster since the output files are spread across the cluster, which makes\nanalyzing them difficult.\nGangliaContext\nGanglia (http://ganglia.info/) is an open source distributed monitoring system for very\nlarge clusters. It is designed to impose very low resource overheads on each node in the\ncluster. Ganglia itself collects metrics, such as CPU and memory usage; by using\nGangliaContext, you can inject Hadoop metrics into Ganglia.\nGangliaContext has one required property, servers, which takes a space- and/or\ncomma-separated list of Ganglia server host-port pairs. Further details on configuring\nthis context can be found on the Hadoop wiki.\nFor a flavor of the kind of information you can get out of Ganglia, see Figure 10-2,\nwhich shows how the number of tasks in the jobtracker’s queue varies over time.\nFigure 10-2. Ganglia plot of number of tasks in the jobtracker queue\nNullContextWithUpdateThread\nBoth FileContext and a GangliaContext push metrics to an external system. However,\nsome monitoring systems—notably JMX—need to pull metrics from Hadoop. Null\nContextWithUpdateThread is designed for this. Like NullContext, it doesn’t publish any\nmetrics, but in addition it runs a timer that periodically updates the metrics stored in\nmemory. This ensures that the metrics are up-to-date when they are fetched by another\nsystem.\n288 | Chapter 10: Administering HadoopAll implementations of MetricsContext except NullContext perform this updating func-\ntion (and they all expose a period property that defaults to five seconds), so you need\nto use NullContextWithUpdateThread only if you are not collecting metrics using another\noutput. If you were using GangliaContext, for example, then it would ensure the metrics\nare updated, so you would be able to use JMX in addition with no further configuration\nof the metrics system. JMX is discussed in more detail shortly.\nCompositeContext\nCompositeContext allows you to output the same set of metrics to multiple contexts,\nsuch as a FileContext and a GangliaContext. The configuration is slightly tricky, and is\nbest shown by an example:\njvm.class=org.apache.hadoop.metrics.spi.CompositeContext\njvm.arity=2\njvm.sub1.class=org.apache.hadoop.metrics.file.FileContext\njvm.fileName=/tmp/jvm_metrics.log\njvm.sub2.class=org.apache.hadoop.metrics.ganglia.GangliaContext\njvm.servers=ip-10-250-59-159.ec2.internal:8649\nThe arity property is used to specify the number of subcontexts; in this case, there are\ntwo. The property names for each subcontext are modified to have a part specifying\nthe subcontext number, hence jvm.sub1.class and jvm.sub2.class.\nJava Management Extensions\nJava Management Extensions (JMX) is a standard Java API for monitoring and man-\naging applications. Hadoop includes several managed beans (MBeans), which expose\nHadoop metrics to JMX-aware applications. There are MBeans that expose the metrics\nin the “dfs” and “rpc” contexts, but none for the “mapred” context (at the time of this\nwriting), or the “jvm” context (as the JVM itself exposes a richer set of JVM metrics).\nThese MBeans are listed in Table 10-3.\nTable 10-3. Hadoop MBeans\nMBean Class Daemons Metrics\nNameNodeActivityM Namenode Namenode activity metrics,\nBean such as the number of create\n    file operations\nFSNamesystemMBean Namenode Namenode status metrics,\n                          such as the number of con-\n                         nected datanodes\nDataNodeActivityM Datanode Datanode activity metrics,\nBean such as number of bytes\n    read\nFSDatasetMBean Datanode Datanode storage metrics,\n                       such as capacity and free\n                      storage space\nMonitoring | 289MBean Class Daemons Metrics\nRpcActivityMBean All daemons that use RPC: namenode, datanode, jobtracker, RPC statistics, such as aver-\n                 tasktracker age processing time\nThe JDK comes with a tool called JConsole for viewing MBeans in a running JVM. It’s\nuseful for browsing Hadoop metrics, as demonstrated in Figure 10-3.\nFigure 10-3. JConsole view of a locally running namenode, showing metrics for the filesystem state\nAlthough you can see Hadoop metrics via JMX using the default metrics\nconfiguration, they will not be updated unless you change the\nMetricsContext implementation to something other than NullContext.\nFor example, NullContextWithUpdateThread is appropriate if JMX is the\nonly way you will be monitoring metrics.\nMany third-party monitoring and alerting systems (such as Nagios or Hyperic) can\nquery MBeans, making JMX the natural way to monitor your Hadoop cluster from an\nexisting monitoring system. You will need to enable remote access to JMX, however,\nand choose a level of security that is appropriate for your cluster. The options here\ninclude password authentication, SSL connections, and SSL client-authentication. See\nthe official Java documentation‖ for an in-depth guide on configuring these options.\n‖ http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n290 | Chapter 10: Administering HadoopAll the options for enabling remote access to JMX involve setting Java system proper-\nties, which we do for Hadoop by editing the conf/hadoop-env.sh file. The following\nconfiguration settings show how to enable password-authenticated remote access to\nJMX on the namenode (with SSL disabled). The process is very similar for other Hadoop\ndaemons:\nexport HADOOP_NAMENODE_OPTS=""-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.password.file=$HADOOP_CONF_DIR/jmxremote.password\n-Dcom.sun.management.jmxremote.port=8004 $HADOOP_NAMENODE_OPTS""\nThe jmxremote.password file lists the usernames and their passwords in plain text; the\nJMX documentation has further details on the format of this file.\nWith this configuration, we can use JConsole to browse MBeans on a remote name-\nnode. Alternatively, we can use one of the many JMX tools to retrieve MBean attribute\nvalues. Here is an example of using the “jmxquery” command-line tool (and Nagios\nplug-in, available from http://code.google.com/p/jmxquery/) to retrieve the number of\nunder-replicated blocks:\n% ./check_jmx -U service:jmx:rmi:///jndi/rmi://namenode-host:8004/jmxrmi -O \\\nhadoop:service=NameNode,name=FSNamesystemState -A UnderReplicatedBlocks \\\n-w 100 -c 1000 -username monitorRole -password secret\nJMX OK - UnderReplicatedBlocks is 0\nThis command establishes a JMX RMI connection to the host namenode-host on port\n8004 and authenticates using the given username and password. It reads the attribute\nUnderReplicatedBlocks of the object named hadoop:service=NameNode,name=FSNamesys\ntemState and prints out its value on the console.# The -w and -c options specify warning\nand critical levels for the value: the appropriate values of these are normally determined\nafter operating a cluster for a while.\nIt’s common to use Ganglia in conjunction with an alerting system like Nagios for\nmonitoring a Hadoop cluster. Ganglia is good for efficiently collecting a large number\nof metrics and graphing them, whereas Nagios and similar systems are good at sending\nalerts when a critical threshold is reached in any of a smaller set of metrics.\n# It’s convenient to use JConsole to find the object names of the MBeans that you want to monitor. Note that\nMBeans for datanode metrics currently contain a random identifier, which makes it difficult to monitor them\nin anything but an ad hoc way. See https://issues.apache.org/jira/browse/HADOOP-4482.\nMonitoring | 291Maintenance\nRoutine Administration Procedures\nMetadata backups\nIf the namenode’s persistent metadata is lost or damaged, the entire filesystem is ren-\ndered unusable, so it is critical that backups are made of these files. You should keep\nmultiple copies of different ages (one hour, one day, one week, and one month, say) to\nprotect against corruption, either in the copies themselves or in the live files running\non the namenode.\nA straightforward way to make backups is to write a script to periodically archive the\nsecondary namenode’s previous.checkpoint subdirectory (under the directory defined\nby the fs.checkpoint.dir property) to an offsite location. The script should additionally\ntest the integrity of the copy. This can be done by starting a local namenode daemon,\nand verifying that it has successfully read the fsimage and edits files into memory (by\nscanning the namenode log for the appropriate success message, for example).*\nData backups\nAlthough HDFS is designed to store data reliably, data loss can occur, just like in any\nstorage system, and thus a backup strategy is essential. With the large data volumes\nthat Hadoop can store, deciding what data to back up, and where to store it is a chal-\nlenge. The key here is to prioritize your data. The highest priority is the data that cannot\nbe regenerated, and which is critical to the business; though data that is straightforward\nto regenerate, or essentially disposable because it is of limited business value, is the\nlowest priority, and you may choose not to make backups of this category of data.\nDo not make the mistake of thinking that HDFS replication is a substi-\ntute for making backups. Bugs in HDFS can cause replicas to be lost. So\ncan hardware failures. Although Hadoop is expressly designed so that\nhardware failure is very unlikely to result in data loss, the possibility can\nnever be completely ruled out, particularly when combined with soft-\nware bugs, or human error.\nWhen it comes to backups, think of HDFS in the same way as you would\nRAID. Although the data will survive the loss of an individual RAID\ndisk, it may not if the RAID controller fails, or is buggy (perhaps over-\nwriting some data), or the entire array is damaged.\n* Hadoop 0.21.0 comes with an Offline Image Viewer, which can be used to check the integrity of the image\nfiles.\n292 | Chapter 10: Administering HadoopIt’s common to have a policy for user directories in HDFS. For example, they may have\nspace quotas, and be backed up nightly. Whatever the policy, make sure your users\nknow what it is, so they know what to expect.\nThe distcp tool is ideal for making backups to other HDFS clusters (preferably running\non a different version of the software, to guard against loss due to bugs in HDFS) or\nother Hadoop filesystems (such as S3 or KFS), since it can copy files in parallel. Alter-\nnatively, you can employ an entirely different storage system for backups, using one of\nthe ways to export data from HDFS described in “Hadoop Filesystems” on page 47.\nFilesystem check (fsck)\nIt is advisable to run HDFS’s fsck tool regularly (for example, daily) on the whole file-\nsystem to proactively look for missing or corrupt blocks. See “Filesystem check\n(fsck)” on page 281.\nFilesystem balancer\nRun the balancer tool (see “balancer” on page 284) regularly to keep the filesystem\ndatanodes evenly balanced.\nCommissioning and Decommissioning Nodes\nAs an administrator of a Hadoop cluster, you will need to add or remove nodes from\ntime to time. For example, to grow the storage available to a cluster, commission new\nnodes. Conversely, sometimes you may wish to shrink a cluster, and to do so, you\ndecommission nodes. It can sometimes be necessary to decommission a node if it is\nmisbehaving, perhaps because it is failing more often than it should or its performance\nis noticeably slow.\nNodes normally run both a datanode and a tasktracker, and both are typically\ncommissioned or decommissioned in tandem.\nCommissioning new nodes\nAlthough commissioning a new node can be as simple as configuring the hdfs-\nsite.xml file to point to the namenode and the mapred-site.xml file to point to the job-\ntracker and starting the datanode and jobtracker daemons, it is generally best to have\na list of authorized nodes.\nIt is a potential security risk to allow any machine to connect to the namenode and act\nas a datanode, since the machine may gain access to data that it is not authorized to\nsee. Furthermore, since such a machine is not a real datanode, it is not under your\ncontrol, and may stop at any time, causing potential data loss. (Imagine what would\nhappen if a number of such nodes were connected, and a block of data was present\nonly on the “alien” nodes?) This scenario is a risk even inside a firewall, through\nMaintenance | 293misconfiguration, so datanodes (and tasktrackers) should be explicitly managed on all\nproduction clusters.\nDatanodes that are permitted to connect to the namenode are specified in a file whose\nname is specified by the dfs.hosts property. The file resides on the namenode’s local\nfilesystem, and it contains a line for each datanode, specified by network address (as\nreported by the datanode—you can see what this is by looking at the namenode’s web\nUI). If you need to specify multiple network addresses for a datanode, put them on one\nline, separated by whitespace.\nSimilarly, tasktrackers that may connect to the jobtracker are specified in a file whose\nname is specified by the mapred.hosts property. In most cases, there is one shared file,\nreferred to as the include file, that both dfs.hosts and mapred.hosts refer to, since nodes\nin the cluster run both datanode and tasktracker daemons.\nThe file (or files) specified by the dfs.hosts and mapred.hosts properties\nis different from the slaves file. The former is used by the namenode and\njobtracker to determine which worker nodes may connect. The slaves\nfile is used by the Hadoop control scripts to perform cluster-wide op-\nerations, such as cluster restarts. It is never used by the Hadoop\ndaemons.\nTo add new nodes to the cluster:\n1. Add the network addresses of the new nodes to the include file.\n2. Update the namenode with the new set of permitted datanodes using this\ncommand:\n% hadoop dfsadmin -refreshNodes\n3. Update the slaves file with the new nodes, so that they are included in future op-\nerations performed by the Hadoop control scripts.\n4. Start the new datanodes.\n5. Restart the MapReduce cluster.†\n6. Check that the new datanodes and tasktrackers appear in the web UI.\nHDFS will not move blocks from old datanodes to new datanodes to balance the cluster.\nTo do this you should run the balancer described in “balancer” on page 284.\nDecommissioning old nodes\nAlthough HDFS is designed to tolerate datanode failures, this does not mean you can\njust terminate datanodes en masse with no ill effect. With a replication level of three,\n† At the time of this writing, there is no command to refresh the set of permitted nodes in the jobtracker.\nConsider setting the mapred.jobtracker.restart.recover property to true to make the jobtracker recover\nrunning jobs after a restart.\n294 | Chapter 10: Administering Hadoopfor example, the chances are very high that you will lose data by simultaneously shutting\ndown three datanodes if they are on different racks. The way to decommission\ndatanodes is to inform the namenode of the nodes that you wish to take out of circu-\nlation, so that it can replicate the blocks to other datanodes before the datanodes are\nshut down.\nWith tasktrackers, Hadoop is more forgiving. If you shut down a tasktracker that is\nrunning tasks, the jobtracker will notice the failure and reschedule the tasks on other\ntasktrackers.\nThe decommissioning process is controlled by an exclude file, which for HDFS is set\nby the dfs.hosts.exclude property, and for MapReduce by the mapred.hosts.exclude\nproperty. It is often the case that these properties refer to the same file. The exclude file\nlists the nodes that are not permitted to connect to the cluster.\nThe rules for whether a tasktracker may connect to the jobtracker are simple: a task-\ntracker may connect only if it appears in the include file and does not appear in the\nexclude file. An unspecified or empty include file is taken to mean that all nodes are in\nthe include file.\nFor HDFS, the rules are slightly different. If a datanode appears in both the include and\nthe exclude file, then it may connect, but only to be decommissioned. Table 10-4 sum-\nmarizes the different combinations for datanodes. As for tasktrackers, an unspecified\nor empty include file means all nodes are included.\nTable 10-4. HDFS include and exclude file precedence\nNode appears in include file Node appears in exclude file Interpretation\nNo No Node may not connect.\nNo Yes Node may not connect.\nYes No Node may connect.\nYes Yes Node may connect and will be decommissioned.\nTo remove nodes from the cluster:\n1. Add the network addresses of the nodes to be decommissioned to the exclude file.\nDo not update the include file at this point.\n2. Restart the MapReduce cluster to stop the tasktrackers on the nodes being\ndecommissioned.\n3. Update the namenode with the new set of permitted datanodes, with this\ncommand:\n% hadoop dfsadmin -refreshNodes\n4. Go to the web UI and check whether the admin state has changed to “Decommis-\nsion In Progress” for the datanodes being decommissioned. They will start copying\ntheir blocks to other datanodes in the cluster.\nMaintenance | 2955. When all the datanodes report their state as “Decommissioned,” then all the blocks\nhave been replicated. Shut down the decommissioned nodes.\n6. Remove the nodes from the include file, and run:\n% hadoop dfsadmin -refreshNodes\n7. Remove the nodes from the slaves file.\nUpgrades\nUpgrading an HDFS and MapReduce cluster requires careful planning. The most im-\nportant consideration is the HDFS upgrade. If the layout version of the filesystem has\nchanged, then the upgrade will automatically migrate the filesystem data and metadata\nto a format that is compatible with the new version. As with any procedure that involves\ndata migration, there is a risk of data loss, so you should be sure that both your data\nand metadata is backed up (see “Routine Administration Procedures” on page 292).\nPart of the planning process should include a trial run on a small test cluster with a\ncopy of data that you can afford to lose. A trial run will allow you to familiarize yourself\nwith the process, customize it to your particular cluster configuration and toolset, and\niron out any snags before running the upgrade procedure on a production cluster. A\ntest cluster also has the benefit of being available to test client upgrades on.\nVersion Compatibility\nAll pre-1.0 Hadoop components have very rigid version compatibility requirements.\nOnly components from the same release are guaranteed to be compatible with each\nother, which means the whole system—from daemons to clients—has to be upgraded\nsimultaneously, in lockstep. This necessitates a period of cluster downtime.\nVersion 1.0 of Hadoop promises to loosen these requirements so that, for example,\nolder clients can talk to newer servers (within the same major release number). In later\nreleases, rolling upgrades may be supported, which would allow cluster daemons to be\nupgraded in phases, so that the cluster would still be available to clients during the\nupgrade.\nUpgrading a cluster when the filesystem layout has not changed is fairly\nstraightforward: install the new versions of HDFS and MapReduce on the cluster (and\non clients at the same time), shut down the old daemons, update configuration files,\nthen start up the new daemons and switch clients to use the new libraries. This process\nis reversible, so rolling back an upgrade is also straightforward.\nAfter every successful upgrade, you should perform a couple of final clean up steps:\n• Remove the old installation and configuration files from the cluster.\n• Fix any deprecation warnings in your code and configuration.\n296 | Chapter 10: Administering HadoopHDFS data and metadata upgrades\nIf you use the procedure just described to upgrade to a new version of HDFS and it\nexpects a different layout version, then the namenode will refuse to run. A message like\nthe following will appear in its log:\nFile system image contains an old layout version -16.\nAn upgrade to version -18 is required.\nPlease restart NameNode with -upgrade option.\nThe most reliable way of finding out whether you need to upgrade the filesystem is by\nperforming a trial on a test cluster.\nAn upgrade of HDFS makes a copy of the previous version’s metadata and data. Doing\nan upgrade does not double the storage requirements of the cluster, as the datanodes\nuse hard links to keep two references (for the current and previous version) to the same\nblock of data. This design makes it straightforward to roll back to the previous version\nof the filesystem, should you need to. You should understand that any changes made\nto the data on the upgraded system will be lost after the rollback completes.\nYou can keep only the previous version of the filesystem: you can’t roll back several\nversions. Therefore, to carry out another upgrade to HDFS data and metadata, you will\nneed to delete the previous version, a process called finalizing the upgrade. Once an\nupgrade is finalized, there is no procedure for rolling back to a previous version.\nIn general, you can skip releases when upgrading (for example, you can upgrade from\nrelease 0.18.3 to 0.20.0 without having to upgrade to a 0.19.x release first), but in some\ncases, you may have to go through intermediate releases. The release notes make it clear\nwhen this is required.\nYou should only attempt to upgrade a healthy filesystem. Before running the upgrade\ndo a full fsck (see “Filesystem check (fsck)” on page 281). As an extra precaution, you\ncan keep a copy of the fsck output that lists all the files and blocks in the system, so\nyou can compare it with the output of running fsck after the upgrade.\nIt’s also worth clearing out temporary files before doing the upgrade, both from the\nMapReduce system directory on HDFS and local temporary files.\nWith these preliminaries out of the way, here is the high-level procedure for upgrading\na cluster when the filesystem layout needs to be migrated:\n1. Make sure that any previous upgrade is finalized before proceeding with another\nupgrade.\n2. Shut down MapReduce and kill any orphaned task processes on the tasktrackers.\n3. Shut down HDFS and backup the namenode directories.\n4. Install new versions of Hadoop HDFS and MapReduce on the cluster and on\nclients.\n5. Start HDFS with the -upgrade option.\nMaintenance | 2976.\n7.\n8.\n9.\nWait until the upgrade is complete.\nPerform some sanity checks on HDFS.\nStart MapReduce.\nRoll back or finalize the upgrade (optional).\nWhile running the upgrade procedure, it is a good idea to remove the Hadoop scripts\nfrom your PATH environment variable. This forces you to be explicit about which version\nof the scripts you are running. It can be convenient to define two environment variables\nfor the new installation directories; in the following instructions, we have defined\nOLD_HADOOP_INSTALL and NEW_HADOOP_INSTALL.\nStart the upgrade. To perform the upgrade, run the following command (this is step 5 in\nthe high-level upgrade procedure):\n% $NEW_HADOOP_INSTALL/bin/start-dfs.sh -upgrade\nThis causes the namenode to upgrade its metadata, placing the previous version in a\nnew directory called previous:\n${dfs.name.dir}/current/VERSION\n/edits\n/fsimage\n/fstime\n/previous/VERSION\n/edits\n/fsimage\n/fstime\nSimilarly, datanodes upgrade their storage directories, preserving the old copy in a\ndirectory called previous.\nWait until the upgrade is complete. The upgrade process is not instantaneous, but you can\ncheck the progress of an upgrade using dfsadmin (upgrade events also appear in the\ndaemons’ logfiles; step 6:\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -upgradeProgress status\nUpgrade for version -18 has been completed.\nUpgrade is not finalized.\nCheck the upgrade. This shows that the upgrade is complete. At this stage, you should run\nsome sanity checks (step 7) on the filesystem (check files and blocks using fsck, basic\nfile operations). You might choose to put HDFS into safe mode while you are running\nsome of these checks (the ones that are read-only) to prevent others from making\nchanges.\nRoll back the upgrade (optional). If you find that the new version is not working correctly,\nyou may choose to roll back to the previous version (step 9). This is only possible if\nyou have not finalized the upgrade.\n298 | Chapter 10: Administering HadoopA rollback reverts the filesystem state to before the upgrade was per-\nformed, so any changes made in the meantime will be lost. In other\nwords, it rolls back to the previous state of the filesystem, rather than\ndowngrading the current state of the filesystem to a former version.\nFirst, shut down the new daemons:\n% $NEW_HADOOP_INSTALL/bin/stop-dfs.sh\nThen start up the old version of HDFS with the -rollback option.\n% $OLD_HADOOP_INSTALL/bin/start-dfs.sh -rollback\nThis command gets the namenode and datanodes to replace their current storage di-\nrectories with their previous copies. The filesystem will be returned to its previous state.\nFinalize the upgrade (optional). When you are happy with the new version of HDFS, you\ncan finalize the upgrade (step 9) to remove the previous storage directories.\nAfter an upgrade has been finalized, there is no way to roll back to the\nprevious version.\nThis step is required before performing another upgrade:\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -finalizeUpgrade\n% $NEW_HADOOP_INSTALL/bin/hadoop dfsadmin -upgradeProgress status\nThere are no upgrades in progress.\nHDFS is now fully upgraded to the new version.\nMaintenance | 299CHAPTER 11\nPig\nPig raises the level of abstraction for processing large datasets. With MapReduce, there\nis a map function and there is a reduce function, and working out how to fit your data\nprocessing into this pattern, which often requires multiple MapReduce stages, can be\na challenge. With Pig the data structures are much richer, typically being multivalued\nand nested; and the set of transformations you can apply to the data are much more\npowerful—they include joins, for example, which are not for the faint of heart in\nMapReduce.\nPig is made up of two pieces:\n• The language used to express data flows, called Pig Latin.\n• The execution environment to run Pig Latin programs. There are currently two\nenvironments: local execution in a single JVM and distributed execution on a Ha-\ndoop cluster.\nA Pig Latin program is made up of a series of operations, or transformations, that are\napplied to the input data to produce output. Taken as a whole, the operations describe\na data flow, which the Pig execution environment translates into an executable repre-\nsentation and then runs. Under the covers, Pig turns the transformations into a series\nof MapReduce jobs, but as a programmer you are mostly unaware of this, which allows\nyou to focus on the data rather than the nature of the execution.\nPig is a scripting language for exploring large datasets. One criticism of MapReduce is\nthat the development cycle is very long. Writing the mappers and reducers, compiling\nand packaging the code, submitting the job(s) and retrieving the results is a time-\nconsuming business, and even with Streaming, which removes the compile and package\nstep, the experience is still involved. Pig’s sweet spot is its ability to process terabytes\nof data simply by issuing a half-dozen lines of Pig Latin from the console. Pig is very\nsupportive of a programmer writing a query, since it provides several commands for\nintrospecting the data structures in your program, as it is written. Even more useful, it\ncan perform a sample run on a representative subset of your input data, so you can see\nwhether there are errors in the processing before unleashing it on the full dataset.\n301Pig was designed to be extensible. Virtually all parts of the processing path are cus-\ntomizable: loading, storing, filtering, grouping, ordering, and joining can all be altered\nby user-defined functions (UDFs). These functions operate on Pig’s nested data model,\nso they can integrate very deeply with Pig’s operators. As another benefit, UDFs tend\nto be more reusable than the libraries developed for writing MapReduce programs.\nPig isn’t suitable for all data processing tasks, however. Like MapReduce, it is designed\nfor batch processing of data. If you want to perform a query that touches only a small\namount of data in a large dataset, then Pig will not perform well, since it is set up to\nscan the whole dataset, or at least large portions of it.\nAlso, Pig doesn’t perform as well as programs written in MapReduce. This is not sur-\nprising, as Pig is a general system that compiles queries into MapReduce jobs, so in-\nevitably there will be some overhead in doing this. The good news, however, is that\nthis overhead will come down as the Pig team optimizes the planner—all without you\nhaving to change your Pig queries.\nInstalling and Running Pig\nPig runs as a client-side application. Even if you want to run Pig on a Hadoop cluster,\nthere is nothing extra to install on the cluster: Pig launches jobs and interacts with\nHDFS (or other Hadoop filesystems) from your workstation.\nInstallation is straightforward. Java 6 is a prerequisite (and on Windows, you will need\nCygwin). Download a stable release from http://hadoop.apache.org/pig/releases.html,\nand unpack the tarball in a suitable place on your workstation:\n% tar xzf pig-x.y.z.tar.gz\nIt’s convenient to add Pig’s binary directory to your command-line path. For example:\n% export PIG_INSTALL=/home/tom/pig-x.y.z\n% export PATH=$PATH:$PIG_INSTALL/bin\nYou also need to set the JAVA_HOME environment variable to point to a suitable Java\ninstallation.\nTry typing pig -help to get usage instructions.\nExecution Types\nPig has two execution types or modes: local mode and Hadoop mode.\nLocal mode\nIn local mode, Pig runs in a single JVM and accesses the local filesystem. This mode is\nsuitable only for small datasets, and when trying out Pig. Local mode does not use\nHadoop. In particular, it does not use Hadoop’s local job runner; instead, Pig translates\nqueries into a physical plan that it executes itself.\n302 | Chapter 11: PigThe execution type is set using the -x or -exectype option. To run in local mode, set\nthe option to local:\n% pig -x local\ngrunt>\nThis starts Grunt, the Pig interactive shell, which is discussed in more detail shortly.\nHadoop mode\nIn Hadoop mode, Pig translates queries into MapReduce jobs and runs them on a\nHadoop cluster. The cluster may be a pseudo- or fully distributed cluster. Hadoop mode\n(with a fully distributed cluster) is what you use when you want to run Pig on large\ndatasets.\nTo use Hadoop mode, you need to tell Pig which version of Hadoop you are using and\nwhere your cluster is running. Pig releases will work against only particular versions of\nHadoop. For example, Pig 0.2.0 will run against only a Hadoop 0.17.x or 0.18.x release.\nThe environment variable PIG_HADOOP_VERSION is used to tell Pig the version of Hadoop\nit is connecting to. For example, the following allows Pig to connect to any 0.18.x\nversion of Hadoop:\n% export PIG_HADOOP_VERSION=18\nNext, you need to point Pig at the cluster’s namenode and jobtracker. If you already\nhave a Hadoop site file (or files) that define fs.default.name and mapred.job.tracker,\nyou can simply add Hadoop’s configuration directory to Pig’s classpath:\n% export PIG_CLASSPATH=$HADOOP_INSTALL/conf/\nAlternatively, you can create a pig.properties file in Pig’s conf directory (this directory\nmay need creating, too), which sets these two properties. Here’s an example for a\npseudo-distributed setup:\nfs.default.name=hdfs://localhost/\nmapred.job.tracker=localhost:8021\nOnce you have configured Pig to connect to a Hadoop cluster, you can launch Pig,\nsetting the -x option to mapreduce, or omitting it entirely, as Hadoop mode is the default:\n% pig\n2009-03-29 21:22:20,489 [main] INFO org.apache.pig.backend.hadoop.executionengine.\nHExecutionEngine - Connecting to hadoop file system at: hdfs://localhost/\n2009-03-29 21:22:20,760 [main] INFO org.apache.pig.backend.hadoop.executionengine.\nHExecutionEngine - Connecting to map-reduce job tracker at: localhost:8021\ngrunt>\nAs you can see from the output, Pig reports the filesystem and jobtracker that it has\nconnected to.\nInstalling and Running Pig | 303Running Pig Programs\nThere are three ways of executing Pig programs, all of which work in both local and\nHadoop mode:\nScript\nPig can run a script file that contains Pig commands. For example,\npig script.pig runs the commands in the local file script.pig. Alternatively, for very\nshort scripts, you can use the -e option to run a script specified as a string on the\ncommand line.\nGrunt\nGrunt is an interactive shell for running Pig commands. Grunt is started when no\nfile is specified for Pig to run, and the -e option is not used. It is also possible to\nrun Pig scripts from within Grunt using run and exec.\nEmbedded\nYou can run Pig programs from Java, much like you can use JDBC to run SQL\nprograms from Java. There are more details on the Pig wiki at http://wiki.apache\n.org/pig/EmbeddedPig.\nGrunt\nGrunt has line-editing facilities like those found in GNU Readline (used in the bash\nshell and many other command-line applications). For instance, the Ctrl-E key com-\nbination will move the cursor to the end of the line. Grunt remembers command his-\ntory, too,* and you can recall lines in the history buffer using Ctrl-P or Ctrl-N (for\nprevious and next), or, equivalently, the up or down cursor keys.\nAnother handy feature is Grunt’s completion mechanism, which will try to complete\nPig Latin keywords and functions when you press the Tab key. For example, consider\nthe following incomplete line:\ngrunt> a = foreach b ge\nIf you press the Tab key at this point, ge will expand to generate, a Pig Latin keyword:\ngrunt> a = foreach b generate\nYou can customize the completion tokens by creating a file named autocomplete, and\nplacing it on Pig’s classpath (such as in the conf directory in Pig’s install directory), or\nin the directory you invoked Grunt from. The file should have one token per line, and\ntokens must not contain any whitespace. Matching is case-sensitive. It can be very\nhandy to add commonly used file paths (especially because Pig does not perform file-\nname completion), or the names of any user-defined functions you have created.\n* History is stored in a file called .pig_history in your home directory.\n304 | Chapter 11: PigYou can get a list of commands using the help command. When you’ve finished your\nGrunt session, you can exit with the quit command.\nPig Latin Editors\nPigPen is an Eclipse plug-in that provides an environment for developing Pig programs.\nIt includes a Pig script text editor, an example generator (equivalent to the ILLUS-\nTRATE command), and a button for running the script on a Hadoop cluster. There is\nalso an operator graph window, which shows a script in graph form, for visualizing the\ndata flow. For full installation and usage instructions, please refer to the Pig wiki at\nhttp://wiki.apache.org/pig/PigPen.\nThere are also Pig Latin syntax highlighters for other editors, including Vim and Text-\nMate. Details are available on the Pig wiki.\nAn Example\nLet’s look at a simple example by writing the program to calculate the maximum re-\ncorded temperature by year for the weather dataset in Pig Latin (just like we did using\nMapReduce in Chapter 2). The complete program is only a few lines long:\n-- max_temp.pig: Finds the maximum temperature by year\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nTo explore what’s going on, we’ll use Pig’s Grunt interpreter, which allows us to enter\nlines and interact with the program to understand what it’s doing. Start up Grunt in\nlocal mode, then enter the first line of the Pig script:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year:chararray, temperature:int, quality:int);\nFor simplicity, the program assumes that the input is tab-delimited text, with each line\nhaving just year, temperature, and quality fields. (Pig actually has more flexibility than\nthis with regard to the input formats it accepts, as you’ll see later.) This line describes\nthe input data we want to process. The year:chararray notation describes the field’s\nname and type; a chararray is like a Java string, and an int is like a Java int. The LOAD\noperator takes a URI argument; here we are just using a local file, but we could refer\nto an HDFS URI. The AS clause (which is optional) gives the fields names to make it\nconvenient to refer to them in subsequent statements.\nThe result of the LOAD operator, indeed any operator in Pig Latin, is a relation, which\nis just a set of tuples. A tuple is just like a row of data in a database table, with multiple\nAn Example | 305fields in a particular order. In this example, the LOAD function produces a set of (year,\ntemperature, quality) tuples that are present in the input file. We write a relation with\none tuple per line, where tuples are represented as comma-separated items in\nparentheses:\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\nRelations are given names, or aliases, so they can be referred to. This relation is given\nthe records alias. We can examine the contents of an alias using the DUMP operator:\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nWe can also see the structure of a relation—the relation’s schema—using the\nDESCRIBE operator on the relation’s alias:\ngrunt> DESCRIBE records;\nrecords: {year: chararray,temperature: int,quality: int}\nThis tells us that records has three fields, with aliases year, temperature, and quality,\nwhich are the names we gave them in the AS clause. The fields have the types given to\nthem in the AS clause, too. We shall examine types in Pig in more detail later.\nThe second statement removes records that have a missing temperature (indicated by\na value of 9999), or an unsatisfactory quality reading. For this small dataset, no records\nare filtered out:\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrunt> DUMP filtered_records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nThe third statement uses the GROUP function to group the records relation by the\nyear field. Let’s use DUMP to see what it produces:\ngrunt> grouped_records = GROUP filtered_records BY year;\ngrunt> DUMP grouped_records;\n(1949,{(1949,111,1),(1949,78,1)})\n(1950,{(1950,0,1),(1950,22,1),(1950,-11,1)})\nWe now have two rows, or tuples, one for each year in the input data. The first field in\neach tuple is the field being grouped by (the year), and the second field is a bag of tuples\nfor that year. A bag is just an unordered collection of tuples, which in Pig Latin is\nrepresented using curly braces.\n306 | Chapter 11: PigBy grouping the data in this way, we have created a row per year, so now all that remains\nis to find the maximum temperature for the tuples in each bag. Before we do this, let’s\nunderstand the structure of the grouped_records relation:\ngrunt> DESCRIBE grouped_records;\ngrouped_records: {group: chararray,filtered_records: {year: chararray,\ntemperature: int,quality: int}}\nThis tells us that the grouping field is given the alias group by Pig, and the second field\nis the same structure as the filtered_records relation that was being grouped. With\nthis information, we can try the fourth transformation:\ngrunt> max_temp = FOREACH grouped_records GENERATE group,\n>>\nMAX(filtered_records.temperature);\nFOREACH processes every row to generate a derived set of rows, using a GENERATE\nclause to define the fields in each derived row. In this example, the first field is group,\nwhich is just the year. The second field is a little more complex. The\nfiltered_records.temperature reference is to the temperature field of the fil\ntered_records bag in the grouped_records relation. MAX is a built-in function for calcu-\nlating the maximum value of fields in a bag. In this case, it calculates the maximum\ntemperature for the fields in each filtered_records bag. Let’s check the result:\ngrunt> DUMP max_temp;\n(1949,111)\n(1950,22)\nSo we’ve successfully calculated the maximum temperature for each year.\nGenerating Examples\nIn this example, we’ve used a small sample dataset with just a handful of rows to make\nit easier to follow the data flow and aid debugging. Creating a cut-down dataset is an\nart, as ideally it should be rich enough to cover all the cases to exercise your queries\n(the completeness property), yet be small enough to reason about by the programmer\n(the conciseness property). Using a random sample doesn’t work well in general, since\njoin and filter operations tend to remove all random data, leaving an empty result,\nwhich is not illustrative of the general flow.\nWith the ILLUSTRATE operator, Pig provides a tool for generating a reasonably com-\nplete and concise dataset. Although it can’t generate examples for all queries (it doesn’t\nsupport LIMIT, SPLIT, or nested FOREACH statements, for example), it can generate\nuseful examples for many queries. ILLUSTRATE works only if the relation has a\nschema.\nHere is the output from running ILLUSTRATE (slightly reformatted to fit the page):\nAn Example | 307grunt> ILLUSTRATE max_temp;\n-------------------------------------------------------------------------------\n| records\n| year: bytearray | temperature: bytearray | quality: bytearray |\n-------------------------------------------------------------------------------\n|\n| 1949\n| 9999\n| 1\n|\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------\n| records\n| year: chararray | temperature: int | quality: int |\n-------------------------------------------------------------------\n|\n| 1949\n| 9999\n| 1\n|\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n-------------------------------------------------------------------\n----------------------------------------------------------------------------\n| filtered_records\n| year: chararray | temperature: int | quality: int |\n----------------------------------------------------------------------------\n|\n| 1949\n| 111\n| 1\n|\n|\n| 1949\n| 78\n| 1\n|\n----------------------------------------------------------------------------\n------------------------------------------------------------------------------------\n| grouped_records\n| group: chararray | filtered_records: bag({year: chararray, |\ntemperature: int,quality: int}) |\n------------------------------------------------------------------------------------\n|\n| 1949\n| {(1949, 111, 1), (1949, 78, 1)}\n|\n------------------------------------------------------------------------------------\n-------------------------------------------\n| max_temp\n| group: chararray | int\n|\n-------------------------------------------\n|\n| 1949\n| 111\n|\n-------------------------------------------\nNotice that Pig used some of the original data (this is important to keep the generated\ndataset realistic), as well as creating some new data. It noticed the special value 9999\nin the query and created a tuple containing this value to exercise the FILTER statement.\nIn summary, the output of the ILLUSTRATE is easy to follow and can help you un-\nderstand what your query is doing.\nComparison with Databases\nHaving seen Pig in action, it might seem that Pig Latin is similar to SQL. The presence\nof such operators as GROUP BY and DESCRIBE reinforces this impression. However,\nthere are several differences between the two languages, and between Pig and RDBMSs\nin general.\nThe most significant difference is that Pig Latin is a data flow programming language,\nwhereas SQL is a declarative programming language. In other words, a Pig Latin pro-\ngram is a step-by-step set of operations on an input relation, in which each step is a\nsingle transformation. By contrast, SQL statements are a set of constraints that taken\ntogether define the output. In many ways, programming in Pig Latin is like working at\n308 | Chapter 11: Pigthe level of an RDBMS query planner, which figures out how to turn a declarative\nstatement into a system of steps.\nRDBMSs store data in tables, with tightly predefined schemas. Pig is more relaxed about\nthe data that it processes: you can define a schema at runtime, but it’s optional. Es-\nsentially, it will operate on any source of tuples (although the source should support\nbeing read in parallel, by being in multiple files, for example), where a UDF is used to\nread the tuples from their raw representation.† The most common representation is a\ntext file with tab-separated fields, and Pig provides a built-in load function for this\nformat. Unlike with a traditional database, there is no data import process to load the\ndata into the RDBMS. The data is loaded from the filesystem (usually HDFS) as the\nfirst step in the processing.\nPig’s support for complex, nested data structures differentiates it from SQL, which\noperates on flatter data structures. Also, Pig’s ability to use UDFs and streaming op-\nerators that are tightly integrated with the language and Pig’s nested data structures\nmakes Pig Latin more customizable than most SQL dialects.\nThere are several features to support online, low-latency queries that RDBMSs have\nthat are absent in Pig, such as transactions and indexes. As mentioned earlier, Pig does\nnot support random reads or queries in the order of tens of milliseconds. Nor does it\nsupport random writes, to update small portions of data; all writes are bulk, streaming\nwrites, just like MapReduce.\nHive is a subproject of Hadoop that sits between Pig and conventional RDBMSs. Like\nPig, Hive is designed to use HDFS for storage, but otherwise there are some significant\ndifferences. Its query language, Hive QL, is based on SQL, and anyone who is familiar\nwith SQL would have little trouble writing queries in Hive QL. Like RDBMSs, Hive\nmandates that all data be stored in tables, with a schema under its management; how-\never, it can associate a schema with preexisting data in HDFS, so the load step is op-\ntional. Hive does not support low-latency queries, a characteristic it shares with Pig.\nHive is discussed further in “Hadoop and Hive at Facebook” on page 414.\nPig Latin\nThis section gives an informal description of the syntax and semantics of the Pig Latin\nprogramming language.‡ It is not meant to offer a complete reference to the\n† Or as the Pig Philosophy has it, “Pigs eat anything.”\n‡ Not to be confused with Pig Latin the language game. English words are translated into Pig Latin by moving\nthe initial consonant sound to the end of the word and adding an “ay” sound. For example, “pig” becomes\n“ig-pay,” and “Hadoop” becomes “Adoop-hay.”\nPig Latin | 309language,§ but there should be enough here for you to get a good understanding of Pig\nLatin’s constructs.\nStructure\nA Pig Latin program consists of a collection of statements. A statement can be thought\nof as an operation, or a command.‖ For example, a GROUP operation is a type of\nstatement:\ngrouped_records = GROUP records BY year;\nThe command to list the files in a Hadoop filesystem is another example of a statement:\nls /\nStatements are usually terminated with a semicolon, as in the example of the GROUP\nstatement. In fact, this is an example of a statement that must be terminated with a\nsemicolon: it is a syntax error to omit it. The ls command, on the other hand, does not\nhave to be terminated with a semicolon. As a general guideline, statements or com-\nmands for interactive use in Grunt do not need the terminating semicolon. This group\nincludes the interactive Hadoop commands, as well as the diagnostic operators like\nDESCRIBE. It’s never an error to add a terminating semicolon, so if in doubt, it’s sim-\nplest to add one.\nStatements that have to be terminated with a semicolon can be split across multiple\nlines for readability:\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nPig Latin has two forms of comments. Double hyphens are single-line comments.\nEverything from the first hyphen to the end of the line is ignored by the Pig Latin\ninterpreter:\n-- My program\nDUMP A; -- What's in A?\nC-style comments are more flexible since they delimit the beginning and end of the\ncomment block with /* and */ markers. They can span lines or be embedded in a single\nline:\n/*\n* Description of my program spanning\n* multiple lines.\n*/\nA = LOAD 'input/pig/join/A';\nB = LOAD 'input/pig/join/B';\n§ Pig Latin does not have a formal language definition as such, but there is a comprehensive guide to the\nlanguage that can be found linked to from the Pig wiki at http://wiki.apache.org/pig/.\n‖ You sometimes these terms being used interchangeably in documentation on Pig Latin. For example,\n“GROUP command, ” “GROUP operation,” “GROUP statement.”\n310 | Chapter 11: PigC = JOIN A BY $0, /* ignored */ B BY $1;\nDUMP C;\nPig Latin has a list of keywords that have a special meaning in the language and cannot\nbe used as identifiers. These include the operators (LOAD, ILLUSTRATE), commands\n(cat, ls), expressions (matches, FLATTEN), and functions (DIFF, MAX)—all of which\nare covered in the following sections.\nPig Latin has mixed rules on case sensitivity. Operators and commands are not case-\nsensitive (to make interactive use more forgiving); however, aliases and function names\nare case-sensitive.\nStatements\nAs a Pig Latin program is executed, each statement is parsed in turn. If there are syntax\nerrors, or other (semantic) problems such as undefined aliases, the interpreter will halt\nand display an error message. The interpreter builds a logical plan for every relational\noperation, which forms the core of a Pig Latin program. The logical plan for the state-\nment is added to the logical plan for the program so far, then the interpreter moves on\nto the next statement.\nIt’s important to note that no data processing takes place while the logical plan of the\nprogram is being constructed. For example, consider again the Pig Latin program from\nthe first example:\n-- max_temp.pig: Finds the maximum temperature by year\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nWhen the Pig Latin interpreter sees the first line containing the LOAD statement, it\nconfirms that it is syntactically and semantically correct, and adds it to the logical plan,\nbut it does not load the data from the file (or even check whether the file exists). Indeed,\nwhere would it load it? Into memory? Even if it did fit into memory, what would it do\nwith the data? Perhaps not all the input data is needed (since later statements filter it,\nfor example), so it would be pointless to load it. The point is that it makes no sense to\nstart any processing until the whole flow is defined. Similarly, Pig validates the GROUP\nand FOREACH ... GENERATE statements, and adds them to the logical plan without\nexecuting them. The trigger for Pig to start processing is the DUMP statement (a STORE\nstatement also triggers processing). At that point, the logical plan is compiled into a\nphysical plan and executed.\nPig Latin | 311The type of physical plan that Pig prepares depends on the execution environment. For\nlocal execution, Pig will create a physical plan that runs in a single local JVM, whereas\nfor execution on Hadoop, Pig compiles the logical plan into a series of MapReduce jobs.\nYou can see the logical and physical plans created by Pig using the\nEXPLAIN command on a relation (EXPLAIN max_temp; for example).\nIn MapReduce mode, EXPLAIN will also show the MapReduce plan,\nwhich shows how the physical operators are grouped into MapReduce\njobs. This is a good way to find out how many MapReduce jobs Pig will\nrun for your query.\nThe relational operators that can be a part of a logical plan in Pig are summarized in\nTable 11-1. We shall go through each operator in more detail in “Data Processing\nOperators” on page 331.\nTable 11-1. Pig Latin relational operators\nCategory Operator Description\nLoading and storing LOAD Loads data from the filesystem or other storage into a relation\nSTORE Saves a relation to the filesystem or other storage\nDUMP Prints a relation to the console\nFILTER Removes unwanted rows from a relation\nDISTINCT Removes duplicate rows from a relation\nFOREACH ... GENERATE Adds or removes fields from a relation\nSTREAM Transforms a relation using an external program\nJOIN Joins two or more relations\nCOGROUP Groups the data in two or more relations\nGROUP Groups the data in a single relation\nCROSS Creates the cross product of two or more relations\nORDER Sorts a relation by one or more fields\nLIMIT Limits the size of a relation to a maximum number of tuples\nUNION Combines two or more relations into one\nSPLIT Splits a relation into two or more relations\nFiltering\nGrouping and joining\nSorting\nCombining and splitting\nThere are other types of statement that are not added to the logical plan. For example,\nthe diagnostic operators, DESCRIBE, EXPLAIN, and ILLUSTRATE are provided to\nallow the user to interact with the logical plan, for debugging purposes (see Ta-\nble 11-2). DUMP is a sort of diagnostic operator too, since it is used only to allow\ninteractive debugging of small result sets, or in combination with LIMIT to retrieve a\n312 | Chapter 11: Pigfew rows from a larger relation. The STORE statement should be used when the size\nof the output is more than a few lines, as it writes to a file, rather than to the console.\nTable 11-2. Pig Latin diagnostic operators\nOperator Description\nDESCRIBE Prints a relation’s schema\nEXPLAIN Prints the logical and physical plans\nILLUSTRATE Shows a sample execution of the logical plan, using a generated subset of the input\nPig Latin provides two statements, REGISTER and DEFINE, to make it possible to\nincorporate user-defined functions into Pig scripts (see Table 11-3).\nTable 11-3. Pig Latin UDF statements\nStatement Description\nREGISTER Registers a JAR file with the Pig runtime\nDEFINE Creates an alias for a UDF, streaming script, or a command specification\nSince they do not process relations, commands are not added to the logical plan; in-\nstead, they are executed immediately. Pig provides commands to interact with Hadoop\nfilesystems (which are very handy for moving data around before or after processing\nwith Pig) and MapReduce, as well as a few utility commands (described in Table 11-4).\nTable 11-4. Pig Latin commands\nCategory Command Description\nHadoop Filesystem cat Prints the contents of one or more files\ncd Changes the current directory\ncopyFromLocal Copies a local file or directory to a Hadoop filesystem\ncopyToLocal Copies a file or directory on a Hadoop filesystem to the local filesystem\ncp Copies a file or directory to another directory\nls Lists files\nmkdir Creates a new directory\nmv Moves a file or directory to another directory\npwd Prints the path of the current working directory\nrm Deletes a file or directory\nrmf Forcibly deletes a file or directory (does not fail if the file or directory does not exist)\nHadoop MapReduce kill Kills a MapReduce job\nUtility help Shows the available commands and options\nquit Exits the interpreter\nset Sets Pig options\nPig Latin | 313The filesystem commands can operate on files or directories in any Hadoop filesystem,\nand they are very similar to the hadoop fs commands (which is not surprising, as both\nare simple wrappers around the Hadoop FileSystem interface). Precisely which Hadoop\nfilesystem is used is determined by the fs.default.name property in the site file for\nHadoop Core. See “The Command-Line Interface” on page 45 for more details on how\nto configure this file.\nThese commands are mostly self-explanatory, except set, which is used to set options\nthat control Pig’s behavior. The debug option is used to turn debug logging on or off\nfrom within a script (you can also control the log level when launching Pig, using the\n-d or -debug option).\ngrunt> set debug on\nAnother useful option is the job.name option, which gives a Pig job a meaningful name,\nmaking it easier to pick out your Pig MapReduce jobs when running on a shared Ha-\ndoop cluster. If Pig is running a script (rather than being an interactive query from\nGrunt), its job name defaults to a value based on the script name.\nExpressions\nAn expression is something that is evaluated to yield a value. Expressions can be used\nin Pig as a part of a statement containing a relational operator. Pig has a rich variety of\nexpressions, many of which will be familiar from other programming languages. They\nare listed in Table 11-5, with brief descriptions and examples. We shall examples of\nmany of these expressions throughout the chapter.\nTable 11-5. Pig Latin expressions\nCategory Expressions Description Examples\nConstant Literal Constant value (see also literals in Table 11-6) 1.0, 'a'\nField (by posi- $n Field in position n (zero-based) $0\ntion) \nField (by name) f Field named f year\nProjection c.$n, c.f Field in container c (relation, bag, or tuple) records.$0, records.year\n                     by position, by name \nMap lookup m#k Value associated with key k in map m items#'Coat'\nCast (t) f Cast of field f to type t (int) year\nArithmetic x + y, x - y Addition, subtraction $1 + $2, $1 - $2\nx * y, x / y Multiplication, division $1 * $2, $1 / $2\nx % y Modulo, the remainder of x divided by y $1 % $2\n+x, -x Unary positive, negation +1, –1\nx ? y : z Bincond/ternary, y if x evaluates to true, z quality == 0 ? 0 : 1\n          otherwise \nConditional\n314 | Chapter 11: PigCategory Expressions Description Examples\nComparison x == y, x != y Equals, not equals quality == 0, tempera\n                                            ture != 9999\nx > y, x < y Greater than, less than quality > 0, quality < 10\nx >= y, x <= y Greater than or equal to, less than or equal to quality >= 1, quality <= 9\nx matches y Pattern matching with regular expression quality matches\n                                                    '[01459]'\nx is null Is null temperature is null\nx is not null Is not null temperature is not null\nx or y Logical or q == 0 or q == 1\nx and y Logical and q == 0 and r == 0\nBoolean\nnot x Logical negation not q matches '[01459]'\nFunctional fn(f1,f2,...) Invocation of function fn on fields f1, f2, isGood(quality)\n                         etc. \nFlatten FLATTEN(f) Removal of nesting from bags and tuples FLATTEN(group)\nTypes\nSo far you have seen some of the simple types in Pig, such as int and chararray. Here\nwe will discuss Pig’s built-in types in more detail.\nPig has four numeric types: int, long, float, and double, which are identical to their\nJava counterparts. There is also a bytearray type, like Java’s byte array type for repre-\nsenting a blob of binary data; and chararray, which, like java.lang.String, represents\ntextual data in UTF-16 format, although it can be loaded or stored in UTF-8 format.\nPig does not have types corresponding to Java’s boolean,# byte, short, or char primitive\ntypes. These are all easily represented using Pig’s int type, or chararray for char.\nThe numeric, textual, and binary types are simple atomic types. Pig Latin also has three\ncomplex types for representing nested structures: tuple, bag, and map. All of Pig Latin’s\ntypes are listed in Table 11-6.\n# Although there is no boolean type for data, Pig has the concept of an expression evaluating to true or false,\nfor testing conditions (such as in a FILTER statement). However, Pig does not allow a boolean expression to\nbe stored in a field.\nPig Latin | 315Table 11-6. Pig Latin types\nCategory Type Description Literal example\nNumeric int 32-bit signed integer 1\nlong 64-bit signed integer 1L\nfloat 32-bit floating-point number 1.0F\ndouble 64-bit floating-point number 1.0\nText chararray Character array in UTF-16 format 'a'\nBinary bytearray Byte array Not supported\nComplex tuple Sequence of fields of any type (1,'pomegranate')\nbag An unordered collection of tuples, possibly with duplicates {(1,'pomegranate'),(2)}\nmap A set of key-value pairs. Keys must be atoms, values may [1#'pomegranate']\n    be any type \nThe complex types are usually loaded from files or constructed using relational oper-\nators. Be aware, however, that the literal form in Table 11-6 is used when a constant\nvalue is created from within a Pig Latin program. The raw form in a file is usually\ndifferent when using the standard PigStorage loader. For example, the representation\nin a file of the bag in Table 11-6 would be {(1,pomegranate),(2)} (note the lack of\nquotes), and with a suitable schema, this would be loaded as a relation with a single\nfield and row, whose value was the bag.\nMaps are always loaded from files, since there is no relational operator in Pig that\nproduces a map. It’s possible to write a UDF to generate maps, if desired.\nAlthough relations and bags are conceptually the same (an unordered collection of\ntuples), in practice Pig treats them slightly differently. A relation is a top-level construct,\nwhereas a bag has to be contained in a relation. Normally you don’t have to worry\nabout this, but there are a few restrictions that can trip up the uninitiated. For example,\nit’s not possible to create a relation from a bag literal. So the following statement fails:\nA = {(1,2),(3,4)}; -- Error\nThe simplest workaround in this case is to load the data from a file using the LOAD\nstatement.\nAs another example, you can’t treat a relation like a bag and project a field into a new\nrelation ($0 refers to the first field of A, using the positional notation):\nB = A.$0;\nInstead, you have to use a relational operator to turn the relation A into relation B:\nB = FOREACH A GENERATE $0;\nIt’s possible that a future version of Pig Latin will remove these inconsistencies and\ntreat relations and bags in the same way.\n316 | Chapter 11: PigSchemas\nA relation in Pig may have an associated schema, which gives the fields in the relation\nnames and types. We’ve seen how an AS clause in a LOAD statement is used to attach\na schema to a relation:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year:int, temperature:int, quality:int);\ngrunt> DESCRIBE records;\nrecords: {year: int,temperature: int,quality: int}\nThis time we’ve declared the year to be an integer, rather than of type chararray, even\nthough the file it is being loaded from is the same. An integer may be more appropriate\nif we needed to manipulate the year arithmetically (to turn it into a timestamp, for\nexample), whereas the chararray representation might be more appropriate when it’s\nbeing used as a simple identifier. Pig’s flexibility in the degree to which schemas are\ndeclared contrasts with schemas in traditional SQL databases, which are declared be-\nfore the data is loaded into to the system. Pig is designed for analyzing plain input files\nwith no associated type information, so it is quite natural to choose types for fields later\nthan you would with an RDBMS.\nIt’s possible to omit type declarations completely, too:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year, temperature, quality);\ngrunt> DESCRIBE records;\nrecords: {year: bytearray,temperature: bytearray,quality: bytearray}\nIn this case, we have specified only the names of the fields in the schema, year, temper\nature, and quality. The types default to bytearray, the most general type, representing\na binary string.\nYou don’t need to specify types for every field; you can leave some to default to byte\narray, as we have done for year in this declaration:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt'\n>>\nAS (year, temperature:int, quality:int);\ngrunt> DESCRIBE records;\nrecords: {year: bytearray,temperature: int,quality: int}\nHowever, if you specify a schema in this way, you do need to specify every field. Also\nthere’s no way to specify the type of a field without specifying the name. On the other\nhand, the schema is entirely optional, and can be omitted by not specifying an AS clause:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample.txt';\ngrunt> DESCRIBE records;\nSchema for records unknown.\nFields in a relation with no schema can be referenced only using positional notation:\n$0 refers to the first field in a relation, $1 to the second, and so on. Their types default\nto bytearray:\nPig Latin | 317grunt> projected_records = FOREACH records GENERATE $0, $1, $2;\ngrunt> DUMP projected_records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> DESCRIBE projected_records;\nprojected_records: {bytearray,bytearray,bytearray}\nAlthough it can be convenient not to have to assign types to fields (particularly in the\nfirst stages of writing a query), doing so can improve the clarity and efficiency of Pig\nLatin programs, and is generally recommended.\nDeclaring a schema as a part of the query is flexible, but doesn’t lend\nitself to schema reuse. A set of Pig queries over the same input data will\noften have the same schema repeated in each query. If the query pro-\ncesses a large number of fields, this repetition can become hard to main-\ntain, since Pig (unlike Hive) doesn’t have a way to associate a schema\nwith data outside of a query. One way to solve this problem is to write\nyour own load function, which encapsulates the schema. This is descri-\nbed in more detail in “A Load UDF” on page 327.\nValidation and nulls\nA SQL database will enforce the constraints in a table’s schema at load time: for ex-\nample, trying to load a string into a column that is declared to be a numeric type will\nfail. In Pig, if the value cannot be cast to the type declared in the schema, then it will\nsubstitute a null value. Let’s see how this works if we have the following input for the\nweather data, which has an “e” character in place of an integer:\n1950\n1950\n1950\n1949\n1949\n0\n22\ne\n111\n78\n1\n1\n1\n1\n1\nPig handles the corrupt line by producing a null for the offending value, which is dis-\nplayed as the absence of a value when dumped to screen (and also when saved using\nSTORE):\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'\n>>\nAS (year:chararray, temperature:int, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,,1)\n(1949,111,1)\n(1949,78,1)\n318 | Chapter 11: PigPig produces a warning for the invalid field (not shown here), but does not halt its\nprocessing. For large datasets, it is very common to have corrupt, invalid, or merely\nunexpected data, and it is generally infeasible to incrementally fix every unparsable\nrecord. Instead, we can pull out all of the invalid records in one go, so we can take\naction on them, perhaps by fixing our program (because they indicate we have made a\nmistake), or by filtering them out (because the data is genuinely unusable).\ngrunt> corrupt_records = FILTER records BY temperature is null;\ngrunt> DUMP corrupt_records;\n(1950,,1)\nNote the use of the is null operator, which is analogous to SQL. In practice, we would\ninclude more information from the original record such as an identifier and the value\nthat could not be parsed, to help our analysis of the bad data.\nWe can find the number of corrupt records using the following idiom for counting the\nnumber of rows in a relation:\ngrunt> grouped = GROUP corrupt_records ALL;\ngrunt> all_grouped = FOREACH grouped GENERATE group, COUNT(corrupt_records);\ngrunt> DUMP all_grouped;\n(all,1L)\nAnother useful technique is to use the SPLIT operator to partition the data into “good”\nand “bad” relations, which can then be analyzed separately:\ngrunt> SPLIT records INTO good_records IF temperature is not null,\n>>\nbad_records IF temperature is null;\ngrunt> DUMP good_records;\n(1950,0,1)\n(1950,22,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> DUMP bad_records;\n(1950,,1)\nGoing back to the case in which temperature’s type was left undeclared, the corrupt\ndata cannot be easily detected, since it doesn’t surface as a null:\ngrunt> records = LOAD 'input/ncdc/micro-tab/sample_corrupt.txt'\n>>\nAS (year:chararray, temperature, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,e,1)\n(1949,111,1)\n(1949,78,1)\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrunt> grouped_records = GROUP filtered_records BY year;\ngrunt> max_temp = FOREACH grouped_records GENERATE group,\n>>\nMAX(filtered_records.temperature);\ngrunt> DUMP max_temp;\n(1949,111.0)\n(1950,22.0)\nPig Latin | 319What happens in this case is that the temperature field is interpreted as a bytearray, so\nthe corrupt field is not detected when the input is loaded. When passed to the MAX\nfunction the temperature field is cast to a double, since MAX works only with numeric\ntypes. The corrupt field can not be represented as a double, so it becomes a null, which\nMAX silently ignores. The best approach is generally to declare types for your data on\nloading, and look for missing or corrupt values in the relations themselves before you\ndo your main processing.\nSometimes corrupt data shows up as smaller tuples since fields are simply missing. You\ncan filter these out by using the SIZE function as follows:\ngrunt> A = LOAD 'input/pig/corrupt/missing_fields';\ngrunt> DUMP A;\n(2,Tie)\n(4,Coat)\n(3)\n(1,Scarf)\ngrunt> B = FILTER A BY SIZE(*) > 1;\ngrunt> DUMP B;\n(2,Tie)\n(4,Coat)\n(1,Scarf)\nSchema merging\nIn Pig, you don’t declare the schema for every new relation in the data flow. In most\ncases, Pig can figure out the resulting schema for the output of a relational operation\nby considering the schema of the input relation.\nHow are schemas propagated to new relations? Some relational operators don’t change\nthe schema, so the relation produced by the LIMIT operator (which restricts a relation\nto a maximum number of tuples), for example, has the same schema as the relation it\noperates on. For other operators, the situation is more complicated. UNION, for ex-\nample, combines two or more relations into one, and tries to merge the input relations\nschemas. If the schemas are incompatible, due to different types or number of fields,\nthen the schema of the result of the UNION is unknown.\nYou can find out the schema for any relation in the data flow using the DESCRIBE\noperator. If you want to redefine the schema for a relation, you can use the FORE-\nACH ... GENERATE operator with AS clauses to define the schema for some or all of\nthe fields of the input relation.\nSee “User-Defined Functions” on page 322 for further discussion of schemas.\nFunctions\nFunctions in Pig come in five types:\n320 | Chapter 11: PigEval function\nA function that takes one or more expressions and returns another expression. An\nexample of a built-in eval function is MAX, which returns the maximum value of the\nentries in a bag. Some eval functions are aggregate functions, which means they\noperate on a bag of data to produce a scalar value; MAX is an example of an aggregate\nfunction. Furthermore, many aggregate functions are algebraic, which means that\nthe result of the function may be calculated incrementally. In MapReduce terms,\nalgebraic functions make use of the combiner, and are much more efficient to cal-\nculate (see “Combiner Functions” on page 29). MAX is an algebraic function,\nwhereas a function to calculate the median of a collection of values is an example\nof a function that is not algebraic.\nFilter function\nA special type of eval function that returns a logical boolean result. As the name\nsuggests, filter functions are used in the FILTER operator to remove unwanted\nrows. They can also be used in other relational operators that take boolean con-\nditions, and in general expressions using boolean or conditional expressions. An\nexample of a built-in filter function is IsEmpty, which tests whether a bag or a map\ncontains any items.\nComparison function\nA function that can impose an ordering on a pair of tuples. You can specify the\ncomparison function to use in an ORDER clause to control the sort order.\nLoad function\nA function that specifies how to load data into a relation from external storage.\nStore function\nA function that specifies how to save the contents of a relation to external storage.\nOften load and store functions are implemented by the same type. For example,\nPigStorage, which loads data from delimited text files, can store data in the same\nformat.\nPig has a small collection of built-in functions, which are listed in Table 11-7.\nTable 11-7. Pig built-in functions\nCategory Function Description\nEval AVG Calculates the average (mean) value of entries in a bag.\nCONCAT Concatenates two byte arrays or two character arrays together.\nCOUNT Calculates the number of entries in a bag.\nDIFF Calculates the set difference of two bags. If the two arguments are not bags, then returns\n    a bag containing both if they are equal; otherwise, returns an empty bag.\nMAX Calculates the maximum value of entries in a bag.\nMIN Calculates the minimum value of entries in a bag.\nPig Latin | 321Category\nFunction Description\nSIZE Calculates the size of a type. The size of numeric types is always one, for character arrays\n    it is the number of characters, for byte arrays the number of bytes, and for containers\n   (tuple, bag, map) it is the number of entries.\nSUM Calculates the sum of the values of entries in a bag.\nTOKENIZE Tokenizes a character array into a bag of its constituent words.\nFilter IsEmpty Tests if a bag or map is empty.\nLoad/Store PigStorage Loads or stores relations using a field-delimited text format. Each line is broken into fields\n                     using a configurable field delimiter (defaults to a tab character) to be stored in the tuple’s\n                      fields. It is the default storage when none is specified.\nBinStorage Loads or stores relations from or to binary files. An internal Pig format is used that uses\n          Hadoop Writable objects.\nBinaryStorage Loads or stores relations containing only single-field tuples with a value of type byte\n             array from or to binary files. The bytes of the bytearray values are stored verbatim.\n            Used with Pig streaming.\nTextLoader Loads relations from a plain-text format. Each line corresponds to a tuple whose single\n          field is the line of text.\nPigDump Stores relations by writing the toString() representation of tuples, one per line. Useful\n       for debugging.\nIf the function you need is not available, you can write your own. Before you do that,\nhowever, have a look in the Piggy Bank, a repository of Pig functions shared by the Pig\ncommunity. There are details on the Pig wiki at http://wiki.apache.org/pig/PiggyBank\non how to browse and obtain the Piggy Bank functions. If the Piggy Bank doesn’t have\nwhat you need, you can write your own function (and if it is sufficiently general, you\nmight consider contributing it to the Piggy Bank so that others can benefit from it, too).\nThese are known as user-defined functions, or UDFs.\nUser-Defined Functions\nPig’s designers realized that the ability to plug-in custom code is crucial for all but the\nmost trivial data processing jobs. For this reason, they made it easy to define and use\nuser-defined functions.\nA Filter UDF\nLet’s demonstrate by writing a filter function for filtering out weather records that do\nnot have a temperature quality reading of satisfactory (or better). The idea is to change\nthis line:\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\n322 | Chapter 11: Pigto:\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\nThis achieves two things: it makes the Pig script more concise, and it encapsulates the\nlogic in one place so that it can be easily reused in other scripts. If we were just writing\nan ad hoc query, then we probably wouldn’t bother to write a UDF. It’s when you start\ndoing the same kind of processing over and over again that you see opportunities for\nreusable UDFs.\nUDFs are written in Java, and filter functions are all subclasses of FilterFunc, which\nitself is a subclass of EvalFunc. We’ll look at EvalFunc in more detail later, but for the\nmoment just note that, in essence, EvalFunc looks like the following class:\npublic abstract class EvalFunc<T> {\npublic abstract T exec(Tuple input) throws IOException;\n}\nEvalFunc’s only abstract method, exec(), takes a tuple and returns a single value, the\n(parameterized) type T. The fields in the input tuple consist of the expressions passed\nto the function; in this case, a single integer. For FilterFunc, T is Boolean, so the method\nshould return true only for those tuples that should not be filtered out.\nFor the quality filter, we write a class, IsGoodQuality, that extends FilterFunc and im-\nplements the exec() method. See Example 11-1. The Tuple class is essentially a list of\nobjects with associated types. Here we are concerned only with the first field (since the\nfunction only has a single argument), which we extract by index using the get() method\non Tuple. The field is an integer, so if it’s not null, we cast it and check whether the\nvalue is one that signifies the temperature was a good reading, returning the appropriate\nvalue, true or false.\nExample 11-1. A FilterFunc UDF to remove records with unsatisfactory temperature quality readings\npackage com.hadoopbook.pig;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.pig.FilterFunc;\nimport\nimport\nimport\nimport\norg.apache.pig.backend.executionengine.ExecException;\norg.apache.pig.data.DataType;\norg.apache.pig.data.Tuple;\norg.apache.pig.impl.logicalLayer.FrontendException;\npublic class IsGoodQuality extends FilterFunc {\n@Override\npublic Boolean exec(Tuple tuple) throws IOException {\nif (tuple == null || tuple.size() == 0) {\nreturn false;\n}\nUser-Defined Functions | 323}\ntry {\nObject object = tuple.get(0);\nif (object == null) {\nreturn false;\n}\nint i = (Integer) object;\nreturn i == 0 || i == 1 || i == 4 || i == 5 || i == 9;\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\nTo use the new function, we first compile it, and package it in a JAR file (in the example\ncode that accompanies this book, we can do this by typing ant pig). Then we tell Pig\nabout the JAR file with the REGISTER operator, which is given the local path to the\nfilename (and is not enclosed in quotes):\ngrunt> REGISTER pig.jar;\nFinally, we can invoke the function:\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND\n>>\ncom.hadoopbook.pig.IsGoodQuality(quality);\nPig resolves function calls by treating the function’s name as a Java classname and\nattempting to load a class of that name. (This, incidentally, is why function names are\ncase-sensitive: because Java classnames are.) When searching for classes, Pig uses a\nclassloader that includes the JAR files that have been registered. When running in dis-\ntributed mode, Pig will ensure that your JAR files get shipped to the cluster.\nFor the UDF in this example, Pig looks for a class with the name com.hadoop\nbook.pig.IsGoodQuality, which it finds in the JAR file we registered.\nResolution of built-in functions proceeds in the same way, except for one difference:\nPig has a set of built-in package names that it searches, so the function call does not\nhave to be a fully qualified name. For example, the function MAX is actually implemented\nby a class MAX in the package org.apache.pig.builtin. This is one of the packages that\nPig looks in, so we can write MAX rather than org.apache.pig.builtin.MAX in our Pig\nprograms.\nWe can’t register our package with Pig, but we can shorten the function name by de-\nfining an alias, using the DEFINE operator:\ngrunt> DEFINE isGood com.hadoopbook.pig.IsGoodQuality();\ngrunt> filtered_records = FILTER records BY temperature != 9999 AND isGood(quality);\nDefining an alias is a good idea if you want to use the function several times in the same\nscript. It’s also necessary if you want to pass arguments to the constructor of the UDF’s\nimplementation class.\n324 | Chapter 11: PigLeveraging types\nThe filter works when the quality field is declared to be of type int, but if the type\ninformation is absent, then the UDF fails! This happens because the field is the default\ntype, bytearray, represented by the DataByteArray class. Because DataByteArray is not\nan Integer, the cast fails.\nThe obvious way to fix this is to convert the field to an integer in the exec() method.\nHowever, there is a better way, which is to tell Pig the types of the fields that the function\nexpects. The getArgToFuncMapping() method on EvalFunc is provided for precisely this\nreason. We can override it to tell Pig that the first field should be an integer:\n@Override\npublic List<FuncSpec> getArgToFuncMapping() throws FrontendException {\nList<FuncSpec> funcSpecs = new ArrayList<FuncSpec>();\nfuncSpecs.add(new FuncSpec(this.getClass().getName(),\nnew Schema(new Schema.FieldSchema(null, DataType.INTEGER))));\n}\nreturn funcSpecs;\nThis method returns a FuncSpec object corresponding to each of the fields of the tuple\nthat are passed to the exec() method. Here there is a single field, and we construct an\nanonymous FieldSchema (the name is passed as null, since Pig ignores the name when\ndoing type conversion). The type is specified using the INTEGER constant on Pig’s Data\nType class.\nWith the amended function, Pig will attempt to convert the argument passed to the\nfunction to an integer. If the field cannot be converted, then a null is passed for the\nfield. The exec() method always returns false if the field is null. For this application\nthis behavior is appropriate, as we want to filter out records whose quality field is\nunintelligible.\nHere’s the final program using the new function:\n-- max_temp_filter_udf.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nAn Eval UDF\nWriting an eval function is a small step up from writing a filter function. Consider a\nUDF (see Example 11-2) for trimming leading and trailing whitespace from\nUser-Defined Functions | 325chararray values, just like the trim() method on java.lang.String. We will use this\nUDF later in the chapter.\nExample 11-2. An EvalFunc UDF to trim leading and trailing whitespace from chararray values\npublic class Trim extends EvalFunc<String> {\n@Override\npublic String exec(Tuple input) throws IOException {\nif (input == null || input.size() == 0) {\nreturn null;\n}\ntry {\nObject object = input.get(0);\nif (object == null) {\nreturn null;\n}\nreturn ((String) object).trim();\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\n@Override\npublic List<FuncSpec> getArgToFuncMapping() throws FrontendException {\nList<FuncSpec> funcList = new ArrayList<FuncSpec>();\nfuncList.add(new FuncSpec(this.getClass().getName(), new Schema(\nnew Schema.FieldSchema(null, DataType.CHARARRAY))));\n}\n}\nreturn funcList;\nThe exec() and getArgToFuncMapping() methods are straightforward; like the ones in\nthe IsGoodQuality UDF.\nWhen you write an eval function, you need to consider what the output’s schema looks\nlike. In the following statement, the schema of B is determined by the function udf:\nB = FOREACH A GENERATE udf($0);\nIf udf creates tuples with scalar fields, then Pig can determine B’s schema through\nreflection. For complex types such as bags, tuples, or maps, Pig needs more help, and\nyou should implement the outputSchema() method to give Pig the information about\nthe output schema.\nThe Trim UDF returns a string, which Pig translates as a chararray, as can be seen from\nthe following session:\ngrunt> DUMP A;\n( pomegranate)\n(banana )\n(apple)\n( lychee )\ngrunt> DESCRIBE A;\n326 | Chapter 11: PigA: {fruit: chararray}\ngrunt> B = FOREACH A GENERATE com.hadoopbook.pig.Trim(fruit);\ngrunt> DUMP B;\n(pomegranate)\n(banana)\n(apple)\n(lychee)\ngrunt> DESCRIBE B;\nB: {chararray}\nA has chararray fields that have leading and trailing spaces. We create B from A by\napplying the Trim function to the first field in A (named fruit). B’s fields are correctly\ninferred to be of type chararray.\nA Load UDF\nWe’ll demonstrate a custom load function that can read plain-text column ranges as\nfields, very much like the Unix cut command. It is used as follows:\ngrunt> records = LOAD 'input/ncdc/micro/sample.txt'\n>>\nUSING com.hadoopbook.pig.CutLoadFunc('16-19,88-92,93-93')\n>>\nAS (year:int, temperature:int, quality:int);\ngrunt> DUMP records;\n(1950,0,1)\n(1950,22,1)\n(1950,-11,1)\n(1949,111,1)\n(1949,78,1)\nThe string passed to CutLoadFunc is the column specification; each comma-separated\nrange defines a field, which is assigned a name and type in the AS clause. Let’s examine\nthe implementation of CutLoadFunc, shown in Example 11-3.\nExample 11-3. A LoadFunc UDF to load tuple fields as column ranges\npublic class CutLoadFunc extends Utf8StorageConverter implements LoadFunc {\nprivate static final Log LOG = LogFactory.getLog(CutLoadFunc.class);\nprivate static final Charset UTF8 = Charset.forName(""UTF-8"");\nprivate static final byte RECORD_DELIMITER = (byte) '\\n';\nprivate\nprivate\nprivate\nprivate\nTupleFactory tupleFactory = TupleFactory.getInstance();\nBufferedPositionedInputStream in;\nlong end = Long.MAX_VALUE;\nList<Range> ranges;\npublic CutLoadFunc(String cutPattern) {\nranges = Range.parse(cutPattern);\n}\n@Override\npublic void bindTo(String fileName, BufferedPositionedInputStream in,\nlong offset, long end) throws IOException {\nUser-Defined Functions | 327this.in = in;\nthis.end = end;\n}\n// Throw away the first (partial) record - it will be picked up by another\n// instance\nif (offset != 0) {\ngetNext();\n}\n@Override\npublic Tuple getNext() throws IOException {\nif (in == null || in.getPosition() > end) {\nreturn null;\n}\n}\nString line;\nwhile ((line = in.readLine(UTF8, RECORD_DELIMITER)) != null) {\nTuple tuple = tupleFactory.newTuple(ranges.size());\nfor (int i = 0; i < ranges.size(); i++) {\ntry {\nRange range = ranges.get(i);\nif (range.getEnd() > line.length()) {\nLOG.warn(String.format(\n""Range end (%s) is longer than line length (%s)"",\nrange.getEnd(), line.length()));\ncontinue;\n}\ntuple.set(i, new DataByteArray(range.getSubstring(line)));\n} catch (ExecException e) {\nthrow new IOException(e);\n}\n}\nreturn tuple;\n}\nreturn null;\n@Override\npublic void fieldsToRead(Schema schema) {\n// Can't use this information to optimize, so ignore it\n}\n}\n@Override\npublic Schema determineSchema(String fileName, ExecType execType,\nDataStorage storage) throws IOException {\n// Cannot determine schema in general\nreturn null;\n}\nWhen Pig is using the Hadoop execution engine, data loading takes place in the mapper,\nso it is important that the input can be split into portions that are independently handled\nby each mapper (see “Input Splits and Records” on page 185 for background).\n328 | Chapter 11: PigIn Pig, load functions are asked to load a portion of the input, specified as a byte range.\nThe start and end offsets are typically not on record boundaries (since they correspond\nto HDFS blocks), so on initialization, in the bindTo() method, the load function needs\nto find the start of the next record boundary. The Pig runtime then calls getNext()\nrepeatedly, and the load function reads tuples from the stream until it gets past the byte\nrange it was asked to load. At this point, it returns null to signal that there are no more\ntuples to be read.\nCutLoadFunc is constructed with a string that specifies the column ranges to use for each\nfield. The logic for parsing this string and creating a list of internal Range objects that\nencapsulates these ranges is contained in the Range class, and is not shown here (it is\navailable in the example code that accompanies this book).\nIn CutLoadFunc’s bindTo() method, we find the next record boundary. If we are at the\nbeginning of the stream, we know the stream is positioned at the first record boundary.\nOtherwise, we call getNext() to discard the first partial line, since it will be handled in\nfull by another instance of CutLoadFunc—the one whose byte range immediately pro-\nceeds the current one. In some cases, the start offset may fall on a line boundary, but\nit is not possible to detect this case, so we always have to discard the first line. This is\nnot a problem, since we always read lines until the position of the input stream is strictly\npast the end offset. In others words, if the end offset is on a line boundary, then we will\nread one further line. In this way we can be sure that no lines are omitted by the load\nfunction. (The situation is analogous to the one described and illustrated in “TextIn-\nputFormat” on page 196.)\nThe role of the getNext() implementation is to turn lines of the input file into Tuple\nobjects. It does this by means of a TupleFactory, a Pig class for creating Tuple instances.\nThe newTuple() method creates a new tuple with the required number of fields, which\nis just the number of Range classes, and the fields are populated using substrings of the\nline, which are determined by the Range objects.\nWe need to think about what to do if the line is shorter than the range asked for. One\noption is to throw an exception, and stop further processing. This is appropriate if your\napplication cannot tolerate incomplete or corrupt records. In many cases, it is better\nto return a tuple with null fields and let the Pig script handle the incomplete data as it\nsees fit. This is the approach we take here, by exiting the for loop if the range end is\npast the end of the line, we leave the current field, and any subsequent fields in the\ntuple, with their default value of null.\nUsing a schema\nLet’s now consider the type of the fields being loaded. If the user has specified a schema,\nthen the fields need converting to the relevant types. However, this is performed lazily\nby Pig, and so the loader should always construct tuples of type bytearrary, using the\nDataByteArray type. The loader function still has the opportunity to do the conversion,\nhowever, since it must implement a collection of conversion methods for this purpose:\nUser-Defined Functions | 329public interface LoadFunc {\n// Cast methods\npublic Integer bytesToInteger(byte[] b) throws IOException;\npublic Long bytesToLong(byte[] b) throws IOException;\npublic Float bytesToFloat(byte[] b) throws IOException;\npublic Double bytesToDouble(byte[] b) throws IOException;\npublic String bytesToCharArray(byte[] b) throws IOException;\npublic Map<Object, Object> bytesToMap(byte[] b) throws IOException;\npublic Tuple bytesToTuple(byte[] b) throws IOException;\npublic DataBag bytesToBag(byte[] b) throws IOException;\n}\n// Other methods omitted\nCutLoadFunc doesn’t implement these methods itself, since it extends Pig’s Utf8Stora\ngeConverter, which provides standard conversions between UTF-8 encoded data and\nPig data types.\nIn some cases, the load function itself can determine the schema. For example, if we\nwere loading self-describing data like XML or JSON, we could create a schema for Pig\nby looking at the data. Alternatively, the load function may determine the schema in\nanother way, such as an external file, or being passed information in its constructor.\nTo support such cases, the load function can provide an implementation of determine\nSchema() that returns a schema. Note, however, that if a user supplies a schema in the\nAS clause of LOAD, then it takes precedence over the schema returned by the load\nfunction’s determineSchema() method.\nFor CutLoadFunc, we return null in determineSchema(), so there is a schema only if the\nuser supplies one.\nFinally, LoadFunc has a fieldsToRead() method that lets the loader function find out\nwhich columns the query is asking for. This is can be a useful optimization for column-\noriented storage, so the loader only loads the columns that are needed by the query.\nThere is no obvious way for CutLoadFunc to load only a subset of columns, since it reads\nthe whole line for each tuple, so we ignore this information.\nAdvanced loading with Slicer\nFor more control over the data loading process, your load function can implement the\nSlicer interface, in addition to LoadFunc. A Slicer implementation is free to use the\nlocation information passed to LOAD in any way it chooses: it doesn’t have to be a\nHadoop path, but could be interpreted as a database reference, or in some other way.\nFurthermore, it is up to the Slicer to split the input dataset into Slice objects, where\neach Slice is processed by a single MapReduce mapper. A custom Slicer can therefore\nsplit the input in a different way to Pig’s standard slicing behavior, which is to break\nfiles into HDFS block-sized slices. For example, a Slicer for processing large image or\nvideo files might create one file per slice.\n330 | Chapter 11: PigYou can learn more about writing a Slicer on the Pig wiki at http://wiki.apache.org/pig/\nUDFManual.\nData Processing Operators\nLoading and Storing Data\nThroughout this chapter we have seen how to load data from external storage for pro-\ncessing in Pig. Storing results is straightforward, too. Here’s an example of using Pig-\nStorage to store tuples as plain-text values separated by a colon character:\ngrunt> STORE A INTO 'out' USING PigStorage(':');\ngrunt> cat out\nJoe:cherry:2\nAli:apple:3\nJoe:banana:2\nEve:apple:7\nOther built-in storage functions are described in Table 11-7.\nFiltering Data\nOnce you have some data loaded into a relation, the next step is often to filter it to\nremove the data that you are not interested in. By filtering early in the processing pipe-\nline, you minimize the amount of data flowing through the system, which can improve\nefficiency.\nFOREACH .. GENERATE\nWe have already seen how to remove rows from a relation using the FILTER operator\nwith simple expressions and a UDF. The FOREACH .. GENERATE operator is used\nto act on every row in a relation. It can be used to remove fields, or to generate new\nones. In this example, we do both:\ngrunt> DUMP A;\n(Joe,cherry,2)\n(Ali,apple,3)\n(Joe,banana,2)\n(Eve,apple,7)\ngrunt> B = FOREACH A GENERATE $0, $2+1, 'Constant';\ngrunt> DUMP B;\n(Joe,3,Constant)\n(Ali,4,Constant)\n(Joe,3,Constant)\n(Eve,8,Constant)\nHere we have created a new relation B with three fields. Its first field is a projection of\nthe first field ($0) of A. B’s second field is the third field of A ($1) with one added to it.\nData Processing Operators | 331B’s third field is a constant field (every row in B has the same third field) with the\nchararray value Constant.\nThe FOREACH .. GENERATE operator has a nested form for to support more complex\nprocessing. In the following example, we compute various statistics for the weather\ndataset:\n-- year_stats.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nrecords = LOAD 'input/ncdc/all/19{1,2,3,4,5}0*'\nUSING com.hadoopbook.pig.CutLoadFunc('5-10,11-15,16-19,88-92,93-93')\nAS (usaf:chararray, wban:chararray, year:int, temperature:int, quality:int);\ngrouped_records = GROUP records BY year PARALLEL 30;\nyear_stats = FOREACH grouped_records {\nuniq_stations = DISTINCT records.usaf;\ngood_records = FILTER records BY isGood(quality);\nGENERATE FLATTEN(group), COUNT(uniq_stations) AS station_count,\nCOUNT(good_records) AS good_record_count, COUNT(records) AS record_count;\n}\nDUMP year_stats;\nUsing the cut UDF we developed earlier, we load various fields from the input dataset\ninto the records relation. Next we group records by year. Notice the PARALLEL key-\nword for setting the number of reducers to use; this is vital when running on a cluster.\nThen we process each group using a nested FOREACH .. GENERATE operator. The\nfirst nested statement creates a relation for the distinct USAF identifiers for stations\nusing the DISTINCT operator. The second nested statement creates a relation for the\nrecords with “good” readings using the FILTER operator, and a UDF. The final nested\nstatement is a GENERATE statement (a nested FOREACH .. GENERATE must always\nhave a GENERATE statement as the last nested statement) that generates the summary\nfields of interest using the grouped records, as well as the relations created in the nested\nblock.\nRunning it on a few years of data, we get the following:\n(1920,8L,8595L,8595L)\n(1950,1988L,8635452L,8641353L)\n(1930,121L,89245L,89262L)\n(1910,7L,7650L,7650L)\n(1940,732L,1052333L,1052976L)\nThe fields are year, number of unique stations, total number of good readings, and total\nnumber of readings. We can see how the number of weather stations and readings grew\nover time.\n332 | Chapter 11: PigSTREAM\nThe STREAM operator allows you to transform data in a relation using an external\nprogram or script. It is named by analogy with Hadoop Streaming, which provides a\nsimilar capability for MapReduce (see “Hadoop Streaming” on page 32).\nSTREAM can use built-in commands with arguments. Here is an example that uses the\nUnix cut command to extract the second field of each tuple in A. Note that the com-\nmand and its arguments are enclosed in backticks:\ngrunt> C = STREAM A THROUGH `cut -f 2`;\ngrunt> DUMP C;\n(cherry)\n(apple)\n(banana)\n(apple)\nThe STREAM operator uses PigStorage to serialize and deserialize relations to and from\nthe program’s standard input and output streams. Tuples in A are converted to tab-\ndelimited lines that are passed to the script. The output of the script is read one line at\na time and split on tabs to create new tuples for the output relation C. You can provide\na custom serializer and deserializer using the DEFINE command.\nPig streaming is most powerful when you write custom processing scripts. The follow-\ning Python script filters out bad weather records:\n#!/usr/bin/env python\nimport re\nimport string\nimport sys\nfor line in sys.stdin:\n(year, temp, q) = string.strip().split(line)\nif (temp != ""9999"" and re.match(""[01459]"", q)):\nprint ""%s\\t%s"" % (year, temp)\nTo use the script, you need to ship it to the cluster. This is achieved via a DEFINE\nclause, which also creates an alias for the STREAM command. The STREAM statement\ncan then refer to the alias, as the following Pig script shows:\n-- max_temp_filter_stream.pig\nDEFINE is_good_quality `is_good_quality.py`\nSHIP ('src/main/ch11/python/is_good_quality.py');\nrecords = LOAD 'input/ncdc/micro-tab/sample.txt'\nAS (year:chararray, temperature:int, quality:int);\nfiltered_records = STREAM records THROUGH is_good_quality\nAS (year:chararray, temperature:int);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nDUMP max_temp;\nData Processing Operators | 333Grouping and Joining Data\nJoining datasets in MapReduce takes some work on the part of the programmer (see\n“Joins” on page 233), whereas Pig has very good built-in support for join operations,\nmaking it much more approachable. Since the large datasets that are suitable for anal-\nysis by Pig (and MapReduce in general), are not normalized, joins are used more in-\nfrequently in Pig than they are in SQL.\nJOIN\nLet’s look at an example of an inner join. Consider the relations A and B:\ngrunt> DUMP A;\n(2,Tie)\n(4,Coat)\n(3,Hat)\n(1,Scarf)\ngrunt> DUMP B;\n(Joe,2)\n(Hank,4)\n(Ali,0)\n(Eve,3)\n(Hank,2)\nWe can join the two relations on the numerical (identity) field in each:\ngrunt> C = JOIN A BY $0, B BY $1;\ngrunt> DUMP C;\n(2,Tie,Joe,2)\n(2,Tie,Hank,2)\n(3,Hat,Eve,3)\n(4,Coat,Hank,4)\nThis is a classic inner join, where each match between the two relations corresponds\nto a row in the result. (It’s actually an equijoin since the join predicate is equality.) The\nresult’s fields are made up of all the fields of all the input relations.\nYou should use the general join operator if all the relations being joined are too large\nto fit in memory. If one of the relations is small enough to fit in memory, there is a\nspecial type of join called a fragment replicate join, which is implemented by distributing\nthe small input to all the mappers, and performing a map-side join using an in-memory\nlookup table against the (fragmented) larger relation. There is a special syntax for telling\nPig to use a fragment replicate join:\ngrunt> C = JOIN A BY $0, B BY $1 USING ""replicated"";\nThe first relation must be the large one, followed by one or more small ones.\n334 | Chapter 11: PigCOGROUP\nJOIN always gives a flat structure: a set of tuples. The COGROUP statement is similar\nto JOIN, but creates a nested set of output tuples. This can be useful if you want to\nexploit the structure in subsequent statements:\ngrunt> D = COGROUP A BY $0, B BY $1;\ngrunt> DUMP D;\n(0,{},{(Ali,0)})\n(1,{(1,Scarf)},{})\n(2,{(2,Tie)},{(Joe,2),(Hank,2)})\n(3,{(3,Hat)},{(Eve,3)})\n(4,{(4,Coat)},{(Hank,4)})\nCOGROUP generates a tuple for each unique grouping key. The first field of each tuple\nis the key, and the remaining fields are bags of tuples from the relations with a matching\nkey. The first bag contains the matching tuples from relation A with the same key.\nSimilarly, the second bag contains the matching tuples from relation B with the same\nkey.\nIf for a particular key a relation has no matching key, then the bag for that relation is\nempty. For example, since no one has bought a scarf (with ID 1), the second bag in the\ntuple for that row is empty. This is an example of an outer join, which is the default\ntype for COGROUP. It can be made explicit using the OUTER keyword; making this\nCOGROUP statement the same as the previous one:\nD = COGROUP A BY $0 OUTER, B BY $1 OUTER;\nYou can suppress rows with empty bags by using the INNER keyword, which gives the\nCOGROUP inner join semantics. The INNER keyword is applied per relation, so the\nfollowing only suppresses rows when relation A has no match (dropping the unknown\nproduct 0 here):\ngrunt> E = COGROUP A BY $0 INNER, B BY $1;\ngrunt> DUMP E;\n(1,{(1,Scarf)},{})\n(2,{(2,Tie)},{(Joe,2),(Hank,2)})\n(3,{(3,Hat)},{(Eve,3)})\n(4,{(4,Coat)},{(Hank,4)})\nWe can flatten this structure to discover who bought each of the items in relation A:\ngrunt> F = FOREACH E GENERATE FLATTEN(A), B.$0;\ngrunt> DUMP F;\n(1,Scarf,{})\n(2,Tie,{(Joe),(Hank)})\n(3,Hat,{(Eve)})\n(4,Coat,{(Hank)})\nUsing a combination of COGROUP, INNER, and FLATTEN (which removes nesting)\nit’s possible to simulate a JOIN:\ngrunt> G = COGROUP A BY $0 INNER, B BY $1 INNER;\ngrunt> H = FOREACH G GENERATE FLATTEN($1), FLATTEN($2);\nData Processing Operators | 335grunt> DUMP H;\n(2,Tie,Joe,2)\n(2,Tie,Hank,2)\n(3,Hat,Eve,3)\n(4,Coat,Hank,4)\nThis gives the same result as JOIN A BY $0, B BY $1.\nIf the join key is composed of several fields, you can specify them all in the BY clauses\nof the JOIN or COGROUP statement. Make sure that the number of fields in each BY\nclause is the same.\nHere’s another example of a join in Pig, in a script for calculating the maximum tem-\nperature of for every station over a time period controlled by the input:\n-- max_temp_station_name.pig\nREGISTER pig.jar;\nDEFINE isGood com.hadoopbook.pig.IsGoodQuality();\nstations = LOAD 'input/ncdc/metadata/stations-fixed-width.txt'\nUSING com.hadoopbook.pig.CutLoadFunc('1-6,8-12,14-42')\nAS (usaf:chararray, wban:chararray, name:chararray);\ntrimmed_stations = FOREACH stations GENERATE usaf, wban,\ncom.hadoopbook.pig.Trim(name);\nrecords = LOAD 'input/ncdc/all/191*'\nUSING com.hadoopbook.pig.CutLoadFunc('5-10,11-15,88-92,93-93')\nAS (usaf:chararray, wban:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND isGood(quality);\ngrouped_records = GROUP filtered_records BY (usaf, wban) PARALLEL 30;\nmax_temp = FOREACH grouped_records GENERATE FLATTEN(group),\nMAX(filtered_records.temperature);\nmax_temp_named = JOIN max_temp BY (usaf, wban), trimmed_stations BY (usaf, wban)\nPARALLEL 30;\nmax_temp_result = FOREACH max_temp_named GENERATE $0, $1, $5, $2;\nSTORE max_temp_result INTO 'max_temp_by_station';\nWe use the cut UDF we developed earlier to load one relation holding the station IDs\n(USAF and WBAN identifiers) and names, and one relation holding all the weather\nrecords, keyed by station ID. We group the filtered weather records by station ID and\naggregate by maximum temperature, before joining with the stations. Finally, we\nproject out the fields we want in the final result: USAF, WBAN, station name, maxi-\nmum temperature.\nHere are a few results for the 1910s:\n228020\n029110\n040650\n99999\n99999\n99999\nSORTAVALA\nVAASA AIRPORT\nGRIMSEY\n322\n300\n378\nThis query could be made more efficient by using a fragment replicate join, as the station\nmetadata is small.\n336 | Chapter 11: PigCROSS\nPig Latin includes the cross product operator (also known as the cartesian product),\nwhich joins every tuple in a relation with every tuple in a second relation (and with\nevery tuple in further relations if supplied). The size of the output is the product of the\nsize of the inputs, potentially making the output very large:\ngrunt> I = CROSS A, B;\ngrunt> DUMP I;\n(2,Tie,Joe,2)\n(2,Tie,Hank,4)\n(2,Tie,Ali,0)\n(2,Tie,Eve,3)\n(2,Tie,Hank,2)\n(4,Coat,Joe,2)\n(4,Coat,Hank,4)\n(4,Coat,Ali,0)\n(4,Coat,Eve,3)\n(4,Coat,Hank,2)\n(3,Hat,Joe,2)\n(3,Hat,Hank,4)\n(3,Hat,Ali,0)\n(3,Hat,Eve,3)\n(3,Hat,Hank,2)\n(1,Scarf,Joe,2)\n(1,Scarf,Hank,4)\n(1,Scarf,Ali,0)\n(1,Scarf,Eve,3)\n(1,Scarf,Hank,2)\nWhen dealing with large datasets, you should try to avoid operations that generate\nintermediate representations that are quadratic (or worse) in size. Computing the cross\nproduct of the whole input dataset is rarely needed, if ever.\nFor example, at first blush one might expect that calculating pairwise document simi-\nlarity in a corpus of documents would require every document pair to be generated\nbefore calculating their similarity. However, if one starts with the insight that most\ndocument pairs have a similarity score of zero (that is, they are unrelated), then we can\nfind a way to a better algorithm.\nIn this case, the key idea is to focus on the entities that we are using to calculate similarity\n(terms in a document, for example), and make them the center of the algorithm. In\npractice, we also remove terms that don’t help discriminate between documents (stop-\nwords) and this reduces the problem space still further. Using this technique to analyze\na set of roughly one million (106) documents generates in the order of one billion\n(109) intermediate pairs,* rather than the one trillion (1012) produced by the naive ap-\nproach (generating the cross product of the input), or the approach with no stopword\nremoval.\n* “Pairwise Document Similarity in Large Collections with MapReduce,” Elsayed, Lin, and Oard (2008, College\nPark, MD: University of Maryland).\nData Processing Operators | 337GROUP\nAlthough COGROUP groups the data in two or more relations, the GROUP statement\ngroups the data in a single relation. GROUP supports grouping by more than equality\nof keys: you can use an expression or user-defined function as the group key. For ex-\nample, consider the following relation A:\ngrunt> DUMP A;\n(Joe,cherry)\n(Ali,apple)\n(Joe,banana)\n(Eve,apple)\nLet’s group by the number of characters in the second field:\ngrunt> B = GROUP A BY SIZE($1);\ngrunt> DUMP B;\n(5L,{(Ali,apple),(Eve,apple)})\n(6L,{(Joe,cherry),(Joe,banana)})\nGROUP creates a relation whose first field is the grouping field, which is given the alias\ngroup. The second field is a bag containing the grouped fields with the same schema as\nthe original relation (in this case, A).\nThere are also two special grouping operations: ALL and ANY. ALL groups all the\ntuples in a relation in a single group, as if the GROUP function was a constant:\ngrunt> C = GROUP A ALL;\ngrunt> DUMP C;\n(all,{(Joe,cherry),(Ali,apple),(Joe,banana),(Eve,apple)})\nNote that there is no BY in this form of the GROUP statement. The ALL grouping is\ncommonly used to count the number of tuples in a relation, as shown in “Validation\nand nulls” on page 318.\nThe ANY keyword is used to group the tuples in a relation randomly, which can be\nuseful for sampling.\nSorting Data\nRelations are unordered in Pig. Consider a relation A:\ngrunt> DUMP A;\n(2,3)\n(1,2)\n(2,4)\nThere is no guarantee which order the rows will be processed in. In particular, when\nretrieving the contents of A using DUMP or STORE, the rows may be written in any\norder. If you want to impose an order on the output you can use the ORDER operator\nto sort a relation by one or more fields. The default sort order compares fields of the\nsame type using the natural ordering, and different types are given an arbitrary, but\ndeterministic, ordering (a tuple is always “less than” a bag, for example). A different\n338 | Chapter 11: Pigordering may be imposed with a USING clause that specifies a UDF that extends Pig’s\nComparisonFunc class.\nThe following example sorts A by the first field in ascending order, and by the second\nfield in descending order:\ngrunt> B = ORDER A BY $0, $1 DESC;\ngrunt> DUMP B;\n(1,2)\n(2,4)\n(2,3)\nAny further processing on a sorted relation does not guarantee to retain its order. For\nexample:\ngrunt> C = FOREACH B GENERATE *;\nEven though relation C has the same contents as relation B, its tuples may be emitted\nin any order by a DUMP or a STORE. It is for this reason that it is usual to perform the\nORDER operation just before retrieving the output.\nThe LIMIT statement is useful for limiting the number of results, as a quick and dirty\nway to get a sample of a relation; prototyping (the ILLUSTRATE command should be\npreferred for generating more representative samples of the data). It can be used im-\nmediately after the ORDER statement to retrieve the first n tuples. Usually LIMIT will\nselect any n tuples from a relation, but when used immediately after an ORDER state-\nment, the order is retained (in an exception to the rule that processing a relation does\nnot retain its order).\ngrunt> D = LIMIT B 2;\ngrunt> DUMP D;\n(1,2)\n(2,4)\nIf the limit is greater than the number of tuples in the relation, all tuples are returned\n(so LIMIT has no effect).\nUsing LIMIT can improve the performance of a query because Pig tries to apply the\nlimit as early as possible in the processing pipeline, to minimize the amount of data\nthat needs to be processed. For this reason, you should always use LIMIT if you are\nnot interested in the entire output.\nCombining and Splitting Data\nSometimes you have several relations that you would like to combine into one. For this,\nthe UNION statement is used. For example:\ngrunt> DUMP A;\n(2,3)\n(1,2)\n(2,4)\ngrunt> DUMP B;\nData Processing Operators | 339(z,x,8)\n(w,y,1)\ngrunt> C = UNION A, B;\ngrunt> DUMP C;\n(2,3)\n(z,x,8)\n(1,2)\n(w,y,1)\n(2,4)\nC is the union of relations A and B, and since relations are unordered, the order of the\ntuples in C is undefined. Also, it’s possible to form the union of two relations with\ndifferent schemas or with different numbers of fields, as we have done here. Pig attempts\nto merge the schemas from the relations that UNION is operating on. In this case, they\nare incompatible, so C has no schema:\ngrunt> DESCRIBE A;\nA: {f0: int,f1: int}\ngrunt> DESCRIBE B;\nB: {f0: chararray,f1: chararray,f2: int}\ngrunt> DESCRIBE C;\nSchema for C unknown.\nIf the output relation has no schema, your script needs to be able to handle tuples that\nvary in the number of fields and/or types.\nThe SPLIT operator is the opposite of UNION; it partitions a relation into two or more\nrelations. See “Validation and nulls” on page 318 for an example of how to use it.\nPig in Practice\nThere are some practical techniques that are worth knowing about when you are de-\nveloping and running Pig programs. This section covers some of them.\nParallelism\nWhen running in Hadoop mode, you need to tell Pig how many reducers you want for\neach job. You do this using a PARALLEL clause for operators that run in the reduce\nphase, which includes all the grouping and joining operators (GROUP, COGROUP,\nJOIN, CROSS), as well as DISTINCT and ORDER. By default the number of reducers\nis one (just like for MapReduce), so it is important to set the degree of parallelism when\nrunning on a large dataset. The following line sets the number of reducers to 30 for the\nGROUP:\ngrouped_records = GROUP records BY year PARALLEL 30;\nA good setting for the number of reduce tasks is slightly fewer than the number of\nreduce slots in the cluster. See “Choosing the Number of Reducers” on page 181 for\nfurther discussion.\n340 | Chapter 11: PigThe number of map tasks is set by the size of the input (with one map per HDFS block),\nand is not affected by the PARALLEL clause.\nParameter Substitution\nIf you have a Pig script that you run on a regular basis, then it’s quite common to want\nto be able to run the same script with different parameters. For example, a script that\nruns daily may use the date to determine which input files it runs over. Pig supports\nparameter substitution, where parameters in the script are substituted with values\nsupplied at runtime. Parameters are denoted by identifiers prefixed with a $ character;\nfor example $input and $output, used in the following script to specify the input and\noutput paths:\n-- max_temp_param.pig\nrecords = LOAD '$input' AS (year:chararray, temperature:int, quality:int);\nfiltered_records = FILTER records BY temperature != 9999 AND\n(quality == 0 OR quality == 1 OR quality == 4 OR quality == 5 OR quality == 9);\ngrouped_records = GROUP filtered_records BY year;\nmax_temp = FOREACH grouped_records GENERATE group,\nMAX(filtered_records.temperature);\nSTORE max_temp into '$output';\nParameters can be specified when launching Pig, using the -param option, one for each\nparameter:\n% pig \\\n-param input=/user/tom/input/ncdc/micro-tab/sample.txt \\\n-param output=/tmp/out \\\nsrc/main/ch11/pig/max_temp_param.pig\nYou can also put parameters in a file, and pass them to Pig using the -param_file option.\nFor example, we can achieve the same result as the previous command by placing the\nparameter definitions in a file:\n# Input file\ninput=/user/tom/input/ncdc/micro-tab/sample.txt\n# Output file\noutput=/tmp/out\nThe pig invocation then becomes:\n% pig \\\n-param_file src/main/ch11/pig/max_temp_param.param \\\nsrc/main/ch11/pig/max_temp_param.pig\nYou can specify multiple parameter files using -param_file repeatedly. You can also\nuse a combination of -param and -param_file options, and if any parameter is defined\nin both a parameter file and on the command line, the last value on the command line\ntakes precedence.\nPig in Practice | 341Dynamic parameters\nFor parameters that are supplied using the -param option, it is easy to make the value\ndynamic by running a command or script. Many Unix shells support command sub-\nstitution for a command enclosed in backticks, and we can use this to make the output\ndirectory date-based:\n% pig \\\n-param input=/user/tom/input/ncdc/micro-tab/sample.txt \\\n-param output=/tmp/`date ""+%Y-%m-%d""`/out \\\nsrc/main/ch11/pig/max_temp_param.pig\nPig also supports backticks in parameter files, by executing the enclosed command in\na shell and using the shell output as the substituted value. If the command or scripts\nexits with a nonzero exit status, then the error message is reported and execution halts.\nBacktick support in parameter files is a useful feature; it means that parameters can be\ndefined in the same way if they are defined in a file or on the command line.\nParameter substitution processing\nParameter substitution occurs as a preprocessing step before the script is run. You can\nsee the substitutions that the preprocessor made by executing Pig with the -dryrun\noption. In dry run mode, Pig performs parameter substitution and generates a copy of\nthe original script with substituted values, but does not execute the script. You can\ninspect the generated script and check that the substitutions look sane (because they\nare dynamically generated, for example) before running it in normal mode.\nAt the time of this writing, Grunt does not support parameter substitution.\n342 | Chapter 11: PigCHAPTER 12\nHBase\nJonathan Gray and Michael Stack\nHBasics\nHBase is a distributed column-oriented database built on top of HDFS. HBase is the\nHadoop application to use when you require real-time read/write random-access to\nvery large datasets.\nAlthough there are countless strategies and implementations for database storage and\nretrieval, most solutions—especially those of the relational variety—are not built with\nvery large scale and distribution in mind. Many vendors offer replication and parti-\ntioning solutions to grow the database beyond the confines of a single node but these\nadd-ons are generally an afterthought and are complicated to install and maintain. They\nalso come at some severe compromise to the RDBMS feature set. Joins, complex quer-\nies, triggers, views, and foreign-key constraints become prohibitively expensive to run\non a scaled RDBMS or do not work at all.\nHBase comes at the scaling problem from the opposite direction. It is built from the\nground-up to scale linearly just by adding nodes. HBase is not relational and does not\nsupport SQL, but given the proper problem space it is able to do what an RDBMS\ncannot: ably host very large, sparsely populated tables on clusters made from com-\nmodity hardware.\nThe canonical HBase use case is the webtable, a table of crawled web pages and their\nattributes (such as language and MIME type) keyed by the web page URL. The webtable\nis large with row counts that run into the billions. Batch analytic and parsing\nMapReduce jobs are continuously run against the webtable deriving statistics and add-\ning new columns of MIME type and parsed text content for later indexing by a search\nengine. Concurrently, the table is randomly accessed by crawlers running at various\nrates updating random rows while random web pages are served in real time as users\nclick on a website’s cached-page feature.\n343Backdrop\nThe HBase project was started toward the end of 2006 by Chad Walters and Jim\nKellerman of Powerset. It was modeled after Google’s “Bigtable: A Distributed Storage\nSystem for Structured Data” by Chang et al. (http://labs.google.com/papers/bigtable\n.html), which had just been published. In February 2007, Mike Cafarella made a code\ndrop of a mostly working system that Jim Kellerman then carried forward.\nThe first HBase release was bundled as part of Hadoop 0.15.0. At the start of 2008,\nHBase became a Hadoop subproject at http://hadoop.apache.org/hbase (or http://hbase\n.org). HBase has been in production use at Powerset since late 2007. Other production\nusers of HBase include WorldLingo, Streamy.com, OpenPlaces, and groups at Yahoo!\nand Adobe.\nConcepts\nIn this section, we provide a quick overview of core HBase concepts. At a minimum, a\npassing familiarity will ease the digestion of all that follows.*\nWhirlwind Tour of the Data Model\nApplications store data into labeled tables. Tables are made of rows and columns. Table\ncells—the intersection of row and column coordinates—are versioned. By default, their\nversion is a timestamp auto-assigned by HBase at the time of cell insertion. A cell’s\ncontent is an uninterpreted array of bytes.\nTable row keys are also byte arrays, so theoretically anything can serve as a row key\nfrom strings to binary representations of longs or even serialized data structures. Table\nrows are sorted by row key, the table’s primary key. By default, the sort is byte-ordered.\nAll table accesses are via the table primary key.†\nRow columns are grouped into column families. All column family members have a\ncommon prefix, so, for example, the columns temperature:air and tempera-\nture:dew_point are both members of the temperature column family, whereas\nstation:identifier belongs to the station family.‡ The column family prefix must be com-\nposed of printable characters. The qualifying tail can be made of any arbitrary bytes.\n* For more detail than is provided here, see the HBase Architecture page http://wiki.apache.org/hadoop/Hbase/\nHbaseArchitecture on the HBase wiki.\n† Though there are facilities in HBase to support secondary indices, these are ancillary rather than core and\nare likely to be moved to their own project.\n‡ In HBase, by convention, the colon character (:) delimits the column family from the column family\nqualifier. It is hardcoded.\n344 | Chapter 12: HBaseA table’s column families must be specified up front as part of the table schema defi-\nnition, but new column family members can be added on demand. For example, a new\ncolumn station:address can be offered by a client as part of an update, and its value\npersisted, as long as the column family station is already in existence on the targeted\ntable.\nPhysically, all column family members are stored together on the filesystem. So, though\nearlier we described HBase as a column-oriented store, it would be more accurate if it\nwere described as a column-family-oriented store. Because tunings and storage speci-\nfications are done at the column family level, it is advised that all column family mem-\nbers have the same general access pattern and size characteristics.\nIn synopsis, HBase tables are like those in an RDBMS, only cells are versioned, rows\nare sorted, and columns can be added on the fly by the client as long as the column\nfamily they belong to preexists.\nRegions\nTables are automatically partitioned horizontally by HBase into regions. Each region\ncomprises a subset of a table’s rows. A region is defined by its first row, inclusive, and\nlast row, exclusive, plus a randomly generated region identifier. Initially a table com-\nprises a single region but as the size of the region grows, after it crosses a configurable\nsize threshold, it splits at a row boundary into two new regions of approximately equal\nsize. Until this first split happens, all loading will be against the single server hosting\nthe original region. As the table grows, the number of its regions grows. Regions are\nthe units that get distributed over an HBase cluster. In this way, a table that is too big\nfor any one server can be carried by a cluster of servers with each node hosting a subset\nof the table’s total regions. This is also the means by which the loading on a table gets\ndistributed. At any one time, the online set of sorted regions comprises the table’s total\ncontent.\nLocking\nRow updates are atomic, no matter how many row columns constitute the row-level\ntransaction. This keeps the locking model simple.\nImplementation\nJust as HDFS and MapReduce are built of clients, slaves and a coordinating master—\nnamenode and datanodes in HDFS and jobtracker and tasktrackers in MapReduce—so\nis HBase characterized with an HBase master node orchestrating a cluster of one or\nmore regionserver slaves (see Figure 12-1). The HBase master is responsible for boot-\nstrapping a virgin install, for assigning regions to registered regionservers, and for re-\ncovering regionserver failures. The master node is lightly loaded. The regionservers\ncarry zero or more regions and field client read/write requests. They also manage region\nsplits informing the HBase master about the new daughter regions for it to manage the\nConcepts | 345offlining of parent region and assignment of the replacement daughters. HBase depends\non ZooKeeper (Chapter 13) and by default it manages a ZooKeeper instance as the\nauthority on cluster state.§\nRegionserver slave nodes are listed in the HBase conf/regionservers file as you would\nlist datanodes and tasktrackers in the Hadoop conf/slaves file. Start and stop scripts are\nlike those in Hadoop using the same SSH-based running of remote commands mech-\nanism. Cluster site-specific configuration is made in the HBase conf/hbase-site.xml and\nconf/hbase-env.sh files, which have the same format as that of their equivalents up in\nthe Hadoop parent project (see Chapter 9).\nWhere there is commonality to be found, HBase directly uses or sub-\nclasses the parent Hadoop implementation, whether a service or type.\nWhen this is not possible, HBase will follow the Hadoop model where\nit can. For example, HBase uses the Hadoop Configuration system so\nconfiguration files have the same format. What this means for you, the\nuser, is that you can leverage any Hadoop familiarity in your exploration\nof HBase. HBase deviates from this rule only when adding its\nspecializations.\nHBase persists data via the Hadoop filesystem API. Since there are multiple implemen-\ntations of the filesystem interface—one for the local filesystem, one for the KFS file-\nsystem, Amazon’s S3, and HDFS (the Hadoop Distributed Filesystem)—HBase can\npersist to any of these implementations. Most experience though has been had using\nHDFS, though by default, unless told otherwise, HBase writes the local filesystem. The\nlocal filesystem is fine for experimenting with your initial HBase install, but thereafter,\nusually the first configuration made in an HBase cluster involves pointing HBase at the\nHDFS cluster to use.\nHBase in operation\nHBase, internally, keeps special catalog tables named -ROOT- and .META. within which\nit maintains the current list, state, recent history, and location of all regions afloat on\nthe cluster. The -ROOT- table holds the list of .META. table regions. The .META. table\nholds the list of all user-space regions. Entries in these tables are keyed using the region’s\nstart row. Row keys, as noted previously, are sorted so finding the region that hosts a\nparticular row is a matter of a lookup to find the first entry whose key is greater than\nor equal to that of the requested row key. As regions transition—are split, disabled/\nenabled, deleted, redeployed by the region load balancer, or redeployed due to a\nregionserver crash—the catalog tables are updated so the state of all regions on the\ncluster is kept current.\n§ HBase can be configured to use an existing ZooKeeper cluster instead.\n346 | Chapter 12: HBaseFigure 12-1. HBase cluster members\nFresh clients connect to the ZooKeeper cluster first to learn the location of -ROOT-.\nClients consult -ROOT- to elicit the location of the .META. region whose scope covers\nthat of the requested row. The client then does a lookup against the found .META. region\nto figure the hosting user-space region and its location. Thereafter the client interacts\ndirectly with the hosting regionserver.\nTo save on having to make three round-trips per row operation, clients cache all they\nlearn traversing -ROOT- and .META. caching locations as well as user-space region start\nand stop rows so they can figure hosting regions themselves without having to go back\nto the .META. table. Clients continue to use the cached entry as they work until there is\na fault. When this happens—the region has moved—the client consults the .META.\nagain to learn the new location. If, in turn, the consulted .META. region has moved, then\n-ROOT- is reconsulted.\nWrites arriving at a regionserver are first appended to a commit log and then are added\nto an in-memory cache. When this cache fills, its content is flushed to the filesystem.\nThe commit log is hosted on HDFS, so it remains available through a regionserver crash.\nWhen the master notices that a regionserver is no longer reachable, it splits the dead\nregionserver’s commit log by region. On reassignment, regions that were on the dead\nregionserver, before they open for business, pick up their just-split file of not yet per-\nsisted edits and replay them to bring themselves up-to-date with the state they had just\nbefore the failure.\nConcepts | 347Reading, the region’s memcache is consulted first. If sufficient versions are found to\nsatisfy the query, we return. Otherwise, flush files are consulted in order, from newest\nto oldest until sufficient versions are found or until we run out of flush files to consult.\nA background process compacts flush files once their number has broached a threshold,\nrewriting many files as one, because the fewer files a read consults, the more performant\nit will be. On compaction, versions beyond the configured maximum, deletes and ex-\npired cells are cleaned out. A separate process running in the regionserver monitors\nflush file sizes splitting the region when they grow in excess of the configured maximum.\nInstallation\nDownload a stable release from the HBase Release page and unpack it somewhere on\nyour filesystem. For example:\n% tar xzf hbase-x.y.z.tar.gz\nAs with Hadoop, you first need to tell HBase where Java is located on your system. If\nyou have the JAVA_HOME environment variable set to point to a suitable Java installation,\nthen that will be used, and you don’t have to configure anything further. Otherwise,\nyou can set the Java installation that HBase uses by editing HBase’s conf/hbase-\nenv.sh, and specifying the JAVA_HOME variable (see Appendix A for some examples) to\npoint to version 1.6.0 of Java.\nHBase, just like Hadoop, requires Java 6.\nFor convenience, add the HBase binary directory to your command-line path. For\nexample:\n% export HBASE_INSTALL=/home/hbase/hbase-x.y.z\n% export PATH=$PATH:$HBASE_INSTALL/bin\nTo get the list of HBase options, type:\n% hbase\nUsage: hbase <command>\nwhere <command> is one of:\nshell\nrun the HBase shell\nmaster\nrun an HBase HMaster node\nregionserver\nrun an HBase HRegionServer node\nrest\nrun an HBase REST server\nthrift\nrun an HBase Thrift server\nzookeeper\nrun a Zookeeper server\nmigrate\nupgrade an hbase.rootdir\nor\nCLASSNAME\nrun the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n348 | Chapter 12: HBaseTest Drive\nTo start a temporary instance of HBase that uses the /tmp directory on the local file-\nsystem for persistence, type\n% start-hbase.sh\nThis will launch two daemons: a standalone HBase instance that persists to the local\nfilesystem—by default, HBase will write to /tmp/hbase-${USERID}—and a single in-\nstance of ZooKeeper to host cluster state.‖\nTo administer your HBase instance, launch the HBase shell by typing:\n% hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nVersion: 0.19.1, r751874, Thu Mar 12 22:54:22 PDT 2009\nhbase(main):001:0>\nThis will bring up a JRuby IRB interpreter that has had some HBase-specific commands\nadded to it. Type help and then RETURN to see the list of shell commands that are\navailable. The list is long. Each listed command includes an example of how its used.\nCommands use Ruby formatting, specifying lists and dictionaries. See the end of the\nhelp screen for a quick tutorial.\nNow let us create a simple table, add some data, and then clean up.\nTo create a table, you must name your table and define its schema. A table’s schema\ncomprises table attributes and the list of table column families. Column families them-\nselves have attributes that you in turn set at schema definition time. Examples of column\nfamily attributes include whether the family content should be compressed on the fil-\nesystem and how many versions of a cell to keep. Schemas can be later edited by off-\nlining the table using the shell disable command, making the necessary alterations\nusing alter, then putting the table back on line with enable.\nTo create a table named test with a single column family name data using defaults for\ntable and column family attributes, enter:\nhbase(main):007:0> create 'test', 'data'\n0 row(s) in 4.3066 seconds\nIf the previous command does not complete successfully, and the shell\ndisplays an error and a stack trace, your install was not successful. Check\nthe master logs under the HBase logs directory for a clue as to where\nthings went awry.\nSee the help output for examples adding table and column family attributes when\nspecifying a schema.\n‖ In standalone mode, HBase master and regionserver are both run in the same JVM.\nInstallation | 349To prove the new table was created successfully, run the list command. This will\noutput all tables in the user space:\nhbase(main):019:0> list\ntest\n1 row(s) in 0.1485 seconds\nTo insert data into three different rows and columns in the data column family, and\nthen list the table content, do the following:\nhbase(main):021:0>\n0 row(s) in 0.0454\nhbase(main):022:0>\n0 row(s) in 0.0035\nhbase(main):023:0>\n0 row(s) in 0.0090\nhbase(main):024:0>\nROW\nrow1\nrow2\nrow3\n3 row(s) in 0.0825\nput 'test', 'row1', 'data:1', 'value1'\nseconds\nput 'test', 'row2', 'data:2', 'value2'\nseconds\nput 'test', 'row3', 'data:3', 'value3'\nseconds\nscan 'test'\nCOLUMN+CELL\ncolumn=data:1, timestamp=1240148026198, value=value1\ncolumn=data:2, timestamp=1240148040035, value=value2\ncolumn=data:3, timestamp=1240148047497, value=value3\nseconds\nNotice how we added three new columns without changing the schema.#\nTo remove the table, you must first disable it before dropping it:\nhbase(main):025:0> disable 'test'\n09/04/19 06:40:13 INFO client.HBaseAdmin: Disabled test\n0 row(s) in 6.0426 seconds\nhbase(main):026:0> drop 'test'\n09/04/19 06:40:17 INFO client.HBaseAdmin: Deleted test\n0 row(s) in 0.0210 seconds\nhbase(main):027:0> list\n0 row(s) in 2.0645 seconds\nShut down your HBase instance by running:\n% stop-hbase.sh\nTo learn how to set up a distributed HBase and point it at a running HDFS, see the\nGetting Started section of the HBase documentation.\nClients\nThere are a number of client options for interacting with an HBase cluster.\n# To quickly load one million 1 k rows, run PerformanceEvaluation as shown here. This will launch a single\nclient running the PerformanceEvaluation sequentialWriter script. It will create a table named TestTable with\na single column family and then populate it. For more on the PerformanceEvaluation scripts, see the HBase\nwiki page on the subject (http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation).\n% hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWriter 1\n350 | Chapter 12: HBaseJava\nHBase, like Hadoop, is written in Java. Later in this chapter, in “Exam-\nple” on page 354, there is sample code for uploading data to an HBase table and for\nreading and updating the uploaded data in HBase tables. In the case of the examples,\nwhere we are interacting with preexisting tables, we use the HBase HTable class as the\ngateway for fetching and updating HBase table instances. Administering your cluster\n(adding, enabling or dropping tables), you would use a different class, HBaseAdmin. Both\nof these classes are in the org.apache.hadoop.hbase.client package.\nMapReduce\nHBase classes and utilities in the org.apache.hadoop.hbase.mapred package facilitate\nusing HBase as a source and/or sink in MapReduce jobs. The TableInputFormat class\nmakes splits on region boundaries so maps are handed a single region to work on. The\nTableOutputFormat will write the result of reduce into HBase. The RowCounter class in\nExample 12-1 can be found in the HBase mapred package. It runs a map task to count\nrows using TableInputFormat.\nExample 12-1. A MapReduce application to count the number of rows in an HBase table\npublic class RowCounter extends Configured implements Tool {\n// Name of this 'program'\nstatic final String NAME = ""rowcounter"";\nstatic class RowCounterMapper\nimplements TableMap<ImmutableBytesWritable, RowResult> {\nprivate static enum Counters {ROWS}\npublic void map(ImmutableBytesWritable row, RowResult value,\nOutputCollector<ImmutableBytesWritable, RowResult> output,\nReporter reporter)\nthrows IOException {\nboolean content = false;\nfor (Map.Entry<byte [], Cell> e: value.entrySet()) {\nCell cell = e.getValue();\nif (cell != null && cell.getValue().length > 0) {\ncontent = true;\nbreak;\n}\n}\nif (!content) {\n// Don't count rows that are all empty values.\nreturn;\n}\n// Give out same value every time. We're only interested in the row/key\nreporter.incrCounter(Counters.ROWS, 1);\n}\npublic void configure(JobConf jc) {\n// Nothing to do.\n}\nClients | 351}\npublic void close() throws IOException {\n// Nothing to do.\n}\npublic JobConf createSubmittableJob(String[] args) throws IOException {\nJobConf c = new JobConf(getConf(), getClass());\nc.setJobName(NAME);\n// Columns are space delimited\nStringBuilder sb = new StringBuilder();\nfinal int columnoffset = 2;\nfor (int i = columnoffset; i < args.length; i++) {\nif (i > columnoffset) {\nsb.append("" "");\n}\nsb.append(args[i]);\n}\n// Second argument is the table name.\nTableMapReduceUtil.initTableMapJob(args[1], sb.toString(),\nRowCounterMapper.class, ImmutableBytesWritable.class, RowResult.class, c);\nc.setNumReduceTasks(0);\n// First arg is the output directory.\nFileOutputFormat.setOutputPath(c, new Path(args[0]));\nreturn c;\n}\nstatic int printUsage() {\nSystem.out.println(NAME +\n"" <outputdir> <tablename> <column1> [<column2>...]"");\nreturn -1;\n}\npublic int run(final String[] args) throws Exception {\n// Make sure there are at least 3 parameters\nif (args.length < 3) {\nSystem.err.println(""ERROR: Wrong number of parameters: "" + args.length);\nreturn printUsage();\n}\nJobClient.runJob(createSubmittableJob(args));\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nHBaseConfiguration c = new HBaseConfiguration();\nint errCode = ToolRunner.run(c, new RowCounter(), args);\nSystem.exit(errCode);\n}\nThis class implements Tool, which is discussed in “GenericOptionsParser, Tool, and\nToolRunner” on page 121, and the HBase TableMap interface, a specialization of\norg.apache.hadoop.mapred.Mapper that sets the map inputs types passed by TableInput\nFormat. The createSubmittableJob() method parses arguments added to the\n352 | Chapter 12: HBaseconfiguration that were passed on the command line figuring the table and columns\nwe are to run RowCounter against. It also invokes the TableMapReduceUtil.initTableMap\nJob() utility method, which among other things such as setting the map class to use,\nsets the input format to TableInputFormat. The map is simple. It checks all columns. If\nall are empty, it doesn’t count the row. Otherwise, it increments Counters.ROWS by one.\nREST and Thrift\nHBase ships with REST and thrift interfaces. These are useful when the interacting\napplication is written in a language other than Java. In both cases, a Java server hosts\nan instance of the HBase client brokering application REST and thrift requests in and\nout of the HBase cluster. This extra work proxying requests and responses means these\ninterfaces are slower than using the Java client directly.\nREST\nTo put up the REST, start it using the following command:\n% hbase-daemon.sh start rest\nThis will start a server instance, by default on port 60050, background it, and catch\nany emissions by the server into a logfile under the HBase logs directory.\nClients can ask for the response to be formatted as JSON or as XML, depending on\nhow the client HTTP Accept header is set. See the REST wiki page for documentation\nand examples of making REST client requests.\nTo stop the REST server, type:\n% hbase-daemon.sh stop rest\nThrift\nSimilarly, start a thrift service by putting up a server to field thrift clients by running\nthe following:\n% hbase-daemon.sh start thrift\nThis will start the server instance, by default on port 9090, background it, and catch\nany emissions by the server into a logfile under the HBase logs directory. The HBase\nthrift documentation* notes the thrift version used generating classes. The HBase thrift\nIDL can be found at src/java/org/apache/hadoop/hbase/thrift/Hbase.thrift in the HBase\nsource code.\nTo stop the thrift server, type:\n% hbase-daemon.sh stop thrift\n* http://hadoop.apache.org/hbase/docs/current/api/org/apache/hadoop/hbase/thrift/package-summary.html\nClients | 353Example\nAlthough HDFS and MapReduce are powerful tools for processing batch operations\nover large datasets, they do not provide ways to read or write individual records effi-\nciently. In this example, we’ll explore HBase as the tool to fill this gap.\nThe existing weather dataset described in previous chapters contains observations for\ntens of thousands of stations over 100 years and this data is growing without bound.\nIn this example, we will build a simple web interface that allows a user to navigate the\ndifferent stations and page through their historical temperature observations in time\norder. For the sake of this example, let us allow that the dataset is massive, that the\nobservations run to the billions, and that the rate at which temperature updates arrive\nis significant—say hundreds to thousands of updates a second from around the world\nacross the whole range of weather stations. Also let us allow that it is a requirement\nthat the web application must display the most up-to-date observation within a second\nor so of receipt.\nThe first size requirement should preclude our use of a simple RDBMS instance and\nmake HBase a candidate store. The second latency requirement rules out plain HDFS.\nA MapReduce job could build initial indices that allowed random-access over all of the\nobservation data but keeping up this index as the updates arrived is not what HDFS\nand MapReduce are good at.\nSchemas\nIn our example, there will be two tables:\nStations\nThis table holds station data. Let the row key be the stationid. Let this table have\na column family info that acts as a key/val dictionary for station information. Let\nthe keys be name, location, and description. This table is static and the info family,\nin this case, closely mirrors a typical RDBMS table design.\nObservations\nThis table holds temperature observations. Let the row key be a composite key of\nstationid + reverse order timestamp. Give this table a column family data that will\ncontain one column airtemp with the observed temperature as the column value.\nOur choice of schema is derived from how we want to most efficiently read from HBase.\nRows and columns are stored in increasing lexicographical order. Though there are\nfacilities for secondary indexing and regular expression matching, they come at a per-\nformance penalty.† It is vital that you understand how you want to most efficiently\nquery your data in order to most effectively store and access it.\n† The bundled HBase secondary index mechanism make use of TransactionalHBase, which is powerful but\nnot performant. If you need good performance, it is recommended, for now, that you manage your own\nsecondary index tables or make use of an external index implementation like Lucene.\n354 | Chapter 12: HBaseFor the stations table, the choice of stationid as key is obvious because we will always\naccess information for a particular station by its id. The observations table, however,\nuses a composite key that adds the observation timestamp at the end. This will group\nall observations for a particular station together, and by using a reverse order timestamp\n(Long.MAX_VALUE - epoch) and storing it as binary, observations for each station will be\nordered with most recent observation first.\nIn the shell, you would define your tables as follows:\nhbase(main):036:0>\n0 row(s) in 0.1304\nhbase(main):037:0>\n0 row(s) in 0.1332\ncreate 'stations', {NAME => 'info', VERSIONS => 1}\nseconds\ncreate 'observations', {NAME => 'data', VERSIONS => 1}\nseconds\nIn both cases, we are interested only in the latest version of a table cell, so set VERSIONS to\n1.\nLoading Data\nThere are a relatively small number of stations, so their static data is easily inserted\nusing any of the available interfaces.\nHowever, let’s assume that there are billions of individual observations to be loaded.\nThis kind of import is normally an extremely complex and long-running database op-\neration, but MapReduce and HBase’s distribution model allow us to make full use of\nthe cluster. Copy the raw input data onto HDFS and then run a MapReduce job that\ncan read the input and write to HBase, eventually reading from and writing to all nodes.\nAt the beginning, as the import runs, your table will have only one region, so writes\nwill go to only one server. As the import progresses and tables split, regions will be\ndistributed across the cluster along with your writes.\nExample 12-2 shows an example MapReduce job that imports observations to HBase\nfrom the same input file used in the previous chapters’ examples.\nExample 12-2. A MapReduce application to import temperature data from HDFS into an HBase table\npublic class HBaseTemperatureImporter extends Configured implements Tool {\n// Inner-class for map\nstatic class HBaseTemperatureMapper<K, V> extends MapReduceBase implements\nMapper<LongWritable, Text, K, V> {\nprivate NcdcRecordParser parser = new NcdcRecordParser();\nprivate HTable table;\npublic void map(LongWritable key, Text value,\nOutputCollector<K, V> output, Reporter reporter)\nthrows IOException {\nparser.parse(value.toString());\nif (parser.isValidTemperature()) {\nbyte[] rowKey = RowKeyConverter.makeObservationRowKey(parser.getStationId(),\nparser.getObservationDate().getTime());\nExample | 355}\n}\n}\nBatchUpdate bu = new BatchUpdate(rowKey);\nbu.put(""data:airtemp"", Bytes.toBytes(parser.getAirTemperature()));\ntable.commit(bu);\npublic void configure(JobConf jc) {\nsuper.configure(jc);\n// Create the HBase table client once up-front and keep it around\n// rather than create on each map invocation.\ntry {\nthis.table = new HTable(new HBaseConfiguration(jc), ""observations"");\n} catch (IOException e) {\nthrow new RuntimeException(""Failed HTable construction"", e);\n}\n}\npublic int run(String[] args) throws IOException {\nif (args.length != 1) {\nSystem.err.println(""Usage: HBaseTemperatureImporter <input>"");\nreturn -1;\n}\nJobConf jc = new JobConf(getConf(), getClass());\nFileInputFormat.addInputPath(jc, new Path(args[0]));\njc.setMapperClass(HBaseTemperatureMapper.class);\njc.setNumReduceTasks(0);\njc.setOutputFormat(NullOutputFormat.class);\nJobClient.runJob(jc);\nreturn 0;\n}\n}\npublic static void main(String[] args) throws Exception {\nint exitCode = ToolRunner.run(new HBaseConfiguration(),\nnew HBaseTemperatureImporter(), args);\nSystem.exit(exitCode);\n}\nHBaseTemperatureImporter has an inner class named HBaseTemperatureMapper that is like\nthe MaxTemperatureMapper class from Chapter 5. The outer class implements Tool and\ndoes the setup to launch the HBaseTemperatureMapper inner class. HBaseTemperatureMap\nper takes the same input as MaxTemperatureMapper and does the same parse—using the\nNcdcRecordParser introduced in Chapter 5—to check for valid temperatures, but rather\nthan add valid temperatures to the output collector as MaxTemperatureMapper does,\ninstead it adds valid temperatures to the observations HBase table into the data:air-\ntemp column. In the configure() method, we create an HTable instance once against\nthe observations table and use it afterward in map invocations talking to HBase.\nThe row key used is created in the makeObservationRowKey() method on RowKey\nConverter from the station ID and observation time:\n356 | Chapter 12: HBasepublic class RowKeyConverter {\nprivate static final int STATION_ID_LENGTH = 12;\n/**\n* @return A row key whose format is: <station_id> <reverse_order_epoch>\n*/\npublic static byte[] makeObservationRowKey(String stationId,\nlong observationTime) {\nbyte[] row = new byte[STATION_ID_LENGTH + Bytes.SIZEOF_LONG];\nBytes.putBytes(row, 0, Bytes.toBytes(stationId), 0, STATION_ID_LENGTH);\nlong reverseOrderEpoch = Long.MAX_VALUE - observationTime;\nBytes.putLong(row, STATION_ID_LENGTH, reverseOrderEpoch);\nreturn row;\n}\n}\nThe conversion takes advantage of the fact that the station ID is a fixed-length string.\nThe Bytes class used in makeObservationRowKey() is from the HBase utility package. It\nincludes methods for converting between byte arrays and common Java and Hadoop\ntypes. In makeObservationRowKey(), the Bytes.putLong() method is used to fill the key\nbyte array. The Bytes.SIZEOF_LONG constant is used for sizing and positioning in the\nrow key array.\nWe can run the program with the following:\n% hbase HBaseTemperatureImporter input/ncdc/all\nOptimization notes\n• Watch for the phenomenon where the import walks in lock-step through the table\nwith all clients in concert pounding one of the table’s regions (and thus, a single\nnode), then moving on to the next, and so on, rather than evenly distributing the\nload over all regions. This is usually brought on by some interaction between sorted\ninput and how the splitter works. Randomizing the ordering of your row keys prior\nto insertion may help. In our example, given the distribution of stationid values\nand how TextInputFormat makes splits, the upload should be sufficiently\ndistributed.‡\n• Only obtain one HTable instance per task. There is a cost instantiating an HTable,\nso if you do this for each insert, you may have a negative impact on performance.\nHence our setup of HTable in the configure() step.\n‡ If a table is new, it will have only one region and initially all updates will be to this single region until it splits.\nThis will happen even if row keys are randomly distributed. This startup phenomenon means uploads run\nslow at first until there are sufficient regions distributed so all cluster members are able to participate in the\nupload. Do not confuse this phenomenon with that of the one described earlier.\nExample | 357• By default, each HTable.commit(BatchUpdate) actually performs the insert without\nany buffering. You can disable HTable auto-flush feature using HTable.setAuto\nFlush(false) and then set the size of configurable write buffer. When the inserts\ncommitted fill the write buffer, it is then flushed. Remember though, you must call\na manual HTable.flushCommits() at the end of each task to ensure that nothing is\nleft unflushed in the buffer. You could do this in an override of the mapper’s\nclose() method.\n• HBase includes TableInputFormat and TableOutputFormat to help with MapReduce\njobs that source and sink HBase (see Example 12-1). One way to write the previous\nexample would have been to use MaxTemperatureMapper from Chapter 5 as is but\nadd a reducer task that takes the output of the MaxTemperatureMapper and feeds it\nto HBase via TableOutputFormat.\nWeb Queries\nTo implement the web application, we will use the HBase Java API directly. Here it\nbecomes clear how important your choice of schema and storage format is.\nThe simplest query will be to get the static station information. This type of query is\nsimple in a traditional database, but HBase gives you additional control and flexibility.\nUsing the info family as a key/value dictionary (column names as keys, column values\nas values), the code would look like this:\npublic Map<String, String> getStationInfo(HTable table, String stationId)\nthrows IOException {\nbyte[][] columns = { Bytes.toBytes(""info:"") };\nRowResult res = table.getRow(Bytes.toBytes(stationId), columns);\nif (res == null) {\nreturn null;\n}\nMap<String, String> resultMap = new HashMap<String, String>();\nresultMap.put(""name"", getValue(res, ""info:name""));\nresultMap.put(""location"", getValue(res, ""info:location""));\nresultMap.put(""description"", getValue(res, ""info:description""));\nreturn resultMap;\n}\nprivate static String getValue(RowResult res, String key) {\nCell c = res.get(key.getBytes());\nif (c == null) {\nreturn """";\n}\nreturn Bytes.toString(c.getValue());\n}\n358 | Chapter 12: HBaseIn this example, getStationInfo() takes an HTable instance and a station ID. To get the\nstation info, we use HTable.getRow(). Use this method when fetching more than one\ncolumn from a row. Passing a column family name info: rather than explicit column\nnames will return all columns on that family.§ The getRow() results are returned in\nRowResult, an implementation of SortedMap with the row key as a data member. It is\nkeyed by the column name as a byte array. Values are Cell data structures that hold a\ntimestamp and the cell content in a byte array. The getStationInfo() method converts\nthe RowResult Map into a more friendly Map of String keys and values.\nWe can already see how there is a need for utility functions when using HBase. There\nare an increasing number of abstractions being built atop HBase to deal with this low-\nlevel interaction, but it’s important to understand how this works and how storage\nchoices make a difference.\nOne of the strengths of HBase over a relational database is that you don’t have to\nprespecify the columns. So, in the future, if each station now has at least these three\nattributes but there are hundreds of optional ones, we can just insert them without\nmodifying the schema. Your applications reading and writing code would of course\nneed to be changed. The example code might change in this case to looping through\nRowResult.entrySet() rather than explicitly grabbing each value from the RowResult.\nWe will make use of HBase scanners for retrieval of observations in our web\napplication.\nHere we are after a Map<ObservationTime, ObservedTemp> result. We will use a\nNavigableMap<Long, Integer> because it is sorted and has a descendingMap() method,\nso we can access observations in both ascending or descending order. The code is in\nExample 12-3.\nExample 12-3. Methods for retrieving a range of rows of weather station observations from an HBase\ntable\npublic NavigableMap<Long, Integer> getStationObservations(HTable table,\nString stationId, long maxStamp, int maxCount) throws IOException {\nbyte[][] columns = { Bytes.toBytes(""data:airtemp"") };\nbyte[] startRow = RowKeyConverter.makeObservationRowKey(stationId, maxStamp);\nRowResult res = null;\nNavigableMap<Long, Integer> resultMap = new TreeMap<Long, Integer>();\nbyte[] airtempColumn = Bytes.toBytes(""data:airtemp"");\nScanner s = table.getScanner(columns, startRow);\nint count = 0;\ntry {\nwhile ((res = s.next()) != null && count++ < maxCount) {\nbyte[] row = res.getRow();\nbyte[] value = res.get(airtempColumn).getValue();\nLong stamp = Long.MAX_VALUE -\nBytes.toLong(row, row.length - Bytes.SIZEOF_LONG, Bytes.SIZEOF_LONG);\n§ A column name without a qualifier designates a column family. The colon is required. If you pass info only\nfor column family name, rather than info:, HBase will complain the column name is incorrectly formatted.\nExample | 359Integer temp = Bytes.toInt(value);\nresultMap.put(stamp, temp);\n}\n}\n} finally {\ns.close();\n}\nreturn resultMap;\n/**\n* Return the last ten observations.\n*/\npublic NavigableMap<Long, Integer> getStationObservations(HTable table,\nString stationId) throws IOException {\nreturn getStationObservations(table, stationId, Long.MAX_VALUE, 10);\n}\nThe getStationObservations() method takes a station ID and a range defined by max\nStamp and a maximum number of rows (maxCount). Note that the NavigableMap that is\nreturned is actually now in descending time order. If you want to read through it in\nascending order, you would make use of NavigableMap.descendingMap().\nThe advantage of storing things as Long.MAX_VALUE - stamp may not be clear in the\nprevious example. It has more use when you want to get the newest observations for a\ngiven offset and limit, which is often the case in web applications. If the observations\nwere stored with the actual stamps, we would be able to get only the oldest observations\nfor a given offset and limit efficiently. Getting the newest would mean getting all of\nthem, and then grabbing them off the end. One of the prime reasons for moving from\nRDBMS to HBase is to allow for these types of “early-out” scenarios.\nScanners\nHBase scanners are like cursors in a traditional database or Java iterators, except—\nunlike the latter—they have to be closed after use. Scanners return rows in order. Users\nobtain a scanner on an HBase table by calling HTable.getScanner(). A number of over-\nloaded variants allow the user to pass a row at which to start the scanner, a row at which\nto stop the scanner on, which columns in a row to return in the row result, and op-\ntionally, a filter to run on the server side.‖ The Scanner interface absent Javadoc is as\nfollows:\npublic interface Scanner extends Closeable, Iterable<RowResult> {\npublic RowResult next() throws IOException;\npublic RowResult [] next(int nbRows) throws IOException;\npublic void close();\n}\n‖ To learn more about the server-side filtering mechanism in HBase, see http://hadoop.apache.org/hbase/\ndocs/current/api/org/apache/hadoop/hbase/filter/package-summary.html.\n360 | Chapter 12: HBaseYou can ask for the next row’s results or a number of rows. Each invocation of\nnext() involves a trip back to the regionserver, so grabbing a bunch of rows at once can\nmake for significant performance savings.#\nHBase Versus RDBMS\nHBase and other column-oriented databases are often compared to more traditional\nand popular relational databases or RDBMSs. Although they differ dramatically in their\nimplementations and in what they set out to accomplish, the fact that they are potential\nsolutions to the same problems means that despite their enormous differences, the\ncomparison is a fair one to make.\nAs described previously, HBase is a distributed, column-oriented data storage system.\nIt picks up where Hadoop left off by providing random reads and writes on top of\nHDFS. It has been designed from the ground up with a focus on scale in every direction:\ntall in numbers of rows (billions), wide in numbers of columns (millions), and to be\nhorizontally partitioned and replicated across thousands of commodity nodes auto-\nmatically. The table schemas mirror the physical storage, creating a system for efficient\ndata structure serialization, storage, and retrieval. The burden is on the application\ndeveloper to make use of this storage and retrieval in the right way.\nStrictly speaking, an RDBMS is a database that follows Codd’s 12 Rules. Typical\nRDBMSs are fixed-schema, row-oriented databases with ACID properties and a so-\nphisticated SQL query engine. The emphasis is on strong consistency, referential in-\ntegrity, abstraction from the physical layer, and complex queries through the SQL lan-\nguage. You can easily create secondary indexes, perform complex inner and outer joins,\ncount, sum, sort, group, and page your data across a number of tables, rows, and\ncolumns.\nFor a majority of small- to medium-volume applications, there is no substitute for the\nease of use, flexibility, maturity, and powerful feature set of available open source\nRDBMS solutions like MySQL and PostgreSQL. However, if you need to scale up in\nterms of dataset size, read/write concurrency, or both, you’ll soon find that the con-\nveniences of an RDBMS come at an enormous performance penalty and make distri-\nbution inherently difficult. The scaling of an RDBMS usually involves breaking Codd’s\nrules, loosening ACID restrictions, forgetting conventional DBA wisdom, and on the\nway losing most of the desirable properties that made relational databases so conven-\nient in the first place.\n# The hbase.client.scanner.caching configuration option is set to 1 by default. Scanners will, under the\ncovers, fetch this many results at a time, bringing them client side, and returning to the server to fetch\nthe next batch only after the current batch has been exhausted. Higher caching values will enable faster\nscanning but will eat up more memory in the client.\nHBase Versus RDBMS | 361Successful Service\nHere is a synopsis of how the typical RDBMS scaling story runs. The following list\npresumes a successful growing service:\nInitial public launch\nMove from local workstation to shared, remote hosted MySQL instance with a\nwell-defined schema.\nService becomes more popular; too many reads hitting the database\nAdd memcached to cache common queries. Reads are now no longer strictly ACID;\ncached data must expire.\nService continues to grow in popularity; too many writes hitting the database\nScale MySQL vertically by buying a beefed up server with 16 cores, 128 GB of RAM,\nand banks of 15 k RPM hard drives. Costly.\nNew features increases query complexity; now we have too many joins\nDenormalize your data to reduce joins. (That’s not what they taught me in DBA\nschool!)\nRising popularity swamps the server; things are too slow\nStop doing any server-side computations.\nSome queries are still too slow\nPeriodically prematerialize the most complex queries, try to stop joining in most\ncases.\nReads are OK, but writes are getting slower and slower\nDrop secondary indexes and triggers (no indexes?).\nAt this point, there are no clear solutions for how to solve your scaling problems. In\nany case, you’ll need to begin to scale horizontally. You can attempt to build some type\nof partitioning on your largest tables, or look into some of the commercial solutions\nthat provide multiple master capabilities.\nCountless applications, businesses, and websites have successfully achieved scalable,\nfault-tolerant, and distributed data systems built on top of RDBMSs and are likely using\nmany of the previous strategies. But what you end up with is something that is no longer\na true RDBMS, sacrificing features and conveniences for compromises and complexi-\nties. Any form of slave replication or external caching introduces weak consistency into\nyour now denormalized data. The inefficiency of joins and secondary indexes means\nalmost all queries become primary key lookups. A multiwriter setup likely means no\nreal joins at all and distributed transactions are a nightmare. There’s now an incredibly\ncomplex network topology to manage with an entirely separate cluster for caching.\nEven with this system and the compromises made, you will still worry about your\nprimary master crashing and the daunting possibility of having 10 times the data and\n10 times the load in a few months.\n362 | Chapter 12: HBaseHBase\nEnter HBase, which has the following characteristics:\nNo real indexes\nRows are stored sequentially, as are the columns within each row. Therefore, no\nissues with index bloat, and insert performance is independent of table size.\nAutomatic partitioning\nAs your tables grow, they will automatically be split into regions and distributed\nacross all available nodes.\nScale linearly and automatically with new nodes\nAdd a node, point it to the existing cluster, and run the regionserver. Regions will\nautomatically rebalance and load will spread evenly.\nCommodity hardware\nClusters are built on $1,000–$5,000 nodes rather than $50,000 nodes. RDBMS are\nhungry I/O, which is the most costly type of hardware.\nFault tolerance\nLots of nodes means each is relatively insignificant. No need to worry about indi-\nvidual node downtime.\nBatch processing\nMapReduce integration allows fully parallel, distributed jobs against your data\nwith locality awareness.\nIf you stay up at night worrying about your database (uptime, scale, or speed), then\nyou should seriously consider making a jump from the RDBMS world to HBase. Utilize\na solution that was intended to scale rather than a solution based on stripping down\nand throwing money at what used to work. With HBase, the software is free, the hard-\nware is cheap, the distribution is intrinsic, and there are no black boxes to step on your\ntoes.\nUse Case: HBase at streamy.com\nStreamy.com is a real-time news aggregator and social sharing platform. With a broad\nfeature set, we started out with a complex implementation on top of PostgreSQL. It’s\na terrific product with a great community and a beautiful codebase. We tried every trick\nin the book to keep things fast as we scaled, going so far as to modify the code directly\nto suit our needs. Originally taking advantage of all RDBMS goodies, we found that\neventually, one by one, we had to let them all go. Along the way, our entire team became\nthe DBA.\nWe did manage to solve many of the issues that we ran into, but there were two that\neventually led to the decision to find another solution from outside the world of\nRDBMS.\nHBase Versus RDBMS | 363Streamy crawls thousands of RSS feeds and aggregates hundreds of millions of items\nfrom them. In addition to having to store these items, one of our more complex queries\nreads a time-ordered list of all items from a set of sources. At the high end, this can run\nto several thousand sources and all of their items all in a single query.\nVery large items tables\nAt first, this was a single items table, but the high number of secondary indexes made\ninserts and updates very slow. We started to divide items up into several one-to-one\nlink tables to store other information, separating static fields from dynamic ones,\ngrouping fields based on how they were queried, and denormalizing everything along\nthe way. Even with these changes, single updates required rewriting the entire record,\nso tracking statistics on items was difficult to scale. The rewriting of records and having\nto update indexes along the way are intrinsic properties of the RDBMS we were using.\nThey could not be decoupled. We partitioned our tables, which was not too difficult\nbecause of the natural partition of time, but the complexity got out of hand fast. We\nneeded another solution!\nVery large sort merges\nPerforming sorted merges of time-ordered lists is common in many Web 2.0 applica-\ntions. An example SQL query might look like this:\nSELECT id, stamp, type FROM streams\nWHERE type IN ('type1','type2','type3','type4',...,'typeN')\nORDER BY stamp DESC LIMIT 10 OFFSET 0;\nAssuming id is a primary key on streams, and that stamp and type have secondary\nindexes, an RDBMS query planner treats this query as follows:\nMERGE (\nSELECT id, stamp, type FROM streams\nWHERE type = 'type1' ORDER BY stamp DESC,\n...,\nSELECT id, stamp, type FROM streams\nWHERE type = 'typeN' ORDER BY stamp DESC\n) ORDER BY stamp DESC LIMIT 10 OFFSET 0;\nThe problem here is that we are after only the top 10 IDs, but the query planner actually\nmaterializes an entire merge and then limits at the end. A simple heapsort across each\nof the types would allow you to “early out” once you have the top 10. In our case, each\ntype could have tens of thousands of IDs in it, so materializing the entire list and sorting\nit was extremely slow and unnecessary. We actually went so far as to write a custom\nPL/Python script that performed a heapsort using a series of queries like the following:\nSELECT id, stamp, type FROM streams\nWHERE type = 'typeN'\nORDER BY stamp DESC LIMIT 1 OFFSET 0;\nIf we ended up taking from typeN (it was the next most recent in the heap), we would\nrun another query:\n364 | Chapter 12: HBaseSELECT id, stamp, type FROM streams\nWHERE type = 'typeN'\nORDER BY stamp DESC LIMIT 1 OFFSET 1;\nIn nearly all cases, this outperformed the native SQL implementation and the query\nplanner’s strategy. In the worst cases for SQL, we were more than an order of magnitude\nfaster using the Python procedure. We found ourselves continually trying to outsmart\nthe query planner.\nAgain, at this point, we really needed another solution.\nLife with HBase\nOur RDBMS-based system was always capable of correctly implementing our require-\nments; the issue was scaling. When you start to focus on scale and performance rather\nthan correctness, you end up short-cutting and optimizing for your domain-specific\nuse cases everywhere possible. Once you start implementing your own solutions to\nyour data problems, the overhead and complexity of an RDBMS gets in your way. The\nabstraction from the storage layer and ACID requirements are an enormous barrier and\nluxury that you cannot always afford when building for scale. HBase is a distributed,\ncolumn-oriented, sorted map store and not much else. The only major part that is\nabstracted from the user is the distribution, and that’s exactly what we don’t want to\ndeal with. Business logic, on the other hand, is very specialized and optimized. With\nHBase not trying to solve all of our problems, we’ve been able to solve them better\nourselves and rely on HBase for scaling our storage, not our logic. It was an extremely\nliberating experience to be able to focus on our applications and logic rather than the\nscaling of the data itself.\nWe currently have tables with hundreds of millions of rows and tens of thousands of\ncolumns; the thought of storing billions of rows and millions of columns is exciting,\nnot scary.\nPraxis\nIn this section, we discuss some of the common issues users run into running an HBase\ninstance that moves beyond basic examples.\nVersions\nFirst, ensure you are running compatible versions of Hadoop and HBase. Compatible\nversions have their major and minor version numbers in common. Although an HBase\n0.18.0 cannot talk to an Hadoop 0.19.0 version cluster—they disagree in their minor\nnumbers—an HBase 0.19.2 can run on a Hadoop 0.19.1 HDFS. Incompatible versions\nwill throw an exception complaining about the version mismatch, if you are lucky. If\nthey cannot talk to each sufficiently to pass versions, you may see your HBase cluster\nhang indefinitely, soon after startup. The mismatch exception or HBase hang can also\nPraxis | 365happen on upgrade if older versions of either HBase or Hadoop can still be found on\nthe CLASSPATH because of imperfect cleanup of the old software.\nLove and Hate: HBase and HDFS\nHBase’s use of HDFS is very different from how its used by MapReduce. In MapReduce,\ngenerally, HDFS files are opened, their content streamed through a map task and then\nclosed. In HBase, data files are opened on cluster startup and kept open so that we\navoid paying the file open costs on each access. Because of this, HBase tends to see\nissues not normally encountered by MapReduce clients:\nRunning out of file descriptors\nBecause we keep files open, on a loaded cluster, it doesn’t take long before we run\ninto system- and Hadoop-imposed limits. For instance, say we have a cluster that\nhas three nodes each running an instance of a datanode and a regionserver and\nwe’re running an upload into a table that is currently at 100 regions and 10 column\nfamilies. Allow that each column family has on average two flush files. Doing the\nmath, we can have 100 × 10 × 2, or 2,000, files open at any one time. Add to this\ntotal miscellaneous other descriptors consumed by outstanding scanners, Java li-\nbraries. Each open file consumes at least one descriptor over on the remote data-\nnode. The default limit on the number of file descriptors per process is 1024. When\nwe exceed the filesystem ulimit, we’ll see the complaint about Too many open\nfiles in logs but often you’ll first see indeterminate behavior in HBase. The fix re-\nquires increasing the file descriptor ulimit count.* You can verify that the HBase\nprocess is running with sufficient file descriptors by looking at the first few lines\nof a regionservers log. It emits vitals on such as the JVM being used and environ-\nment settings such as the file descriptor ulimit.\nRunning out of datanode threads\nSimilarly, the Hadoop datanode has an upper bound of 256 on the number of\nthreads it can run at any one time. Given the same table statistics quoted in the\npreceding bullet, it’s easy to see how we can exceed this upper bound relatively\nearly, given that in the datanode as of this writing each open connection to a file\nblock consumes a thread.† If you look in the datanode log, you’ll see a complaint\nlike xceiverCount 258 exceeds the limit of concurrent xcievers 256 but again, you’ll\nlikely see HBase act erratically before you encounter this log entry. Increase the\ndfs.datanode.max.xcievers (note that the property name is misspelled) count in\nHDFS and restart your cluster.‡\n* See the HBase FAQ (http://wiki.apache.org/hadoop/Hbase/FAQ) for how to up the ulimit on your cluster.\n† See HADOOP-3856 Asynchronous IO Handling in Hadoop and HDFS.\n‡ See the HBase troubleshooting guide (http://wiki.apache.org/hadoop/Hbase/Troubleshooting) for more detail\non this issue.\n366 | Chapter 12: HBaseBad blocks\nThe DFSClient hosted in your long-running regionserver will tend to mark file\nblocks as bad if, on an access, the server is currently heavily loaded. Since blocks\nby default are replicated three times, the regionserver DFSClient will move on to\nthe next replica. But if this replica is accessed during a time of heavy loading, we\nnow have two of the three blocks marked as bad. If the third block is found to be\nbad, we start see complaint No live nodes contain current block in regionserver logs.\nDuring startup, there is lots of churn and contention as regions are opened and\ndeployed. During this time, the No live nodes contain current block can come on\nquickly. At an extreme, set dfs.datanode.socket.write.timeout to zero. Note that\nthis configuration needs to be set in a location that can be seen by the HBase\nDFSClient; set it in the hbase-site.xml or by symlinking the hadoop-site.xml (or hdfs-\nsite.xml in recent versions) into your HBase conf directory.§\nUI\nHBase runs a web server on the master to present a view on the state of your running\ncluster. By default, it listens on port 60010. The master UI displays a list of basic at-\ntributes such as software versions, cluster load, request rates, lists of cluster tables and\nparticipating regionservers. Click on a regionserver in the master UI and you are taken\nto the web server running on the individual regionserver. It lists the regions this server\nis carrying and basic metrics such as resources consumed and request rates.\nMetrics\nHadoop has a metrics system that can be used emitting vitals over a period to a con-\ntext (this is covered in “Metrics” on page 286). Enabling Hadoop metrics, and in par-\nticular tying them to Ganglia, helps the development of view on what is happening on\nyour cluster currently and in the recent past. HBase also adds metrics of its own—\nrequest rates, counts of vitals, resources used—that can be caught by a Hadoop context.\nSee the file hadoop-metrics.properties under the HBase conf directory.‖\nSchema Design\nHBase tables are like those in an RDBMS, except that cells are versioned, rows are\nsorted, and columns can be added on the fly by the client as long as the column family\nthey belong to preexists. The following factors should be considered when designing\nschemas for HBase. The other property to keep in mind when designing schemas is\n§ Again, see the HBase troubleshooting guide (http://wiki.apache.org/hadoop/Hbase/Troubleshooting) for more\ndetail on this issue.\n‖ Yes, this file named for Hadoop, though it’s for setting up HBase metrics.\nPraxis | 367that a defining attribute of column(-family)-oriented stores, like HBase, is that it can\nhost wide and sparsely populated tables at no incurred cost.#\nJoins\nThere is no native database join facility in HBase, but wide tables can make it so that\nthere is no need for database joins pulling from secondary or tertiary tables. A wide\nrow can sometimes be made to hold all data that pertains to a particular primary key.\nRow keys\nTake time designing your row key. In the weather data example in this chapter, the\ncompound row key has a station prefix that served to group temperatures by station.\nThe reversed timestamp suffix made it so temperatures could be scanned ordered from\nmost recent to oldest. A smart compound key can be used clustering data in ways\namenable to how it will be accessed.\nDesigning compound keys, you may have to zero-pad number components so row keys\nsort properly. Otherwise, you will run into the issue where 10 sorts before 2 when only\nbyte-order is considered (02 sorts before 10).\nIf your keys are integers, use a binary representation rather than persist the string ver-\nsion of a number—it consumes less space.\n# “Column-Stores for Wide and Sparse Data” by Daniel J. Abadi.\n368 | Chapter 12: HBaseCHAPTER 13\nZooKeeper\nSo far in this book, we have been studying large-scale data processing. This chapter is\ndifferent: it is about building general distributed applications using Hadoop’s distrib-\nuted coordination service, called ZooKeeper.\nWriting distributed applications is hard. It’s hard primarily because of partial failure.\nWhen a message is sent across the network between two nodes and the network fails,\nthe sender does not know whether the receiver got the message. It may have gotten\nthrough before the network failed, or it may not have. Or perhaps the receiver’s process\ndied. The only way that the sender can find out what happened is to reconnect to the\nreceiver and ask it. This is partial failure: when we don’t even know if an operation\nfailed.\nZooKeeper can’t make partial failures go away, since they are intrinsic to distributed\nsystems. It certainly does not hide partial failures, either.* But what ZooKeeper does\ndo is give you a set of tools to build distributed applications that can safely handle\npartial failures.\nZooKeeper also has the following characteristics:\nZooKeeper is simple\nZooKeeper is, at its core, a stripped-down filesystem that exposes a few simple\noperations, and some extra abstractions such as ordering and notifications.\nZooKeeper is expressive\nThe ZooKeeper primitives are a rich set of building blocks that can be used to build\na large class of coordination data structures and protocols. Examples include: dis-\ntributed queues, distributed locks, and leader election among a group of peers.\n* This is the message of J. Waldo et al., “A Note on Distributed Computing,” (1994), http://research.sun.com/\ntechrep/1994/smli_tr-94-29.pdf. That is, distributed programming is fundamentally different from local\nprogramming, and the differences cannot simply be papered over.\n369ZooKeeper is highly available\nZooKeeper runs on a collection of machines and is designed to be highly available,\nso applications can depend on it. ZooKeeper can help you avoid introducing single\npoints of failure into your system, so you can build a reliable application.\nZooKeeper facilitates loosely coupled interactions\nZooKeeper interactions support participants that do not need to know about one\nanother. For example, ZooKeeper can be used as a rendezvous mechanism so that\nprocesses that otherwise don’t know of each other’s existence (or network details)\ncan discover each other and interact. Coordinating parties may not even be con-\ntemporaneous, since one process may leave a message in ZooKeeper that is read\nby another after the first has shut down.\nZooKeeper is a library\nZooKeeper provides an open source, shared repository of implementations and\nrecipes of common coordination patterns. Individual programmers are spared the\nburden of writing common protocols themselves (which are often difficult to get\nright). Over time the community can add to, and improve, the libraries, which is\nto everyone’s benefit.\nZooKeeper is highly performant, too. At Yahoo!, where it was created, ZooKeeper’s\nthroughput has been benchmarked at approximately 10,000 operations per second for\nwrite-dominant workloads. For workloads where reads dominate, which is the norm,\nthe throughput is several times higher.\nInstalling and Running ZooKeeper\nWhen trying out ZooKeeper for the first time, it’s simplest to run it in standalone mode\nwith a single ZooKeeper server. You can do this on a development machine, for exam-\nple. ZooKeeper requires Java 6 to run, so make sure you have it installed first. If you\nare running ZooKeeper on Windows (Windows is supported only as a development\nplatform, not as a production platform), you need to install Cygwin, too.\nDownload a stable release of ZooKeeper from the Apache ZooKeeper releases page at\nhttp://hadoop.apache.org/zookeeper/releases.html, and unpack the tarball in a suitable\nlocation:\n% tar xzf zookeeper-x.y.z.tar.gz\nZooKeeper provides a few binaries to run and interact with the service, and it’s con-\nvenient to put the directory containing the binaries on your command-line path:\n% export ZOOKEEPER_INSTALL=/home/tom/zookeeper-x.y.z\n% export PATH=$PATH:$ZOOKEEPER_INSTALL/bin\nBefore running the ZooKeeper service, we need to set up a configuration file. The con-\nfiguration file is conventionally called zoo.cfg and placed in the conf subdirectory (al-\n370 | Chapter 13: ZooKeeperthough you can also place it in /etc/zookeeper, or in the directory defined by the\nZOOCFGDIR environment variable, if set). Here’s an example:\ntickTime=2000\ndataDir=/Users/tom/zookeeper\nclientPort=2181\nThis is a standard Java properties file, and the three properties defined in this example\nare the minimum required for running ZooKeeper in standalone mode. Briefly,\ntickTime is the basic time unit in ZooKeeper (specified in milliseconds), dataDir is the\nlocal filesystem location where ZooKeeper stores persistent data, and clientPort is the\nport the ZooKeeper listens on for client connections (2181 is a common choice). You\nshould change dataDir to an appropriate setting for your system.\nWith a suitable configuration defined, we are now ready to start a local ZooKeeper\nserver:\n% zkServer.sh start\nTo check whether ZooKeeper is running, send the ruok command (“Are you OK?”) to\nthe client port using nc (telnet works, too):\n% echo ruok | nc localhost 2181\nimok\nThat’s ZooKeeper saying “I’m OK.” There are other commands, known as the “four-\nletter words,” for interacting with ZooKeeper. Most are queries: dump lists sessions and\nephemeral znodes, envi lists server properties, reqs lists outstanding requests, and\nstat lists service statistics and connected clients. However, you can also update Zoo-\nKeeper’s state: srst resets the service statistics, and kill shuts down ZooKeeper if\nissued from the host running the ZooKeeper server.\nFor more extensive ZooKeeper monitoring, have a look at its JMX support, which is\ncovered in the ZooKeeper documentation (linked from http://hadoop.apache.org/zoo\nkeeper/).\nAn Example\nImagine a group of servers that provide some service to clients. We want clients to be\nable to locate one of the servers, so they can use the service. One of the challenges is\nmaintaining the list of servers in the group.\nThe membership list clearly cannot be stored on a single node in the network, as the\nfailure of that node would mean the failure of the whole system (we would like the list\nto be highly available). Suppose for a moment that we had a robust way of storing the\nlist. We would still have the problem of how to remove a server from the list if it failed.\nSome process needs to be responsible for removing failed servers, but note that it can’t\nbe the servers themselves, since they are no longer running!\nAn Example | 371What we are describing is not a passive distributed data structure, but an active one,\nand one that can change the state of an entry when some external event occurs. Zoo-\nKeeper provides this service, so let’s see how to build this group membership applica-\ntion (as it is known) with it.\nGroup Membership in ZooKeeper\nOne way of understanding ZooKeeper is to think of it as providing a high-availability\nfilesystem. It doesn’t have files and directories, but a unified concept of a node, called\na znode, which acts both as a container of data (like a file) and a container of other\nznodes (like a directory). Znodes form a hierarchical namespace, and a natural way to\nbuild a membership list is to create a parent znode with the name of the group, and\nchild znodes with the name of the group members (servers). This is shown in Fig-\nure 13-1.\nFigure 13-1. ZooKeeper znodes\nIn this example, we won’t store data in any of the znodes, but in a real application, you\ncould imagine storing data about the members in their znodes, such as hostname.\nCreating the Group\nLet’s introduce ZooKeeper’s Java API by writing a program to create a znode for the\ngroup, /zoo in this example. See Example 13-1.\nExample 13-1. A program to create a znode representing a group in ZooKeeper\npublic class CreateGroup implements Watcher {\n372 | Chapter 13: ZooKeeperprivate static final int SESSION_TIMEOUT = 5000;\nprivate ZooKeeper zk;\nprivate CountDownLatch connectedSignal = new CountDownLatch(1);\npublic void connect(String hosts) throws IOException, InterruptedException {\nzk = new ZooKeeper(hosts, SESSION_TIMEOUT, this);\nconnectedSignal.await();\n}\n@Override\npublic void process(WatchedEvent event) { // Watcher interface\nif (event.getState() == KeeperState.SyncConnected) {\nconnectedSignal.countDown();\n}\n}\npublic void create(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\nString createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\nSystem.out.println(""Created "" + createdPath);\n}\npublic void close() throws InterruptedException {\nzk.close();\n}\n}\npublic static void main(String[] args) throws Exception {\nCreateGroup createGroup = new CreateGroup();\ncreateGroup.connect(args[0]);\ncreateGroup.create(args[1]);\ncreateGroup.close();\n}\nWhen the main() method is run, it creates a CreateGroup instance and then calls its\nconnect() method. This method instantiates a new ZooKeeper object, the main class of\nthe client API and the one that maintains the connection between the client and the\nZooKeeper service. The constructor takes three arguments: the first is the host address\n(and optional port, which defaults to 2181) of the ZooKeeper service;† the second is\nthe session timeout in milliseconds (which we set to 5 seconds), explained in more\ndetail later; and the third is an instance of a Watcher object. The Watcher object receives\ncallbacks from ZooKeeper to inform it of various events. In this case, CreateGroup is a\nWatcher, so we pass this to the ZooKeeper constructor.\nWhen a ZooKeeper instance is created, it starts a thread to connect to the ZooKeeper\nservice. The call to the constructor returns immediately, so it is important to wait for\n† For a replicated ZooKeeper service, this parameter is the comma-separated list of servers (host and optional\nport) in the ensemble.\nAn Example | 373the connection to be established before using the ZooKeeper object. We make use of\nJava’s CountDownLatch class (in the java.util.concurrent package) to block until the\nZooKeeper instance is ready. This is where the Watcher comes in. The Watcher interface\nhas a single method:\npublic void process(WatchedEvent event);\nWhen the client has connected to ZooKeeper, the Watcher receives a call to its\nprocess() method with an event indicating that it has connected. On receiving a con-\nnection event (represented by the Watcher.Event.KeeperState enum, with value\nSyncConnected), we decrement the counter in the CountDownLatch, using its count\nDown() method. The latch was created with a count of one, representing the number of\nevents that need to occur before it releases all waiting threads. After calling count\nDown() once, the counter reaches zero and the await() method returns.\nThe connect() method has now returned, and the next method to be invoked on the\nCreateGroup is the create() method. In this method, we create a new ZooKeeper znode\nusing the create() method on the ZooKeeper instance. The arguments it takes are the\npath (represented by a string), the contents of the znode (a byte array, null here), an\naccess control list (or ACL for short, which here is a completely open ACL, allowing\nany client to read or write the znode), and the nature of the znode to be created.\nZnodes may be ephemeral or persistent. An ephemeral znode will be deleted by the\nZooKeeper service when the client that created it disconnects, either by explicitly dis-\nconnecting or if the client terminates for whatever reason. A persistent znode, on the\nother hand, is not deleted when the client disconnects. We want the znode representing\na group to live longer than the lifetime of the program that creates it, so we create a\npersistent znode.\nThe return value of the create() method is the path that was created by ZooKeeper.\nWe use it to print a message that the path was successfully created. We will see how\nthe path returned by create() may differ from the one passed in to the method when\nwe look at sequential znodes.\nTo see the program in action, we need to have ZooKeeper running on the local machine,\nand then we can type:\n% export CLASSPATH=build/classes:$ZOOKEEPER_INSTALL/*:$ZOOKEEPER_INSTALL/lib/*:\\\n$ZOOKEEPER_INSTALL/conf\n% java CreateGroup localhost zoo\nCreated /zoo\nJoining a Group\nThe next part of the application is a program to register a member in a group. Each\nmember will run as a program and join a group. When the program exits, it should be\nremoved from the group, which we can do by creating an ephemeral znode that rep-\nresents it in the ZooKeeper namespace.\n374 | Chapter 13: ZooKeeperThe JoinGroup program implements this idea, and its listing is in Example 13-2. The\nlogic for creating and connecting to a ZooKeeper instance has been refactored into a base\nclass, ConnectionWatcher, and appears in Example 13-3.\nExample 13-2. A program that joins a group\npublic class JoinGroup extends ConnectionWatcher {\npublic void join(String groupName, String memberName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName + ""/"" + memberName;\nString createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE,\nCreateMode.EPHEMERAL);\nSystem.out.println(""Created "" + createdPath);\n}\npublic static void main(String[] args) throws Exception {\nJoinGroup joinGroup = new JoinGroup();\njoinGroup.connect(args[0]);\njoinGroup.join(args[1], args[2]);\n}\n}\n// stay alive until process is killed or thread is interrupted\nThread.sleep(Long.MAX_VALUE);\nExample 13-3. A helper class that waits for the connection to ZooKeeper to be established\npublic class ConnectionWatcher implements Watcher {\nprivate static final int SESSION_TIMEOUT = 5000;\nprotected ZooKeeper zk;\nprivate CountDownLatch connectedSignal = new CountDownLatch(1);\npublic void connect(String hosts) throws IOException, InterruptedException {\nzk = new ZooKeeper(hosts, SESSION_TIMEOUT, this);\nconnectedSignal.await();\n}\n@Override\npublic void process(WatchedEvent event) {\nif (event.getState() == KeeperState.SyncConnected) {\nconnectedSignal.countDown();\n}\n}\n}\npublic void close() throws InterruptedException {\nzk.close();\n}\nThe code for JoinGroup is very similar to CreateGroup. It creates an ephemeral znode as\na child of the group znode in its join() method, then simulates doing work of some\nAn Example | 375kind by sleeping until the process is forcibly terminated. Later, you will see that upon\ntermination, the ephemeral znode is removed by ZooKeeper.\nListing Members in a Group\nNow we need a program to find the members in a group (see Example 13-4).\nExample 13-4. A program to list the members in a group\npublic class ListGroup extends ConnectionWatcher {\npublic void list(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\n}\n}\ntry {\nList<String> children = zk.getChildren(path, false);\nif (children.isEmpty()) {\nSystem.out.printf(""No members in group %s\\n"", groupName);\nSystem.exit(1);\n}\nfor (String child : children) {\nSystem.out.println(child);\n}\n} catch (KeeperException.NoNodeException e) {\nSystem.out.printf(""Group %s does not exist\\n"", groupName);\nSystem.exit(1);\n}\npublic static void main(String[] args) throws Exception {\nListGroup listGroup = new ListGroup();\nlistGroup.connect(args[0]);\nlistGroup.list(args[1]);\nlistGroup.close();\n}\nIn the list() method, we call getChildren() with a znode path and a watch flag to\nretrieve a list of child paths for the znode, which we print out. Placing a watch on a\nznode causes the registered Watcher to be triggered if the znode changes state. Although\nwe’re not using it here, watching a znode’s children would permit a program to get\nnotifications of members joining or leaving the group, or of the group being deleted.\nWe catch KeeperException.NoNodeException, which is thrown in the case when the\ngroup’s znode does not exist.\nLet’s see ListGroup in action. As expected, the zoo group is empty, since we haven’t\nadded any members yet:\n% java ListGroup localhost zoo\nNo members in group zoo\n376 | Chapter 13: ZooKeeperWe can use JoinGroup to add some members. We launch them as background pro-\ncesses, since they don’t terminate on their own (due to the sleep statement):\n%\n%\n%\n%\njava JoinGroup localhost zoo duck &\njava JoinGroup localhost zoo cow &\njava JoinGroup localhost zoo goat &\ngoat_pid=$!\nThe last line saves the process ID of the Java process running the program that adds\ngoat as a member. We need to remember the ID so that we can kill the process in a\nmoment, after checking the members:\n% java ListGroup localhost zoo\ngoat\nduck\ncow\nTo remove a member, we kill its process:\n% kill $goat_pid\nAnd a few seconds later, it has disappeared from the group because the process’s Zoo-\nKeeper session has terminated (the timeout was set to 5 seconds) and its associated\nephemeral node has been removed:\n% java ListGroup localhost zoo\nduck\ncow\nLet’s stand back and see what we’ve built here. We have a way of building up a list of\na group of nodes that are participating in a distributed system. The nodes may have no\nknowledge of each other. A service that wants to use the nodes in the list to perform\nsome work, for example, can discover the nodes without them being aware of the serv-\nice’s existence.\nFinally, note that group membership is not a substitution for handling network errors\nwhen communicating with a node. Even if a node is a group member, communications\nwith it may fail, and such failures must be handled in the usual manner.\nZooKeeper command-line tools\nZooKeeper comes with a command-line tool for interacting with the ZooKeeper name-\nspace. We can use it to list the znodes under the /zoo znode as follows:\n% zkCli.sh localhost ls /zoo\nProcessing ls\nWatchedEvent: Server state change. New state: SyncConnected\n[duck, cow]\nYou can run the command without arguments to display usage instructions.\nAn Example | 377Deleting a Group\nTo round off the example, let’s see how to delete a group. The ZooKeeper class provides\na delete() method that takes a path and a version number. ZooKeeper will delete a\nznode only if the version number specified is the same as the version number of the\nznode it is trying to delete, an optimistic locking mechanism that allows clients to detect\nconflicts over znode modification. You can bypass the version check, however, by using\na version number of –1 to delete the znode regardless of its version number.\nThere is no recursive delete operation in ZooKeeper, so you have to delete child znodes\nbefore parents. This is what we do in the DeleteGroup class, which will remove a group\nand all its members (Example 13-5).\nExample 13-5. A program to delete a group and its members\npublic class DeleteGroup extends ConnectionWatcher {\npublic void delete(String groupName) throws KeeperException,\nInterruptedException {\nString path = ""/"" + groupName;\n}\n}\ntry {\nList<String> children = zk.getChildren(path, false);\nfor (String child : children) {\nzk.delete(path + ""/"" + child, -1);\n}\nzk.delete(path, -1);\n} catch (KeeperException.NoNodeException e) {\nSystem.out.printf(""Group %s does not exist\\n"", groupName);\nSystem.exit(1);\n}\npublic static void main(String[] args) throws Exception {\nDeleteGroup deleteGroup = new DeleteGroup();\ndeleteGroup.connect(args[0]);\ndeleteGroup.delete(args[1]);\ndeleteGroup.close();\n}\nFinally, we can delete the zoo group that we created earlier:\n% java DeleteGroup localhost zoo\n% java ListGroup localhost zoo\nGroup zoo does not exist\nThe ZooKeeper Service\nZooKeeper is a highly available, high-performance coordination service. In this section,\nwe look at the nature of the service it provides: its model, operations, and\nimplementation.\n378 | Chapter 13: ZooKeeperData Model\nZooKeeper maintains a hierarchical tree of nodes called znodes. A znode stores data\nand has an associated ACL. ZooKeeper is designed for coordination (which typically\nuses small data files), not high-volume data storage, so there is a limit of 1 MB on the\namount of data that may be stored in any znode.\nData access is atomic. A client reading the data stored at a znode will never receive only\nsome of the data; the data will be delivered in its entirety (or the read will fail). Similarly,\na write will replace all the data associated with a znode. ZooKeeper guarantees that the\nwrite will either succeed or fail; there is no such thing as a partial write, where only\nsome of the data written by the client is stored. ZooKeeper does not support an append\noperation. These characteristics contrast with HDFS, which is designed for high-\nvolume data storage, with streaming data access, and provides an append operation.\nZnodes are referenced by paths, which in ZooKeeper are represented as slash-delimited\nUnicode character strings, like filesystem paths in Unix. Paths must be absolute, so\nthey must begin with a slash character. Furthermore, they are canonical, which means\nthat each path has a single representation, and so paths do not undergo resolution. For\nexample, in Unix, a file with the path /a/b can equivalently be referred to by the\npath /a/./b, since “.” refers to the current directory at the point it is encountered in the\npath. In ZooKeeper “.” does not have this special meaning, and is actually illegal as a\npath component (as is “..” for the parent of the current directory).\nPath components are composed of Unicode characters, with a few restrictions (these\nare spelled out in the ZooKeeper reference documentation). The string “zookeeper” is\na reserved word, and may not be used as a path component. In particular, ZooKeeper\nuses the /zookeeper subtree to store management information, such as information on\nquotas.\nNote that paths are not URIs, and they are represented in the Java API by a\njava.lang.String, rather than the Hadoop Path class (or by the java.net.URI class, for\nthat matter).\nZnodes have some properties that are very useful for building distributed applications,\nwhich we discuss in the following sections.\nEphemeral znodes\nZnodes can be one of two types: ephemeral or persistent. A znode’s type is set at creation\ntime and may not be changed later. An ephemeral znode is deleted by ZooKeeper when\nthe creating client’s session ends. By contrast, a persistent znode is not tied to the client’s\nsession, and is deleted only when explicitly deleted by a client (not necessarily the one\nthat created it). An ephemeral znode may not have children, not even ephemeral ones.\nEven though ephemeral nodes are tied to a client session, they are visible to all clients\n(subject to their ACL policy, of course).\nThe ZooKeeper Service | 379Ephemeral znodes are ideal for building applications that needs to know when certain\ndistributed resources are available. The example earlier in this chapter uses ephemeral\nznodes to implement a group membership service, so any process can discover the\nmembers of the group at any particular time.\nSequence numbers\nA sequential znode is given a sequence number by ZooKeeper as a part of its name. If\na znode is created with the sequential flag set, then the value of a monotonically in-\ncreasing counter (maintained by the parent znode) is appended to its name.\nIf a client asks to create a sequential znode with the name /a/b-, for example, then the\nznode created may actually have the name /a/b-3.‡ If, later on, another sequential znode\nwith the name /a/b- is created, then it will be given a unique name with a larger value\nof the counter—for example, /a/b-5. In the Java API, the actual path given to sequential\nznodes is communicated back to the client as the return value of the create() call.\nSequence numbers can be used to impose a global ordering on events in a distributed\nsystem, and may be used by the client to infer the ordering. In “A Lock Serv-\nice” on page 398, you will learn how to use sequential znodes to build a shared lock.\nWatches\nWatches allow clients to get notifications when a znode changes in some way. Watches\nare set by operations on the ZooKeeper service, and are triggered by other operations\non the service. For example, a client might call the exists operation on a znode, placing\na watch on it at the same time. If the znode doesn’t exist, then the exists operation\nwill return false. If, some time later, the znode is created by a second client, then the\nwatch is triggered, notifying the first client of the znode’s creation. You will see precisely\nwhich operations trigger others in the next section.\nWatchers are triggered only once.§ To receive multiple notifications, a client needs to\nreregister the watch. If the client in the previous example wishes to receive further\nnotifications for the znode’s existence (to be notified when it is deleted, for example),\nit needs to call the exists operation again to set a new watch.\nThere is an example in “A Configuration Service” on page 391 demonstrating how to\nuse watches to update configuration across a cluster.\nOperations\nThere are nine basic operations in ZooKeeper, listed in Table 13-1.\n‡ It is conventional (but not required) to have a trailing dash on path names for sequential nodes, to make their\nsequence numbers easy to read and parse (by the application).\n§ Except for callbacks for connection events, which do not need re-registration.\n380 | Chapter 13: ZooKeeperTable 13-1. Operations in the ZooKeeper service\nOperation Description\ncreate Creates a znode (the parent znode must already exist)\ndelete Deletes a znode (the znode may not have any children)\nexists Tests whether a znode exists and retrieves its metadata\ngetACL, setACL Gets/sets the ACL for a znode\ngetChildren Gets a list of the children of a znode\ngetData, setData Gets/sets the data associated with a znode\nsync Synchronizes a client’s view of a znode with ZooKeeper\nUpdate operations in ZooKeeper are conditional. A delete or setData operation has to\nspecify the version number of the znode that is being updated (which is found from a\nprevious exists call). If the version number does not match, the update will fail. Up-\ndates are a nonblocking operation, so a client that loses an update (because another\nprocess updated the znode in the meantime) can decide whether to try again or take\nsome other action, and it can do so without blocking the progress of any other process.\nAlthough ZooKeeper can be viewed as a filesystem, there are some filesystem primitives\nthat it does away with in the name of simplicity. Because files are small and are written\nand read in their entirety, there is no need to provide open, close, or seek operations.\nThe sync operation is not like fsync() in POSIX filesystems. As men-\ntioned earlier, writes in ZooKeeper are atomic, and a successful write\noperation is guaranteed to have been written to persistent storage on a\nmajority of ZooKeeper servers. However, it is permissible for reads to\nlag the latest state of ZooKeeper service, and the sync operation exists\nto allow a client to bring itself up-to-date. This topic is covered in more\ndetail in the section on “Consistency” on page 386.\nAPIs\nThere are two core language bindings for ZooKeeper clients, one for Java and one for\nC; there are also contrib bindings for Perl, Python, and REST clients. For each binding,\nthere is a choice between performing operations synchronously or asynchronously.\nWe’ve already seen the synchronous Java API. Here’s the signature for the exists op-\neration, which returns a Stat object encapsulating the znode’s metadata, or null if the\nznode doesn’t exist:\npublic Stat exists(String path, Watcher watcher) throws KeeperException,\nInterruptedException\nThe asynchronous equivalent, which is also found in the ZooKeeper class, looks like this:\npublic void exists(String path, Watcher watcher, StatCallback cb, Object ctx)\nThe ZooKeeper Service | 381In the Java API, all the asynchronous methods have void return types, since the result\nof the operation is conveyed via a callback. The caller passes a callback implementation,\nwhose method is invoked when a response is received from ZooKeeper. In this case,\nthe callback is the StatCallback interface, which has the following method:\npublic void processResult(int rc, String path, Object ctx, Stat stat);\nThe rc argument is the return code, corresponding to the codes defined by KeeperEx\nception. A nonzero code represents an exception, in which case the stat parameter will\nbe null. The path and ctx arguments correspond to the equivalent arguments passed\nby the client to the exists() method, and can be used to identify the request for which\nthis callback is a response. The ctx parameter can be an arbitrary object that may be\nused by the client when the path does not give enough context to disambiguate the\nrequest. If not needed, it may be set to null.\nThere are actually two C shared libraries. The single-threaded library, zookeeper_st,\nsupports only the asynchronous API and is intended for platforms where the pthread\nlibrary is not available or stable. Most developers will use the multithreaded library,\nzookeeper_mt, as it supports both the synchronous and asynchronous APIs. For details\non how to build and use the C API, please refer to the README file in the src/c directory\nof the ZooKeeper distribution.\nShould I Use the Synchronous or Asynchronous API?\nBoth APIs offer the same functionality, so the one you use is largely a matter of style.\nThe asynchronous API is appropriate if you have an event-driven programming model,\nfor example.\nThe asynchronous API allows you to pipeline requests, which in some scenarios can\noffer better throughput. Imagine that you want to read a large batch of znodes and\nprocess them independently. Using the synchronous API, each read would block until\nit returned, whereas with the asynchronous API, you can fire off all the asynchronous\nreads very quickly and process the responses in a separate thread as they come back.\nWatch triggers\nThe read operations exists, getChildren, and getData may have watches set on them,\nand the watches are triggered by write operations: create, delete, and setData. ACL\noperations do not participate in watches. When a watch is triggered, a watch event is\ngenerated, and the watch event’s type depends both on the watch and the operation\nthat triggered it:\n• A watch set on an exists operation will be triggered when the znode being watched\nis created, deleted, or has its data updated.\n382 | Chapter 13: ZooKeeper• A watch set on a getData operation will be triggered when the znode being watched\nis deleted or has its data updated. No trigger can occur on creation, since the znode\nmust already exist for the getData operation to succeed.\n• A watch set on a getChildren operation will be triggered when a child of the znode\nbeing watched is created or deleted, or when the znode itself is deleted. You can\ntell whether the znode or its child was deleted by looking at the watch event type:\nNodeDeleted shows the znode was deleted, and NodeChildrenChanged indicates that\nit was a child that was deleted.\nThe combinations are summarized in Table 13-2.\nTable 13-2. Watch creation operations and their corresponding triggers\nWatch trigger\nWatch creation\ncreate\nznode\nexists\ndelete\nchild\nNodeCreated\nchild\nNodeDeleted \nNodeChildren\nChanged\nNodeData\nChanged\nNodeDeleted \ngetData\ngetChildren\nznode\nsetData\nNodeData\nChanged\nNodeDeleted\nNodeChildren\nChanged\nA watch event includes the path of the znode that was involved in the event, so for\nNodeCreated and NodeDeleted events, you can tell which node was created or deleted\nsimply by inspecting the path. To discover which children have changed after a Node\nChildrenChanged event, you need to call getChildren again to retrieve the new list of\nchildren. Similarly, to discover the new data for a NodeDataChanged event, you need to\ncall getData. In both of these cases, the state of the znodes may have changed between\nreceiving the watch event and performing the read operation, so you should bear this\nin mind when writing applications.\nACLs\nA znode is created with a list of ACLs, which determines who can perform certain\noperations on it.\nACLs depend on authentication, the process by which the client identifies itself to\nZooKeeper. There are a few authentication schemes that ZooKeeper provides:\ndigest\nThe client is identified by a username and password.\nhost\nThe client is identified by his hostname.\nThe ZooKeeper Service | 383ip\nThe client is identified by his IP address.\nClients may authenticate themselves after establishing a ZooKeeper session. Authen-\ntication is optional, although a znode’s ACL may require an authenticated client, in\nwhich case the client must authenticate itself to access the znode. Here is an example\nof using the digest scheme to authenticate with a username and password:\nzk.addAuthInfo(""digest"", ""tom:secret"".getBytes());\nAn ACL is the combination of an authentication scheme, an identity for that scheme,\nand a set of permissions. For example, if we wanted to give clients in the domain\nexample.com read access to a znode, we would set an ACL on the znode with the\nhost scheme, an ID of example.com, and READ permission. In Java, we would create the\nACL object as follows:\nnew ACL(Perms.READ, new Id(""host"", ""example.com""));\nThe full set of permissions are listed in Table 13-3. Note that the exists operation is\nnot governed by an ACL permission, so any client may call exists to find the Stat for\na znode or to discover that a znode does not in fact exist.\nTable 13-3. ACL permissions\nACL permission Permitted operations\nCREATE create (a child znode)\nREAD getChildren\ngetData\nWRITE setData\nDELETE delete (a child znode)\nADMIN setACL\nThere are a number of predefined ACLs defined in the ZooDefs.Ids class, including\nOPEN_ACL_UNSAFE, which gives all permissions (except ADMIN permission) to everyone.\nIn addition, ZooKeeper has a pluggable authentication mechanism, which makes it\npossible to integrate third-party authentication systems if needed.\nImplementation\nThe ZooKeeper service can run in two modes. In standalone mode, there is a single\nZooKeeper server, which is useful for testing due to its simplicity (it can even be em-\nbedded in unit tests), but provides no guarantees of high-availability or resilience. In\nproduction, ZooKeeper runs in replicated mode, on a cluster of machines called an\nensemble. ZooKeeper achieves high-availability through replication, and can provide a\nservice as long as a majority of the machines in the ensemble are up. For example, in a\nfive-node ensemble, any two machines can fail and the service will still work because\n384 | Chapter 13: ZooKeepera majority of three remain. Note that a six-node ensemble can also tolerate only two\nmachines failing, since with three failures the remaining three do not constitute a ma-\njority of the six. For this reason, it is usual to have an odd number of machines in an\nensemble.\nConceptually, ZooKeeper is very simple: all it has to do is ensure that every modification\nto the tree of znodes is replicated to a majority of the ensemble. If a minority of the\nmachines fail, then a minimum of one machine will survive with the latest state. The\nother remaining replicas will eventually catch up with this state.\nThe implementation of this simple idea, however, is nontrivial. ZooKeeper uses a pro-\ntocol called Zab that runs in two phases, which may be repeated indefinitely:\nPhase 1: Leader election\nThe machines in an ensemble go through a process of electing a distinguished\nmember, called the leader. The other machines are termed followers. This phase is\nfinished once a majority (or quorum) of followers have synchronized their state\nwith the leader.\nPhase 2: Atomic broadcast\nAll write requests are forwarded to the leader, which broadcasts the update to the\nfollowers. When a majority have persisted the change, the leader commits the up-\ndate, and the client gets a response saying the update succeeded. The protocol for\nachieving consensus is designed to be atomic, so a change either succeeds or fails.\nIt resembles two-phase commit.\nDoes ZooKeeper Use Paxos?\nNo. ZooKeeper’s Zab protocol is not the same as the well-known Paxos algorithm\n(Leslie Lamport, “Paxos Made Simple,” ACM SIGACT News [Distributed Computing\nColumn] 32, 4 [Whole Number 121, December 2001] 51–58.). Zab is similar, but it\ndiffers in several aspects of its operation, such as relying on TCP for its message ordering\nguarantees.\nZab is described in “A simple totally ordered broadcast protocol” by Benjamin Reed\nand Flavio Junqueira (Proceedings of the Second Workshop on Large-Scale Distributed\nSystems and Middleware [LADIS’08] 2008, IBM TJ Watson Research Center, York-\ntown Heights, NY, USA. To appear in ACM International Conference Proceedings\nSeries, ACM Press, 2009. ISBN: 978-1-60558-296-2).\nGoogle’s Chubby Lock Service (Mike Burrows, “The Chubby Lock Service for Loosely-\nCoupled Distributed Systems,” November 2006, http://labs.google.com/papers/chubby\n.html), which shares similar goals with ZooKeeper, is based on Paxos.\nThe ZooKeeper Service | 385If the leader fails, the remaining machines hold another leader election and continue\nas before with the new leader. If the old leader later recovers, it then starts as a follower.\nLeader election is very fast, around 200 ms according to one published result,‖ so per-\nformance does not noticeably degrade during an election.\nAll machines in the ensemble write updates to disk before updating their in-memory\ncopy of the znode tree. Read requests may be serviced from any machine, and since\nthey involve only a lookup from memory, they are very fast.\nConsistency\nUnderstanding the basis of ZooKeeper’s implementation helps in understanding the\nconsistency guarantees that the service makes. The terms “leader” and “follower” for\nthe machines in an ensemble are apt, for they make the point that a follower may lag\nthe leader by a number of updates. This is a consequence of the fact that only a majority\nand not all of the ensemble needs to have persisted a change before it is committed. A\ngood mental model for ZooKeeper is of clients connected to ZooKeeper servers that\nare following the leader. A client may actually be connected to the leader, but it has no\ncontrol over this, and cannot even know if this is the case.# See Figure 13-2.\nFigure 13-2. Reads are satisfied by followers, while writes are committed by the leader\n‖ Reported by Yahoo! at http://hadoop.apache.org/zookeeper/docs/current/zookeeperOver.html.\n# It is possible to configure ZooKeeper so that the leader does not accept client connections. In this case, its\nonly job is to coordinate updates. Do this by setting the leaderServes property to no. This is recommended\nfor ensembles of more than three servers.\n386 | Chapter 13: ZooKeeperEvery update made to the znode tree is given a globally unique identifier, called a\nzxid (which stands for “ZooKeeper transaction ID”). Updates are ordered, so if zxid\nz1 is less than z2, then z1 happened before z2, according to ZooKeeper, which is the\nsingle authority on ordering in the distributed system.\nThe following guarantees for data consistency flow from ZooKeeper’s design:\nSequential consistency\nUpdates from any particular client are applied in the order that they are sent. This\nmeans that if a client updates the znode z to the value a, and in a later operation,\nit updates z to the value b, then no client will ever see z with value a after it has\nseen it with value b (if no other updates are made to z).\nAtomicity\nUpdates either succeed or fail. This means that if an update fails, no client will ever\nsee it.\nSingle system image\nA client will see the same view of the system regardless of the server it connects to.\nThis means that if a client connects to a new server during the same session, it will\nnot see an older state of the system than the one it saw with the previous server.\nWhen a server fails and a client tries to connect to another in the ensemble, a server\nthat is behind the one that failed will not accept connections from the client until\nit has caught up with the failed server.\nDurability\nOnce an update has succeeded, it will persist and will not be undone. This means\nupdates will survive server failures.\nTimeliness\nThe lag in any client’s view of the system is bounded, so it will not be out of date\nby more than some multiple of tens of seconds. This means that rather than allow\na client to see data that is very stale, a server will shut down, forcing the client to\nswitch to a more up-to-date server.\nFor performance reasons, reads are satisfied from a ZooKeeper’s server’s memory and\ndo not participate in the global ordering of writes. This property can lead to the ap-\npearance of inconsistent ZooKeeper states from clients that communicate through a\nmechanism outside ZooKeeper.\nFor example, client A updates znode z from a to a', A tells B to read z, B reads the value\nof z as a, not a'. This is perfectly compatible with the guarantees that ZooKeeper makes\n(this condition that it does not promise is called “Simultaneously Consistent Cross-\nClient Views”). To prevent this condition from happening, B should call sync on z,\nbefore reading z’s value. The sync operation forces the ZooKeeper server that B is con-\nnected to to “catch up” with the leader, so that when B reads z’s value it will be the one\nthat A set (or a later value).\nThe ZooKeeper Service | 387Slightly confusingly, the sync operation is only available as an asyn-\nchronous call. The reason for this is that you don’t need to wait for it to\nreturn, since ZooKeeper guarantees that any subsequent operation will\nhappen after the sync completes on the server, even if the operation is\nissued before the sync completes.\nSessions\nA ZooKeeper client is configured with the list of servers in the ensemble. On startup,\nit tries to connect to one of the servers in the list. If the connection fails, it tries another\nserver in the list, and so on, until it either successfully connects to one of them, or fails\nif all ZooKeeper servers are unavailable.\nOnce a connection has been made with a ZooKeeper server, the server creates a new\nsession for the client. A session has a timeout period that is decided on by the appli-\ncation that creates it. If the server hasn’t received a request within the timeout period,\nit may expire the session. Once a session has expired, it may not be reopened, and any\nephemeral nodes associated with the session will be lost. Although session expiry is a\ncomparatively rare event, since sessions are long-lived, it is important for applications\nto handle it (which you will see how in “The Resilient ZooKeeper Applica-\ntion” on page 394).\nSessions are kept alive by the client sending ping requests (also known as heartbeats)\nwhenever the session is idle for longer than a certain period. (Pings are automatically\nsent by the ZooKeeper client library, so your code doesn’t need to worry about main-\ntaining the session.) The period is chosen to be low enough to detect server failure\n(manifested by a read timeout) and reconnect to another server within the session\ntimeout period.\nFailover to another ZooKeeper server is handled automatically by the ZooKeeper client,\nand, crucially, sessions (and associated ephemeral znodes) are still valid after another\nserver takes over from the failed one.\nDuring failover, the application will receive notifications of disconnections and con-\nnections to the service. Watch notifications will not be delivered while the client is\ndisconnected, but they will be delivered when the client successfully reconnects. Also,\nif the application tries to perform an operation while the client is reconnecting to\nanother server, the operation will fail. This underlines the importance of handling con-\nnection loss exceptions in real-world ZooKeeper applications (described in “The Re-\nsilient ZooKeeper Application” on page 394).\nTime\nThere are several time parameters in ZooKeeper. The tick time is the fundamental period\nof time in ZooKeeper and is used by servers in the ensemble to define the schedule on\nwhich their interactions run. Other settings are defined in terms of tick time, or are at\n388 | Chapter 13: ZooKeeperleast constrained by it. The session timeout, for example, may not be less than 2 ticks\nor more than 20. If you attempt to set a session timeout outside this range, it will be\nmodified to fall within the range.\nA common tick time setting is 2 seconds (2,000 milliseconds). This translates to an\nallowable session timeout of between 4 and 40 seconds. There are a few considerations\nin selecting a session timeout.\nA low session timeout leads to faster detection of machine failure. In the group mem-\nbership example, the session timeout is the time it takes for a failed machine to be\nremoved from the group. Beware of setting the session timeout too low, however, since\na busy network can cause packets to be delayed and may cause inadvertent session\nexpiry. In such an event, a machine would appear to “flap”: leaving and then rejoining\nthe group repeatedly in a short space of time.\nApplications that create more complex ephemeral state should favor longer session\ntimeouts, as the cost of reconstruction is higher. In some cases, it is possible to design\nthe application so it can restart within the session timeout period, and avoid session\nexpiry. (This might be desirable to perform maintenance or upgrades.) Every session\nis given a unique identity and password by the server, and if these are passed to Zoo-\nKeeper while a connection is being made, it is possible to recover a session (as long as\nit hasn’t expired). An application can therefore arrange a graceful shutdown, whereby\nit stores the session identity and password to stable storage before restarting the proc-\ness, retrieving the stored session identity and password, and recovering the session.\nYou should view this feature as an optimization, which can help avoid expire sessions.\nIt does not remove the need to handle session expiry, which can still occur if a machine\nfails unexpectedly, or even if an application is shut down gracefully but does not restart\nbefore its session expires—for whatever reason.\nAs a general rule, the larger the ZooKeeper ensemble, the larger the session timeout\nshould be. Connection timeouts, read timeouts, and ping periods are all defined inter-\nnally as a function of the number of servers in the ensemble, so as the ensemble grows,\nthese periods decrease. Consider increasing the timeout if you experience frequent\nconnection loss. You can monitor ZooKeeper metrics—such as request latency\nstatistics—using JMX.\nStates\nThe ZooKeeper object transitions through different states in its lifecycle (see Fig-\nure 13-3). You can query its state at any time by using the getState() method:\npublic States getState()\nStates is an enum representing the different states that a ZooKeeper object may be in.\n(Despite the enum’s name, an instance of ZooKeeper may only be in one state at a time.)\nA newly constructed ZooKeeper instance is in the CONNECTING state, while it tries to\nThe ZooKeeper Service | 389Figure 13-3. ZooKeeper state transitions\nestablish a connection with the ZooKeeper service. Once a connection is established,\nit goes into the CONNECTED state.\nA client using the ZooKeeper object can receive notifications of the state transitions by\nregistering a Watcher object. On entering the CONNECTED state, the watcher receives a\nWatchedEvent whose KeeperState value is SyncConnected.\nA ZooKeeper Watcher object serves double duty: it can be used to be\nnotified of changes in the ZooKeeper state (as described in this section),\nand it can be used to be notified of changes in znodes (described in\n“Watch triggers” on page 382). The (default) watcher passed in to the\nZooKeeper object constructor is used for state changes, but znode\nchanges may either use a dedicated instance of Watcher (by passing one\nin to the appropriate read operation), or they may share the default one\nif using the form of the read operation that takes a boolean flag to specify\nwhether to use a watcher.\nThe ZooKeeper instance may disconnect and reconnect to the ZooKeeper service, mov-\ning between the CONNECTED and CONNECTING states. If it disconnects, the watcher receives\na Disconnected event. Note that these state transitions are initiated by the ZooKeeper\ninstance itself, and it will automatically try to reconnect if the connection is lost.\n390 | Chapter 13: ZooKeeperThe ZooKeeper instance may transition to a third state, CLOSED, if either the close()\nmethod is called or the session times out as indicated by a KeeperState of type\nExpired. Once in the CLOSED state, the ZooKeeper object is no longer considered to be\nalive (this can be tested using the isAlive() method on States), and cannot be reused.\nTo reconnect to the ZooKeeper service, the client must construct a new ZooKeeper\ninstance.\nBuilding Applications with ZooKeeper\nHaving covered ZooKeeper in some depth, let’s turn back to writing some useful ap-\nplications with it.\nA Configuration Service\nOne of the most basic services that a distributed application needs is a configuration\nservice so that common pieces of configuration information can be shared by machines\nin a cluster. At the simplest level, ZooKeeper can act as a highly available store for\nconfiguration, allowing application participants to retrieve or update configuration\nfiles. Using ZooKeeper watches, it is possible to create an active configuration service,\nwhere interested clients are notified of changes in configuration.\nLet’s write such a service. We make a couple of assumptions that simplify the imple-\nmentation (they could be removed with a little more work). First, the only configuration\nvalues we need to store are strings, and keys are just znode paths, so we use a znode to\nstore each key-value pair. Second, there is a single client that performs updates at any\none time. Among other things, this model fits with the idea of a master (such as the\nnamenode in HDFS) that wishes to update information that its workers need to follow.\nWe wrap the code up in a class called ActiveKeyValueStore:\npublic class ActiveKeyValueStore extends ConnectionWatcher {\nprivate static final Charset CHARSET = Charset.forName(""UTF-8"");\n}\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), -1);\n}\n}\nThe contract of the write() method is that a key with the given value is written to\nZooKeeper. It hides the difference between creating a new znode and updating an ex-\nisting znode with a new value, by testing first for the znode using the exists operation\nBuilding Applications with ZooKeeper | 391and then performing the appropriate operation. The other detail worth mentioning is\nthe need to convert the string value to a byte array, for which we just use the\ngetBytes() method with a UTF-8 encoding.\nTo illustrate the use of the ActiveKeyValueStore, consider a ConfigUpdater class that\nupdates a configuration property with a value. The listing appears in Example 13-6.\nExample 13-6. An application that updates a property in ZooKeeper at random times\npublic class ConfigUpdater {\npublic static final String PATH = ""/config"";\nprivate ActiveKeyValueStore store;\nprivate Random random = new Random();\npublic ConfigUpdater(String hosts) throws IOException, InterruptedException {\nstore = new ActiveKeyValueStore();\nstore.connect(hosts);\n}\npublic void run() throws InterruptedException, KeeperException {\nwhile (true) {\nString value = random.nextInt(100) + """";\nstore.write(PATH, value);\nSystem.out.printf(""Set %s to %s\\n"", PATH, value);\nTimeUnit.SECONDS.sleep(random.nextInt(10));\n}\n}\n}\npublic static void main(String[] args) throws Exception {\nConfigUpdater configUpdater = new ConfigUpdater(args[0]);\nconfigUpdater.run();\n}\nThe program is simple. A ConfigUpdater has an ActiveKeyValueStore that connects to\nZooKeeper in ConfigUpdater’s constructor. The run() method loops forever, updating\nthe /config znode at random times with random values.\nNext, let’s look at how to read the /config configuration property. First we add a read\nmethod to ActiveKeyValueStore:\npublic String read(String path, Watcher watcher) throws InterruptedException,\nKeeperException {\nbyte[] data = zk.getData(path, watcher, null/*stat*/);\nreturn new String(data, CHARSET);\n}\nThe getData() method of ZooKeeper takes the path, a Watcher, and a Stat object. The\nStat object is filled in with values by getData(), and is used to pass information back\nto the caller. In this way, the caller can get both the data and the metadata for a znode,\nalthough in this case, we pass a null Stat because we are not interested in the metadata.\n392 | Chapter 13: ZooKeeperAs a consumer of the service, ConfigWatcher (see Example 13-7) creates an ActiveKey\nValueStore, and after starting, calls the store’s read() method (in its displayConfig()\nmethod) passing a reference to itself as the watcher. It displays the initial value of the\nconfiguration that it reads.\nExample 13-7. An application that watches for updates of a property in ZooKeeper and prints them\nto the console\npublic class ConfigWatcher implements Watcher {\nprivate ActiveKeyValueStore store;\npublic ConfigWatcher(String hosts) throws IOException, InterruptedException {\nstore = new ActiveKeyValueStore();\nstore.connect(hosts);\n}\npublic void displayConfig() throws InterruptedException, KeeperException {\nString value = store.read(ConfigUpdater.PATH, this);\nSystem.out.printf(""Read %s as %s\\n"", ConfigUpdater.PATH, value);\n}\n@Override\npublic void process(WatchedEvent event) {\nif (event.getType() == EventType.NodeDataChanged) {\ntry {\ndisplayConfig();\n} catch (InterruptedException e) {\nSystem.err.println(""Interrupted. Exiting."");\nThread.currentThread().interrupt();\n} catch (KeeperException e) {\nSystem.err.printf(""KeeperException: %s. Exiting.\\n"", e);\n}\n}\n}\npublic static void main(String[] args) throws Exception {\nConfigWatcher configWatcher = new ConfigWatcher(args[0]);\nconfigWatcher.displayConfig();\n}\n}\n// stay alive until process is killed or thread is interrupted\nThread.sleep(Long.MAX_VALUE);\nWhen the ConfigUpdater updates the znode, ZooKeeper causes the watcher to fire with\nan event type of EventType.NodeDataChanged. ConfigWatcher acts on this event in its\nprocess() method by reading and displaying the latest version of the config.\nBecause watches are one-time signals, we tell ZooKeeper of the new watch each time\nwe call read() on ActiveKeyValueStore—this ensures we see future updates. Further-\nmore, we are not guaranteed to receive every update, since between the receipt of the\nwatch event and the next read, the znode may have been updated, possibly many times,\nBuilding Applications with ZooKeeper | 393and as the client has no watch registered during that period, it is not notified. For the\nconfiguration service, this is not a problem because clients care only about the latest\nvalue of a property, as it takes precedence over previous values, but in general you\nshould be aware of this potential limitation.\nLet’s see the code in action. Launch the ConfigUpdater in one terminal window:\n% java ConfigUpdater localhost\nSet /config to 79\nSet /config to 14\nSet /config to 78\nThen launch the ConfigWatcher in another window immediately afterward:\n% java ConfigWatcher localhost\nRead /config as 79\nRead /config as 14\nRead /config as 78\nThe Resilient ZooKeeper Application\nThe first of the Fallacies of Distributed Computing* states that “The network is relia-\nble.” As they stand, the programs so far have been assuming a reliable network, so when\nthey run on a real network, they can fail in several ways. Let’s examine possible failure\nmodes, and what we can do to correct them so that our programs are resilient in the\nface of failure.\nEvery ZooKeeper operation in the Java API declares two types of exception in its throws\nclause: InterruptedException and KeeperException.\nInterruptedException\nAn InterruptedException is thrown if the operation is interrupted. There is a standard\nJava mechanism for canceling blocking methods, which is to call interrupt() on the\nthread from which the blocking method was called. A successful cancelation will result\nin an InterruptedException. ZooKeeper adheres to this standard, so you can cancel a\nZooKeeper operation in this way. Classes or libraries that use ZooKeeper should usually\npropagate the InterruptedException so that their clients can cancel their operations.†\nAn InterruptedException does not indicate a failure, but rather that the operation has\nbeen canceled, so in the configuration application, it is appropriate to propagate the\nexception, causing the application to terminate.\n* See http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing.\n† For more detail, see the excellent article “Dealing with InterruptedException” by Brian Goetz.\n394 | Chapter 13: ZooKeeperKeeperException\nA KeeperException is thrown if the ZooKeeper server signals an error, or if there is a\ncommunication problem with the server. There are various subclasses of KeeperExcep\ntion for different error cases. For example, KeeperException.NoNodeException is a sub-\nclass of KeeperException that is thrown if you try to perform an operation on a znode\nthat doesn’t exist.\nEvery subclass of KeeperException has a corresponding code with information about\nthe type of error. For example, for KeeperException.NoNodeException the code is Keep\nerException.Code.NONODE (an enum value).\nThere are two ways then to handle KeeperException: either catch KeeperException and\ntest its code to determine what remedying action to take, or catch the equivalent\nKeeperException subclasses and perform the appropriate action in each catch block.\nKeeperExceptions fall into three broad categories.\nState exceptions. A state exception occurs when the operation fails because it cannot be\napplied to the znode tree. State exceptions usually happen because another process is\nmutating a znode at the same time. For example, a setData operation with a version\nnumber will fail with a KeeperException.BadVersionException if the znode is updated\nby another process first, since the version number does not match. The programmer is\nusually aware that this kind of conflict is possible, and will code to deal with it.\nSome state exceptions indicate an error in the program, such as KeeperExcep\ntion.NoChildrenForEphemeralsException, which is thrown when trying to create a child\nznode of an ephemeral znode.\nRecoverable exceptions. Recoverable exceptions are those from which the application can\nrecover within the same ZooKeeper session. A recoverable exception is manifested by\nKeeperException.ConnectionLossException, which means that the connection to Zoo-\nKeeper has been lost. ZooKeeper will try to reconnect, and in most cases the recon-\nnection will succeed and ensure that the session is intact.\nHowever, ZooKeeper cannot tell whether the operation that failed with KeeperExcep\ntion.ConnectionLossException was applied. This is an example of partial failure (which\nwe introduced at the beginning of the chapter). The onus is therefore on the program-\nmer to deal with the uncertainty, and the action that should be taken depends on the\napplication.\nAt this point, it is useful to make a distinction between idempotent and nonidempo-\ntent operations. An idempotent operation is one that may be applied one or more times\nwith the same result, such as a read request or an unconditional setData. These can\nsimply be retried.\nA nonidempotent operation cannot be indiscriminately retried, as the effect of applying\nit multiple times is not the same as applying it once. The program needs a way of\ndetecting whether its update was applied by encoding information in the znode’s path\nBuilding Applications with ZooKeeper | 395name or its data. We shall discuss how to deal with failed nonidempotent operations\nin “Recoverable exceptions” on page 399, when we look at the implementation of a\nlock service.\nUnrecoverable exceptions. In some cases, the ZooKeeper session becomes invalid—\nperhaps because of a timeout or because the session was closed (both get a KeeperEx\nception.SessionExpiredException), or perhaps because authentication failed (Keeper\nException.AuthFailedException). In any case, all ephemeral nodes associated with the\nsession will be lost, so the application needs to rebuild its state before reconnecting to\nZooKeeper.\nA reliable configuration service\nGoing back to the write() method in ActiveKeyValueStore, recall that it is composed\nof an exists operation followed by either a create or a setData:\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), -1);\n}\n}\nTaken as a whole, the write() method is idempotent, so we can afford to uncondi-\ntionally retry it. Here’s a modified version of the write() method that retries in a loop.\nIt is set to try a maximum number of retries (MAX_RETRIES) and sleeps for\nRETRY_PERIOD_SECONDS between each attempt:\npublic void write(String path, String value) throws InterruptedException,\nKeeperException {\nint retries = 0;\nwhile (true) {\ntry {\nStat stat = zk.exists(path, false);\nif (stat == null) {\nzk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE,\nCreateMode.PERSISTENT);\n} else {\nzk.setData(path, value.getBytes(CHARSET), stat.getVersion());\n}\n} catch (KeeperException.SessionExpiredException e) {\nthrow e;\n} catch (KeeperException e) {\nif (retries++ == MAX_RETRIES) {\nthrow e;\n}\n// sleep then retry\nTimeUnit.SECONDS.sleep(RETRY_PERIOD_SECONDS);\n396 | Chapter 13: ZooKeeper}\n}\n}\nThe code is careful not to retry KeeperException.SessionExpiredException, since when\na session expires, the ZooKeeper object enters the CLOSED state, from which it can never\nreconnect (refer to Figure 13-3). We simply rethrow the exception‡ and let the caller\ncreate a new ZooKeeper instance, so that the whole write() method can be retried. A\nsimple way to create a new instance is to create a new ConfigUpdater (which we’ve\nactually renamed ResilientConfigUpdater) to recover from an expired session:\npublic static void main(String[] args) throws Exception {\nwhile (true) {\ntry {\nResilientConfigUpdater configUpdater =\nnew ResilientConfigUpdater(args[0]);\nconfigUpdater.run();\n} catch (KeeperException.SessionExpiredException e) {\n// start a new session\n} catch (KeeperException e) {\n// already retried, so exit\ne.printStackTrace();\nbreak;\n}\n}\n}\nAn alternative way of dealing with session expiry would be to look for a KeeperState\nof type Expired in the watcher (that would be the ConnectionWatcher in the example\nhere), and create a new connection when this is detected. This way, we would just keep\nretrying in the write() method, even if we got a KeeperException.SessionExpiredExcep\ntion, since the connection should eventually be re-established. Regardless of the precise\nmechanics of how we recover from an expired session, the important point is that it is\na different kind of failure from connection loss and needs to be handled differently.\nThere’s actually another failure mode that we’ve ignored here. When\nthe ZooKeeper object is created, it tries to connect to a ZooKeeper server.\nIf the connection fails or times out, then it tries another server in the\nensemble. If, after trying all of the servers in the ensemble, it can’t con-\nnect, then it throws an IOException. The likelihood of all ZooKeeper\nservers being unavailable is low; nevertheless, some applications may\nchoose to retry the operation in a loop until ZooKeeper is available.\nThis is just one strategy for retry handling—there are many others, such as using ex-\nponential backoff where the period between retries is multiplied by a constant each\n‡ Another way of writing the code would be to have a single catch block, just for KeeperException, and a test\nto see whether its code has the value KeeperException.Code.SESSIONEXPIRED. Which method you use is a\nmatter of style, since they both behave in the same way.\nBuilding Applications with ZooKeeper | 397time. The org.apache.hadoop.io.retry package in Hadoop Core is a set of utilities for\nadding retry logic into your code in a reusable way, and may be helpful for building\nZooKeeper applications.\nA Lock Service\nA distributed lock is a mechanism for providing mutual exclusion between a collection\nof processes. At any one time, only a single process may hold the lock. Distributed locks\ncan be used for leader election in a large distributed system, where the leader is the\nprocess that holds the lock at any point in time.\nDo not confuse ZooKeeper’s own leader election with a general leader\nelection service, which can be built using ZooKeeper primitives. Zoo-\nKeeper’s own leader election is not exposed publicly, unlike the type of\ngeneral leader election service we are describing here, which is designed\nto be used by distributed systems that need to agree upon a master\nprocess.\nTo implement a distributed lock using ZooKeeper, we use sequential znodes to impose\nan order on the processes vying for the lock. The idea is simple: first designate a lock\nznode, typically describing the entity being locked on, say /leader; then clients that want\nto acquire the lock create sequential ephemeral znodes as children of the lock znode.\nAt any point in time, the client with the lowest sequence number holds the lock. For\nexample, if two clients create znodes at around the same time, /leader/lock-1\nand /leader/lock-2, then the client that created /leader/lock-1 holds the lock, since its\nznode has the lowest sequence number. The ZooKeeper service is the arbiter of order,\nsince it assigns the sequence numbers.\nThe lock may be released simply by deleting the znode /leader/lock-1; alternatively, if\nthe client process dies, it will be deleted by virtue of it being an ephemeral znode. The\nclient that created /leader/lock-2 will then hold the lock, since it has the next lowest\nsequence number. It will be notified that it has the lock by creating a watch that fires\nwhen znodes go away.\nThe pseudocode for lock acquisition is as follows:\n1. Create an ephemeral sequential znode named lock- under the lock znode and re-\nmember its actual path name (the return value of the create operation).\n2. Get the children of the lock znode and set a watch.\n3. If the path name of the znode created in 1 has the lowest number of the children\nreturned in 2, then the lock has been acquired. Exit.\n4. Wait for the notification from the watch set in 2 and go to step 2.\n398 | Chapter 13: ZooKeeperThe herd effect\nAlthough this algorithm is correct, there are some problems with it. The first problem\nis that this implementation suffers from the herd effect. Consider hundreds or thou-\nsands of clients, all trying to acquire the lock. Each client places a watch on the lock\nznode for changes in its set of children. Every time the lock is released, or another\nprocess starts the lock acquisition process, the watch fires and every client receives a\nnotification. The “herd effect” refers to a large number of clients being notified of the\nsame event, when only a small number of them can actually proceed. In this case, only\none client will successfully acquire the lock, and the process of maintaining and sending\nwatch events to all clients causes traffic spikes, which put pressure on the ZooKeeper\nservers.\nTo avoid the herd effect, we need to refine the condition for notification. The key\nobservation for implementing locks is that a client needs to be notified only when the\nchild znode with the previous sequence number goes away, not when any child znode\nis deleted (or created). In our example, if clients have created the znodes /leader/\nlock-1, /leader/lock-2, and /leader/lock-3, then the client holding /leader/lock-3 only\nneeds to be notified when /leader/lock-2 disappears. It does not need to be notified\nwhen /leader/lock-1 disappears, or when a new znode /leader/lock-4 is added.\nRecoverable exceptions\nAnother problem with the lock algorithm as it stands is that it doesn’t handle the case\nwhen the create operation fails due to connection loss. Recall that in this case we do\nnot know if the operation succeeded or failed. Creating a sequential znode is a\nnonidempotent operation, so we can’t simply retry, since if the first create had succee-\nded, we would have an orphaned znode that would never be deleted (until the client\nsession ended, at least). Deadlock would be the unfortunate result.\nThe problem is that after reconnecting, the client can’t tell whether it created any of\nthe child znodes. By embedding an identifier in the znode name, if it suffers a connection\nloss, it can check to see whether any of the children of the lock node have its identifier\nin their name. If a child contains its identifier, it knows that the create operation suc-\nceeded, and it shouldn’t create another child znode. If no child has the identifier in its\nname, then the client can safely create a new sequential child znode.\nThe client’s session identifier is a long integer that is unique for the ZooKeeper service\nand therefore ideal for the purpose of identifying a client across connection loss events.\nThe session identifier can be obtained by calling the getSessionId() method on the\nZooKeeper Java class.\nThe ephemeral sequential znode should be created with a name of the form lock-\n<sessionId>-, so that when the sequence number is appended by ZooKeeper, the name\nbecomes lock-<sessionId>-<sequenceNumber>. The sequence numbers are unique to the\nparent, not to the name of the child, so this technique allows the child znodes to identify\ntheir creators as well as impose an order of creation.\nBuilding Applications with ZooKeeper | 399Unrecoverable exceptions\nIf a client’s ZooKeeper session expires, the ephemeral znode created by the client will\nbe deleted, effectively relinquishing the lock, or at least forfeiting the client’s turn to\nacquire the lock. The application using the lock should realize that it no longer holds\nthe lock, clean up its state, then start again, by creating a new lock object, and trying\nto acquire it. Notice that it is the application that controls this process, not the lock\nimplementation, since it cannot second-guess how the application needs to clean up\nits state.\nImplementation\nImplementing a distributed lock correctly is a delicate matter, since accounting for all\nof the failure modes is nontrivial. ZooKeeper comes with a production-quality lock\nimplementation in Java called WriteLock (from ZooKeeper 3.2.0 onward) that is very\neasy for clients to use.\nMore Distributed Data Structures and Protocols\nThere are many distributed data structures and protocols that can be built with Zoo-\nKeeper, such as barriers, queues, and two-phase commit. One interesting thing to note\nis that these are synchronous protocols, even though we use asynchronous ZooKeeper\nprimitives (such as notifications) to build them.\nThe ZooKeeper website (http://hadoop.apache.org/zookeeper/) describes several such\ndata structures and protocols in pseudocode. At the time of this writing, standard im-\nplementations were not available as a part of ZooKeeper, but over time it is expected\nthat they will be added to the codebase.\nBookKeeper\nBookKeeper is a highly available and reliable logging service. It can be used to provide\nwrite-ahead logging, which is a common technique for ensuring data integrity in storage\nsystems. In a system using write-ahead logging, every write operation is written to the\ntransaction log before it is applied. Using this procedure, we don’t have to write the\ndata to permanent storage after every write operation because in the event of a system\nfailure, the latest state may be recovered by replaying the transaction log for any writes\nthat had not been applied.\nBookKeeper clients create logs called ledgers, and each record appended to a ledger is\ncalled a ledger entry, which is simply a byte array. Ledgers are managed by bookies,\nwhich are servers that replicate the ledger data. Note that ledger data is not stored in\nZooKeeper, only metadata is.\nTraditionally, the challenge has been to make systems that use write-ahead logging\nrobust in the face of failure of the node writing the transaction log. This is usually done\nby replicating the transaction log in some manner. Hadoop’s HDFS namenode, for\n400 | Chapter 13: ZooKeeperinstance, writes its edit log to multiple disks, one of which is typically an NFS mounted\ndisk. However, in the event of failure of the primary, failover is still manual. By pro-\nviding logging as a highly available service, BookKeeper promises to make failover\ntransparent, since it can tolerate the loss of bookie servers.\nBookKeeper is provided in the contrib directory of the ZooKeeper distribution, where\nyou can find more information on how to use it.\nZooKeeper in Production\nIn production, you should run ZooKeeper in replicated mode. Here we will cover some\nof the considerations for running an ensemble of ZooKeeper servers. However, this\nsection is not exhaustive, so you should consult the ZooKeeper Administrator’s Guide\n(http://hadoop.apache.org/zookeeper/docs/current/) for detailed up-to-date instructions,\nincluding supported platforms, recommended hardware, maintenance procedures, and\nconfiguration properties.\nResilience and Performance\nZooKeeper machines should be located to minimize the impact of machine and network\nfailure. In practice, this means that servers should be spread across racks, power sup-\nplies, and switches, so that the failure of any one of these does not cause the ensemble\nto lose a majority of its servers. ZooKeeper replies on having low-latency connections\nbetween all of the servers in the ensemble, so for that reason an ensemble should be\nconfined to a single data center.\nZooKeeper is a highly available system, and it is critical that it can perform its functions\nin a timely manner. Therefore, ZooKeeper should run on machines that are dedicated\nto ZooKeeper alone. Having other applications contend for resources can cause Zoo-\nKeeper’s performance to degrade significantly.\nConfigure ZooKeeper to keep its transaction log on a different disk drive from its snap-\nshots. By default, both go in the directory specified by the dataDir property, but by\nspecifying a location for dataLogDir, the transaction log will be written there. By having\nits own dedicated device (not just a partition) a ZooKeeper server can maximize the\nrate at which it writes log entries to disk, which is does sequentially, without seeking.\nSince all writes go through the leader, write throughput does not scale by adding servers,\nso it is crucial that writes are as fast as possible.\nIf the process swaps to disk, performance will suffer adversely. This can be avoided by\nsetting the Java heap size to less than the amount of physical memory available on the\nmachine. The ZooKeeper scripts will source a file called java.env from its configuration\ndirectory, and this can be used to set the JVMFLAGS environment variable to set the heap\nsize (and any other desired JVM arguments).\nZooKeeper in Production | 401Configuration\nEach server in the ensemble of ZooKeeper servers has a numeric identifier that is unique\nwithin the ensemble, and must fall between 1 and 255. The server number is specified\nin plain text in a file named myid in the directory specified by the dataDir property.\nSetting each server number is only half of the job. We also need to give all the servers\nall the identities and network locations of the others in the ensemble. The ZooKeeper\nconfiguration file must include a line for each server, of the form:\nserver.n=hostname:port:port\nThe value of n is replaced by the server number. There are two port settings: the first\nis the port that followers use to connect to the leader, and the second is used for leader\nelection. Here is a sample configuration for a three-machine replicated ZooKeeper\nensemble:\ntickTime=2000\ndataDir=/disk1/zookeeper\ndataLogDir=/disk2/zookeeper\nclientPort=2181\ninitLimit=5\nsyncLimit=2\nserver.1=zookeeper1:2888:3888\nserver.2=zookeeper2:2888:3888\nserver.3=zookeeper3:2888:3888\nServers listen on three ports: 2181 for client connections; 2888 for follower connections,\nif they are the leader; and 3888 for other server connections during the leader election\nphase. When a ZooKeeper server starts up, it reads the myid file to determine which\nserver it is, then reads the configuration file to determine the ports it should listen on,\nas well as the network addresses of the other servers in the ensemble.\nClients connecting to this ZooKeeper ensemble should use zookeeper1:2181,zoo\nkeeper2:2181,zookeeper3:2181 as the host string in the constructor for the ZooKeeper\nobject.\nIn replicated mode, there are two extra mandatory properties: initLimit and\nsyncLimit, both measured in multiples of tickTime.\ninitLimit is the amount of time to allow for followers to connect to and sync with the\nleader. If a majority of followers fail to sync within this period, then the leader renounces\nits leadership status and another leader election takes place. If this happens often (and\nyou can discover if this is the case because it is logged), it is a sign that the setting is too\nlow.\nsyncLimit is the amount of time to allow a follower to sync with the leader. If a follower\nfails to sync within this period, it will restart itself. Clients that were attached to this\nfollower will connect to another one.\n402 | Chapter 13: ZooKeeperThese are the minimum settings needed to get up and running with a cluster of Zoo-\nKeeper servers. There are, however, more configuration options, particularly for tuning\nperformance, documented in the ZooKeeper Administrator’s Guide.\nZooKeeper in Production | 403CHAPTER 14\nCase Studies\nHadoop Usage at Last.fm\nLast.fm: The Social Music Revolution\nFounded in 2002, Last.fm is an Internet radio and music community website that offers\nmany services to its users, such as free music streams and downloads, music and event\nrecommendations, personalized charts, and much more. There are about 25 million\npeople who use Last.fm every month, generating huge amounts of data that need to be\nprocessed. One example of this is users transmitting information indicating which\nsongs they are listening to (this is known as “scrobbling”). This data is processed and\nstored by Last.fm, so the user can access it directly (in the form of charts), and it is also\nused to make decisions about users’ musical tastes and compatibility, and artist and\ntrack similarity.\nHadoop at Last.fm\nAs Last.fm’s service developed and the number of users grew from thousands to mil-\nlions, storing, processing and managing all the incoming data became increasingly\nchallenging. Fortunately, Hadoop was quickly becoming stable enough and was en-\nthusiastically adopted as it became clear how many problems it solved. It was first used\nat Last.fm in early 2006 and was put into production a few months later. There were\nseveral reasons for adopting Hadoop at Last.fm:\n• The distributed filesystem provided redundant backups for the data stored on it\n(e.g., web logs, user listening data) at no extra cost.\n• Scalability was simplified through the ability to add cheap, commodity hardware\nwhen required.\n• The cost was right (free) at a time when Last.fm had limited financial resources.\n• The open source code and active community meant that Last.fm could freely mod-\nify Hadoop to add custom features and patches.\n405• Hadoop provided a flexible framework for running distributed computing algo-\nrithms with a relatively easy learning curve.\nHadoop has now become a crucial part of Last.fm’s infrastructure, currently consisting\nof two Hadoop clusters spanning over 50 machines, 300 cores, and 100 TB of disk\nspace. Hundreds of daily jobs are run on the clusters performing operations, such as\nlogfile analysis, evaluation of A/B tests, ad hoc processing, and charts generation. This\ncase study will focus on the process of generating charts, as this was the first usage of\nHadoop at Last.fm and illustrates the power and flexibility that Hadoop provides over\nother approaches when working with very large datasets.\nGenerating Charts with Hadoop\nLast.fm uses user-generated track listening data to produce many different types of\ncharts, such as weekly charts for tracks, per country and per user. A number of Hadoop\nprograms are used to process the listening data and generate these charts, and these\nrun on a daily, weekly, or monthly basis. Figure 14-1 shows an example of how this\ndata is displayed on the site; in this case, the weekly top tracks.\nFigure 14-1. Last.fm top tracks chart\nListening data typically arrives at Last.fm from one of two sources:\n406 | Chapter 14: Case Studies• A user plays a track of her own (e.g., listening to an MP3 file on a PC or other\ndevice), and this information is sent to Last.fm using either the official Last.fm\nclient application or one of many hundreds of third-party applications.\n• A user tunes into one of Last.fm’s Internet radio stations and streams a song to her\ncomputer. The Last.fm player or website can be used to access these streams and\nextra functionality is made available to the user, allowing her to love, skip, or ban\neach track that she listens to.\nWhen processing the received data, we distinguish between a track listen submitted by\na user (the first source above, referred to as a scrobble from here on) and a track listened\nto on the Last.fm radio (the second source, mentioned earlier, referred to as a radio\nlisten from here on). This distinction is very important in order to prevent a feedback\nloop in the Last.fm recommendation system, which is based only on scrobbles. One of\nthe most fundamental Hadoop jobs at Last.fm takes the incoming listening data and\nsummarizes it into a format that can be used for display purposes on the Last.fm website\nas well as input to other Hadoop programs. This is achieved by the Track Statistics\nprogram, which is the example described in the following sections.\nThe Track Statistics Program\nWhen track listening data is submitted to Last.fm, it undergoes a validation and\nconversion phase, the end result of which is a number of space-delimited text files\ncontaining the user ID, the track ID, the number of times the track was scrobbled, the\nnumber of times the track was listened to on the radio, and the number of times it was\nskipped. Table 14-1 contains sample listening data, which is used in the following\nexamples as input to the Track Statistics program (the real data is gigabytes in size and\nincludes many more fields that have been omitted here for simplicity’s sake).\nTable 14-1. Listening data\nUserId TrackId Scrobble Radio Skip\n111115 222 0 1 0\n111113 225 1 0 0\n111117 223 0 1 1\n111115 225 1 0 0\nThese text files are the initial input provided to the Track Statistics program, which\nconsists of two jobs that calculate various values from this data and a third job that\nmerges the results (see Figure 14-2).\nThe Unique Listeners job calculates the total number of unique listeners for a track by\ncounting the first listen by a user and ignoring all other listens by the same user. The\nSum job accumulates the total listens, scrobbles, radio listens, and skips for each track\nby counting these values for all listens by all users. Although the input format of these\nHadoop Usage at Last.fm | 407Figure 14-2. TrackStats jobs\ntwo jobs is identical, two separate jobs are needed, as the Unique Listeners job is re-\nsponsible for emitting values per track per user, and the Sum job emits values per track.\nThe final “Merge” job is responsible for merging the intermediate output of the two\nother jobs into the final result. The end results of running the program are the following\nvalues per track:\n•\n•\n•\n•\n•\nNumber of unique listeners\nNumber of times the track was scrobbled\nNumber of times the track was listened to on the radio\nNumber of times the track was listened to in total\nNumber of times the track was skipped on the radio\nEach job and its MapReduce phases are described in more detail next. Please note that\nthe provided code snippets have been simplified due to space constraints; for download\ndetails for the full code listings, refer to the preface.\nCalculating the number of unique listeners\nThe Unique Listeners job calculates, per track, the number of unique listeners.\nUniqueListenerMapper. The UniqueListenersMapper processes the space-delimited raw lis-\ntening data and emits the user ID associated with each track ID:\npublic void map(LongWritable position, Text rawLine, OutputCollector<IntWritable,\nIntWritable> output, Reporter reporter) throws IOException {\nString[] parts = (rawLine.toString()).split("" "");\nint scrobbles = Integer.parseInt(parts[TrackStatisticsProgram.COL_SCROBBLES]);\nint radioListens = Integer.parseInt(parts[TrackStatisticsProgram.COL_RADIO]);\n408 | Chapter 14: Case Studies}\n// if track somehow is marked with zero plays - ignore\nif (scrobbles <= 0 && radioListens <= 0) {\nreturn;\n}\n// if we get to here then user has listened to track,\n// so output user id against track id\nIntWritable trackId = new IntWritable(\nInteger.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]));\nIntWritable userId = new IntWritable(\nInteger.parseInt(parts[TrackStatisticsProgram.COL_USERID]));\noutput.collect(trackId, userId);\nUniqueListenersReducer. The UniqueListenersReducers receives a list of user IDs per track\nID, and puts these IDs into a Set to remove any duplicates. The size of this set is then\nemitted (i.e., the number of unique listeners) for each track ID. Storing all the reduce\nvalues in a Set runs the risk of running out of memory if there are many values for a\ncertain key. This hasn’t happened in practice, but to overcome this, an extra\nMapReduce step could be introduced to remove all the duplicate values or a secondary\nsort could be used. (For more details, see “Secondary Sort” on page 227.)\npublic void reduce(IntWritable trackId, Iterator<IntWritable> values,\nOutputCollector<IntWritable, IntWritable> output, Reporter reporter)\nthrows IOException {\n}\nSet<Integer> userIds = new HashSet<Integer>();\n// add all userIds to the set, duplicates automatically removed (set contract)\nwhile (values.hasNext()) {\nIntWritable userId = values.next();\nuserIds.add(Integer.valueOf(userId.get()));\n}\n// output trackId -> number of unique listeners per track\noutput.collect(trackId, new IntWritable(userIds.size()));\nTable 14-2 shows the sample input data for the job. The map output appears in Ta-\nble 14-3 and the reduce output in Table 14-4.\nTable 14-2. Job input\nLine of file UserId TrackId\nLongWritable IntWritable 0 \nScrobbled\nRadio play Skip\nIntWritableBoolean Boolean Boolean\n11115 222 0 1 0\n1 11113 225 1 0 0\n2 11117 223 0 1 1\n3 11115 225 1 0 0\nHadoop Usage at Last.fm | 409Table 14-3. Mapper output\nTrackId UserId\nIntWritable IntWritable\n222 11115\n225 11113\n223 11117\n225 11115\nTable 14-4. Reducer output\nTrackId #listeners\nIntWritable IntWritable\n222 1\n225 2\n223 1\nSumming the track totals\nThe Sum job is relatively simple; it just adds up the values we are interested in for each\ntrack.\nSumMapper. The input data is again the raw text files, but in this case, it is handled quite\ndifferently. The desired end result is a number of totals (unique listener count, play\ncount, scrobble count, radio listen count, skip count) associated with each track. To\nsimplify things, we use an intermediate TrackStats object generated using Hadoop\nRecord I/O, which implements WritableComparable (so it can be used as output) to hold\nthese values. The mapper creates a TrackStats object and sets the values on it for each\nline in the file, except for the unique listener count, which is left empty (it will be filled\nin by the final merge job):\npublic void map(LongWritable position, Text rawLine,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\n}\nString[] parts = (rawLine.toString()).split("" "");\nint trackId = Integer.parseInt(parts[TrackStatisticsProgram.COL_TRACKID]);\nint scrobbles = Integer.parseInt(parts[TrackStatisticsProgram.COL_SCROBBLES]);\nint radio = Integer.parseInt(parts[TrackStatisticsProgram.COL_RADIO]);\nint skip = Integer.parseInt(parts[TrackStatisticsProgram.COL_SKIP]);\n// set number of listeners to 0 (this is calculated later)\n// and other values as provided in text file\nTrackStats trackstat = new TrackStats(0, scrobbles + radio, scrobbles, radio, skip);\noutput.collect(new IntWritable(trackId), trackstat);\nSumReducer. In this case, the reducer performs a very similar function to the mapper—\nit sums the statistics per track and returns an overall total:\n410 | Chapter 14: Case Studiespublic void reduce(IntWritable trackId, Iterator<TrackStats> values,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\n}\nTrackStats sum = new TrackStats(); // holds the totals for this track\nwhile (values.hasNext()) {\nTrackStats trackStats = (TrackStats) values.next();\nsum.setListeners(sum.getListeners() + trackStats.getListeners());\nsum.setPlays(sum.getPlays() + trackStats.getPlays());\nsum.setSkips(sum.getSkips() + trackStats.getSkips());\nsum.setScrobbles(sum.getScrobbles() + trackStats.getScrobbles());\nsum.setRadioPlays(sum.getRadioPlays() + trackStats.getRadioPlays());\n}\noutput.collect(trackId, sum);\nTable 14-5 shows the input data for the job (the same as for the Unique Listeners job).\nThe map output appears in Table 14-6 and the reduce output in Table 14-7.\nTable 14-5. Job input\nLine UserId TrackId Scrobbled Radio play Skip\nLongWritable IntWritable IntWritable Boolean Boolean Boolean\n0 11115 222 0 1 0\n1 11113 225 1 0 0\n2 11117 223 0 1 1\n3 11115 225 1 0 0\nTable 14-6. Map output\nTrackId #listeners #plays #scrobbles #radio plays #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 1 1 0 0\n223 0 1 0 1 1\n225 0 1 1 0 0\nTable 14-7. Reduce output\nTrackId #listeners #plays #scrobbles #radio plays #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 2 2 0 0\n223 0 1 0 1 1\nHadoop Usage at Last.fm | 411Merging the results\nThe final job needs to merge the output from the two previous jobs: the number of\nunique listeners per track and the statistics per track. In order to be able to merge these\ndifferent inputs, two different mappers (one for each type of input) are used. The two\nintermediate jobs are configured to write their results to different paths, and the\nMultipleInputs class is used to specify which mapper will process which files. The\nfollowing code shows how the JobConf for the job is set up to do this:\nMultipleInputs.addInputPath(conf, sumInputDir,\nSequenceFileInputFormat.class, IdentityMapper.class);\nMultipleInputs.addInputPath(conf, listenersInputDir,\nSequenceFileInputFormat.class, MergeListenersMapper.class);\nIt is possible to use a single mapper to handle different inputs, but the example solution\nis more convenient and elegant.\nMergeListenersMapper. This mapper is used to process the UniqueListenerJob’s output of\nunique listeners per track. It creates a TrackStats object in a similar manner to the\nSumMapper, but this time, it fills in only the unique listener count per track and leaves\nthe other values empty:\npublic void map(IntWritable trackId, IntWritable uniqueListenerCount,\nOutputCollector<IntWritable, TrackStats> output, Reporter reporter)\nthrows IOException {\nTrackStats trackStats = new TrackStats();\ntrackStats.setListeners(uniqueListenerCount.get());\noutput.collect(trackId, trackStats);\n}\nTable 14-8 shows some input for the mapper; the corresponding output is shown in\nTable 14-9.\nTable 14-8. MergeListenersMapper input\nTrackId #listeners\nIntWritable IntWritable\n222 1\n225 2\n223 1\nTable 14-9. MergeListenersMapper output\nTrackId #listeners #plays #scrobbles #radio #skips\n222 1 0 0 0 0\n225 2 0 0 0 0\n223 2 0 0 0 0\n412 | Chapter 14: Case StudiesIdentityMapper. The IdentityMapper is configured to process the SumJob’s output of\nTrackStats objects and, as no additional processing is required, it directly emits the\ninput data (see Table 14-10).\nTable 14-10. IdentityMapper input and output\nTrackId #listeners #plays #scrobbles #radio #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 0 1 0 1 0\n225 0 2 2 0 0\n223 0 1 0 1 1\nSumReducer. The two mappers above emit values of the same type: a TrackStats object\nper track, with different values filled in. The final reduce phase can reuse the\nSumReducer described earlier to create a TrackStats object per track, sum up all the\nvalues, and emit it (see Table 14-11).\nTable 14-11. Final SumReducer output\nTrackId #listeners #plays #scrobbles #radio #skips\nIntWritable IntWritable IntWritable IntWritable IntWritable IntWritable\n222 1 1 0 1 0\n225 2 2 2 0 0\n223 1 1 0 1 1\nThe final output files are then accumulated and copied to a server where a web service\nmakes the data available to the Last.fm website for display. An example of this is shown\nin Figure 14-3, where the total number of listeners and plays are displayed for a track.\nFigure 14-3. TrackStats result\nHadoop Usage at Last.fm | 413Summary\nHadoop has become an essential part of Last.fm’s infrastructure and is used to generate\nand process a wide variety of datasets ranging from web logs to user listening data. The\nexample covered here has been simplified considerably in order to get the key concepts\nacross; in real-world usage the input data has a more complicated structure and the\ncode that processes it is more complex. Hadoop itself, while mature enough for pro-\nduction use, is still in active development and new features and improvements are\nadded by the Hadoop community every week. We at Last.fm are happy to be part of\nthis community as a contributor of code and ideas, and as end users of a great piece of\nopen source technology.\n—Adrian Woodhead and Marc de Palol\nHadoop and Hive at Facebook\nIntroduction\nHadoop can be used to form core backend batch and near real-time computing infra-\nstructures. It can also be used to store and archive massive datasets. In this case study,\nwe will explore backend data architectures and the role Hadoop can play in them. We\nwill describe hypothetical Hadoop configurations, potential uses of Hive—an open\nsource data warehousing and SQL infrastructure built on top of Hadoop—and the\ndifferent kinds of business and product applications that have been built using this\ninfrastructure.\nHadoop at Facebook\nHistory\nThe amount of log and dimension data in Facebook that needs to be processed and\nstored has exploded as the usage of the site has increased. A key requirement for any\ndata processing platform for this environment is the ability to be able to scale rapidly\nin tandem. Further, engineering resources being limited, the system should be very\nreliable and easy to use and maintain.\nInitially, data warehousing at Facebook was performed entirely on an Oracle instance.\nAfter we started hitting scalability and performance problems, we investigated whether\nthere were open source technologies that could be used in our environment. As part of\nthis investigation, we deployed a relatively small Hadoop instance and started pub-\nlishing some of our core datasets into this instance. Hadoop was attractive because\nYahoo! was using it internally for its batch processing needs and also because we were\nfamiliar with the simplicity and scalability of the MapReduce model as popularized by\nGoogle.\n414 | Chapter 14: Case StudiesOur initial prototype was very successful: the engineers loved the ability to process\nmassive amounts of data in reasonable timeframes, an ability that we just did not have\nbefore. They also loved being able to use their favorite programming language for pro-\ncessing (using Hadoop streaming). Having our core datasets published in one\ncentralized data store was also very convenient. At around the same time, we started\ndeveloping Hive. This made it even easier for users to process data in the Hadoop cluster\nby being able to express common computations in the form of SQL, a language with\nwhich most engineers and analysts are familiar.\nAs a result, the cluster size and usage grew leaps and bounds, and today Facebook is\nrunning the second largest Hadoop cluster in the world. As of this writing, we hold\nmore than 2 PB of data in Hadoop and load more than 10 TB of data into it every day.\nOur Hadoop instance has 2,400 cores and about 9 TB of memory and runs at 100%\nutilization at many points during the day. We are able to scale out this cluster rapidly\nin response to our growth, and we have been able to take advantage of open source by\nmodifying Hadoop where required to suit our needs. We have contributed back to open\nsource, both in the form of contributions to some core components of Hadoop as well\nas by open-sourcing Hive, which is now a Hadoop subproject.\nUse cases\nThere are at least four interrelated but distinct classes of uses for Hadoop at Facebook:\n• Producing daily and hourly summaries over large amounts of data. These summa-\nries are used for a number of different purposes within the company:\n— Reports based on these summaries are used by engineering and nonengineering\nfunctional teams to drive product decisions. These summaries include reports\non growth of the users, page views, and average time spent on the site by the\nusers.\n— Providing performance numbers about advertisement campaigns that are run\non Facebook.\n— Backend processing for site features such as people you may like and applica-\ntions you may like.\n• Running ad hoc jobs over historical data. These analyses help answer questions\nfrom our product groups and executive team.\n• As a de facto long-term archival store for our log datasets.\n• To look up log events by specific attributes (where logs are indexed by such at-\ntributes), which is used to maintain the integrity of the site and protect users against\nspambots.\nData architecture\nFigure 14-4 shows the basic components of our architecture and the data flow within\nthese components.\nHadoop and Hive at Facebook | 415Figure 14-4. Data warehousing architecture at Facebook\nAs shown in Figure 14-4, the following components are used in processing data:\nScribe\nLog data is generated by web servers as well as internal services such as the Search\nbackend. We use Scribe, an open source log collection service developed in Face-\nbook that deposits hundreds of log datasets with daily volume in tens of terabytes\ninto a handful of NFS servers.\nHDFS\nA large fraction of this log data is copied into one central HDFS instance. Dimen-\nsion data is also scraped from our internal MySQL databases and copied over into\nHDFS daily.\nHive/Hadoop\nWe use Hive, a Hadoop subproject developed in Facebook, to build a data ware-\nhouse over all the data collected in HDFS. Files in HDFS, including log data from\nScribe and dimension data from the MySQL tier, are made available as tables with\nlogical partitions. A SQL-like query language provided by Hive is used in conjunc-\ntion with MapReduce to create/publish a variety of summaries and reports, as well\nas to perform historical analysis over these tables.\nTools\nBrowser-based interfaces built on top of Hive allow users to compose and launch\nHive queries (which in turn launch MapReduce jobs) using just a few mouse clicks.\n416 | Chapter 14: Case StudiesTraditional RDBMS\nWe use Oracle and MySQL databases to publish these summaries. The volume of\ndata here is relatively small, but the query rate is high and needs real-time response.\nDataBee\nAn in-house ETL workflow software that is used to provide a common framework\nfor reliable batch processing across all data processing jobs.\nData from the NFS tier storing Scribe data is continuously replicated to the HDFS\ncluster by copier jobs. The NFS devices are mounted on the Hadoop tier and the copier\nprocesses run as map-only jobs on the Hadoop cluster. This makes it easy to scale the\ncopier processes and also makes them fault-resilient. Currently we copy over 6 TB per\nday from Scribe to HDFS in this manner. We also download up to 4 TB of dimension\ndata from our MySQL tier to HDFS every day. These are also conveniently arranged\non the Hadoop cluster, as map-only jobs that copy data out of MySQL boxes.\nHadoop configuration\nThe central philosophy behind our Hadoop deployment is consolidation. We use a\nsingle HDFS instance, and a vast majority of processing is done in a single MapReduce\ncluster (running a single jobtracker). The reasons for this are fairly straightforward:\n• We can minimize the administrative overheads by operating a single cluster.\n• Data does not need to be duplicated. All data is available in a single place for all\nthe use cases described previously.\n• By using the same compute cluster across all departments, we get tremendous\nefficiencies.\n• Our users work in a collaborative environment, so requirements in terms of quality\nof service are not onerous (yet).\nWe also have a single shared Hive metastore (using a MySQL database) that holds\nmetadata about all the Hive tables stored in HDFS.\nHypothetical Use Case Studies\nIn this section, we will describe some typical problems that are common for large web-\nsites, which are difficult to solve through traditional warehousing technologies, simply\nbecause the costs and scales involved are prohibitively high. Hadoop and Hive can\nprovide a more scalable and more cost-effective solution in such situations.\nAdvertiser insights and performance\nOne of the most common uses of Hadoop is to produce summaries from large volumes\nof data. It is very typical of large ad networks such as Facebook ad network, Google\nAdSense, and many others to provide advertisers with standard aggregated statistics\nabout their ads that help the advertisers to tune their campaigns effectively. Computing\nHadoop and Hive at Facebook | 417advertisement performance numbers on large datasets is a very data-intensive opera-\ntion, and the scalability and cost advantages of Hadoop and Hive can really help in\ncomputing these numbers in a reasonable time frame and at a reasonable cost.\nMany ad networks provide standardized CPC- and CPM-based ad-units to the adver-\ntisers. The CPC ads are cost-per-click ads: the advertiser pays the ad network amounts\nthat are dependent on the number of clicks that the particular ad gets from the users\nvisiting the site. The CPM ads, on the other hand, bill the advertisers amounts that are\nproportional to the number of users that see the ad on the site. Apart from these stand-\nardized ad units, in the last few years ads that have more dynamic content that is tailored\nto each individual user have also become common in the online advertisement industry.\nYahoo! does this through SmartAds, whereas Facebook provides its advertisers with\nSocial Ads. The latter allows the advertisers to embed information from a user’s net-\nwork of friends; for example, a Nike ad may refer to a friend of the user who recently\nfanned Nike and shared that information with his friends on Facebook. In addition,\nFacebook also provides Engagement Ad units to the advertisers, wherein the users can\nmore effectively interact with the ad, be it by commenting on it or by playing embedded\nvideos. In general, there is a wide variety of ads that are provided to the advertisers by\nthe online ad networks, and this variety also adds yet another dimension to the various\nkinds of performance numbers that the advertisers are interested in getting about their\ncampaigns.\nAt the most basic level, advertisers are interested in knowing the total and the number\nof unique users that have seen the ad or have clicked on it. For more dynamic ads, they\nmay even be interested in getting the breakdown of these aggregated numbers by the\nkind of dynamic information shown in the ad unit or the kind of engagement action\nundertaken by the users on the ad. For example, a particular advertisement may have\nbeen shown 100,000 times to 30,000 unique users. Similarly a video embedded inside\nan Engagement Ad may have been watched by 100,000 unique users. In addition, these\nperformance numbers are typically reported for each ad, campaign, and account. An\naccount may have multiple campaigns with each campaign running multiple ads on\nthe network. Finally, these numbers are typically reported for different time durations\nby the ad networks. Typical durations are daily, rolling week, month to date, rolling\nmonth, and sometimes even for the entire lifetime of the campaign. Moreover, adver-\ntisers also look at the geographic breakdown of these numbers among other ways of\nslicing and dicing this data, such as what percentage of the total viewers or clickers of\na particular ad are in the Asia Pacific region.\nAs is evident, there are four predominant dimension hierarchies: the account, cam-\npaign, and ad dimension; the time period; the type of interaction; and the user dimen-\nsion. The last of these is used to report unique numbers, whereas the other three are\nthe reporting dimensions. The user dimension is also used to create aggregated geo-\ngraphic profiles for the viewers and clickers of ads. All this information in totality allows\nthe advertisers to tune their campaigns to improve their effectiveness on any given ad\nnetwork. Aside from the multidimensional nature of this set of pipelines, the volumes\n418 | Chapter 14: Case Studiesof data processed and the rate at which this data is growing on a daily basis make this\ndifficult to scale without a technology like Hadoop for large ad networks. As of this\nwriting, for example, the ad log volume that is processed for ad performance numbers\nat Facebook is approximately 1 TB per day of (uncompressed) logs. This volume has\nseen a 30-fold increase since January 2008, when the volumes were in the range of 30\nGB per day. Hadoop’s ability to scale with hardware has been a major factor behind\nthe ability of these pipelines to keep up with this data growth with minor tweaking of\njob configurations. Typically, these configuration changes involve increasing the num-\nber of reducers for the Hadoop jobs that are processing the intensive portions of these\npipelines. The largest of these stages currently run with 400 reducers (an increase of\neight times from the 50 reducers that were being used in January 2008).\nAd hoc analysis and product feedback\nApart from regular reports, another primary use case for a data warehousing solution\nis to be able to support ad hoc analysis and product feedback solutions. Any typical\nwebsite, for example, makes product changes, and it is typical for product managers\nor engineers to understand the impact of a new feature, based on user engagement as\nwell as on the click-through rate on that feature. The product team may even wish to\ndo a deeper analysis on what is the impact of the change based on various regions and\ncountries, such as whether this change increases the click-through rate of the users in\nU.S. or whether it reduces the engagement of users in India. A lot of this type of analysis\ncould be done with Hadoop by using Hive and regular SQL. The measurement of click-\nthrough rate can be easily expressed as a join of the impressions and clicks for the\nparticular link related to the feature. This information can be joined with geographic\ninformation to compute the effect of product changes on different regions. Subse-\nquently one can compute average click-through rate for different geographic regions\nby performing aggregations over them. All of these are easily expressible in Hive using\na couple of SQL queries (that would in turn generate multiple Hadoop jobs). If only an\nestimate were required, the same queries can be run for a sample set of the users using\nsampling functionality natively supported by Hive. Some of this analysis needs the use\nof custom map and reduce scripts in conjunction with the Hive SQL and that is also\neasy to plug into a Hive query.\nA good example of a more complex analysis is estimating the peak number of users\nlogging into the site per minute for the entire past year. This would involve sampling\npage view logs (because the total page view data for a popular website is huge), grouping\nit by time and then finding the number of new users at different time points via a custom\nreduce script. This is a good example where both SQL and MapReduce are required\nfor solving the end user problem and something that is possible to achieve easily with\nHive.\nHadoop and Hive at Facebook | 419Data analysis\nHive and Hadoop can be easily used for training and scoring for data analysis applica-\ntions. These data analysis applications can span multiple domains such as popular\nwebsites, bioinformatics companies, and oil exploration companies. A typical example\nof such an application in the online ad network industry would be the prediction of\nwhat features of an ad makes it more likely to be noticed by the user. The training phase\ntypically would involve identifying the response metric and the predictive features. In\nthis case, a good metric to measure the effectiveness of an ad could be its click-through\nrate. Some interesting features of the ad could be the industry vertical that it belongs\nto, the content of the ad, the placement of the ad on the page, and so on. Hive is easily\nuseful for assembling training data and then feeding the same into a data analysis engine\n(typically R or user programs written in MapReduce). In this particular case, different\nad performance numbers and features can be structured as tables in Hive. One can\neasily sample this data (sampling is required as R can only handle limited data volume)\nand perform the appropriate aggregations and joins using Hive queries to assemble a\nresponse table that contains the most important ad features that determine the effec-\ntiveness of an advertisement. However, since sampling loses information, some of the\nmore important data analysis applications use parallel implementations of popular data\nanalysis kernels using MapReduce framework.\nOnce the model has been trained, it may be deployed for scoring on a daily basis. The\nbulk of the data analysis tasks do not perform daily scoring though. Many of them are\nad hoc in nature and require one-time analysis that can be used as input into product\ndesign process.\nHive\nOverview\nWhen we started using Hadoop, we very quickly became impressed by its scalability\nand availability. However, we were worried about widespread adoption, primarily be-\ncause of the complexity involved in writing MapReduce programs in Java (as well as\nthe cost of training users to write them). We were aware that a lot of engineers and\nanalysts in the company understood SQL as a tool to query and analyze data and that\na lot of them were proficient in a number of scripting languages like PHP and Python.\nAs a result, it was imperative for us to develop software that could bridge this gap\nbetween the languages that the users were proficient in and the languages required to\nprogram Hadoop.\nIt was also evident that a lot of our datasets were structured and could be easily parti-\ntioned. The natural consequence of these requirements was a system that could model\ndata as tables and partitions and that could also provide a SQL-like language for query\nand analysis. Also essential was the ability to plug in customized MapReduce programs\nwritten in the programming language of the user’s choice into the query. This system\n420 | Chapter 14: Case Studieswas called Hive. Hive is a data warehouse infrastructure built on top of Hadoop and\nserves as the predominant tool that is used to query the data stored in Hadoop at\nFacebook. In the following sections, we describe this system in more detail.\nData organization\nData is organized consistently across all datasets and is stored compressed, partitioned,\nand sorted:\nCompression\nAlmost all datasets are stored as sequence files using gzip codec. Older datasets are\nrecompressed to use the bzip codec that gives substantially more compression than\ngzip. Bzip is slower than gzip, but older data is accessed much less frequently and\nthis performance hit is well worth the savings in terms of disk space.\nPartitioning\nMost datasets are partitioned by date. Individual partitions are loaded into Hive,\nwhich loads each partition into a separate HDFS directory. In most cases, this\npartitioning is based simply on datestamps associated with scribe logfiles. How-\never, in some cases, we scan data and collate them based on timestamp available\ninside a log entry. Going forward, we are also going to be partitioning data on\nmultiple attributes (for example, country and date).\nSorting\nEach partition within a table is often sorted (and hash-partitioned) by unique ID\n(if one is present). This has a few key advantages:\n• It is easy to run sampled queries on such datasets.\n• We can build indexes on sorted data.\n• Aggregates and joins involving unique IDs can be done very efficiently on such\ndatasets.\nLoading data into this long-term format is done by daily MapReduce jobs (and is dis-\ntinct from the near real-time data import processes).\nQuery language\nThe Hive Query language is very SQL-like. It has traditional SQL constructs like joins,\ngroup bys, where, select, from clauses and from clause subqueries. It tries to convert\nSQL commands into a set of MapReduce jobs. Apart from the normal SQL clauses, it\nhas a bunch of other extensions, like the ability to specify custom mapper and reducer\nscripts in the query itself, the ability to insert into multiple tables, partitions, HDFS or\nlocal files while doing a single scan of the data and the ability to run the query on data\nsamples rather than the full dataset (this ability is fairly useful while testing queries).\nThe Hive metastore stores the metadata for a table and provides this metadata to the\nHive compiler for converting SQL commands to MapReduce jobs. Through partition\nHadoop and Hive at Facebook | 421pruning, map-side aggregations, and other features, the compiler tries to create plans\nthat can optimize the runtime for the query.\nData pipelines using Hive\nAdditionally, the ability provided by Hive in terms of expressing data pipelines in SQL\ncan and has provided the much needed flexibility in putting these pipelines together in\nan easy and expedient manner. This is especially useful for organizations and products\nthat are still evolving and growing. Many of the operations needed in processing data\npipelines are the well-understood SQL operations like join, group by, and distinct ag-\ngregations. With Hive’s ability to convert SQL into a series of Hadoop MapReduce\njobs, it becomes fairly easy to create and maintain these pipelines. We illustrate these\nfacets of Hive in this section by using an example of a hypothetical ad network and\nshowing how some typical aggregated reports needed by the advertisers can be com-\nputed using Hive. As an example, assuming that an online ad network stores informa-\ntion on ads in a table named dim_ads and stores all the impressions served to that ad in\na table named impression_logs in Hive, with the latter table being partitioned by date,\nthe daily impression numbers (both unique and total by campaign, that are routinely\ngiven by ad networks to the advertisers) for 2008-12-01 are expressible as the following\nSQL in Hive:\nSELECT a.campaign_id, count(1), count(DISTINCT b.user_id)\nFROM dim_ads a JOIN impression_logs b ON(b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01'\nGROUP BY a.campaign_id;\nThis would also be the typical SQL statement that one could use in other RDBMSs such\nas Oracle, DB2, and so on.\nIn order to compute the daily impression numbers by ad and account from the same\njoined data as earlier, Hive provides the ability to do multiple group bys simultaneously\nas shown in the following query (SQL-like but not strictly SQL):\nFROM(\nSELECT a.ad_id, a.campaign_id, a.account_id, b.user_id\nFROM dim_ads a JOIN impression_logs b ON (b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01') x\nINSERT OVERWRITE DIRECTORY 'results_gby_adid'\nSELECT x.ad_id, count(1), count(DISTINCT x.user_id) GROUP BY x.ad_id\nINSERT OVERWRITE DIRECTORY 'results_gby_campaignid'\nSELECT x.campaign_id, count(1), count(DISTINCT x.user_id) GROUP BY x.campaign_id\nINSERT OVERWRITE DIRECTORY 'results_gby_accountid'\nSELECT x.account_id, count(1), count(DISTINCT x.user_id) GROUP BY x.account_id;\nIn one of the optimizations that is being added to Hive, the query can be converted into\na sequence of Hadoop MapReduce jobs that are able to scale with data skew. Essen-\ntially, the join is converted into one MapReduce job and the three group bys are con-\nverted into four MapReduce jobs with the first one generating a partial aggregate on\nunique_id. This is especially useful because the distribution of impression_logs over\nunique_id is much more uniform as compared to ad_id (typically in an ad network, a\n422 | Chapter 14: Case Studiesfew ads dominate in that they are shown more uniformly to the users). As a result,\ncomputing the partial aggregation by unique_id allows the pipeline to distribute the\nwork more uniformly to the reducers. The same template can be used to compute\nperformance numbers for different time periods by simply changing the date predicate\nin the query.\nComputing the lifetime numbers can be more tricky though, as using the strategy de-\nscribed previously, one would have to scan all the partitions of the impression_logs\ntable. Therefore, in order to compute the lifetime numbers, a more viable strategy is to\nstore the lifetime counts on a per ad_id, unique_id grouping every day in a partition of\nan intermediate table. The data in this table combined with the next days\nimpression_logs can be used to incrementally generate the lifetime ad performance\nnumbers. As an example, in order to get the impression numbers for 2008-12-01, the\nintermediate table partition for 2008-11-30 is used. The Hive queries that can be used\nto achieve this are as follows:\nINSERT OVERWRITE lifetime_partial_imps PARTITION(dateid='2008-12-01')\nSELECT x.ad_id, x.user_id, sum(x.cnt)\nFROM (\nSELECT a.ad_id, a.user_id, a.cnt\nFROM lifetime_partial_imps a\nWHERE a.dateid = '2008-11-30'\nUNION ALL\nSELECT b.ad_id, b.user_id, 1 as cnt\nFROM impression_log b\nWHERE b.dateid = '2008-12-01'\n) x\nGROUP BY x.ad_id, x.user_id;\nThis query computes the partial sums for 2008-12-01, which can be used for computing\nthe 2008-12-01 numbers as well as the 2008-12-02 numbers (not shown here). The\nSQL is converted to a single Hadoop MapReduce job that essentially computes the\ngroup by on the combined stream of inputs. This SQL can be followed by the following\nHive query, which computes the actual numbers for different groupings (similar to the\none in the daily pipelines):\nFROM(\nSELECT a.ad_id, a.campaign_id, a.account_id, b.user_id, b.cnt\nFROM dim_ads a JOIN lifetime_partial_imps b ON (b.ad_id = a.ad_id)\nWHERE b.dateid = '2008-12-01') x\nINSERT OVERWRITE DIRECTORY 'results_gby_adid'\nSELECT x.ad_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.ad_id\nINSERT OVERWRITE DIRECTORY 'results_gby_campaignid'\nSELECT x.campaign_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.campaign_id\nINSERT OVERWRITE DIRECTORY 'results_gby_accountid'\nSELECT x.account_id, sum(x.cnt), count(DISTINCT x.user_id) GROUP BY x.account_id;\nHive and Hadoop are batch processing systems that cannot serve the computed data\nwith the same latency as a usual RDBMS such as Oracle or MySQL. Therefore, on many\noccasions, it is still useful to load the summaries generated through Hive and Hadoop\nHadoop and Hive at Facebook | 423to a more traditional RDBMS for serving this data to users through different BI tools\nor even though a web portal.\nProblems and Future Work\nFair sharing\nHadoop clusters typically run a mix of production daily jobs that need to finish com-\nputation within a reasonable time frame as well as ad hoc jobs that may be of different\npriorities and sizes. In typical installations, these jobs tend to run overnight, when\ninterference from ad hoc jobs run by users is minimal. However, overlap between large\nad hoc and production jobs is often unavoidable and, without adequate safeguards,\ncan impact the latency of production jobs. ETL processing also contains several near\nreal-time jobs that must be performed at hourly intervals (these include processes to\ncopy Scribe data from NFS servers as well as hourly summaries computed over some\ndatasets). It also means that a single rogue job can bring down the entire cluster and\nput production processes at risk.\nThe fair-sharing Hadoop jobscheduler, developed at Facebook and contributed back\nto Hadoop, provides a solution to many of these issues. It reserves guaranteed compute\nresources for specific pools of jobs while at the same time letting idle resources be used\nby everyone. It also prevents large jobs from hogging cluster resources by allocating\ncompute resources in a fair manner across these pools. Memory can become one of the\nmost contended resources in the cluster. We have made some changes to Hadoop so\nthat if the JobTracker is low on memory, Hadoop job submissions are throttled. This\ncan allow the user processes to run with reasonable per-process memory limits, and it\nis possible to put in place some monitoring scripts in order to prevent MapReduce jobs\nfrom impacting HDFS daemons (due primarily to high memory consumption) running\non the same node. Log directories are stored in separate disk partitions and cleaned\nregularly, and we think it can also be useful to put MapReduce intermediate storage in\nseparate disk partitions as well.\nSpace management\nCapacity management continues to be a big challenge—utilization is increasing at a\nfast rate with growth of data. Many growing companies with growing datasets have the\nsame pain. In many situations, much of this data is temporary in nature. In such cases,\none can use retention settings in Hive and also recompress older data in bzip format to\nsave on space. Although configurations are largely symmetrical from a disk storage\npoint of view, adding a separate tier of high-storage-density machines to hold older\ndata may prove beneficial. This will make it cheaper to store archival data in Hadoop.\nHowever, access to such data should be transparent. We are currently working on a\ndata archival layer to make this possible and to unify all the aspects of dealing with\nolder data.\n424 | Chapter 14: Case StudiesScribe-HDFS integration\nCurrently, Scribe writes to a handful of NFS filers from where data is picked up and\ndelivered to HDFS by custom copier jobs as described earlier. We are working on\nmaking Scribe write directly to another HDFS instance. This will make it very easy to\nscale and administer Scribe. Due to the high uptime requirements for Scribe, its target\nHDFS instance is likely to be different from the production HDFS instance (so that it\nis isolated from any load/downtime issues due to user jobs).\nImprovements to Hive\nHive is still under active development. A number of key features are being worked on\nsuch as order by and having clause support, more aggregate functions, more built in\nfunctions, datetime, data type, and so on. At the same time, a number of performance\noptimizations are being worked on, such as predicate pushdown and common subex-\npression elimination. On the integration side, JDBC and ODBC drivers are being de-\nveloped in order to integrate with OLAP and BI tools. With all these optimizations, we\nhope that we can unlock the power of MapReduce and Hadoop and bring it closer to\nnonengineering communities as well within Facebook. For more information on this\nproject, please visit http://hadoop.apache.org/hive/.\n—Joydeep Sen Sarma and Ashish Thusoo\nNutch Search Engine\nBackground\nNutch is a framework for building scalable Internet crawlers and search engines. It’s\nan Apache Software Foundation project, and a subproject of Lucene, and it’s available\nunder the Apache 2.0 license.\nWe won’t go deeply into the anatomy of a web crawler as such—the purpose of this\ncase study is to show how Hadoop can be used to implement various complex pro-\ncessing tasks typical for a search engine. Interested readers can find plenty of Nutch-\nspecific information on the official site of the project (http://lucene.apache.org/nutch).\nSuffice to say that in order to create and maintain a search engine, one needs the fol-\nlowing subsystems:\nDatabase of pages\nThis database keeps track of all pages known to the crawler and their status, such\nas the last time it visited the page, its fetching status, refresh interval, content\nchecksum, etc. In Nutch terminology, this database is called CrawlDb.\nNutch Search Engine | 425List of pages to fetch\nAs crawlers periodically refresh their view of the Web, they download new pages\n(previously unseen) or refresh pages that they think already expired. Nutch calls\nsuch a list of candidate pages prepared for fetching a fetchlist.\nRaw page data\nPage content is downloaded from remote sites, and stored locally in the original\nuninterpreted format, as a byte array. This data is called the page content in Nutch.\nParsed page data\nPage content is then parsed using a suitable parser—Nutch provides parsers for\ndocuments in many popular formats, such as HTML, PDF, Open Office and Mi-\ncrosoft Office, RSS, and others.\nLink graph database\nThis database is necessary to compute link-based page ranking scores, such as\nPageRank. For each URL known to Nutch, it contains a list of other URLs pointing\nto it, and their associated anchor text (from HTML <a href="".."">anchor\ntext</a> elements). This database is called LinkDb.\nFull-text search index\nThis is a classical inverted index, built from the collected page metadata and from\nthe extracted plain-text content. It is implemented using the excellent Lucene li-\nbrary (http://lucene.apache.org/java).\nWe briefly mentioned before that Hadoop began its life as a component in Nutch,\nintended to improve its scalability and to address clear performance bottlenecks caused\nby a centralized data processing model. Nutch was also the first public proof-of-concept\napplication ported to the framework that would later become Hadoop, and the effort\nrequired to port Nutch algorithms and data structures to Hadoop proved to be sur-\nprisingly small. This probably encouraged the following development of Hadoop as a\nseparate subproject with the aim of providing a reusable framework for applications\nother than Nutch.\nCurrently nearly all Nutch tools process data by running one or more MapReduce jobs.\nData Structures\nThere are several major data structures maintained in Nutch, and they all make use of\nHadoop I/O classes and formats. Depending on the purpose of the data, and the way\nit’s accessed once it’s created, the data is kept either using Hadoop map files or\nsequence files.\nSince the data is produced and processed by MapReduce jobs, which in turn run several\nmap and reduce tasks, its on-disk layout corresponds to the common Hadoop output\nformats, that is, MapFileOutputFormat and SequenceFileOutputFormat. So to be precise,\nwe should say that data is kept in several partial map files or sequence files, with as\n426 | Chapter 14: Case Studiesmany parts as there were reduce tasks in the job that created the data. For simplicity,\nwe omit this distinction in the following sections.\nCrawlDb\nCrawlDb stores the current status of each URL, as a map file of <url, CrawlDatum>,\nwhere keys use Text and values use a Nutch-specific CrawlDatum class (which imple-\nments the Writable interface).\nIn order to provide a quick random access to the records (sometimes useful for diag-\nnostic reasons, when users want to examine individual records in CrawlDb), this data\nis stored in map files and not in sequence files.\nCrawlDb is initially created using the Injector tool, which simply converts a plain-text\nrepresentation of the initial list of URLs (called the seed list) to a map file in the format\ndescribed earlier. Subsequently it is updated with the information from the fetched and\nparsed pages—more on that shortly.\nLinkDb\nThis database stores the incoming link information for every URL known to Nutch. It\nis a map file of <url, Inlinks>, where Inlinks is a list of URL and anchor text data. It’s\nworth noting that this information is not immediately available during page collection,\nbut the reverse information is available, namely that of outgoing links from a page. The\nprocess of inverting this relationship is implemented as a MapReduce job, described\nshortly.\nSegments\nSegments in Nutch parlance correspond to fetching and parsing a batch of URLs.\nFigure 14-5 presents how segments are created and processed.\nA segment (which is really a directory in a filesystem) contains the following parts\n(which are simply subdirectories containing MapFileOutputFormat or SequenceFileOut\nputFormat data):\ncontent\nContains the raw data of downloaded pages, as a map file of <url, Content>. Nutch\nuses a map file here, because it needs fast random access in order to present a cached\nview of a page.\ncrawl_generate\nContains the list of URLs to be fetched, together with their current status retrieved\nfrom CrawlDb, as a sequence file of <url, CrawlDatum>. This data uses sequence\nfile, first because it’s processed sequentially, and second because we couldn’t sat-\nisfy the map file invariants of sorted keys. We need to spread URLs that belong to\nthe same host as far apart as possible to minimize the load per target host, and this\nmeans that records are sorted more or less randomly.\nNutch Search Engine | 427crawl_fetch\nContains status reports from the fetching, that is, whether it was successful, what\nwas the response code, etc. This is stored in a map file of <url, CrawlDatum>.\ncrawl_parse\nThe list of outlinks for each successfully fetched and parsed page is stored here so\nthat Nutch can expand its crawling frontier by learning new URLs.\nparse_data\nMetadata collected during parsing; among others, the list of outgoing links (out-\nlinks) for a page. This information is crucial later on to build an inverted graph (of\nincoming links—inlinks).\nparse_text\nPlain-text version of the page, suitable for indexing in Lucene. These are stored as\na map file of <url, ParseText> so that Nutch can access them quickly when\nbuilding summaries (snippets) to display the list of search results.\nNew segments are created from CrawlDb when the Generator tool is run (1 in Fig-\nure 14-5), and initially contain just a list of URLs to fetch (the crawl_generate subdir-\nectory). As this list is processed in several steps, the segment collects output data from\nthe processing tools in a set of subdirectories.\nFigure 14-5. Segments\nFor example, the content part is populated by a tool called Fetcher, which downloads\nraw data from URLs on the fetchlist (2). This tool also saves the status information in\ncrawl_fetch so that this data can be used later on for updating the status of the page in\nCrawlDb.\nThe remaining parts of the segment are populated by the Parse segment tool (3), which\nreads the content section, selects appropriate content parser based on the declared (or\n428 | Chapter 14: Case Studiesdetected) MIME type, and saves the results of parsing in three parts: crawl_parse,\nparse_data, and parse_text. This data is then used to update the CrawlDb with new\ninformation (4) and to create the LinkDb (5).\nSegments are kept around until all pages present in them are expired. Nutch applies a\nconfigurable maximum time limit, after which a page is forcibly selected for refetching;\nthis helps the operator phase out all segments older than this limit (because he can be\nsure that by that time all pages in this segment would have been refetched).\nSegment data is used to create Lucene indexes ([6]—primarily the parse_text and\nparse_data parts), but it also provides a data storage mechanism for quick retrieval of\nplain text and raw content data. The former is needed so that Nutch can generate\nsnippets (fragments of document text best matching a query); the latter provides the\nability to present a “cached view” of the page. In both cases, this data is accessed directly\nfrom map files in response to requests for snippet generation or for cached content. In\npractice, even for large collections the performance of accessing data directly from map\nfiles is quite sufficient.\nSelected Examples of Hadoop Data Processing in Nutch\nThe following sections present relevant details of some Nutch tools to illustrate how\nthe MapReduce paradigm is applied to a concrete data processing task in Nutch.\nLink inversion\nHTML pages collected during crawling contain HTML links, which may point either\nto the same page (internal links) or to other pages. HTML links are directed from source\npage to target page. See Figure 14-6.\nFigure 14-6. Link inversion\nHowever, most algorithms for calculating a page’s importance (or quality) need the\nopposite information, that is, what pages contain outlinks that point to the current\npage. This information is not readily available when crawling. Also, the indexing proc-\ness benefits from taking into account the anchor text on inlinks so that this text may\nsemantically enrich the text of the current page.\nNutch Search Engine | 429As mentioned earlier, Nutch collects the outlink information and then uses this data\nto build a LinkDb, which contains this reversed link data in the form of inlinks and\nanchor text.\nThis section presents a rough outline of the implementation of the LinkDb tool—many\ndetails have been omitted (such as URL normalization and filtering) in order to present\na clear picture of the process. What’s left gives a classical example of why the\nMapReduce paradigm fits so well with the key data transformation processes required\nto run a search engine. Large search engines need to deal with massive web graph data\n(many pages with a lot of outlinks/inlinks), and the parallelism and fault tolerance\noffered by Hadoop make this possible. Additionally, it’s easy to express the link inver-\nsion using the map-sort-reduce primitives, as illustrated next.\nThe snippet below presents the job initialization of the LinkDb tool:\nJobConf job = new JobConf(configuration);\nFileInputFormat.addInputPath(job, new Path(segmentPath, ""parse_data""));\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(LinkDb.class);\njob.setReducerClass(LinkDb.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(Inlinks.class);\njob.setOutputFormat(MapFileOutputFormat.class);\nFileOutputFormat.setOutputPath(job, newLinkDbPath);\nAs we can see, the source data for this job is the list of fetched URLs (keys) and the\ncorresponding ParseData records that contain among others the outlink information\nfor each page, as an array of outlinks. An outlink contains both the target URL and the\nanchor text.\nThe output from the job is again a list of URLs (keys), but the values are instances of\ninlinks, which is simply a specialized Set of inlinks that contain target URLs and anchor\ntext.\nPerhaps surprisingly, URLs are typically stored and processed as plain text and not as\njava.net.URL or java.net.URI instances. There are several reasons for this: URLs ex-\ntracted from downloaded content usually need normalization (e.g., converting host-\nnames to lowercase, resolving relative paths), are often broken or invalid, or refer to\nunsupported protocols. Many normalization and filtering operations are better ex-\npressed as text patterns that span several parts of a URL. Also, for the purpose of link\nanalysis, we may still want to process and count invalid URLs.\nLet’s take a closer look now at the map() and reduce() implementations—in this case,\nthey are simple enough to be implemented in the body of the same class:\npublic void map(Text fromUrl, ParseData parseData,\nOutputCollector<Text, Inlinks> output, Reporter reporter) {\n...\nOutlink[] outlinks = parseData.getOutlinks();\nInlinks inlinks = new Inlinks();\nfor (Outlink out : outlinks) {\n430 | Chapter 14: Case Studies}\n}\ninlinks.clear(); // instance reuse to avoid excessive GC\nString toUrl = out.getToUrl();\nString anchor = out.getAnchor();\ninlinks.add(new Inlink(fromUrl, anchor));\noutput.collect(new Text(toUrl), inlinks);\nYou can see from this listing that for each Outlink our map() implementation produces\na pair of <toUrl, Inlinks>, where Inlinks contains just a single Inlink containing\nfromUrl and the anchor text. The direction of the link has been inverted.\nSubsequently, these one-element-long Inlinks are aggregated in the reduce() method:\npublic void reduce(Text toUrl, Iterator<Inlinks> values,\nOutputCollector<Text, Inlinks> output, Reporter reporter) {\nInlinks result = new Inlinks();\nwhile (values.hasNext()) {\nresult.add(values.next());\n}\noutput.collect(toUrl, result);\n}\nFrom this code, it’s obvious that we have got exactly what we wanted—that is, a list\nof all fromUrls that point to our toUrl, together with their anchor text. The inversion\nprocess has been accomplished.\nThis data is then saved using the MapFileOutputFormat and becomes the new version of\nLinkDb.\nGeneration of fetchlists\nLet’s take a look now at a more complicated use case. Fetchlists are produced from the\nCrawlDb (which is a map file of <url, crawlDatum>, with the crawlDatum containing a\nstatus of this URL), and they contain URLs ready to be fetched, which are then pro-\ncessed by the Nutch Fetcher tool. Fetcher is itself a MapReduce application (described\nshortly). This means that the input data (partitioned in N parts) will be processed by\nN map tasks—the Fetcher tool enforces that SequenceFileInputFormat should not fur-\nther split the data in more parts than there are already input partitions. We mentioned\nearlier briefly that fetchlists need to be generated in a special way so that the data in\neach part of the fetchlist (and consequently processed in each map task) meets certain\nrequirements:\n1. All URLs from the same host need to end up in the same partition. This is required\nso that Nutch can easily implement in-JVM host-level blocking to avoid over-\nwhelming target hosts.\n2. URLs from the same host should be as far apart as possible (i.e., well mixed with\nURLs from other hosts) in order to minimize the host-level blocking.\nNutch Search Engine | 4313. There should be no more than x URLs from any single host so that large sites with\nmany URLs don’t dominate smaller sites (and URLs from smaller sites still have a\nchance to be scheduled for fetching).\n4. URLs with high scores should be preferred over URLs with low scores.\n5. There should be at most y URLs in total in the fetchlist.\n6. The number of output partitions should match the optimum number of fetching\nmap tasks.\nIn this case, two MapReduce jobs are needed to satisfy all these requirements, as illus-\ntrated in Figure 14-7. Again, in the following listings, we are going to skip some details\nof these steps for the sake of brevity.\nFigure 14-7. Generation of fetchlists\nStep 1: Select, sort by score, limit by URL count per host. In this step, Nutch runs a MapReduce\njob to select URLs that are considered eligible for fetching, and to sort them by their\nscore (a floating-point value assigned to each URL, e.g., a PageRank score). The input\ndata comes from CrawlDb, which is a map file of <url, datum>. The output from this\njob is a sequence file with <score, <url, datum>>, sorted in descending order by score.\nFirst, let’s look at the job setup:\nFileInputFormat.addInputPath(job, crawlDbPath);\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(Selector.class);\njob.setPartitionerClass(Selector.class);\njob.setReducerClass(Selector.class);\nFileOutputFormat.setOutputPath(job, tempDir);\njob.setOutputFormat(SequenceFileOutputFormat.class);\njob.setOutputKeyClass(FloatWritable.class);\njob.setOutputKeyComparatorClass(DecreasingFloatComparator.class);\njob.setOutputValueClass(SelectorEntry.class);\nThe Selector class implements three functions: mapper, reducer, and partitioner. The\nlast function is especially interesting: Selector uses a custom Partitioner to assign\nURLs from the same host to the same reduce task so that we can satisfy criteria 3–5\n432 | Chapter 14: Case Studiesfrom the previous list. If we didn’t override the default partitioner, URLs from the same\nhost would end up in different partitions of the output, and we wouldn’t be able to\ntrack and limit the total counts, because MapReduce tasks don’t communicate between\nthemselves. As it is now, all URLs that belong to the same host will end up being\nprocessed by the same reduce task, which means we can control how many URLs per\nhost are selected.\nIt’s easy to implement a custom partitioner so that data that needs to be processed in\nthe same task ends up in the same partition. Let’s take a look first at how the\nSelector class implements the Partitioner interface (which consists of a single\nmethod):\n/** Partition by host. */\npublic int getPartition(FloatWritable key, Writable value, int numReduceTasks) {\nreturn hostPartitioner.getPartition(((SelectorEntry)value).url, key, numReduceTasks);\n}\nThe method returns an integer number from 0 to numReduceTasks - 1. It simply replaces\nthe key with the original URL from SelectorEntry to pass the URL (instead of score)\nto an instance of PartitionUrlByHost, where the partition number is calculated:\n/** Hash by hostname. */\npublic int getPartition(Text key, Writable value, int numReduceTasks) {\nString urlString = key.toString();\nURL url = null;\ntry {\nurl = new URL(urlString);\n} catch (MalformedURLException e) {\nLOG.warn(""Malformed URL: '"" + urlString + ""'"");\n}\nint hashCode = (url == null ? urlString : url.getHost()).hashCode();\n// make hosts wind up in different partitions on different runs\nhashCode ^= seed;\n}\nreturn (hashCode & Integer.MAX_VALUE) % numReduceTasks;\nAs you can see from the code snippet, the partition number is a function of only the\nhost part of the URL, which means that all URLs that belong to the same host will end\nup in the same partition.\nThe output from this job is sorted in decreasing order by score. Since there are many\nrecords in CrawlDb with the same score, we couldn’t use MapFileOutputFormat because\nwe would violate the map file’s invariant of strict key ordering.\nObservant readers will notice that as we had to use something other than the original\nkeys, but we still want to preserve the original key-value pairs, we use here a Selector\nEntry class to pass the original key-value pairs to the next step of processing.\nSelector.reduce() keeps track of the total number of URLs and the maximum number\nof URLs per host, and simply discards excessive records. Please note that the enforce-\nment of the total count limit is necessarily approximate. We calculate the limit for the\nNutch Search Engine | 433current task as the total limit divided by the number of reduce tasks. But we don’t know\nfor sure from within the task that it is going to get an equal share of URLs; indeed, in\nmost cases, it doesn’t because of the uneven distribution of URLs among hosts. How-\never, for Nutch this approximation is sufficient.\nStep 2: Invert, partition by host, sort randomly. In the previous step, we ended up with a se-\nquence file of <score, selectorEntry>. Now we have to produce a sequence file of\n<url, datum> and satisfy criteria 1, 2, and 6 just described. The input data for this step\nis the output data produced in step 1.\nThe following is a snippet showing the setup of this job:\nFileInputFormat.addInputPath(job, tempDir);\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(SelectorInverseMapper.class);\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(SelectorEntry.class);\njob.setPartitionerClass(PartitionUrlByHost.class);\njob.setReducerClass(PartitionReducer.class);\njob.setNumReduceTasks(numParts);\nFileOutputFormat.setOutputPath(job, output);\njob.setOutputFormat(SequenceFileOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(CrawlDatum.class);\njob.setOutputKeyComparatorClass(HashComparator.class);\nThe SelectorInverseMapper class simply discards the current key (the score value), ex-\ntracts the original URL and uses it as a key, and uses the SelectorEntry as the value.\nCareful readers may wonder why we don’t go one step further, extracting also the\noriginal CrawlDatum and using it as the value—more on this shortly.\nThe final output from this job is a sequence file of <Text, CrawlDatum>, but our output\nfrom the map phase uses <Text, SelectorEntry>. We have to specify that we use dif-\nferent key/value classes for the map output, using the setMapOutputKeyClass() and\nsetMapOutputValueClass() setters—otherwise, Hadoop assumes that we use the same\nclasses as declared for the reduce output (this conflict usually would cause a job to fail).\nThe output from the map phase is partitioned using PartitionUrlByHost class so that\nit again assigns URLs from the same host to the same partition. This satisfies require-\nment 1.\nOnce the data is shuffled from map to reduce tasks, it’s sorted by Hadoop according\nto the output key comparator, in this case the HashComparator. This class uses a simple\nhashing scheme to mix URLs in a way that is least likely to put URLs from the same\nhost close to each other.\nIn order to meet requirement 6, we set the number of reduce tasks to the desired number\nof Fetcher map tasks (the numParts mentioned earlier), keeping in mind that each reduce\npartition will be used later on to create a single Fetcher map task.\n434 | Chapter 14: Case StudiesPartitionReducer class is responsible for the final step, that is, to convert <url,\nselectorEntry> to <url, crawlDatum>. A surprising side effect of using HashCompara\ntor is that several URLs may be hashed to the same hash value, and Hadoop will call\nreduce() method passing only the first such key—all other keys considered equal will\nbe discarded. Now it becomes clear why we had to preserve all URLs in SelectorEn\ntry records, because now we can extract them from the iterated values. Here is the\nimplementation of this method:\npublic void reduce(Text key, Iterator<SelectorEntry> values,\nOutputCollector<Text, CrawlDatum> output, Reporter reporter) throws IOException {\n// when using HashComparator, we get only one input key in case of hash collisions\n// so use only URLs extracted from values\nwhile (values.hasNext()) {\nSelectorEntry entry = values.next();\noutput.collect(entry.url, entry.datum);\n}\n}\nFinally, the output from reduce tasks is stored as a SequenceFileOutputFormat in a Nutch\nsegment directory, in a crawl_generate subdirectory. This output satisfies all criteria\nfrom 1 to 6.\nFetcher: A multi-threaded MapRunner in action\nThe Fetcher application in Nutch is responsible for downloading the page content from\nremote sites. As such, it is important that the process uses every opportunity for par-\nallelism, in order to minimize the time it takes to crawl a fetchlist.\nThere is already one level of parallelism present in Fetcher—multiple parts of the input\nfetchlists are assigned to multiple map tasks. However, in practice this is not sufficient:\nsequential download of URLs, from different hosts (see the earlier section on HashCom\nparator), would be a tremendous waste of time. For this reason the Fetcher map tasks\nprocess this data using multiple worker threads.\nHadoop uses the MapRunner class to implement the sequential processing of input data\nrecords. The Fetcher class implements its own MapRunner that uses multiple threads to\nprocess input records in parallel.\nLet’s begin with the setup of the job:\njob.setSpeculativeExecution(false);\nFileInputFormat.addInputPath(job, ""segment/crawl_generate"");\njob.setInputFormat(InputFormat.class);\njob.setMapRunnerClass(Fetcher.class);\nFileOutputFormat.setOutputPath(job, segment);\njob.setOutputFormat(FetcherOutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(NutchWritable.class);\nNutch Search Engine | 435First, we turn off speculative execution. We can’t run several map tasks downloading\ncontent from the same hosts because it would violate the host-level load limits (such\nas the number of concurrent requests and the number of requests per second).\nNext, we use a custom InputFormat implementation that prevents Hadoop from split-\nting partitions of input data into smaller chunks (splits), thus creating more map tasks\nthan there are input partitions. This again ensures that we control host-level access\nlimits.\nOutput data is stored using a custom OutputFormat implementation, which creates sev-\neral output map files and sequence files created using data contained in NutchWrita\nble values. The NutchWritable class is a subclass of GenericWritable, able to pass in-\nstances of several different Writable classes declared in advance.\nThe Fetcher class implements the MapRunner interface, and we set this class as the job’s\nMapRunner implementation. The relevant parts of the code are listed here:\npublic void run(RecordReader<Text, CrawlDatum> input,\nOutputCollector<Text, NutchWritable> output,\nReporter reporter) throws IOException {\nint threadCount = getConf().getInt(""fetcher.threads.fetch"", 10);\nfeeder = new QueueFeeder(input, fetchQueues, threadCount * 50);\nfeeder.start();\n}\nfor (int i = 0; i < threadCount; i++) {\nnew FetcherThread(getConf()).start();\n}\ndo {\ntry {\nThread.sleep(1000);\n} catch (InterruptedException e) {}\nreportStatus(reporter);\n} while (activeThreads.get() > 0);\n// spawn threads\n// wait for threads to exit\nFetcher reads many input records in advance, using the QueueFeeder thread that puts\ninput records into a set of per-host queues. Then several FetcherThread instances are\nstarted, which consume items from per-host queues, while QueueFeeder keeps reading\ninput data to keep the queues filled. Each FetcherThread consumes items from any\nnonempty queue.\nIn the meantime, the main thread of the map task spins around waiting for all threads\nto finish their job. Periodically it reports the status to the framework to ensure that\nHadoop doesn’t consider this task to be dead and kill it. Once all items are processed,\nthe loop is finished and the control is returned to Hadoop, which considers this map\ntask to be completed.\nIndexer: Using custom OutputFormat\nThis is an example of a MapReduce application that doesn’t produce sequence file or\nmap file output—instead, the output from this application is a Lucene index. Again,\n436 | Chapter 14: Case Studiesas MapReduce applications may consist of several reduce tasks, the output from this\napplication may consist of several partial Lucene indexes.\nNutch Indexer tool uses information from CrawlDb, LinkDb, and Nutch segments\n(fetch status, parsing status, page metadata, and plain-text data), so the job setup sec-\ntion involves adding several input paths:\nFileInputFormat.addInputPath(job, crawlDbPath);\nFileInputFormat.addInputPath(job, linkDbPath);\n// add segment data\nFileInputFormat.addInputPath(job, ""segment/crawl_fetch"");\nFileInputFormat.addInputPath(job, ""segment/crawl_parse"");\nFileInputFormat.addInputPath(job, ""segment/parse_data"");\nFileInputFormat.addInputPath(job, ""segment/parse_text"");\njob.setInputFormat(SequenceFileInputFormat.class);\njob.setMapperClass(Indexer.class);\njob.setReducerClass(Indexer.class);\nFileOutputFormat.setOutputPath(job, indexDir);\njob.setOutputFormat(OutputFormat.class);\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(LuceneDocumentWrapper.class);\nAll corresponding records for a URL dispersed among these input locations need to be\ncombined to create Lucene documents to be added to the index.\nThe Mapper implementation in Indexer simply wraps input data, whatever its source\nand implementation class, in a NutchWritable, so that the reduce phase may receive\ndata from different sources, using different classes, and still be able to consistently\ndeclare a single output value class (as NutchWritable) from both map and reduce steps.\nThe Reducer implementation iterates over all values that fall under the same key (URL),\nunwraps the data (fetch CrawlDatum, CrawlDb CrawlDatum, LinkDb Inlinks, Parse\nData and ParseText) and, using this information, builds a Lucene document, which is\nthen wrapped in a Writable LuceneDocumentWrapper and collected. In addition to all\ntextual content (coming either from the plain-text data or from metadata), this docu-\nment also contains a PageRank-like score information (obtained from CrawlDb data).\nNutch uses this score to set the boost value of Lucene document.\nThe OutputFormat implementation is the most interesting part of this tool:\npublic static class OutputFormat extends\nFileOutputFormat<WritableComparable, LuceneDocumentWrapper> {\npublic RecordWriter<WritableComparable, LuceneDocumentWrapper>\ngetRecordWriter(final FileSystem fs, JobConf job,\nString name, final Progressable progress) throws IOException {\nfinal Path out = new Path(FileOutputFormat.getOutputPath(job), name);\nfinal IndexWriter writer = new IndexWriter(out.toString(),\nnew NutchDocumentAnalyzer(job), true);\nreturn new RecordWriter<WritableComparable, LuceneDocumentWrapper>() {\nboolean closed;\npublic void write(WritableComparable key, LuceneDocumentWrapper value)\nNutch Search Engine | 437}\nthrows IOException {\nDocument doc = value.get();\nwriter.addDocument(doc);\nprogress.progress();\n// unwrap & index doc\npublic void close(final Reporter reporter) throws IOException {\n// spawn a thread to give progress heartbeats\nThread prog = new Thread() {\npublic void run() {\nwhile (!closed) {\ntry {\nreporter.setStatus(""closing"");\nThread.sleep(1000);\n} catch (InterruptedException e) { continue; }\ncatch (Throwable e) { return; }\n}\n}\n};\n}\n}\ntry {\nprog.start();\n// optimize & close index\nwriter.optimize();\nwriter.close();\n} finally {\nclosed = true;\n}\n};\nWhen an instance of RecordWriter is requested, the OutputFormat creates a new Lucene\nindex by opening an IndexWriter. Then, for each new output record collected in the\nreduce method, it unwraps the Lucene document from LuceneDocumentWrapper value\nand adds it to the index.\nWhen a reduce task is finished, Hadoop will try to close the RecordWriter. In this case,\nthe process of closing may take a long time, because we would like to optimize the\nindex before closing. During this time, Hadoop may conclude that the task is hung,\nsince there are no progress updates, and it may attempt to kill it. For this reason, we\nfirst start a background thread to give reassuring progress updates, and then proceed\nto perform the index optimization. Once the optimization is completed, we stop the\nprogress updater thread. The output index is now created, optimized, and is closed,\nand ready for use in a searcher application.\nSummary\nThis short overview of Nutch necessarily omits many details, such as error handling,\nlogging, URL filtering and normalization, dealing with redirects or other forms of\n“aliased” pages (such as mirrors), removing duplicate content, calculating PageRank\n438 | Chapter 14: Case Studiesscoring, etc. You can find this and much more information on the official page of the\nproject and on the wiki (http://wiki.apache.org/nutch).\nToday, Nutch is used by many organizations and individual users. Still, operating a\nsearch engine requires nontrivial investment in hardware, integration, and customiza-\ntion, and the maintenance of the index, so in most cases Nutch is used to build com-\nmercial vertical- or field-specific search engines.\nNutch is under active development, and the project follows closely new releases of\nHadoop. As such, it will continue to be a practical example of a real-life application\nthat uses Hadoop at its core, with excellent results.\n—Andrzej Białecki\nLog Processing at Rackspace\nRackspace Hosting has always provided managed systems for enterprises, and in that\nvein, Mailtrust became Rackspace’s mail division in the Fall 2007. Rackspace currently\nhosts email for over 1 million users and thousands of companies on hundreds of servers.\nRequirements/The Problem\nTransferring the mail generated by Rackspace customers through the system generates\na considerable “paper” trail, in the form of around 150 GB per day of logs in various\nformats. It is extremely helpful to aggregate that data for growth planning purposes\nand to understand how customers use our applications, and the records are also a boon\nfor troubleshooting problems in the system.\nIf an email fails to be delivered, or a customer is unable to log in, it is vital that our\ncustomer support team is able to find enough information about the problem to begin\nthe debugging process. To make it possible to find that information quickly, we cannot\nleave the logs on the machines that generated them or in their original format. Instead,\nwe use Hadoop to do a considerable amount of processing, with the end result being\nLucene indexes that customer support can query.\nLogs\nTwo of our highest volume log formats are produced by the Postfix mail transfer agent\nand Microsoft Exchange Server. All mail that travels through our systems touches\nPostfix at some point, and the majority of messages travel through multiple Postfix\nservers. The Exchange environment is independent by necessity, but one class of Postfix\nmachines acts as an added layer of protection, and uses SMTP to transfer messages\nbetween mailboxes hosted in each environment.\nThe messages travel through many machines, but each server only knows enough about\nthe destination of the mail to transfer it to the next responsible server. Thus, in order\nto build the complete history of a message, our log processing system needs to have a\nLog Processing at Rackspace | 439global view of the system. This is where Hadoop helps us immensely: as our system\ngrows, so does the volume of logs. For our log processing logic to stay viable, we had\nto ensure that it would scale, and MapReduce was the perfect framework for that\ngrowth.\nBrief History\nEarlier versions of our log processing system were based on MySQL, but as we gained\nmore and more logging machines, we reached the limits of what a single MySQL server\ncould process. The database schema was already reasonably denormalized, which\nwould have made it less difficult to shard, but MySQL’s partitioning support was still\nvery weak at that point in time. Rather than implementing our own sharding and pro-\ncessing solution around MySQL, we chose to use Hadoop.\nChoosing Hadoop\nAs soon as you shard the data in a RDBMS system, you lose a lot of the advantages of\nSQL for performing analysis of your dataset. Hadoop gives us the ability to easily proc-\ness all of our data in parallel using the same algorithms we would for smaller datasets.\nCollection and Storage\nLog collection\nThe servers generating the logs we process are distributed across multiple data centers,\nbut we currently have a single Hadoop cluster, located in one of those data centers (see\nFigure 14-8). In order to aggregate the logs and place them into the cluster, we use the\nUnix syslog replacement syslog-ng and some simple scripts to control the creation of\nfiles in Hadoop.\nWithin a data center, syslog-ng is used to transfer logs from a source machine to a load-\nbalanced set of collector machines. On the collectors, each type of log is aggregated into\na single stream, and lightly compressed with gzip (step A in Figure 14-8). From remote\ncollectors, logs can be transferred through an SSH tunnel cross-data center to collectors\nthat are local to the Hadoop cluster (step B).\n440 | Chapter 14: Case StudiesFigure 14-8. Hadoop data flow at Rackspace\nOnce the compressed log stream reaches a local collector, it can be written to Hadoop\n(step C). We currently use a simple Python script that buffers input data to disk, and\nperiodically pushes the data into the Hadoop cluster using the Hadoop command-line\ninterface. The script copies the log buffers to input folders in Hadoop when they reach\na multiple of the Hadoop block size, or when enough time has passed.\nThis method of securely aggregating logs from different data centers was developed\nbefore SOCKS support was added to Hadoop via the hadoop.rpc.socket.fac\ntory.class.default parameter and SocksSocketFactory class. By using SOCKS support\nand the HDFS API directly from remote collectors, we could eliminate one disk write\nand a lot of complexity from the system. We plan to implement a replacement using\nthese features in future development sprints.\nOnce the raw logs have been placed in Hadoop, they are ready for processing by our\nMapReduce jobs.\nLog storage\nOur Hadoop cluster currently contains 15 datanodes with commodity CPUs and three\n500 GB disks each. We use a default replication factor of three for files that need to\nsurvive for our archive period of six months, and two for anything else.\nThe Hadoop namenode uses hardware identical to the datanodes. To provide reason-\nably high availability, we use two secondary namenodes, and a virtual IP that can easily\nbe pointed at any of the three machines with snapshots of the HDFS. This means that\nin a failover situation, there is potential for us to lose up to 30 minutes of data, de-\npending on the ages of the snapshots on the secondary namenodes. This is acceptable\nfor our log processing application, but other Hadoop applications may require lossless\nfailover by using shared storage for the namenode’s image.\nLog Processing at Rackspace | 441MapReduce for Logs\nProcessing\nIn distributed systems, the sad truth of unique identifiers is that they are rarely actually\nunique. All email messages have a (supposedly) unique identifier called a message-id\nthat is generated by the host where they originated, but a bad client could easily send\nout duplicates. In addition, since the designers of Postfix could not trust the message-\nid to uniquely identify the message, they were forced to come up with a separate ID\ncalled a queue-id, which is guaranteed to be unique only for the lifetime of the message\non a local machine.\nAlthough the message-id tends to be the definitive identifier for a message, in Postfix\nlogs, it is necessary to use queue-ids to find the message-id. Looking at the second line\nin Example 14-1 (which is formatted to better fit the page), you will see the hex string\n1DBD21B48AE, which is the queue-id of the message that the log line refers to. Because\ninformation about a message (including its message-id) is output as separate lines when\nit is collected (potentially hours apart), it is necessary for our parsing code to keep state\nabout messages.\nExample 14-1. Postfix log lines\nNov 12 17:36:54 gate8.gate.sat.mlsrvr.com postfix/smtpd[2552]: connect from hostname\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/qmgr[9489]: 1DBD21B48AE:\nfrom=<mapreduce@rackspace.com>, size=5950, nrcpt=1 (queue active)\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/smtpd[28085]: disconnect from\nhostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/smtpd[22593]: too many errors\nafter DATA from hostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/smtpd[22593]: disconnect from\nhostname\nNov 12 17:36:54 gate10.gate.sat.mlsrvr.com postfix/smtpd[10311]: connect from\nhostname\nNov 12 17:36:54 relay2.relay.sat.mlsrvr.com postfix/smtp[28107]: D42001B48B5:\nto=<mapreduce@rackspace.com>, relay=hostname[ip], delay=0.32, delays=0.28/0/0/0.04,\ndsn=2.0.0, status=sent (250 2.0.0 Ok: queued as 1DBD21B48AE)\nNov 12 17:36:54 gate20.gate.sat.mlsrvr.com postfix/smtpd[27168]: disconnect from\nhostname\nNov 12 17:36:54 gate5.gate.sat.mlsrvr.com postfix/qmgr[1209]: 645965A0224: removed\nNov 12 17:36:54 gate2.gate.sat.mlsrvr.com postfix/smtp[15928]: 732196384ED: to=<m\napreduce@rackspace.com>, relay=hostname[ip], conn_use=2, delay=0.69, delays=0.04/\n0.44/0.04/0.17, dsn=2.0.0, status=sent (250 2.0.0 Ok: queued as 02E1544C005)\nNov 12 17:36:54 gate2.gate.sat.mlsrvr.com postfix/qmgr[13764]: 732196384ED: removed\nNov 12 17:36:54 gate1.gate.sat.mlsrvr.com postfix/smtpd[26394]: NOQUEUE: reject: RCP\nT from hostname 554 5.7.1 <mapreduce@rackspace.com>: Client host rejected: The\nsender's mail server is blocked; from=<mapreduce@rackspace.com> to=<mapred\nuce@rackspace.com> proto=ESMTP helo=<mapreduce@rackspace.com>\nFrom a MapReduce perspective, each line of the log is a single key-value pair. In phase\n1, we need to map all lines with a single queue-id key together, and then reduce them\nto determine if the log message values indicate that the queue-id is complete.\n442 | Chapter 14: Case StudiesSimilarly, once we have a completed queue-id for a message, we need to group by the\nmessage-id in phase 2. We Map each completed queue-id with its message-id as key,\nand a list of its log lines as the value. In Reduce, we determine whether all of the queue-\nids for the message-id indicate that the message left our system.\nTogether, the two phases of the mail log MapReduce job and their InputFormat and\nOutputFormat form a type of staged event-driven architecture (SEDA). In SEDA, an ap-\nplication is broken up into multiple “stages,” that are separated by queues. In a Hadoop\ncontext, a queue could be either an input folder in HDFS that a MapReduce job con-\nsumes from, or the implicit queue that MapReduce forms between the Map and Reduce\nsteps.\nIn Figure 14-9, the arrows between stages represent the queues, with a dashed arrow\nbeing the implicit MapReduce queue. Each stage can send a key-value pair (SEDA calls\nthem events or messages) to another stage via these queues.\nFigure 14-9. MapReduce chain\nPhase 1: Map. During the first phase of our Mail log processing job, the inputs to the Map\nstage are either a line number key and log message value or a queue-id key to an array\nof log-message values. The first type of input is generated when we process a raw logfile\nLog Processing at Rackspace | 443from the queue of input files, and the second type is an intermediate format that rep-\nresents the state of a queue-id we have already attempted to process, but which was\nrequeued because it was incomplete.\nIn order to accomplish this dual input, we implemented a Hadoop InputFormat that\ndelegates the work to an underlying SequenceFileRecordReader or LineRecordReader,\ndepending on the file extension of the input FileSplit. The two input formats come\nfrom different input folders (queues) in HDFS.\nPhase 1: Reduce. During this phase, the Reduce stage determines whether the queue-id\nhas enough lines to be considered completed. If the queue-id is completed, we output\nthe message-id as key, and a HopWritable object as value. Otherwise, the queue-id is set\nas the key, and the array of log lines is requeued to be Mapped with the next set of raw\nlogs. This will continue until we complete the queue-id, or until it times out.\nThe HopWritable object is a POJO that implements Hadoop’s\nWritable interface. It completely describes a message from the viewpoint\nof a single server, including the sending address and IP, attempts to\ndeliver the message to other servers, and typical message header\ninformation.\nThis split output is accomplished with an OutputFormat implementation that is some-\nwhat symmetrical with our dual InputFormat. Our MultiSequenceFileOutputFormat was\nimplemented before the Hadoop API added a MultipleSequenceFileOutputFormat in\nr0.17.0, but fulfills the same type of goal: we needed our Reduce output pairs to go to\ndifferent files depending on characteristics of their keys.\nPhase 2: Map. In the next stage of the Mail log processing job, the input is a message-id\nkey, with a HopWritable value from the previous phase. This stage does not contain any\nlogic: instead, it simply combines the inputs from the first phase using the standard\nSequenceFileInputFormat and IdentityMapper.\nPhase 2: Reduce. In the final Reduce stage, we want to see whether all of the HopWrita\nbles we have collected for the message-id represent a complete message path through\nour system. A message path is essentially a directed graph (which is typically acyclic,\nbut it may contain loops if servers are misconfigured). In this graph, a vertex is a server,\nwhich can be labeled with multiple queue-ids, and attempts to deliver the message from\none server to another are edges. For this processing, we use the JGraphT graph library.\nFor output, we again use the MultiSequenceFileOutputFormat. If the Reducer decides\nthat all of the queue-ids for a message-id create a complete message path, then the\nmessage is serialized and queued for the SolrOutputFormat. Otherwise, the\nHopWritables for the message are queued for phase 2: Map stage to be reprocessed with\nthe next batch of queue-ids.\n444 | Chapter 14: Case StudiesThe SolrOutputFormat contains an embedded Apache Solr instance—in the fashion that\nwas originally recommended by the Solr wiki—to generate an index on local disk.\nClosing the OutputFormat then involves compressing the disk index to the final desti-\nnation for the output file. This approach has a few advantages over using Solr’s HTTP\ninterface or using Lucene directly:\n• We can enforce a Solr schema\n• Map and Reduce remain idempotent\n• Indexing load is removed from the Search nodes\nWe currently use the default HashPartitioner class to decide which Reduce task will\nreceive particular keys, which means that the keys are semirandomly distributed. In a\nfuture iteration of the system, we’d like to implement a new Partitioner to split by\nsending address instead (our most common search term). Once the indexes are split\nby sender, we can use the hash of the address to determine where to merge or query\nfor an index, and our search API will only need to communicate with the relevant nodes.\nMerging for near-term search\nAfter a set of MapReduce phases have completed, a different set of machines are notified\nof the new indexes, and can pull them for merging. These Search nodes are running\nApache Tomcat and Solr instances to host completed indexes, along with a service to\npull and merge the indexes to local disk (step D in Figure 14-8).\nEach compressed file from SolrOutputFormat is a complete Lucene index, and Lucene\nprovides the IndexWriter.addIndexes() methods for quickly merging multiple indexes.\nOur MergeAgent service decompresses each new index into a Lucene RAMDirectory or\nFSDirectory (depending on size), merges them to local disk, and sends a <commit/>\nrequest to the Solr instance hosting the index to make the changed index visible to\nqueries.\nSharding. The Query/Management API is a thin layer of PHP code that handles sharding\nthe output indexes across all of the Search nodes. We use a simple implementation of\nconsistent hashing to decide which Search nodes are responsible for each index file.\nCurrently, indexes are sharded by their creation time, and then by their hashed file-\nname, but we plan to replace the filename hash with a sending address hash at some\npoint in the future (see phase 2: Reduce).\nBecause HDFS already handles replication of the Lucene indexes, there is no need to\nkeep multiple copies available in Solr. Instead, in a failover situation, the Search node\nis completely removed, and other nodes become responsible for merging the indexes.\nSearch results. With this system, we’ve achieved a 15-minute turnaround time from log\ngeneration to availability of a search result for our Customer Support team.\nOur search API supports the full Lucene query syntax, so we commonly see complex\nqueries like:\nLog Processing at Rackspace | 445sender:""mapreduce@rackspace.com"" -recipient:""hadoop@rackspace.com""\nrecipient:""@rackspace.com"" short-status:deferred timestamp:[1228140900 TO 2145916799]\nEach result returned by a query is a complete serialized message path, which indicates\nwhether individual servers and recipients received the message. We currently display\nthe path as a 2D graph (Figure 14-10) that the user can interact with to expand points\nof interest, but there is a lot of room for improvement in the visualization of this data.\nFigure 14-10. Data tree\nArchiving for analysis\nIn addition to providing short-term search for Customer Support, we are also interested\nin performing analysis of our log data.\nEvery night, we run a series of MapReduce jobs with the day’s indexes as input. We\nimplemented a SolrInputFormat that can pull and decompress an index, and emit each\ndocument as a key-value pair. With this InputFormat, we can iterate over all message\npaths for a day, and answer almost any question about our mail system, including:\n• Per domain data (viruses, spam, connections, recipients)\n• Most effective spam rules\n446 | Chapter 14: Case Studies•\n•\n•\n•\nLoad generated by specific users\nReasons for message bounces\nGeographical sources of connections\nAverage latency between specific machines\nSince we have months of compressed indexes archived in Hadoop, we are also able to\nretrospectively answer questions that our nightly log summaries leave out. For instance,\nwe recently wanted to determine the top sending IP addresses per month, which we\naccomplished with a simple one-off MapReduce job.\n—Stu Hood\nCascading\nCascading is an open source Java library and application programming interface (API)\nthat provides an abstraction layer for MapReduce. It allows developers to build com-\nplex, mission-critical data processing applications that run on Hadoop clusters.\nThe Cascading project began in the summer of 2007. Its first public release, version\n0.1, launched in January 2008. Version 1.0 was released in January 2009. Binaries,\nsource code, and add-on modules can be downloaded from the project website, http:\n//www.cascading.org/.\n“Map” and “Reduce” operations offer powerful primitives. However, they tend to be\nat the wrong level of granularity for creating sophisticated, highly composable code\nthat can be shared among different developers. Moreover, many developers find it dif-\nficult to “think” in terms of MapReduce when faced with real-world problems.\nTo address the first issue, Cascading substitutes the “keys” and “values” used in Map-\nReduce with simple field names and a data tuple model, where a tuple is simply a list\nof values. For the second issue, Cascading departs from Map and Reduce operations\ndirectly by introducing higher-level abstractions as alternatives: Functions, Filters, Ag-\ngregators, and Buffers.\nOther alternatives began to emerge at about the same time as the project’s initial public\nrelease, but Cascading was designed to complement them. Consider that most of these\nalternative frameworks impose pre- and post-conditions, or other expectations.\nFor example, in several other MapReduce tools, you must preformat, filter, or import\nyour data into the Hadoop Filesystem (HDFS) prior to running the application. That\nstep of preparing the data must be performed outside of the programming abstraction.\nIn contrast, Cascading provides means to prepare and manage your data as integral\nparts of the programming abstraction.\nThis case study begins with an introduction to the main concepts of Cascading, then\nfinishes with an overview of how ShareThis uses Cascading in its infrastructure.\nCascading | 447Please see the Cascading User Guide on the project website for a more in-depth pre-\nsentation of the Cascading processing model.\nFields, Tuples, and Pipes\nThe MapReduce model uses keys and values to link input data to the Map function,\nthe Map function to the Reduce function, and the Reduce function to the output data.\nBut as we know, real-world Hadoop applications are usually more than one MapRe-\nduce job chained together. Consider the canonical word count example implemented\nin MapReduce. If you needed to sort the numeric counts in descending order, not an\nunlikely requirement, it would need to be done in a second MapReduce job.\nSo, in the abstract, keys and values not only bind Map to Reduce, but Reduce to the\nnext Map, and then to the next Reduce, and so on (Figure 14-11). That is, key-value\npairs are sourced from input files and stream through chains of Map and Reduce op-\nerations and finally rest in an output file. When you implement enough of these chained\nMapReduce applications, you start to see a well-defined set of key/value manipulations\nused over and over again to modify the key/value data stream.\nFigure 14-11. Counting and sorting in MapReduce\nCascading simplifies this by abstracting away keys and values and replacing them with\ntuples that have corresponding field names, similar in concept to tables and column\nnames in a relational database. And during processing, streams of these Fields and\nTuples are then manipulated as they pass through user-defined operations linked to-\ngether by Pipes (Figure 14-12).\n448 | Chapter 14: Case StudiesFigure 14-12. Pipes linked by Fields and Tuples\nSo, MapReduce keys and values are reduced to:\nFields\nFields are a collection of either String names (like “first_name”), numeric positions\n(like 2, or –1, for the third and last position, respectively), or a combination of\nboth, very much like column names. So Fields are used to declare the names of\nvalues in a Tuple, and to select values by name from a Tuple. The later is like a\nSQL select call.\nTuple\nA Tuple is simply an array of java.lang.Comparable objects. A Tuple is very much\nlike a database row or record.\nAnd the Map and Reduce operations are abstracted behind one or more Pipe instances\n(Figure 14-13):\nEach\nThe Each pipe processes a single input Tuple at a time. It may apply either a Func\ntion or a Filter operation (described shortly) to the input tuple.\nGroupBy\nThe GroupBy pipe groups tuples on grouping fields. It behaves just like the SQL\ngroup by statement. It can also merge multiple input tuple streams into a single\nstream, if they all share the same field names.\nCoGroup\nThe CoGroup pipe both joins multiple tuple streams together by common field\nnames, and it also groups the tuples by the common grouping fields. All standard\njoin types (inner, outer, etc.) and custom joins can be used across two or more\ntuple streams.\nEvery\nThe Every pipe processes a single grouping of tuples at a time, where the group\nwas grouped by a GroupBy or CoGroup pipe. The Every pipe may apply either an\nAggregator or a Buffer operation to the grouping.\nCascading | 449SubAssembly\nThe SubAssembly pipe allows for nesting of assemblies inside a single pipe, which\ncan in turn be nested in more complex assemblies.\nFigure 14-13. Pipe types\nAll these pipes are chained together by the developer into “pipe assemblies” in which\neach assembly can have many input Tuple streams (sources) and many output Tuple\nstreams (sinks) (see Figure 14-14).\nFigure 14-14. A simple PipeAssembly\nOn the surface, this might seem more complex than the traditional MapReduce model.\nAnd admittedly there are more concepts here than Map, Reduce, Key, and Value. But\nin practice, there are many more concepts that must all work in tandem to provide\ndifferent behaviors.\nFor example, if a developer wanted to provide a “secondary sorting” of reducer values,\nshe would need to implement Map, Reduce, a “composite” Key (two Keys nested in a\nparent Key), Value, Partitioner, an “output value grouping” Comparator, and an “out-\nput key” Comparator, all of which would be coupled to one another in varying ways,\nand very likely nonreusable in subsequent applications.\n450 | Chapter 14: Case StudiesIn Cascading, this would be one line of code: new GroupBy(<previous>, <grouping\nfields>, <secondary sorting fields>), where previous is the pipe that came before.\nOperations\nAs mentioned earlier, Cascading departs from MapReduce by introducing alternative\noperations that either are applied to individual Tuples or groups of Tuples (Fig-\nure 14-15):\nFunction\nA Function operates on individual input Tuples and may return zero or more output\nTuples for every one input. Functions are applied by the Each pipe.\nFilter\nA Filter is a special kind of function that returns a boolean value indicating\nwhether the current input Tuple should be removed from the Tuple stream. A\nFunction could serve this purpose, but the Filter is optimized for this case, and\nmany filters can be grouped by “logical” filters like And, Or, Xor, and Not, rapidly\ncreating more complex filtering operations.\nAggregator\nAn Aggregator performs some operation against a group of Tuples, where the\ngrouped Tuples are grouped by a common set of field values. For example, all\nTuples having the same “last-name” value. Common Aggregator implementations\nwould be Sum, Count, Average, Max, and Min.\nBuffer\nA Buffer is similar to the Aggregator, except it is optimized to act as a “sliding\nwindow” across all the Tuples in a unique grouping. This is useful when the de-\nveloper needs to efficiently insert missing values in an ordered set of Tuples (like\na missing date or duration), or create a running average. Usually Aggregator is the\noperation of choice when working with groups of Tuples, since many\nAggregators can be chained together very efficiently, but sometimes a Buffer is the\nbest tool for the job.\nCascading | 451Figure 14-15. Operation types\nOperations are bound to pipes when the pipe assembly is created (Figure 14-16).\nFigure 14-16. An assembly of operations\nThe Each and Every pipes provide a simple mechanism for selecting some or all values\nout of an input tuple before being passed to its child operation. And there is a simple\nmechanism for merging the operation results with the original input Tuple to create\nthe output Tuple. Without going into great detail, this allows for each operation to\nonly care about argument Tuple values and fields, not the whole set of fields in the\ncurrent input Tuple. Subsequently, operations can be reusable across applications the\nsame way Java methods can be reusable.\nFor example, in Java, a method declared as concatenate(String first, String\nsecond) is more abstract than concatenate(Person person). In the second case, the\nconcatenate() function must “know” about the Person object; in the first case, it is\nagnostic to where the data came from. Cascading operations exhibit this same quality.\nTaps, Schemes, and Flows\nIn many of the previous diagrams, there are references to “sources” and “sinks.” In\nCascading, all data is read from or written to Tap instances, but is converted to and\nfrom Tuple instances via Scheme objects:\n452 | Chapter 14: Case StudiesTap\nA Tap is responsible for the “how” and “where” parts of accessing data. For exam-\nple, is the data on HDFS or the local filesystem? In Amazon S3 or over HTTP?\nScheme\nA Scheme is responsible for reading raw data and converting it to a Tuple and/or\nwriting a Tuple out into raw data, where this “raw” data can be lines of text, Ha-\ndoop binary sequence files, or some proprietary format.\nNote that Taps are not part of a pipe assembly, and so they are not a type of Pipe.\nBut they are connected with pipe assemblies when they are made cluster-executable.\nWhen a pipe assembly is connected with the necessary number of source and sink Tap\ninstances, we get a Flow. A Flow is created when a pipe assembly is connected with its\nrequired number of source and sink taps, and the Taps either emit or capture the field\nnames the pipe assembly expects. That is, if a Tap emits a Tuple with the field name\n“line” (by reading data from a file on HDFS), the head of the pipe assembly must be\nexpecting a “line” value as well. Otherwise, the process that connects the pipe assembly\nwith the Taps will immediately fail with an error.\nSo pipe assemblies are really data process definitions, and are not “executable” on their\nown. They must be connected to source and sink Tap instances before they can run on\na cluster. This separation between Taps and pipe assemblies is part of what makes\nCascading so powerful.\nIf you think of pipe assemblies like a Java class, then a Flow is like a Java Object instance\n(Figure 14-17). That is, the same pipe assembly can be “instantiated” many times into\nnew Flows, in the same application, without fear of any interference between them.\nThis allows pipe assemblies to be created and shared like standard Java libraries.\nFigure 14-17. A Flow\nCascading | 453Cascading in Practice\nNow that we know what Cascading is and have a good idea how it works, what does\nan application written in Cascading look like? See Example 14-2.\nExample 14-2. Word count and sort\nScheme sourceScheme =\nnew TextLine(new Fields(""line""));\nTap source =\nnew Hfs(sourceScheme, inputPath);\nScheme sinkScheme = new TextLine();\nTap sink =\nnew Hfs(sinkScheme, outputPath, SinkMode.REPLACE);\nPipe assembly = new Pipe(""wordcount"");\nString regexString = ""(?<!\\\\pL)(?=\\\\pL)[^ ]*(?<=\\\\pL)(?!\\\\pL)"";\nFunction regex = new RegexGenerator(new Fields(""word""), regexString);\nassembly =\nnew Each(assembly, new Fields(""line""), regex);\nassembly =\nnew GroupBy(assembly, new Fields(""word""));\nAggregator count = new Count(new Fields(""count""));\nassembly = new Every(assembly, count);\nassembly =\nnew GroupBy(assembly, new Fields(""count""), new Fields(""word""));\nFlowConnector flowConnector = new FlowConnector();\nFlow flow =\nflowConnector.connect(""word-count"", source, sink, assembly);\nflow.complete();\nWe create a new Scheme that can read simple text files, and emits a new Tuple for\neach line in a field named “line,” as declared by the Fields instance.\nWe create a new Scheme that can write simple text files, and expects a Tuple with any\nnumber of fields/values. If more than one value, they will be tab-delimited in the\noutput file.\nWe create source and sink Tap instances that reference the input file and output\ndirectory, respectively. The sink Tap will overwrite any file that may already exist.\nWe construct the head of our pipe assembly, and name it “wordcount.” This name\nis used to bind the source and sink taps to the assembly. Multiple heads or tails\nwould require unique names.\nWe construct an Each pipe with a function that will parse the “line” field into a new\nTuple for each word encountered.\n454 | Chapter 14: Case StudiesWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\nvalue in the field “word.”\nWe construct an Every pipe with an Aggregator that will count the number of\nTuples in every unique word group. The result is stored in a field named “count.”\nWe construct a GroupBy pipe that will create a new Tuple grouping for each unique\nvalue in the field “count,” and secondary sort each value in the field “word.” The\nresult will be a list of “count” and “word” values with “count” sorted in increasing\norder.\nWe connect the pipe assembly to its sources and sinks into a Flow, and then execute\nthe Flow on the cluster.\nIn the example, we count the words encountered in the input document, and we sort\nthe counts in their natural order (ascending). And if some words have the same “count”\nvalue, these words are sorted in their natural order (alphabetical).\nOne obvious problem with this example is that some words might have uppercase\nletters; for example, “the” and “The” when the word comes at the beginning of a sen-\ntence. So we might decide to insert a new operation to force all the words to\nlowercase, but we realize that all future applications that need to parse words from\ndocuments should have the same behavior, so we decide to create a reusable pipe\nSubAssembly, just like we would by creating a subroutine in a traditional application\n(see Example 14-3).\nExample 14-3. Creating a SubAssembly\npublic class ParseWordsAssembly extends SubAssembly\n{\npublic ParseWordsAssembly(Pipe previous)\n{\nString regexString = ""(?<!\\\\pL)(?=\\\\pL)[^ ]*(?<=\\\\pL)(?!\\\\pL)"";\nFunction regex = new RegexGenerator(new Fields(""word""), regexString);\nprevious = new Each(previous, new Fields(""line""), regex);\nString exprString = ""word.toLowerCase()"";\nFunction expression =\nnew ExpressionFunction(new Fields(""word""), exprString, String.class);\nprevious = new Each(previous, new Fields(""word""), expression);\n}\nsetTails(previous);\n}\nWe subclass the SubAssembly class, which is itself a kind of Pipe.\nWe create a Java expression function that will call toLowerCase() on the String value\nin the field named “word.” We must also pass in the Java type the expression expects\n“word” to be, in this case, String. (http://www.janino.net/ is used under the covers.)\nCascading | 455We must tell the SubAssembly superclass where the tail ends of our pipe subassembly\nare.\nFirst, we create a SubAssembly pipe to hold our “parse words” pipe assembly. Since this\nis a Java class, it can be reused in any other application, as long as there is an incoming\nfield named “word” (Example 14-4). Note that there are ways to make this function\neven more generic, but they are covered in the Cascading User Guide.\nExample 14-4. Extending word count and sort with a SubAssembly\nScheme sourceScheme = new TextLine(new Fields(""line""));\nTap source = new Hfs(sourceScheme, inputPath);\nScheme sinkScheme = new TextLine(new Fields(""word"", ""count""));\nTap sink = new Hfs(sinkScheme, outputPath, SinkMode.REPLACE);\nPipe assembly = new Pipe(""wordcount"");\nassembly =\nnew ParseWordsAssembly(assembly);\nassembly = new GroupBy(assembly, new Fields(""word""));\nAggregator count = new Count(new Fields(""count""));\nassembly = new Every(assembly, count);\nassembly = new GroupBy(assembly, new Fields(""count""), new Fields(""word""));\nFlowConnector flowConnector = new FlowConnector();\nFlow flow = flowConnector.connect(""word-count"", source, sink, assembly);\nflow.complete();\nWe replace the Each from the previous example with our ParseWordsAssembly pipe.\nFinally, we just substitute in our new SubAssembly right where the previous Every and\nword parser function was used in the previous example. This nesting can continue as\ndeep as necessary.\nFlexibility\nTake a step back and see what this new model has given us. Or better yet, what it has\ntaken away.\nYou see, we no longer think in terms of MapReduce jobs, or Mapper and Reducer\ninterface implementations, and how to bind or link subsequent MapReduce jobs to the\nones that precede them. During runtime, the Cascading “planner” figures out the op-\ntimal way to partition the pipe assembly into MapReduce jobs, and manages the link-\nages between them (Figure 14-18).\n456 | Chapter 14: Case StudiesFigure 14-18. How a Flow translates to chained MapReduce jobs\nBecause of this, developers can build applications of arbitrary granularity. They can\nstart with a small application that just filters a logfile, but then can iteratively build up\nmore features into the application as needed.\nSince Cascading is an API and not a syntax like strings of SQL, it is more flexible. First\noff, developers can create domain-specific languages (DSLs) using their favorite lan-\nguage, like Groovy, JRuby, Jython, Scala, and others (see the project site for examples).\nSecond, developers can extend various parts of Cascading, like allowing custom Thrift\nor JSON objects to be read and written to and allowing them to be passed through the\nTuple stream.\nHadoop and Cascading at ShareThis\nShareThis is a sharing network that makes it simple to share any online content. With\nthe click of a button on a web page or browser plug-in, ShareThis allows users to\nseamlessly access their contacts and networks from anywhere online and share the\ncontent through email, IM, Facebook, Digg, mobile SMS, etc. without ever leaving the\ncurrent page. Publishers can deploy the ShareThis button to tap the service’s universal\nsharing capabilities to drive traffic, stimulate viral activity, and track the sharing of\nonline content. ShareThis also simplifies social media services by reducing clutter on\nweb pages and providing instant distribution of content across social networks, affiliate\ngroups, and communities.\nAs ShareThis users share pages and information through the online widgets, a contin-\nuous stream of events enter the ShareThis network. These events are first filtered and\nprocessed, and then handed to various backend systems, including AsterData,\nHypertable, and Katta.\nThe volume of these event can be huge, too large to process with traditional systems.\nThis data can also be very “dirty” thanks to “injection attacks” from rogue systems,\nbrowser bugs, or faulty widgets. For this reason, ShareThis chose to deploy Hadoop as\nthe preprocessing and orchestration frontend to their backend systems. They also chose\nto use Amazon Web Services to host their servers, on the Elastic Computing Cloud\nCascading | 457(EC2), and provide long term storage, on the Simple Storage Service (S3), with an eye\ntoward leveraging Elastic MapReduce (EMR).\nIn this overview, we will focus on the “log processing pipeline” (Figure 14-19). The log\nprocessing pipeline simply takes data stored in an S3 bucket, processes it (described\nshortly), and stores the results back into another bucket. Simple Queue Service (SQS)\nis used to coordinate the events that mark the start and completion of data processing\nruns. Downstream other processes pull data that load AsterData, pull URL lists from\nHypertable to source a web crawl, or pull crawled page data to create Lucene indexes\nfor use by Katta. Note that Hadoop is central to the ShareThis architecture. It is used\nto coordinate the processing and movement of data between architectural components.\nFigure 14-19. The ShareThis log processing pipeline\nWith Hadoop as the frontend, all the event logs can be parsed, filtered, cleaned, and\norganized by a set of rules before ever being loaded into the AsterData cluster or used\nby any other component. AsterData is a clustered data warehouse that can support\nlarge datasets and allow for complex ad hoc queries using a standard SQL syntax.\nShareThis chose to clean and prepare the incoming datasets on the Hadoop cluster and\nthen to load that data into the AsterData cluster for ad hoc analysis and reporting.\nThough possible with AsterData, it made a lot of sense to use Hadoop as the first stage\nin the processing pipeline to offset load on the main data warehouse.\nCascading was chosen as the primary data processing API to simplify the development\nprocess, codify how data is coordinated between architectural components, and pro-\nvide the developer-facing interface to those components. This represents a departure\nfrom more “traditional” Hadoop use cases, which essentially just query stored data.\nInstead, Cascading and Hadoop together provide better and simpler structure to the\ncomplete solution, end-to-end, and thus more value to the users.\n458 | Chapter 14: Case StudiesFor developers, Cascading made it easy to start with a simple unit test (by subclassing\ncascading.ClusterTestCase) that did simple text parsing and then to layer in more\nprocessing rules while keeping the application logically organized for maintenance.\nCascading aided this organization in a couple of ways. First, standalone operations\n(Functions, Filters, etc.) could be written and tested independently. Second, the ap-\nplication was segmented into stages: one for parsing, one for rules, and a final stage for\nbinning/collating the data, all via the SubAssembly base class described earlier.\nThe data coming from the ShareThis loggers looks a lot like Apache logs with date/\ntimestamps, share URLs, referrer URLs, and a bit of metadata. To use the data for\nanalysis downstream the URLs needed to be un-packed (parsing query-string data,\ndomain names, etc.). So a top-level SubAssembly was created to encapsulate the parsing,\nand child SubAssemblies were nested inside to handle specific fields if they were suf-\nficiently complex to parse.\nThe same was done for applying rules. As every Tuple passed through the rules SubAs\nsembly, it was marked as “bad” if any of the rules were triggered. Along with the “bad”\ntag, a description of why the record was bad was added to the Tuple for later review.\nFinally, a splitter SubAssembly was created to do two things. First, to allow for the\ntuple stream to split into two, one stream for “good” data and one for “bad” data.\nSecond, the splitter binned the data into intervals, such as every hour. To do this, only\ntwo operations were necessary. The first to create the interval from the timestamp value\nalready present in the stream, and the second to use the interval and good/bad metadata\nto create a directory path; for example, “05/good/” where “05” is 5am and “good”\nmeans the tuple passed all the rules. This path would then be used by the Cascading\nTemplateTap, a special Tap that can dynamically output tuple streams to different\nlocations based on values in the Tuple. In this case, the TemplateTap used the “path”\nvalue to create the final output path.\nThe developers also created a fourth SubAssembly, this one to apply Cascading Asser-\ntions during unit testing. These assertions double-checked that rules and parsing Sub-\nAssemblies did their job.\nIn the unit test in Example 14-5, we see the splitter isn’t being tested, but it is added in\nanother integration test not shown.\nExample 14-5. Unit testing a Flow\npublic void testLogParsing() throws IOException\n{\nHfs source = new Hfs(new TextLine(new Fields(""line"")), sampleData);\nHfs sink =\nnew Hfs(new TextLine(), outputPath + ""/parser"", SinkMode.REPLACE);\nPipe pipe = new Pipe(""parser"");\n// split ""line"" on tabs\npipe = new Each(pipe, new Fields(""line""), new RegexSplitter(""\\t""));\nCascading | 459pipe = new LogParser(pipe);\npipe = new LogRules(pipe);\n// testing only assertions\npipe = new ParserAssertions(pipe);\nFlow flow = new FlowConnector().connect(source, sink, pipe);\nflow.complete(); // run the test flow\n// verify there are 98 tuples, 2 fields, and matches the regex pattern\n// for TextLine schemes the tuples are { ""offset"", ""line }\nvalidateLength(flow, 98, 2, Pattern.compile(""^[0-9]+(\\\\t[^\\\\t]*){19}$""));\n}\nFor integration and deployment, many of the features built into Cascading allowed for\neasier integration with external systems and for greater process tolerance.\nIn production, all the SubAssemblies are joined and planned into a Flow, but instead\nof just source and sink Taps, trap Taps were planned in (Figure 14-20). Normally when\nan operation throws an exception from a remote Mapper or Reducer task, the Flow\nwill fail and kill all its managed MapReduce jobs. When a Flow has traps, any excep-\ntions are caught and the data causing the exception is saved to the Tap associated with\nthe current trap. Then the next Tuple is processed without stopping the Flow. Sometimes\nyou want your Flows to fail on errors, but in this case, the ShareThis developers knew\nthey could go back and look at the “failed” data and update their unit tests while the\nproduction system kept running. Losing a few hours of processing time was worse than\nlosing a couple of bad records.\nUsing Cascading’s event listeners, Amazon SQS could be integrated. When a Flow\nfinishes, a message is sent to notify other systems that there is data ready to be picked\nup from Amazon S3. On failure, a different message is sent, alerting other processes.\nFigure 14-20. The ShareThis log processing Flow\n460 | Chapter 14: Case StudiesThe remaining downstream processes pick up where the log processing pipeline leaves\noff on different independent clusters. The log processing pipeline today runs once a\nday, so there is no need to keep a 100-node cluster sitting around for the 23 hours it\nhas nothing to do. So it is decommissioned and recommissioned 24 hours later.\nIn the future, it would be trivial to increase this interval on smaller clusters to every 6\nhours, or 1 hour, as the business demands. Independently, other clusters are booting\nand shutting down at different intervals based on the needs of the business unit re-\nsponsible for that component. For example, the web crawler component (using Bixo,\na Cascading-based web-crawler toolkit developed by EMI and ShareThis) may run\ncontinuously on a small cluster with a companion Hypertable cluster. This on-demand\nmodel works very well with Hadoop where each cluster can be tuned for the kind of\nworkload it is expected to handle.\nSummary\nHadoop is a very powerful platform for processing and coordinating the movement of\ndata across various architectural components. Its only drawback is that the primary\ncomputing model is MapReduce.\nCascading aims to help developers build powerful applications quickly and simply,\nthrough a well-reasoned API, without needing to think in MapReduce, while leaving\nthe heavy lifting of data distribution, replication, distributed process management, and\nliveness to Hadoop.\nRead more about Cascading, join the online community, and download sample appli-\ncations by visiting the project website at http://www.cascading.org/.\n—Chris K Wensel\nTeraByte Sort on Apache Hadoop\nThis article is reproduced from the http://sortbenchmark.org/YahooHadoop.pdf, which\nwas written in May 2008. Jim Gray and his successors define a family of benchmarks to\nfind the fastest sort programs every year. TeraByte Sort and other sort benchmarks are\nlisted with winners over the years at http://sortbenchmark.org/. In April 2009, Arun\nMurthy and I won the minute sort (where the aim is to sort as much data as possible in\nunder one minute) by sorting 500 GB in 59 seconds on 1,406 Hadoop nodes. We also\nsorted a terabyte in 62 seconds on the same cluster. The cluster we used in 2009 was\nsimilar to the hardware listed below, except that the network was much better with only\n2-to-1 oversubscription between racks instead of 5-to-1 in the previous year. We also used\nLZO compression on the intermediate data between the nodes. We also sorted a petabyte\n(1015 bytes) in 975 minutes on 3,658 nodes, for an average rate of 1.03 TB/minute. See\nhttp://developer.yahoo.net/blogs/hadoop/2009/05/hadoop_sorts_a_petabyte_in_162\n.html for more details about the 2009 results.\nTeraByte Sort on Apache Hadoop | 461Apache Hadoop is an open source software framework that dramatically simplifies\nwriting distributed data-intensive applications. It provides a distributed filesystem,\nwhich is modelled after the Google File System,* and a MapReduce† implementation\nthat manages distributed computation. Since the primary primitive of MapReduce is a\ndistributed sort, most of the custom code is glue to get the desired behavior.\nI wrote three Hadoop applications to run the terabyte sort:\n1. TeraGen is a MapReduce program to generate the data.\n2. TeraSort samples the input data and uses MapReduce to sort the data into a total\norder.\n3. TeraValidate is a MapReduce program that validates the output is sorted.\nThe total is around 1,000 lines of Java code, which will be checked in to the Hadoop\nexample directory.\nTeraGen generates output data that is byte-for-byte equivalent to the C version including\nthe newlines and specific keys. It divides the desired number of rows by the desired\nnumber of tasks and assigns ranges of rows to each map. The map jumps the random\nnumber generator to the correct value for the first row and generates the following rows.\nFor the final run, I configured TeraGen to use 1,800 tasks to generate a total of 10 billion\nrows in HDFS, with a block size of 512 MB.\nTeraSort is a standard MapReduce sort, except for a custom partitioner that uses a\nsorted list of N−1 sampled keys that define the key range for each reduce. In particular,\nall keys such that sample[i−1] <= key < sample[i] are sent to reduce i. This guarantees\nthat the output of reduce i are all less than the output of reduce i+1. To speed up the\npartitioning, the partitioner builds a two-level trie that quickly indexes into the list of\nsample keys based on the first two bytes of the key. TeraSort generates the sample keys\nby sampling the input before the job is submitted and writing the list of keys into HDFS.\nI wrote an input and output format, which are used by all 3 applications, that read and\nwrite the text files in the right format. The output of the reduce has replication set to\n1, instead of the default 3, because the contest does not require the output data be\nreplicated on to multiple nodes. I configured the job with 1,800 maps and 1,800 reduces\nand io.sort.mb, io.sort.factor, fs.inmemory.size.mb, and task heap size sufficient that\ntransient data was never spilled to disk other at the end of the map. The sampler used\n100,000 keys to determine the reduce boundaries, although as can be seen in Fig-\nure 14-21, the distribution between reduces was hardly perfect and would benefit from\nmore samples. You can see the distribution of running tasks over the job run in Fig-\nure 14-22.\n* S. Ghemawat, H. Gobioff, and S.-T. Leung. “The Google File System.” In 19th Symposium on Operating\nSystems Principles (October 2003), Lake George, NY: ACM.\n† J. Dean and S. Ghemawat. “MapReduce: Simplified Data Processing on Large Clusters.” In Sixth Symposium\non Operating System Design and Implementation (December 2004), San Francisco, CA.\n462 | Chapter 14: Case StudiesFigure 14-21. Plot of reduce output size versus finish time\nFigure 14-22. Number of tasks in each phase across time\nTeraByte Sort on Apache Hadoop | 463TeraValidate ensures that the output is globally sorted. It creates one map per file in\nthe output directory, and each map ensures that each key is less than or equal to the\nprevious one. The map also generates records with the first and last keys of the file, and\nthe reduce ensures that the first key of file i is greater than the last key of file i−1. Any\nproblems are reported as output of the reduce with the keys that are out of order.\nThe cluster I ran on was:\n•\n•\n•\n•\n•\n•\n•\n•\n•\n910 nodes\n2 quad core Xeons at 2.0ghz per a node\n4 SATA disks per a node\n8 G RAM per a node\n1 gigabit Ethernet on each node\n40 nodes per a rack\n8 gigabit Ethernet uplinks from each rack to the core\nRed Hat Enterprise Linux Server release 5.1 (kernel 2.6.18)\nSun Java JDK 1.6.0_05-b13\nThe sort completed in 209 seconds (3.48 minutes). I ran Hadoop trunk (pre-0.18.0) with\npatches for HADOOP-3443 and HADOOP-3446, which were required to remove in-\ntermediate writes to disk. Although I had the 910 nodes mostly to myself, the network\ncore was shared with another active 2,000-node cluster, so the times varied a lot de-\npending on the other activity.\n—Owen O’Malley, Yahoo!\n464 | Chapter 14: Case StudiesAPPENDIX A\nInstalling Apache Hadoop\nIt’s easy to install Hadoop on a single machine to try it out. (For installation on a cluster,\nplease refer to Chapter 9.) The quickest way is to download and run a binary release\nfrom an Apache Software Foundation Mirror.\nIn this appendix, we cover how to install Hadoop Core, HDFS, and MapReduce. In-\nstructions for installing Pig, HBase, and ZooKeeper are included in the relevant chapter\n(Chapters 11, 12, and 13).\nPrerequisites\nHadoop is written in Java, so you will need to have Java installed on your machine,\nversion 6 or later. Sun's JDK is the one most widely used with Hadoop, although others\nhave been reported to work.\nHadoop runs on Unix and on Windows. Linux is the only supported production plat-\nform, but other flavors of Unix (including Mac OS X) can be used to run Hadoop for\ndevelopment. Windows is only supported as a development platform, and additionally\nrequires Cygwin to run. During the Cygwin installation process, you should include\nthe openssh package if you plan to run Hadoop in pseudo-distributed mode (see fol-\nlowing explanation).\nInstallation\nStart by deciding which user you’d like to run Hadoop as. For trying out Hadoop or\ndeveloping Hadoop programs, it is simplest to run Hadoop on a single machine using\nyour own user account.\nDownload a stable release, which is packaged as a gzipped tar file, from the Apache\nHadoop releases page (http://hadoop.apache.org/core/releases.html) and unpack it\nsomewhere on your filesystem:\n% tar xzf hadoop-x.y.z.tar.gz\n465Before you can run Hadoop, you need to tell it where Java is located on your system.\nIf you have the JAVA_HOME environment variable set to point to a suitable Java installa-\ntion, that will be used, and you don’t have to configure anything further. Otherwise,\nyou can set the Java installation that Hadoop uses by editing conf/hadoop-env.sh, and\nspecifying the JAVA_HOME variable. For example, on my Mac I changed the line to read:\nexport JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6.0/Home\nto point to version 1.6.0 of Java. On Ubuntu, the equivalent line is:\nexport JAVA_HOME=/usr/lib/jvm/java-6-sun\nIt’s very convenient to create an environment variable that points to the Hadoop in-\nstallation directory (HADOOP_INSTALL, say) and to put the Hadoop binary directory on\nyour command-line path. For example:\n% export HADOOP_INSTALL=/home/tom/hadoop-x.y.z\n% export PATH=$PATH:$HADOOP_INSTALL/bin\nCheck that Hadoop runs by typing:\n% hadoop version\nHadoop 0.20.0\nSubversion https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.20 -r 763504\nCompiled by ndaley on Thu Apr 9 05:18:40 UTC 2009\nConfiguration\nEach component in Hadoop is configured using an XML file. Core properties go in\ncore-site.xml, HDFS properties go in hdfs-site.xml, and MapReduce properties go in\nmapred-site.xml. These files are all located in the conf subdirectory.\nIn earlier versions of Hadoop, there was a single site configuration file\nfor the Core, HDFS, and MapReduce components, called hadoop-\nsite.xml. From release 0.20.0 onward this file has been split into three:\none for each component. The property names have not changed, just\nthe configuration file they have to go in. You can see the default settings\nfor all the properties that are governed by these configuration files by\nlooking in the docs directory of your Hadoop installation for HTML files\ncalled core-default.html, hdfs-default.html, and mapred-default.html.\nHadoop can be run in one of three modes:\nStandalone (or local) mode\nThere are no daemons running and everything runs in a single JVM. Standalone\nmode is suitable for running MapReduce programs during development, since it\nis easy to test and debug them.\n466 | Appendix A: Installing Apache HadoopPseudo-distributed mode\nThe Hadoop daemons run on the local machine, thus simulating a cluster on a\nsmall scale.\nFully distributed mode\nThe Hadoop daemons run on a cluster of machines. This setup is described in\nChapter 9.\nTo run Hadoop in a particular mode, you need to do two things: set the appropriate\nproperties, and start the Hadoop daemons. Table A-1 shows the minimal set of prop-\nerties to configure each mode. In standalone mode, the local filesystem and the local\nMapReduce job runner are used, while in the distributed modes the HDFS and Map-\nReduce daemons are started.\nTable A-1. Key configuration properties for different modes\nComponent Property Standalone Pseudo-distributed Fully distributed\nCore fs.default.name file:/// (default) hdfs://localhost/ hdfs://namenode/\nHDFS dfs.replication N/A 1 3 (default)\nMapReduce mapred.job.tracker local (default) localhost:8021 jobtracker:8021\nYou can read more about configuration in “Hadoop Configuration” on page 251.\nStandalone Mode\nIn standalone mode, there is no further action to take, since the default properties are\nset for standalone mode, and there are no daemons to run.\nPseudo-Distributed Mode\nThe configuration files should be created with the following contents, and placed in\nthe conf directory (although you can place configuration files in any directory as long\nas you start the daemons with the --config option).\n<?xml version=""1.0""?>\n<!-- core-site.xml -->\n<configuration>\n<property>\n<name>fs.default.name</name>\n<value>hdfs://localhost/</value>\n</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- hdfs-site.xml -->\n<configuration>\n<property>\n<name>dfs.replication</name>\n<value>1</value>\nConfiguration | 467</property>\n</configuration>\n<?xml version=""1.0""?>\n<!-- mapred-site.xml -->\n<configuration>\n<property>\n<name>mapred.job.tracker</name>\n<value>localhost:8021</value>\n</property>\n</configuration>\nConfiguring SSH\nIn pseudo-distributed mode, we have to start daemons, and to do that, we need to have\nSSH installed. Hadoop doesn’t actually distinguish between pseudo-distributed and\nfully distributed modes: it merely starts daemons on the set of hosts in the cluster\n(defined by the slaves file) by SSH-ing to each host and starting a daemon process.\nPseudo-distributed mode is just a special case of fully distributed mode in which the\n(single) host is localhost, so we need to make sure that we can SSH to localhost and log\nin without having to enter a password.\nFirst, make sure that SSH is installed and a server is running. On Ubuntu, for example,\nthis is achieved with:\n% sudo apt-get install ssh\nOn Windows with Cygwin, you can set up an SSH server (after having\ninstalled the openssh package) by running ssh-host-config -y.\nThen to enable password-less login, generate a new SSH key with an empty passphrase:\n% ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n% cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nTest this with:\n% ssh localhost\nYou should be logged in without having to type a password.\nFormatting the HDFS filesystem\nBefore it can be used, a brand-new HDFS installation needs to be formatted. The for-\nmatting process creates an empty filesystem by creating the storage directories and the\ninitial versions of the namenode’s persistent data structures. Datanodes are not in-\nvolved in the initial formatting process, since the namenode manages all of the filesys-\ntem’s metadata, and datanodes can join or leave the cluster dynamically. For the same\nreason, you don’t need to say how large a filesystem to create, since this is determined\n468 | Appendix A: Installing Apache Hadoopby the number of datanodes in the cluster, which can be increased as needed, long after\nthe filesystem was formatted.\nFormatting HDFS is quick to do. Just type the following:\n% hadoop namenode -format\nStarting and stopping the daemons\nTo start the HDFS and MapReduce daemons, type:\n% start-dfs.sh\n% start-mapred.sh\nIf you have placed configuration files outside the default conf directory,\nstart the daemons with the --config option, which takes an absolute\npath to the configuration directory:\n% start-dfs.sh --config path-to-config-directory\n% start-mapred.sh --config path-to-config-directory\nThree daemons will be started on your local machine: a namenode, a secondary name-\nnode, and a datanode. You can check whether the daemons started successfully by\nlooking at the logfiles in the logs directory (in the Hadoop installation directory), or by\nlooking at the web UIs, at http://localhost:50030/ for the jobtracker, and at http://lo-\ncalhost:50070/ for the namenode. You can also use Java’s jps command to see whether\nthey are running.\nStopping the daemons is done in the obvious way:\n% stop-dfs.sh\n% stop-mapred.sh\nFully Distributed Mode\nSetting up a cluster of machines brings many additional considerations, so this mode\nis covered in Chapter 9.\nConfiguration | 469APPENDIX B\nCloudera’s Distribution for Hadoop\nby Matt Massie and Todd Lipcon, Cloudera\nCloudera’s Distribution for Hadoop is based on the most recent stable version of\nApache Hadoop with numerous patches, backports, and updates. Cloudera shares this\ndistribution in a number of different formats: compressed tar files, RPMs, Debian\npackages, and Amazon EC2 AMIs. Cloudera’s Distribution for Hadoop is free, released\nunder the Apache 2.0 license and available at http://www.cloudera.com/hadoop/.\nCloudera has an online configurator at http://www.cloudera.com/configurator to make\nsetting up a Hadoop cluster easy (Figure B-1). The configurator has a simple wizard-\nlike interface that asks targeted questions about your cluster. When you’ve finished,\nthe configurator generates customized Hadoop packages and places them in a package\nrepository for you. You can manage any number of clusters and return at a later time\nto update your active configurations.\nTo simplify package management, Cloudera shares RPMs from a yum repository and\nDebian packages from an apt repository. Cloudera’s Distribution for Hadoop enables\nyou to install and configure Hadoop on each machine in your cluster by running a\nsingle, simple command. Kickstart users benefit even more by being able to commission\nentire Hadoop clusters automatically without any manual intervention.\nPrerequisites\nCloudera’s Distribution for Hadoop requires Sun Java 6 or later to be installed. The\nSun Java Debian and RPM packages require that you agree to the Sun license before\nuse. For a Debian-based system, you will want to enable the non-free apt repository,\nas it contains the sun-java6-* packages. For a Red Hat–based system, download the\nSun Java RPM package from http://java.sun.com/javase/downloads/.\nBefore you can use your favorite package manager (e.g., yum, apt-get, aptitude) to\ninstall Cloudera packages, you’ll need to add the Cloudera repositories to your list of\nyum and/or apt sources.\n471Figure B-1. Cloudera’s on-line configurator makes it easy to set up a Hadoop cluster\nPlease refer to http://www.cloudera.com/hadoop/ for up-to-date instructions on the sim-\nplest way to satisfy these prerequisites.\n472 | Appendix B: Cloudera’s Distribution for HadoopStandalone Mode\nTo install Hadoop standalone mode, run the following command on Red Hat–based\nsystems:\n% yum install hadoop\nOr on Debian-based systems, run the command:\n% apt-get install hadoop\nThe hadoop package include a man page. To read the man page, run the command:\n% man hadoop\nIf you want to install the full Hadoop documentation on a machine, install the hadoop-\ndocs package. On Red Hat–based systems, run the command:\n% yum install hadoop-docs\nTo install the documentation on Debian-based systems, run the command:\n% apt-get install hadoop-doc\nPseudo-Distributed Mode\nTo install Hadoop in pseudo-distributed mode, run the following command on Red\nHat–based systems:\n% yum install hadoop-conf-pseudo\nOr on Debian-based systems, run the command:\n% apt-get install hadoop-conf-pseudo\nOnce you’ve installed the Hadoop pseudo-distributed configuration package, you start\nthe Hadoop services by running the same command on both Red Hat– and Debian-\nbased systems:\n% for x in namenode secondarynamenode datanode jobtracker tasktracker ;\ndo /etc/init.d/hadoop-$x start ; done\nThere is no need to worry about creating a hadoop user or formatting HDFS, as that is\nhandled automatically by the hadoop-conf-pseudo package. You can use Hadoop im-\nmediately after installing the package and starting the Hadoop services. The hadoop-\nconf-pseudo package will also ensure that your Hadoop services are started at system\nboot.\nFully Distributed Mode\nFor details about deploying a fully distributed Hadoop cluster, visit Cloudera’s Distri-\nbution for Hadoop web page at http://www.cloudera.com/hadoop/.\nFully Distributed Mode | 473When you run Cloudera’s online configurator, it creates a personalized apt or yum re-\npository to hold the configuration packages for every cluster you manage. For example,\nlet’s say you gave one of your clusters the name mycluster. To see a list of all the con-\nfiguration packages for mycluster, run the following command on Red Hat–based\nsystems:\n% yum search hadoop-conf-mycluster\nor on Debian-based systems, run the command:\n% apt-cache search hadoop-conf-mycluster\nThese commands will return a list of configuration packages for the mycluster cluster.\nThe number and types of configuration packages depends on how you answered the\nquestions posed by the Cloudera configurator. Some of the packages will be generated\nfor specific hosts in your cluster; others will be for groups or classes of machines in\nyour cluster. For host-specific configurations, the fully qualified hostname will be\nadded to the package name. For example, there may be a configuration for myhost.mydo\nmain in the mycluster cluster. To install Hadoop on myhost.mydomain on Red Hat–based\nsystems, run the command:\n% yum install hadoop-conf-mycluster-myhost.mydomain\nor on Debian-based systems, run the command:\n% apt-get install hadoop-conf-mycluster-myhost.mydomain\nThe Hadoop configuration packages will ensure that your services are set up to run at\nsystem boot.\nHadoop-Related Packages\nCloudera’s Distribution for Hadoop allows you to easily deploy tools built on top of\nHadoop like Hive and Pig. Hive is a data warehouse infrastructure that allows you to\nquery data in Hadoop with a query language based on SQL. For more information on\nHive, see “Hadoop and Hive at Facebook” on page 414. Pig is a platform for analyzing\nlarge datasets using a high-level language; it is covered in Chapter 11.\nTo install Hive and Pig on Red Hat–based systems, run the command:\n% yum install hadoop-hive hadoop-pig\nTo install Hive and Pig on Debian-based systems, run the command:\n% apt-get install hadoop-hive hadoop-pig\nMore Hadoop-related packages will be added to Cloudera’s Distribution for Hadoop\nover time.\n474 | Appendix B: Cloudera’s Distribution for HadoopAPPENDIX C\nPreparing the NCDC Weather Data\nThis section gives a runthrough of the steps taken to prepare the raw weather data files\nso they are in a form that is amenable for analysis using Hadoop. If you want to get a\ncopy of the data to process using Hadoop, you can do so by following the instructions\ngiven at the website which accompanies this book at http://hadoopbook.com/. The rest\nof this section explains how the raw weather data files were processed.\nThe raw data is provided as a collection of tar files, compressed with bzip2. Each year\nof readings comes in a separate file. Here’s a partial directory listing of the files:\n1901.tar.bz2\n1902.tar.bz2\n1903.tar.bz2\n...\n2000.tar.bz2\nEach tar file contains a file for each weather station’s readings for the year, compressed\nwith gzip. (The fact that the files in the archive are compressed makes the bzip2 com-\npression on the archive itself redundant.) For example:\n% tar jxf 1901.tar.bz2\n% ls -l 1901 | head\n011990-99999-1950.gz\n011990-99999-1950.gz\n...\n011990-99999-1950.gz\nSince there are tens of thousands of weather stations, the whole dataset is made up of\nlarge number of relatively small files. It’s generally easier and more efficient to process\na smaller number of relatively large files in Hadoop (see “Small files and CombineFi-\nleInputFormat” on page 190), so in this case, I concatenated the decompressed files for\na whole year into a single file, named by the year. I did this using a MapReduce program,\nto take advantage of its parallel processing capabilities. Let’s take a closer look at the\nprogram.\n475The program has only a map function: no reduce function is needed since the map does\nall the file processing in parallel with no combine stage. The processing can be done\nwith a Unix script so the Streaming interface to MapReduce is appropriate in this case;\nsee Example C-1.\nExample C-1. Bash script to process raw NCDC data files and store in HDFS\n#!/usr/bin/env bash\n# NLineInputFormat gives a single line: key is offset, value is S3 URI\nread offset s3file\n# Retrieve file from S3 to local disk\necho ""reporter:status:Retrieving $s3file"" >&2\n$HADOOP_INSTALL/bin/hadoop fs -get $s3file .\n# Un-bzip and un-tar the local file\ntarget=`basename $s3file .tar.bz2`\nmkdir -p $target\necho ""reporter:status:Un-tarring $s3file to $target"" >&2\ntar jxf `basename $s3file` -C $target\n# Un-gzip each station file and concat into one file\necho ""reporter:status:Un-gzipping $target"" >&2\nfor file in $target/*/*\ndo\ngunzip -c $file >> $target.all\necho ""reporter:status:Processed $file"" >&2\ndone\n# Put gzipped version into HDFS\necho ""reporter:status:Gzipping $target and putting in HDFS"" >&2\ngzip -c $target.all | $HADOOP_INSTALL/bin/hadoop fs -put - gz/$target.gz\nThe input is a small text file (ncdc_files.txt) listing all the files to be processed (the files\nstart out on S3, so the files are referenced using S3 URIs that Hadoop understands).\nHere is a sample:\ns3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2\ns3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2\n...\ns3n://hadoopbook/ncdc/raw/isd-2000.tar.bz2\nBy specifying the input format to be NLineInputFormat, each mapper receives one line\nof input, which contains the file it has to process. The processing is explained in the\nscript, but, briefly, it unpacks the bzip2 file, and then concatenates each station file into\na single file for the whole year. Finally, the file is gzipped and copied into HDFS. Note\nthe use of hadoop fs -put - to consume from standard input.\nStatus messages are echoed to standard error with a reporter:status prefix so that they\nget interpreted as a MapReduce status update. This tells Hadoop that the script is\nmaking progress, and is not hanging.\n476 | Appendix C: Preparing the NCDC Weather DataThe script to run the Streaming job is as follows:\n% hadoop jar $HADOOP_INSTALL/contrib/streaming/hadoop-*-streaming.jar \\\n-D mapred.reduce.tasks=0 \\\n-D mapred.map.tasks.speculative.execution=false \\\n-D mapred.task.timeout=12000000 \\\n-input ncdc_files.txt \\\n-inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \\\n-output output \\\n-mapper load_ncdc_map.sh \\\n-file load_ncdc_map.sh\nI set the number of reduce tasks to zero, since this is a map-only job. I also turned off\nspeculative execution so duplicate tasks didn’t write the same files (although the ap-\nproach discussed in “Task side-effect files” on page 173 would have worked, too). The\ntask timeout was set high so that Hadoop didn’t kill tasks that are taking a long time\n(for example, when unarchiving files, or copying to HDFS, when no progress is\nreported).\nLast, the files were archived on S3 by copying them from HDFS using distcp.\nPreparing the NCDC Weather Data | 477Index\nA\nack queue in HDFS, 67\nACLs (access control lists)\nfor Hadoop services, 264\nZooKeeper, 374, 383\npermissions, 384\nActiveKeyValueStore class (example), 391\nad hoc analysis and product feedback\n(hypothetical use case), 419\nadministration procedures, routine, 292–293\nadvertiser insights and performance\n(hypothetical use case), 418\naggregate functions, 321\nalgebraic functions, 321\naliases for relations, 306\nALL and ANY ALL groups, 338\nalter command (HBase), 349\nAmazon Elastic Compute Cloud (EC2), 269,\n458\nAmazon Simple Storage Service (S3), 458\nAmazon Web Services, Public Data Sets, 2\nanalysis of data, 3\nAnt, packaging program in JAR file, 132\nANY keyword, 338\nApache Commons Logging API, 142\nApache Hadoop, 465–469\nconfiguration, 466–469\nmodes and properties for, 466\npseudo-distributed mode, 467–469\nstandalone mode, 467\nhome page, xvi\ninstallation, 465\nprerequisites, 465\nTeraByte sort on, 462\nApache Hadoop project, 10, 12\nApache Lucene project, 9\nApache Nutch, 9\nApache Thrift services, 49\nApache ZooKeeper, 372\n(see also ZooKeeper)\nreleases page, 370\nAPIs in ZooKeeper, 381\narchive files, copying to tasks, 239\narchive tool, 72\narchives (see HAR files)\nArrayWritable class, 95\nAsterData, 457\nAstrometry.net project, 2\nasynchronous API in ZooKeeper, 381\natomic broadcast phase, 385\nattempt IDs, 133\naudit logging, 280\naudit logs (HDFS), 143\nauthorization, service-level, 264\nautofs tool, 250\nAvro, 12, 103\nawk, using to analyze weather data, 17\nB\nbackups of data, 292\nbad blocks, HBase using HDFS, 367\nbad_files directory, 77\nbag type, 316\nbalancer tool, 284, 293\nBaldeschwieler, Eric, 11\nbenchmarks, 267\nother widely used Hadoop benchmarks,\n269\nWe’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n479TeraByteSort and other sort benchmarks,\n461\nTestDFSIO, benchmarking HDFS, 267\nbinary input, 199\nbinary output, 203\nblacklisting of tasktracker, 161\nblock compression in sequence files, 109\nblocks, 43\nbenefits for distributed filesystem, 43\nBookKeeper logging service, 400\nbuffer size for I/O operations, 264\nburning a Hadoop cluster, 266\nBY clauses, JOIN or COGROUP statements,\n336\nbytearray type, 317, 325\nByteArrayOutputStream objects, 87\nBytes class, 357\nBytesWritable class, 94\nBzip2 compression, 78\nC\nC language, ZooKeeper client binding, 381\nC++\ncompiling and running MapReduce\nprogram, 38\nmap and reduce functions, 36\ncaching\nCodecPool class, 82\ndistributed cache mechanism, 239\nHBase Scanners, 361\nCafarella, Mike, 344\ncartesian product, 337\nCascading, 447–461\napplication example, word count and sort,\n454\ncreating a SubAssembly, 455\nextending word count and sort application\nwith a SubAssembly, 456\nfields, tuples, and pipes, 448\nflexibility for application development, 456\nHadoop and Cascading at ShareThis, 457–\n461\noperations, 451\nproject website, 461\nTaps, Schemes, and Flows, 452\ncascading.ClusterTestCase class, 459\ncase sensitivity (Pig Latin), 311\ncells, versioning in HBase, 344\nChainMapper class, 243\n480 | Index\nChainReducer class, 243\nchararray type, 305, 315\ntrimming whitespace from values, 326\ncheckpointing process for filesystem metadata,\n275\nChecksumFileSystem class, 77\nchecksums, 75\nclient side, 76\nChubby Lock Service, 385\nChukwa, 285\ndefined, 13\nclientPort property (ZooKeeper), 371\nCloudera’s Distribution for Hadoop, 471–474\nfully distributed mode, 473\non-line configurator, 471\npackages related to Hadoop, 474\nprerequisites, 471\npseudo-distributed mode, 473\nstandalone mode, 473\ncluster membership, properties for, 264\nclusters, 245–271\naddresses for namenode and jobtracker,\nhadoopcluster.xml file, 119\nbalancing, 71\nbenchmarking, 266–269\nHadoop configuration, 251–266\ndaemon addresses and ports, 263\nenvironment settings, 254–258\nimportant daemon properties, 258–263\nmanaging, 252\nother properties, 264\nHadoop in the cloud, 269\nHBase cluster members, 346\nHBase, configuration files, 346\nmini-clusters, 131\nnetwork topology, 247\nrack awareness, 248\nRAID and, 246\nrunning a job on, 132–145\ndebugging the job, 139–145\nlaunching the job, 132\nMapReduce web UI, 134–136\npackaging program as JAR file, 132\nretrieving results, 136\nrunning Hadoop on Amazon EC2, 269\nlaunching a cluster, 270\nMapReduce job, 271\nsetup, 270\nterminating a cluster, 271setup and installation, 249–251\ncreating Hadoop user, 250\ninstalling Hadoop, 250\ninstalling Java, 249\nspecification, 245\nSSH configuration, 251\nuser jobs as benchmarks, 269\nusername, setting for a cluster, 120\nCodd’s rules, 361\ncode examples from this book, download site,\nxvi\nCodecPool class, 82\ncodecs, 79–83\ncompression codec properties, 81\ninferring using CompressionCodecFactory,\n80\nCOGROUP statement, 335\njoin key in BY clause, 336\nusing combination of COGROUP, INNER,\nand FLATTEN, 335\ncoherency model (filesystem), 68\ncollections, Writable collection types, 95\ncolumn families (HBase), 344, 359\nCombineFileInputFormat class, 190\ncombiner functions, 29\nsetting in Streaming Ruby program, 35\nspecifying, 31\ncommand line\nrunning jobs from, helper classes for, 121–\n124\nrunning local job driver, 128\nZooKeeper tool, 377\nconf switch, 120\ncommands\nHBase shell, 349\nPig Latin, 313\nZooKeeper, 371\ncomments in Pig Latin, 310\ncommit logs, HBase regionservers, 347\ncommodity hardware, 42\nComparable interface, 88\ncomparators, 88\ncustom comparators in Java secondary sort\nprogram, 231\ncustom RawComparator, 100\nFirstComparator custom class (example),\n237\nKeyFieldBasedComparator, 232\nRawComparator class, 220\ncomparison functions, 321\nComparisonFunc class, 339\ncompletion, job, 158\nCompositeContext class, 289\nCompositeInputFormat class, 234\ncompression, 77–86\ncodecs, 79–83\ndetails in sequence files, 109\ndetermining which format to use, 84\ninput splits and, 83\nlisting of compression formats, 78\nmap output written to disk, 164\nnative libraries for, 81\nusing in MapReduce, 84\nCompressionCodec interface, 79\ncompressing and decompressing streams,\n79\nCompressionCodecFactory class, 80\ncompressors and decompressors, reusing, 82\nconf command line switch, 120\nlaunching cluster job, 132\nConfigUpdater class (example), 392\nConfigurable interface, 121\nconfiguration API, 116–118\ncombining resources to define, 117\nexample file (configuration-1.xml), 116\nvariable expansion, 118\nConfiguration class, 53, 116\nconfiguration files, 118–120\nHadoop, 251\nHadoop site configuration files, typical set,\n258–260\nHBase, 346\nzoo.cfg, 371\nconfiguration service (ZooKeeper), 391–394\nreliable service, 396–398\nconfiguration tuning, shuffle and sort, 166\nmap side, 167\nconfiguration, ZooKeeper in production, 402\nConfigured class, 121\nConfigWatcher class (example), 393\nconnection events, ZooKeeper, 374\nConnectionWatcher class (example), 375\nconsistency, ZooKeeper service, 386\ncontext objects, new Java MapReduce API, 25\ncontexts for metrics, 286\ncopy phase of reduce tasks, 164\nCore, 12\ncore-default.xml file, 117\nIndex | 481core-site.xml file, 117, 252\ncounters, 211–218\nbuilt-in, 211–213\nmetrics versus, 286\nspilled records, 166\nuser-defined Java counters, 213–217\ndynamic, 215\nMaxTemperatureWithCounters class\n(example), 213\nreadable names, 215\nretrieving, 216\nuser-defined Streaming counters, 218\nCounters class, 217\ncounting in MapReduce, 448\nCrawlDb (Nutch), 425, 427\nCRC-32 (cyclic redundancy check), 75\nCreateGroup objects, 373\nCROSS operator, 337\ncustom Writable, implementing, 96–101\ncustom comparators, 100\nimplementing RawComparator for speed,\n99\nCutLoadFunc (example UDF), 327–330\nCutting, Doug, 9, 11\nD\ndata analysis, 3\nhypothetical use case, 420\ndata backups, 292\ndata integrity, 75–77\nChecksumFileSystem, 77\nin HDFS, 75\nLocalFileSystem, 76\ndata locality, 7\ndata locality optimization, 28\ndata pipelines using Hive, 422\ndata processing operators (Pig), 331–340\ncombining and splitting data, 339\nfiltering data, 331–334\ngrouping and joining data, 334–338\nsorting data, 338\ndata queue, 67\ndata structures, file-based, 103–114\nMapFile class, 110–114\nSequenceFile class, 103–110\ndata types\nconfiguration properties, 116, 117\nJava primitives, ObjectWritable wrapper\nfor, 95\n482 | Index\nJava primitives, Writable wrappers for, 89\nleveraging in Pig filter UDF, 325\nMapReduce, 175–184\nconflicts in, 178\ndefault MapReduce job, 178\nPig Latin, 315\ndata, sources of, 1\ndata-local tasks, 156\ndatabase input and output, 201\ndatabases, 309\n(see also RDBMS)\ncomparison with Pig, 308\nstorage and retrieval of data, 343\nDataBlockScanner, 76\nDataByteArray class, 325\ndataDir property (ZooKeeper), 371, 401\ndataLogDir property (ZooKeeper), 401\ndatanodes, 44\nblock distribution over, balancing, 284\nblock scanner, 283\nclient reading data from, 63\ncommissioning, 294\ndecommissioning, 295\ndirectory structure, 277\npermitted to connect to namenode, 294\nrole in client file write to HDFS, 67\nrunning out of datanode threads, 366\nstorage of replicas on, 67\nverification of data, 76\nwriting of file to, visibility to other readers,\n69\nDataOutput interface, 88\nDataOutputStream objects, 87\nDataStreamer class, 67\nDataType class, 325\nDBInputFormat class, 201\nDBOutputFormat class, 201\ndebugging jobs, 139–145\nhandling malformed data, 143\nusing remote debugger, 144\nDEFINE operator, 324\ndelete operations in ZooKeeper, 381\nDESCRIBE operator, 306\ndeserialization\ndefined, 86\nexample, 88\nDeserializer objects, 101\ndevelopment environment, configuring, 118–\n124helper classes for running jobs from\ncommand line, 121–124\ndfs.block.size property, 265\ndfs.data.dir property, 260\ndfs.datanode.du.reserved property, 265\ndfs.datanode.http.address property, 264\ndfs.datanode.ipc property, 263\ndfs.hosts property, 264, 294\ndfs.http.address property, 71, 264\ndfs.name.dir property, 260\ndfs.permissions property, 47\ndfs.replication property, 45\ndfs.replication.min property, 67\ndfs.secondary.http.address property, 264\ndfsadmin tool, 280\nchecking progress of upgrade, 298\ncommands, listed, 280\nsafe mode commands, 279\nDFSClient class, bad blocks on HDFS and,\n367\nDFSInputStream class, 63\nDFSOutputStream class, 66\ndiagnostic operators (Pig Latin), 313\ndirectories\ncreating, 57\ndeleting, 62\nlisting in HDFS, 46\nspecified as input path, 187\ntemporary directory for MapReduce output,\n173\ndisable command (HBase shell), 349\ndistcp tool, 70, 271\ncluster balance and, 71\nusing for backups, 293\ndistributed cache, 239\nDistributedCache API, 242\nusing to share metadata file for station\nnames (example), 240–242\ndistributed computation, 6\ndistributed filesystems, 41\nDistributedFileSystem class, 47, 51, 63\n(see also FileSystem class)\nrole in client write to HDFS, 66\nsetVerifyChecksum( ) method, 76\ndistributive functions, 31\nDNSToSwitchMapping interface, 248\ndomain-specific languages (DSLs), 457\ndryrun option (Pig), 342\ndsh tool, 258\nDSLs (domain-specific languages), 457\ndump command, 371\nDUMP operator, 306\nDUMP statement, order and, 339\ndynamic counters, 215\ndynamic parameters, 342\nE\nedit log, HDFS, 260, 274\nenable command (HBase shell), 349\nensemble (ZooKeeper), 385\nenums, 215\nenvi command (ZooKeeper), 371\nenvironment properties, task, 172\nenvironment settings, 254–258\nJava, 256\nmemory, 254\nSSH, 257\nsystem logfiles, 256\nenvironment variables\nHADOOP_CLASSPATH, 24\nsetting for Makefile, C++ MapReduce\nprogram, 38\neval functions, 321\nUDF (user-defined function), 326–327\nEvalFunc class, 323\ngetArgToFuncMapping( ) method, 325\nEventType.NodeDataChanged, 393\nexceptions in ZooKeeper, 394\nInterruptedException, 394\nKeeperException, 395\nlocks and, 399\nexclude file, 295\nprecedence in HDFS, 295\nexists operation in ZooKeeper, 381\nsignature for, 381\nwatches on, 382\nexpressions (Pig Latin), 314\nF\nFacebook, Hadoop and Hive at, 414–417\nfailover, ZooKeeper service, 388\nfailures, 159–161\njobtracker, 161\npartial failure, distributed applications, 369\ntask, 160\nskipping bad records, 171\ntasktracker, 161\nIndex | 483Fair Scheduler, 162\nfair sharing for jobs, 424\nFetcher application, multi-threaded\nMapRunner, 435\nfetchlists (Nutch)\ndefined, 426\ngeneration of, 431–438\nfields, 449\nFieldSelectionMapReduce class, 243\nfile descriptors, running out of, 366\nfile mode, 46\nfile permissions, 47\nfile-based data structures (see data structures,\nfile-based)\nFileContext class, 287\nFileInputFormat class, 186\ncomputeSplitSize( ) method, 189\ninput splits, 188\nstatic methods to set JobConf's input paths,\n187\nFileOutputFormat class, 22, 23, 173\nfiles\ncopying local file to Hadoop filesystem and\nshowing progress, 56\ncopying to and from HDFS, 45\ndeleting, 62\nlisting on HDFS, 46\nparallel copying with distcp, 70\nprocessing whole file as a record, 192\nsmall, packaging in SequenceFile, 103\nworking with small files, using\nCombineFileInputFormat, 190\nFileStatus class, 58, 59\nfilesystem blocks (see blocks)\nfilesystem check (see fsck utility)\nFileSystem class, 47, 51–62\nconcrete implementations, listed, 47\ncreating directories, 57\ndeleting files or directories, 62\nexists( ) method, 59\ngetting file metadata in FileStatus object,\n58\nlisting files, 59\nmethods for processing globs, 60\nreading data from Hadoop URLs, 51\nreading data using, 52–55\nwriting data, 56\nfilesystem commands (Pig Latin), 314\nFilesystem in Userspace (FUSE), 50\n484 | Index\nFileSystemCat class (example), 53\nfilesystems, 41\ndefault Hadoop configuration for default\nfilesystem, 119\nHadoop, 47–51\nlisted, 47\nHBase persistence to, 346\nraw filesystem underlying FileSystem, 77\nZooKeeper as a filesystem, 381\nfilter functions, 321\nUDF (user-defined function), 322–326\nleveraging types, 325\nFILTER operator, 331\nFilterFunc class, 323\nfiltering, 331–334\ninput path, 188\nserver-side filtering in HBase, 360\nusing FOREACH ... GENERATE operator,\n331\nusing STREAM operator, 333\nfinal properties, 117\nFLATTEN expression, 335\nFlows, 453\ntranslation into chained MapReduce jobs,\n456\nflush files, HBase regions, 348\nfollowers (ZooKeeper), 385\nFOREACH ... GENERATE operator, 331\nFOREACH statements, 307\nfragment replicate join, 334\nfs and jt command line options, 128\nfs command, 46\nconf option, 120\ntext option, 108\ngetmerge option, 138\nfs.checkpoint.dir property, 261\nfs.default.name property, 260, 263\nfs.trash.interval property, 265\nfsck utility, 44, 281\nchecking HDFS upgrade, 298\nfinding blocks for a file, 283\nhandling files with corrupt or missing\nblocks, 282\nrunning before HDFS upgrades, 297\nrunning regularly for filesystem\nmaintenance, 293\nFSDataInputStream class, 54, 63\nimplementation of PositionedReadable\ninterface, 55FSDataOutputStream class, 57, 66\nsync( ) method, 69\nfsimage file, 274\nfsync system call in Unix, 69\nFTP interface to HDFS, 51\nFTPFileSystem interface, 47, 51\nfully-distributed mode, 467\ninstalling Cloudera’s Distribution for\nHadoop, 473\nFuncSpec objects, 325\nfunctions in Pig, 320\nbuilt-in functions, listed, 321\nresolution of function calls, 324\ntypes of functions, 321\nUDFs (user-defined functions), 322–331\nFUSE (Filesystem in Userspace), 50\nG\nGanglia, 288\nusing with altering system to monitor\nHadoop clusters, 291\nGangliaContext class, 288\nGENERATE statement, 332\nGenericOptionsParser class, 121\nlisting of supported options, 122\nusing distributed cache via, 242\nfs and jt options, 128\nGenericWritable class, 95\nNutchWritable subclass, 436\ngetChildren operation, watches on, 383\ngetData operation, watches on, 383\nGFS (Google filesystem), 9\nglobbing, 60\nfile globs and their expansions, 61\nglob characters and their meanings, 60\nGoogle\nBigtable, 344\nChubby Lock Service, 385\nGFS, 9\nMapReduce, 8\ngraph-based problems, 8\nGrid Computing, 6\nGROUP function, 306\ngroup names\nsetting, 120\ngrouping data\nCOGROUP statement, 335\nGROUP operator, 338\nPARALLEL clause for grouping operators in\nreduce phase, 340\ngroups\ncreating in ZooKeeper (example), 372–374\ndeleting in ZooKeeper, 378\njoining in ZooKeeper, 374–376\nlisting members in ZooKeeper, 376–377\nmembership in ZooKeeper, 372\nGrunt, 304\ngzip compression, 78\nH\nHadoop\nApache Hadoop project and subprojects,\n12\nconfiguration, 251–266\ndaemon addresses and ports, 263\nenvironment settings, 254–258\nfiles controlling, 251\nimportant daemon properties, 258–263\nmanagement of, 252\nother properties, 264\ndownloading and installing, 250\nHBase subproject, 344\nhistory of, 9–12\nstorage and analysis of data, 3\nHadoop and Cascading at ShareThis, 457–461\nHadoop and Hive at Facebook, 414\ndata warehousing architecture at Facebook,\n415\nHadoop configuration, 417\nHadoop use cases, 415\nHadoop, history at Facebook, 414\nHadoop Distributed Filesystem (see HDFS)\nHadoop in the cloud, 269\nHadoop in the Cloud\non Amazon EC2, 269\nHadoop mode (Pig), 303\nHadoop usage at Last.fm, 405–414\ngenerating charts with Hadoop, 406\nHadoop at Last.fm, 405\nTrack Statistics Program, 407–413\nHadoop Workflow Scheduler (HWS), 151\nhadoop-conf-pseudo package, 473\nhadoop-env.sh file, 252\nhadoop-metrics.properties file, 252\nhadoop.job.ugi property, 120\nhadoop.security.authorization property, 264\nhadoop.tmp.dir property, 261\nIndex | 485HadoopPipes::runTask method, 38\nHADOOP_CLASSPATH environment\nvariable, 24\nHADOOP_INSTALL environment variable,\n38\nHADOOP_LOG_DIR setting, 256\nHADOOP_MASTER setting, 257\nhanging tasks, 160\n.har file extension, 72\nHAR files (Hadoop Archives), 72\nlimitations of, 73\nhardware\ncommodity hardware, Hadoop on, 42\nspecification for Hadoop clusters, 245\nHarFileSystem, 47\nHashComparator objects, 434\nHashPartitioner class, 98, 181\ndefault, using to sort SequenceFile with\nIntWritable keys, 219\nHBase, 343–368\nbrief history of, 344\nclients, 350–354\nJava, 351\nusing REST and thrift, 353\ndata model, 344\nlocking, 345\nregions, 345\ndefined, 13\nexample, 354–360\nloading data from HDFS into table, 355–\n358\nschemas for Stations and Observations\ntables, 354\nweb queries, 358–360\nimplementation\nclients, slaves, and coordinating master,\n346\noperation of HBase, 346\ninstallation, 348–350\ntesting, 349\nlisting command-line options, 348\npractical issues running HBase instance,\n365–368\nHBase and HDFS, 366\nmetrics, 367\nschema design, 368\nUI, 367\nversions of Hadoop and HBase, 366\nRDBMS versus, 361\n486 | Index\nHBase characteristics, 363\nsuccessful RDBMS service, 362–365\nuse case, HBase at streamy.com, 363\nshutting down an instance, 350\nTableInputFormat and\nTableOutputFormat, 202\nHBaseAdmin class, 351\nHDFS (Hadoop Distributed Filesystem), 3, 41–\n73\naudit logging, 280\nbackups, 292\nbalancer for block distribution over\ndatanodes, 284\nbenchmarking with TestDFSIO, 267\nblock size, property for, 265\nblocks, 43\nclient reading file from, 63\nclient writing files to, 66\ncoherency model, 68\ncommand-line interface, 45\nbasic filesystem operations, 45\ndata integrity, 75\ndatanode block scanner, 283\ndefined, 12\ndesign of, 41\ndfsadmin tool, 280\nfile permissions, 47\nformatting, 469\nfsck utility, 281\nHAR files (Hadoop Archives), 71\nHBase persistence to, 346\nHBase use of, problems with, 366\nHTTP and FTP interfaces, 50\nimportant daemon properties, 260\ninclude and exclude file precedence, 295\ninstallation, MapReduce installation and,\n250\nkeeping clusters balanced, 71\nnamenodes and datanodes, 44\nparallel copying with distcp, 70\npersistent data structures, 273–278\ndatanode directory structure, 277\nfilesystem image and edit log, 274\nnamenode directory structure, 273\nsecondary namenode directory\nstructure, 276\nrelationship between input splits and\nblocks, 197\nsafe mode, 278Scribe integration with, 425\nstarting and stopping the daemon, 469\nupgrading, 297\nchecking the upgrade, 298\nchecking upgrade progress, 298\nfinalizing the upgrade, 299\nrolling back the upgrade, 298\nstarting the upgrade, 298\nsteps in procedure, 297\nwriting of reduce phase output to, 166\nhdfs-default.xml file, 121\nhdfs-site.xml file, 121, 252\nhelp command (HBase), 349\nherd effect, 399\nHftpFileSystem, 47\nHigh Performance Computing (HPC), 6\nhistory of Hadoop, 9–12\nHive, 309, 474\ndefined, 13\nat Facebook, 416\nimprovements to, 425\nuse case study, 421–424\ndata organization, 421\ndata pipelines using Hive, 422\nHive Query Language, 422\nHPC (High Performance Computing), 6\nHPROF profiler, 147\nHsftpFileSystem, 47\nHTable class, 351\ngetRow( ) method, 359\ngetScanner( ) method, 360\noptimization in HBase application, 357\nHTTP Accept header, 353\nHTTP interface to HDFS, 51\nHTTP job completion notification, 159\nHTTP server properties, 263\nHWS (Hadoop Workflow Scheduler), 151\nHypertable, 457\nhypothetical use case studies, 417\nad hoc analysis and product feedback, 419\nadvertiser insights and performance, 418\ndata analysis, 420\nI\nI/O (input/output), 75–114, 201\n(see also input formats; output formats)\ncompression\nusing in MapReduce, 84\ndata integrity, 75–77\nfile compression, 77–86\nfile-based data structures, 103–114\ninput formats, 184–202\nmap function, Java MapReduce, 21\nreduce function, Java MapReduce, 22\nserialization, 86–103\nsetting types for MapReduce jobs, 178\nStreaming MapReduce jobs, 182\nwriting output from map and reduce tasks,\n173\nidempotent and nonidempotent operations,\n395\nidentifiers (IDs), 133\nzxid, 387\nIdentityMapper class, 180\nTrack Statistics Program, 413\nIdentityReducer class, 182\nIDL (interface description language), 102\nILLUSTRATE operator, 307\nimage analysis, 8\ninclude file, 294\nprecedence in HDFS, 295\nIndexer tool in Nutch, custom OutputFormat,\n437\nindexing for Text class, 91\nindexing, secondary HBase index, 354\nIndexWriter class, addIndexes( ) methods,\n445\ninitialization, MapReduce jobs, 155\ninitLimit property, 402\nINNER keyword, 335\ninput formats\nbinary input, 199\ndatabase input, 201\ninput splits and records, 185–196\nmultiple inputs, 200\ntext input, 196–199\ninput paths\nproperties for input paths and filters, 188\nsetting with FileInputFormat methods, 187\ninput splits, 27, 185\ncontrolling size of, examples, 189\ncreation by FileInputFormat, 188\nfile split properties, 192\npreventing splitting, 191\nrelationship to HDFS blocks, 197\nsupport by compression formats, 78, 83\nInputFormat interface, 185\nhierarchy of classes implementing, 186\nIndex | 487InputSampler class, 225\nInputSplit interface, 185\nintegrity (see data integrity)\ninter-process communication, use of\nserialization, 86\ninterface description language (IDL), 102\nInterruptedException class, 394\nIntervalSampler objects, 226\nIntSumReducer class, 243\nIntWritable class, 21, 87\nobtaining comparator for and comparing,\n89\nreusing instances, 148\nInverseMapper class, 243\nio.bytes.per.checksum property, 75, 76\nio.compression.codecs property, 81\nio.file.buffer.size property, 167, 264\nio.serializations property, 101\nio.sort.factor property, 167, 168\nio.sort.mb property, 167\nio.sort.record.percent property, 167\nio.sort.spill.percent property, 167\nIOUtils class, closeStream( ) method, 52\nIsolationRunner class, 145\nitems tables, very large, 364\nJ\nJAR files\nadding to classpath of mapper and reducer\ntasks, 239\ncopying to tasks, 239\nin Streaming MapReduce API, 35\npackaging program as, 132\nJava\nenums, 215\nHBase client, 351–353\ninstalling, 249\nrunning Pig programs from, 304\nSun's JDK, 465\nJava API documentation for Hadoop, xvi\nJava Management Extensions (see JMX)\nJava MapReduce, 20–27\napplication to run MapReduce job, 22\nMaxTemperatureMapper class (example),\n20\nMaxTemperatureReducer class (example),\n21\nnew API in Hadoop 0.20.0, 25–27\nStreaming versus, 33\n488 | Index\ntesting running of MapReduce job, 23\nJava Object Serialization, 101\nJava Virtual Machines (see JVMs)\njava.env file, 401\njava.library.path property, 82\nJavaSerialization class, 101\nJAVA_HOME setting, 256\nfor HBase, 348\nJBOD (Just a Bunch of Disks) configuration,\n246\nJConsole tool, viewing MBeans in running\nJVM, 290\nJMX (Java Management Extensions), 289\nenabling remote access to, 290\nretrieving MBean attribute values, 291\njob history, 134\njob history logs (MapReduce), 143\njob IDs, 25, 133\ngetting for new job, 154\njob page, 136\njob run, anatomy of, 153–159\njob completion, 158\njob initialization, 155\njob submission, 153\nprogress and status updates, 157\ntask assignment, 155\ntask execution, 156\njob schedulers, 266\njob scheduling, 162\njob.end.notification.url property, 159\nJobClient class\nDistributedCache and, 242\ngetJob( ) method, 158\ngetSplits( ) method, 186\nrunJob( ) method, 23, 132, 153, 215\nsetJobPriority( ) method, 162\nsubmitJob( ) method, 154\nJobConf class, 22\nmethods to get lists of available cache files,\n242\nsetNumTasksToExecutePerJvm( ) method,\n170\nsetOutputKeyComparatorClass( ) method,\n220\nsetter methods for MapReduce types, 176\nusing for side data distribution, 238\nJobControl class, 151\njobs, 27\ndecomposing problem into, 149killing, 161\nrunning dependent jobs, 151\ntuning, 145–149\nuser jobs as cluster benchmarks, 269\njobtracker, 9, 27\ncluster specifications and, 247\nfailure of, 161\nrunning on localhost, 119\ntasktrackers connecting to, 294\nJobTracker class, 9, 153\ngetNewJobId( ) method, 154\nsubmitJob( ) method, 155\njobtracker page, 134\nJOIN statement\nCOGROUP versus, 335\njoin key in BY clause, 336\njoins, 233–238\nCOGROUP versus JOIN statements, 335\ndataset size and partitioning, 233\nexample of join in Pig, 336\nHBase and, 368\ninner join of two datasets, 233\nmap-side, 233\nPARALLEL clause for joining operators in\nreduce phase, 340\nin Pig, 334\nreduce-side, 235\nusing CROSS operator, 337\nJRuby IRB interpreter, 349\njt and fs command line options, 128\nJUnit 4 assertions, 88\nJust a Bunch of Disks (JBOD) configuration,\n246\nJVMFLAGS environment variable, 401\nJVMs (Java Virtual Machines)\nexit of child JVM in task failure, 160\nlaunch by TaskRunner, 156\nmemory given to JVMs running map and\nreduce tasks, 166\nmemory, setting, 266\nreuse for subsequent tasks, 170\nK\nKatta, 457\nKeeperException class, 395\nrecoverable exceptions, 395\nstate exceptions, 395\nunrecoverable exceptions, 396\nKeeperException.NoNodeException, 376\nKeeperState objects, 390\nKellerman, Jim, 344\nKeyFieldBasedComparator objects, 232\nKeyFieldBasedPartitioner objects, 232\nkeys and values\nin C++ Pipes MapReduce API, 37\nin Streaming MapReduce API, 33\nsorting in MapReduce, 227\nin Streaming, 183\nKeyValueTextInputFormat class, 197\nkill command, 371\nKosmosFileSystem, 47\nL\nLast.fm, 405\n(see also Hadoop usage at Last.fm)\nLazyOutputFormat class, 210\nleader election phase, 385, 398\nZooKeeper server numbers and, 402\nLIMIT statement, limiting number of results,\n339\nlinear chain of jobs, 151\nlink inversion, 429–431\nLinkDb\nimplementation, 430\nLinkDb (Nutch), 427\nLinux\nautomated installation tools, 249\nHadoop on, 465\nsetting up NFS on, 250\nlist command (HBase), 350\nlists, Writable collection implementations, 96\nload functions, 321\nUDF (user defined function)\nadvanced loading with Slicer, 330\nusing a schema, 329\nUDF (user-defined function), 327–331\nLOAD operator, 305\nloading data into HBase table, 355\nlocal job runner, 127–131\nfixing mapper, 129\nrunning the driver, 128\ntesting the driver, 130\nwriting driver to run job, 127\nlocal mode (Pig), 302\nLocalFileSystem class, 47\nclient-side checksumming, 76\nlock service (ZooKeeper), 398–400\nherd effect, 399\nIndex | 489implementation, 400\npseudocode for lock acquisition, 398\nrecoverable exceptions and, 399\nunrecoverable exceptions and, 400\nlocking in HBase, 345\nlog processing at Rackspace, 439–447\nbrief history, 440\nchoosing Hadoop, 440\ncollection and storage, 440\nMapReduce for logs, 442–447\nrequirements/problem, 439\nlog4j.properties file, 252\nlogging, 285\naudit logging, 280\nBookKeeper service, 400\ncompression format for logfiles, 84\ngetting stack traces, 286\nHadoop user logs, 142\nin Java, using Apache Commons Logging\nAPI, 142\nsetting levels, 286\nShareThis log processing, 458–461\nsystem logfiles produced by Hadoop, 256\nusing SequenceFile for logfiles, 103\nlogical plan for Pig Latin statements, 311\nLong.MAX_VALUE stamp, 360\nLongSumReducer class, 243\nLongWritable class, 21\nlow-latency data access, HDFS and, 42\nLucene library, 426\nLucene project, 9\nM\nmachine learning algorithms, 8\nMailtrust (see Rackspace)\nmaintenance, 292–299\ncommissioning and decommissioning\nnodes, 293\nroutine administrative procedures, 292\nupgrades, 296–299\nMakefile, C++ MapReduce program, 38\nmalformed data, handling by mapper\napplication, 143\nmap functions\ncompressing output, 85\ngeneral form, 175\nsecondary sort in Python, 231\nmap tasks, 27\n490 | Index\nconfiguration properties for shuffle tuning,\n167\nshuffle and sort, 163\nskipping bad records, 171\nmap type (Pig), 316\nmap-side joins, 233\nmap.input.file property, 195\nMapFile class, 110–114\napplication for partitioned MapFile\nlookups, 221–223\nconverting SequenceFile to, 113\nreading with MapFile.Reader instance, 112\nwriting with MapFile.Writer instance, 110\nMapFile.Reader objects, 222\nMapFileOutputFormat class, 203\nstatic methods for lookups against\nMapReduce output, 221\nMapper interface, 20, 21\nconfigure( ) method, 192\nHBase TableMap interface and, 353\nmappers, 7\nadding debugging to, 139\ndefault mapper, IdentityMapper, 180\ngetting information about file input splits,\n192\nhandling malformed data, 143\nparser class for, 129\ntagging station and weather records in\nreduce-side join, 235\nunit testing, 124–126\nusing utility parser class, 130\nmapred-default.xml file, 121\nmapred-site.xml file, 121, 252\nmapred.child.java.opts property, 262, 266\nmapred.child.ulimit property, 266\nmapred.combiner.class property, 178\nmapred.compress.map.output property, 167\nmapred.hosts property, 264, 294\nmapred.inmem.merge.threshold property,\n167, 168\nmapred.input.dir property, 188\nmapred.input.format.class property, 176\nmapred.input.pathFilter.class property, 188\nmapred.job.id property, 172\nmapred.job.priority property, 162\nmapred.job.reduce.input.buffer.percent\nproperty, 167, 168\nmapred.job.reuse.jvm.num.tasks property,\n170mapred.job.shuffle.input.buffer.percent\nproperty, 168\nmapred.job.shuffle.merge.percent property,\n168\nmapred.job.tracker property, 128, 262, 263\nmapred.job.tracker.http.address property, 264\nmapred.jobtracker.taskScheduler property,\n162\nmapred.line.input.format.linespermap\nproperty, 198\nmapred.local.dir property, 145, 262\nmapred.map.max.attempts property, 160\nmapred.map.output.compression.codec\nproperty, 167\nmapred.map.runner.class property, 178\nmapred.map.tasks.speculative.execution\nproperty, 169\nmapred.mapoutput.key.class property, 176\nmapred.mapper.class property, 178\nmapred.max.split.size property, 188\nmapred.min.split.size property, 188\nmapred.output.compression.type property, 85\nmapred.output.format.class property, 178\nmapred.output.key.class property, 176\nmapred.output.key.comparator.class property,\n178\nmapred.output.value.class property, 176\nmapred.output.value.groupfn.class property,\n178\nmapred.partitioner.class property, 178\nmapred.reduce.copy.backoff property, 168\nmapred.reduce.max.attempts property, 160\nmapred.reduce.parallel.copies property, 168\nmapred.reduce.tasks property, 155\nmapred.reduce.tasks.speculative.execution\nproperty, 169\nmapred.reducer.class property, 178\nmapred.submit.replication property, 154\nmapred.system.dir property, 262\nmapred.task.id property, 172\nmapred.task.is.map property, 172\nmapred.task.partition property, 172\nmapred.task.tracker.http.address property,\n264\nmapred.task.tracker.report.address property,\n263\nmapred.tasktracker.map.tasks.maximum\nproperty, 122, 262\nmapred.tasktracker.reduce.tasks.maximum\nproperty, 262\nmapred.textoutputformat.separator property,\n183\nmapred.tip.id property, 172\nmapred.userlog.limit.kb property, 142\nmapred.usrlog.retain.hours property, 142\nmapred.work.output.dir property, 174\nMapReduce programming in Hadoop, 15–39\nanalysis of data, 4\napplication counting rows in HBase table,\n351–353\napplication importing data from HDFS into\nHBase table, 355–358\nbenchmarking MapReduce with sort, 268\nCascading and, 447\ncombiner functions, 29\ncomparison to other systems, 4\nGrid Computing, 6\nRDBMS, 4\nvolunteer computing, 8\ncompression and input splits, 83\ncompression, using, 84–86\ncontrol script starting daemons, 253\ncounters, 211–218\ncounting and sorting in, 448\ndata flow, 19\ndata flow for large inputs, 27\ndefinition of MapReduce, 12\ndeveloping an application, 115–151\nconfiguration API, 116–118\nconfiguring development environment,\n118–124\nrunning job on a cluster, 132–145\nrunning locally on test data, 127–131\ntranslating problem into MapReduce\nworkflow, 149–151\ntuning a job, 145–149\nwriting unit test, 124–127\nenvironment settings, 255\nfailures, 159–161\nHadoop Pipes, 36–39\nHadoop Streaming, 32–36\nHAR files as input, 72, 73\nhow Flow translates into chained\nMapReduce jobs, 456\nimportant daemon properties, 261\ninput formats, 184–202\ninstallation, HDFS installation and, 250\nIndex | 491introduction of MapReduce, 10\nJava MapReduce, 20–27\njob scheduling, 162, 266\njoins, 233–238\nmap and reduce functions, 18\nMapReduce library classes, listed, 243\nMapReduce types, 175–184\nnew Java MapReduce API, 25–27\noutput formats, 202–210\nrunning a job, 153–159\nrunning a job on Amazon EC2, 271\nrunning distributed job, 32\nshuffle and sort, 163–168\nside data distribution, 238–242\nsorting, 218–233\nstarting and stopping the daemon, 469\ntask execution, 168–174\nusing for logs at Rackspace, 442–447\nweather dataset, 15\nMapRunnable interface\nMapRunner implementation, 181\nMultithreadedMapRunner\nimplementation, 186\nMapRunner class, 181, 186\nFetcher application in Nutch, 435\nMapWritable class, 95\nexample with different types for keys and\nvalues, 96\nmaster node (HBase), 346\nmasters file, 252\nMAX function, resolution of, 324\nMBeans, 289\nretrieving attribute values with JMX, 291\nmemory\nenvironment settings for, 254\nlimits for tasks, 266\nmemory buffers\nmap task, 163\nreduce tasktracker, 165\nmerges\nmap task file output in reduce task, 165\nvery large sort merges, 364\nMessage Passing Interface (MPI), 6\n.META. table, 346\nmetadata\nencapsulation in FileStatus class, 58\nHDFS blocks and, 43\nHDFS, upgrading, 297\npassing to tasks, 238\n492 | Index\nznode, 381\nmetrics, 286–289\nCompositeContext class, 289\ncontexts for, 286\ncounters versus, 286\nFileContext class, 287\nGangliaContext class, 288\nHadoop and HBase, 367\nmonitoring in ZooKeeper, 389\nNullContextWithUpdateThread class, 288\nMetricsContext interface, 287\nmin.num.spills.for.combine property, 167\nMiniDFSCluster class, 131\nMiniMPCluster class, 131\nmock object frameworks, 124\nmonitoring, 285–291\nlogging, 285\nmetrics, 286–289\nusing Java Management Extensions (JMX),\n289\nMPI (Message Passing Interface), 6\nmulti named output, 209\nMultipleInputs class, 200\nspecifying which mapper processes which\nfiles, 412\nuse in reduce-side joins, 235\nMultipleOutputFormat class, 203\ndifferences from MultipleOutputs, 210\nweather dataset partitioning (example),\n205–207\nMultipleOutputs class, 203\ndifferences from MultipleOutputFormat,\n210\nusing to partition weather dataset\n(example), 207–209\nMultithreadedMapRunner objects, 186\nMyLifeBits project, 2\nN\nnamenodes, 44\nchoosing of datanodes to store replicas on,\n67\ncluster specifications and, 247\ndatanode permitted to connect to, 294\ndirectory structure, 273\nfilesystem image and edit log, 274\nrole in client file write to HDFS, 66\nrole in client reading data from HDFS, 63\nrunning in safe mode, 278entering and leaving safe mode, 279\nrunning on localhost, 119\nsecondary, directory structure, 276\nNativeS3FileSystem, 47\nNavigableMap class, 359\nNCDC (National Climatic Data Center) data\nformat, 15\nNCDC weather data, preparing, 475–477\nNDFS (Nutch Distributed Filesystem), 9\nnetwork addresses, Hadoop daemons, 263\nnetwork topology\nAmazon EC2, 270\nHadoop and, 64, 247\nreplication factor and, 68\nNew York Times, use of Hadoop, 10\nNFS filesystem, 250\nNLineInputFormat class, 174, 198\nspecifying for NCDC files, 476\nnodes\ncommissioning and decommissioning, 293\ncommissioning new nodes, 293\ndecommissioning, 295\nznodes, 372\nnormalization of data, 6\nnull values, Pig Latin schemas and, 318\nNullContext class, 287\nNullContextWithUpdateThread class, 288\nNullWritable class, 95\nNumberFormatException, 125\nNutch Distributed Filesystem (NDFS), 9\nNutch search engine, 9, 425–439\nbackground, 425\ndata structures, 426–429\nHadoop data processing examples in, 429–\n438\ngeneration of fetchlists, 431–438\nlink inversion, 429–431\nsummary, 439\nNutchWritable class, 436\nO\nObjectWritable class, 95\noptimization notes\nHBase application, 357\ntuning a MapReduce job, 145–149\nORDER operator, 339\nOUTER keyword, 335\noutput formats, 202–210\nbinary output, 203\ndatabase output, 201\nlazy output, 210\nmultiple outputs, 203–210\ntext output, 202\nOutputCollector class, 21\ncreating mock replacement, 125\nmock replacement for, 124\npurpose of, 175\nOutputCommitter objects, 173\nOutputFormat class\ncustom implementation used by Nutch\nIndexer, 437\nOutputFormat interface\nclass hierarchy, 202\nP\nPARALLEL clause for operators running in\nreduce phase, 340\nPARALLEL keyword, 332\nparam option (Pig), 341\nparameter substitution, 341\ndynamic parameters, 342\nprocessing, 342\nparameter sweep, 198\nparam_file option (Pig), 341\nparsers, writing parser class for use with\nmapper, 129\npartial failure, 7\nZooKeeper and, 369, 395\npartial sort, 219–223\nPartitioner interface, 433\npartitioners\nHashPartitioner, 98, 181, 219\nKeyFieldBasedPartitioner, 232\nKeyPartitioner custom class, 237\nTotalOrderPartitioner, 225\nPartitionReducer class, 435\npartitions\nmap task output, 164\nnumber rigidly fixed by application, 204\npartitioner respecting total order of output,\n223–227\npartitioning weather dataset (example),\n203\nPartitionUrlByHost class (Nutch), 433\nPathFilter interface, 62, 188\npaths, znode, 379\npattern matching\nfile globs, 60\nIndex | 493using PathFilter, 62\nPaxos, 385\nperformance, ZooKeeper, 401\npermissions for file and directories, 47\nphysical plan for Pig statement execution, 311\nPig, 301–342, 474\ncomparison with databases, 308\ncomponents of, 301\ndata processing operators, 331–340\ndefined, 13\nexample program finding maximum\ntemperature by year, 305–307\nexecution types or modes, 302\nHadoop mode, 303\nlocal mode, 303\ngenerating examples, 307\nGrunt, 304\ninstalling, 302\nparallelism, 340\nparameter substitution, 341\nPig Latin editors, 305\nrunning programs, 304\nUDFs (user-defined functions), 322–331\nPig Latin, 309–322\ncomments, 310\nexpressions, 314\nfunctions, 320\nkeywords, 311\nschemas, 317–320\nstatements, 310, 311–314\ncommands, 313\ndiagnostic operators, 313\nrelational operators, 312\nUDF (user-defined function), 313\ntypes, 315\nPiggy Bank, functions in, 322\nPigStorage function, 331\nuse by STREAM operator, 333\nPipes, 36–39\nassembly of operations, 452\ncompiling and running C++ MapReduce\nprogram, 38\ncreating SubAssembly pipe (Cascading),\n456\nPipe types in Cascading, 449\nrelationship of executable to tasktracker and\nits child, 156\nusing Unix pipes to test Ruby map function\nin Streaming, 34\n494 | Index\nPLATFORM environment variable, 38\nports\nconfiguration in ZooKeeper, 402\nHadoop daemons, 263\nZooKeeper client connections, 371\nPositionedReadable interface, 55\nPostfix log lines, 442\npriority, setting for jobs, 162\nproblems and future work (Facebook), 424\nprofiling tasks, 146–149\nHPROF profiler, 147\nother profilers, 149\nprogress\nMapReduce jobs and tasks, 157\nshowing in file copying, 56\nProgressable interface, 56\nproperties\nconfiguration, 116\nconfiguration for different modes, 467\nconfiguration of MapReduce types, 176\nconfiguration tuning for shuffle, 166\nmap side, 167\nreduce side, 168\ncontrolling size of input splits, 188\nfile split, 192\nHTTP server, 263\nimportant HDFS daemon properties, 261\nimportant MapReduce daemon properties,\n262\ninput path and filter, 188\nmap output compression, 85\nRPC server, 263\nsafe mode, 279\nspeculative execution, 169\nStreaming separator properties for key-value\npairs, 183\nsystem, 117\ntask environment, 172\ntask JVM reuse, 170\nZooKeeper configuration, 371\npseudo-distributed mode, 38, 465, 467\nconfiguration files, 467\nconfiguring SSH, 468\nformatting HDFS filesystem, 469\ninstalling Cloudera’s Distribution for\nHadoop, 473\nstarting and stopping daemons, 469\nPublic Data Sets, Amazon Web Services, 2\nPythonmap function for secondary sort, 231\nPython, map and reduce functions, 35\nQ\nquery languages\nHive Query Language, 422\nPig, SQL, and Hive, 308\nquorum (ZooKeeper), 385\nR\nrack awareness, clusters and, 248\nrack-local tasks, 156\nRackspace, 439\n(see also log processing at Rackspace)\nMailtrust division, 4\nRAID (Redundant Array of Independent\nDisks), Hadoop clusters and, 246\nRandomSampler objects, 226\nRandomWriter objects, 268\nRawComparator class, 88\ncontrolling sort order for keys, 220\ncustom implementation, 100\nimplementing (example), 99\nRawLocalFileSystem class, 77\nRDBMS (Relational DataBase Management\nSystems), 4\ncomparison to MapReduce, 5\nHBase versus, 361–365\nHBase characteristics, scaling and, 363\ntypical RDBMS scaling story for\nsuccessful service, 362\nuse case, HBase at streamy.com, 363\nPig versus, 308\nread operations in ZooKeeper, 382\nreading/writing data in parallel to/from\nmultiple disks, 3\nrecord compression in sequence files, 109\nRecordReader class, 186\nWholeFileRecordReader custom\nimplementation, 193\nrecords, 185\ncorrupt, skipping in task execution, 171\nlogical records for TextInputFormat, 196\nprocessing a whole file as a record, 192\nrecoverable exceptions in ZooKeeper, 395,\n399\nreduce functions\ngeneral form, 175\nsecondary sort in Python, 232\nreduce tasks, 27\nconfiguration properties for shuffle tuning,\n168\nnumber of, 28\nshuffle and sort, 164\nskipping bad records, 171\nreduce-side joins, 235\napplication to join weather records with\nstation names, 237\nmappers for tagging station and weather\nrecords, 235\nReducer interface, implementation (example),\n21\nreducers, 7\ndefault reducer, IdentityReducer, 182\njoining tagged station records with tagged\nweather records (example), 236\nspecifying number in Pig, 340\nwriting unit test for, 126\nRegexMapper class, 243\nregions in HBase tables, 345\nregionservers (HBase), 346\ncommit log, 347\nREGISTER operator, 324\nregular expressions, using with PathFilter, 62\nrelational operators (Pig Latin), 312\nrelations (Pig), 306\nbags versus, 316\npropagation of schemas to new relations,\n320\nschema associated with, 317\nremote debugging, 144\nremote procedure calls (RPCs), 86\nreplicas, placement of, 67\nreplicated mode (ZooKeeper), 385, 401\nreplication factor, 44, 46, 154\nReporter class\ndynamic counters, 215\npurpose of, 175\nreqs command, 371\nreserved storage space, property for, 265\nREST interface for HBase, 353\nretries, ZooKeeper object, write( ) method,\n396\nROOT table, 346\nrow keys, design in HBase, 368\nRowCounter class, 351\nRowKeyConverter class (example), 356\nIndex | 495RowResult class, 359\nnext( ) method, 361\nRPC server properties, 263\nRPCs (remote procedure calls), 86\nrsync tool, 252\ndistributing configuration files to all nodes\nof a cluster, 257\nRuby, map and reduce functions, in Streaming\nMapReduce API, 33\nRunningJob objects, 158\nretrieving a counter, 217\nruok command (ZooKeeper), 371\nS\nS3FileSystem, 47\nsafe mode, 278\nentering and leaving, 279\nproperties, 279\nSampler interface, 225\nsamplers, 226\nScanner interface, 360\nScanners (HBase), 359\nscheduling, job, 162\nFair Scheduler, 162\nschemas (HBase), 361\ndefining for tables, 349\ndesign of, 368\nStations and Observations tables (example),\n354\nschemas (Pig Latin), 317–320\nmerging, 320\nusing in load UDF, 329\nvalidation and nulls, 318\nSchemes (Cascading), 453\nScribe-HDFS integration, 425\nScriptBasedMapping class, 249\nsearch engines, 10\n(see also Nutch search engine)\nApache Lucene and Nutch projects, 9\nbuilding web search engine from scratch, 9\nsecondary namenode, 45\nsecondary sort, 227\n(see also sorting)\nin reduce-side joins, 235\nSEDA (staged event-driven architecture), 443\nSeekable interface, 54\nsegments (in Nutch), 427\nSelector class (Nutch), 433\nSelectorInverseMapper class (Nutch), 434\n496 | Index\nsemi-structured data, 5\nseparators, key-value pairs\nkey.value.separator.in.input.line property,\n197\nin Streaming, 183\nSequenceFile class, 103–110\ncharacteristics of, 200\nconverting to MapFile, 113\ndisplaying with command-line interface,\n108\nformat, 109\nreading with SequenceFile.Reader instance,\n105\nsorting and merging sequence files, 108\nusing WholeFileInputFormat to package\nfiles into, 194\nwriting with SequenceFile.Writer instance,\n103\nSequenceFileAsBinaryInputFormat class, 200\nSequenceFileAsBinaryOutputFormat class,\n203\nSequenceFileAsTextInputFormat class, 200\nSequenceFileInputFormat class, 200\nSequenceFileOutputFormat class, 203\nsequential znodes, 380\nusing in distributed lock implementation,\n398\nserialization, 86–103\nframeworks for, 101\nJava Object Serialization, 101\nserialization IDL, 102\nrelations to and from program IO streams,\n333\nof side data in job configuration, 238\nuse in remote procedure calls (RPCs), 86\nWritable classes, 89\nWritable interface, 87–89\nSerialization interface, 101\nSerializer objects, 101\nservers, ZooKeeper, numeric identifier, 402\nservice-level authorization, 264\nsession IDs (ZooKeeper client), 399\nsessions (ZooKeeper), 388\nSETI@home, 8\nsets, emulation of, 96\nsharding, 445\nshared-nothing architecture, 7\nShareThis, Hadoop and Cascading at, 457–\n461shell, filesystem, 49\nshell, launching for HBase, 349\nshuffle and sort, 163, 218\n(see also sorting)\nconfiguration tuning, 166\nmap side, 163\nreduce tasks, 164\nside data\ndefined, 238\ndistribution using distributed cache, 239–\n242\ndistribution using job configuration, 238\nside effects, task side-effect files, 173\nsingle named output, 209\nSkipBadRecords class, 172\nskipping mode, 171\nslaves file, 252\nSlicer interface, 330\nSocksSocketFactory class, 441\nSolrInputFormat objects, 446\nSolrOutputFormat objects, 445\nsort merges, very large, 364\nsort phase of reduce tasks, 165\nSortedMap interface, 359\nSortedMapWritable class, 95\nsorting, 218–233\n(see also shuffle and sort)\nbenchmarking MapReduce with, 268\nin Pig, 338\nin MapReduce, 448\npartial sorts, 219–223\napplication for partitioned MapFile\nlookups, 221–223\nsorting sequence file with IntWritable\nkeys, 219\npreparing for, converting weather data into\nSequenceFile format, 218\nsecondary sort, 227–233\nin Streaming, 231\nJava code for, 228–231\nTeraByte sort on Apache Hadoop, 461\ntotal sort, 223–227\nspace management, 424\nspeculative execution, 169\nspills, task memory buffers, 163\nreduce task, 165\nSPLIT operator, 319, 340\nsplits (see input splits)\nSQL\ndata pipelines in, 422\nPig Latin versus, 308\nsrst command, 371\nSSH\nconfiguration, 251\nconfiguring for pseudo-distributed mode,\n468\nenvironmental settings, 257\nstack traces, 286\nstaged event-driven architecture (SEDA), 443\nstandalone mode, 466\ninstalling Cloudera’s Distribution for\nHadoop, 473\nZooKeeper service, 385\nstandby namenode, 45\nstat command, 371\nStat objects, 381\nStatCallback interface, 382\nstate exceptions in ZooKeeper, 395\nstatements (Pig Latin), 310, 311–314\ncommands, 313\ndiagnostic operators, 313\nrelational operators, 312\nUDF (user-defined function), 313\nStates enum, 390\nstates, ZooKeeper object, 389\nstatus\nMapReduce jobs and tasks, 157\npropagation of updates through\nMapReduce system, 158\nstorage and analysis of data, 3\nstore functions, 321\nPigStorage, 331\nSTORE statement, order and, 339\nSTREAM operator, 333\nStreaming, 32–36\ndefault MapReduce job, 182\ndistributed cache and, 239\nenvironment variables, 173\nkeys and values, 183\nPython map and reduce functions, 35\nrelationship of executable to tasktracker and\nits child, 156\nRuby map and reduce functions, 33\nscript to process raw NCDC files and store\nin HDFS, 477\nsecondary sort, 231\ntask failures, 160\nuser-defined counters, 218\nIndex | 497streaming data access in HDFS, 42\nstreaming in Pig, custom processing script,\n333\nstreams, compressing and decompressing with\nCompressionCodec, 79\nStreamXmlRecordReader class, 199\nString class, 91\nconversion of Text objects to Strings, 94\nText class versus, 92\nznode paths, 379\nStringifier class, 238\nstructured data, 5\nSubAssembly class (Cascading), 455\nsubmission of a job, 153\nsuper-user, 47\nsync markers in SequenceFiles, 109\nsync operation in ZooKeeper, 381\nsync( ) method, FSDataOutputStream class,\n69\nsynchronous API in ZooKeeper, 381\nsyncLimit property, 402\nsystem daemon logs, 143\nsystem properties, 117\nconfiguration properties defined in terms of,\n118\nT\ntab character, 33, 34\nTableInputFormat class, 202, 351\nTableMap interface, 353\nTableMapReduceUtil class,\ninitTableMapJob( ) method, 353\nTableOutputFormat class, 202, 351\ntables\ncreating in HBase, 349\ndescription of HBase tables, 344\nremoving in HBase, 350\nTaps (Cascading), 453\ntask details page, 141\ntask execution, 156, 168–174\nenvironment, 172\nStreaming environment variables, 173\ntask side-effect files, 173\nJVM reuse, 170\nskipping bad records, 171\nspeculative, 169\nStreaming and Pipes, 156\ntask IDs, 25, 133\ntask logs (MapReduce), 143\n498 | Index\nTaskRunner objects, 156\ntasks\nassignment to tasktracker, 155\ncreating list of tasks to run, 155\nfailures, 160\nkilling attempts, 161\nmap and reduce, 27\nmaximum number of attempts to run, 160\nmemory limits for, 266\nprofiling, 146–149\nprogress of, 157\nstatus of, 158\ntasks page, 140\nTaskTracker class, 153\ntasktracker.http.threads property, 167\ntasktrackers, 27, 153\nblacklisted, 161\nfailure of, 161\npermitted to connect to jobtracker, 294\nreducers fetching map output from, 164\nTCP/IP server, 263\ntemporary directory for MapReduce task\noutputs (datanodes), 173\nTeraByte sort on Apache Hadoop, 461\nTeraGen application, 462\nTeraSort application, 462\nTeraValidate application, 464\nTestDFSIO, benchmarking HDFS, 267\ntesting\nunit testing log flow at ShareThis, 459\nwriting unit test for mapper, 124–126\nwriting unit test for reducer, 126\nTestInputFormat objects, skipping bad\nrecords, 171\nText class, 21, 91–94\nconversion of SequenceFile keys and values\nto, 200\nconverting Text objects to Strings, 94\nindexing, 91\niterating over Unicode characters in Text\nobjects, 93\nmutability of Text objects, 94\nreusing Text objects, 148\nString class versus, 92\ntext input, 196–199, 196\n(see also TextInputFormat class)\nKeyValueTextInputFormat class, 197\nNLineInputFormat class, 198\nXML, 199text output, 202\nTextInputFormat class, 196\ndefault Streaming job and, 182\nnonsplittable example, 191\nTextOutputFormat class, 202\ndefault output format of MapReduce jobs,\n182\nthreads\ncopier threads for reduce task, 164\ndatanode, running out of, 366\nnumber of worker threads serving map\noutput file partitions, 164\nThrift API, 49\ninstallation and usage instructions, 49\nthrift service, using with HBase, 353\ntick time, 388\ntickTime property, 402\ntickTime property (ZooKeeper), 371\ntime parameters in ZooKeeper, 388\ntimeout period for tasks, 160\nTokenCounterMapper class, 243\nTool interface, 121, 353\nexample implementation\n(ConfigurationPrinter), 121\nToolRunner class, 122\nlisting of supported options, 122\ntopology.node.switch.mapping.impl property,\n248\ntotal sort, 223–227\nTotalOrderPartitioner class, 225\nTrack Statistics Program (Hadoop at Last.fm),\n407\nIdentityMapper, 413\nMergeListenerMapper, 412\nmerging results from previous jobs, 412\nresults, 413\nSumMapper, 410\nsumming track totals, 410\nSumReducer, 410, 413\nUnique Listeners job, 408\nUniqueListenerMapper, 408\nUniqueListenerReducer, 409\ntrash, 265\nexpunging, 265\nTrim UDF (example), 326–327\ntuning jobs, 145–149\nchecklist for, 145\nprofiling tasks, 146–149\nTupleFactory class, 329\nTuples (Cascading), 449\ntuples (Pig), 306\nTwoDArrayWritable class, 95\nU\nUDF statements (Pig Latin), 313\nUDFs (user-defined functions) in Pig, 322–331\neval UDF, 326–327\nfilter UDF, 322–326\nleveraging types, 325\nload UDF, 327–331\nUI, 367\n(see also web UI for MapReduce)\nHBase, 367\nulimit count for file descriptors, 366\nUnicode, 92\niteration over characters in Text object, 93\nznode paths, 379\nUNION statement, 339\nUnix\nHadoop on, 465\nproduction platform for Hadoop, 246\nstreams, 32\nUnix tools, analyzing weather data, 17\nunrecoverable exceptions in ZooKeeper, 396,\n400\nunstructured data, 5\nupdate operations in ZooKeeper, 381\nupgrades, 296–299\nchecking, 298\nclean up after, 296\nfinalizing, 299\nHDFS data and metadata, 297\nrolling back, 298\nstarting, 298\nversion compatibility, 296\nwaiting for completion of, 298\nURIs\nadding fragment identifiers to file URIs with\nDistributedCache, 242\nremapping file URIs to\nRawLocalFileSystem, 77\nS3, 476\nznode paths versus, 379\nURLCat class (example), 52\nURLs, reading data from, 51\nuser identity, setting, 120\nuser, creating for Hadoop, 250\nUTF-8 character encoding, 91\nIndex | 499Utf8StorageConverter class, 330\nV\nvalidation, Pig Latin schemas and, 318\nvariable expansion, 118\nversioned cells in HBase, 344\nversions\nHadoop and HBase, compatibility, 366\nHadoop components, compatibility of, 296\nvery large files, 41\nvoid return types in ZooKeeper, 382\nvolunteer computing, 8\nW\nWalters, Chad, 344\nWatcher interface\nConnectionWatcher class (example), 375\nCreateGroup (example), 373\nfunctions of, 390\nprocess( ) method, 374\nWatcher.Event.KeeperState enum, 374\nwatches, 380\ncreation operations and corresponding\ntriggers, 383\non read operations, 382\nweather dataset, 15\nanalyzing with Unix tools, 17\nNCDC format, 15\nweb page for this book, xviii\nweb queries in HBase, 358–360\nmethods retrieving range of rows from\nHBase table, 359\nusing Scanners, 359\nweb search engines\nApache Lucene and Nutch, 9\nbuilding from scratch, 9\nweb UI for MapReduce, 134–136, 139\njob page, 136\njobtracker page, 134\ntask details page, 141\ntasks page, 140\nWebDAV, 50\nwebinterface.private.actions property, 141\nWebMap, 11\nwebtable, 343\nwhoami command, 120\nWholeFileInputFormat class, 192\n500 | Index\nusing to package small files into\nSequenceFiles, 194\nWindows, Hadoop on, 465\nwork units, 8\nworkflows, MapReduce, 149–151\ndecomposing problem into MapReduce\njobs, 149\nrunning dependent jobs, 151\nWritable classes\nBytesWritable, 94\ncollections, 95\nimplementing custom, 96–101\nNullWritable, 95\nObjectWritable and GenericWritable, 95\nText, 91–94\nwrappers for Java primitives, 89\nWritable interface, 87–89\nWritableComparable interface, 88, 220\nWritableSerialization class, 101\nwrite operations in ZooKeeper, 382\nWriteLock class, 400\nwriters, multiple, HDFS and, 42\nX\nXML, text input as XML documents, 199\nY\nYahoo!, Hadoop at, 10\nZ\nZab protocol, 385\nzettabytes, 1\nznodes, 372, 379\nACLs (access control lists), 383\ndeleting, 378\ndeletion of, watch event types and, 383\nephemeral, 379\nephemeral and persistent, 374\npaths, 379\nprogram creating znode to represent group,\n372–374\nsequence numbers, 380\nversion number, 381\nwatches on, 380\nzoo.cfg file, 371\nZOOCFGDIR environment variable, 371\nZooDefs.Ids class, 384\nZooKeeper, 369–403Administrator's Guide, 401\nbuilding applications with, 391–401\nconfiguration service, 391–394\ndistributed data structures and\nprotocols, 400\nlock service, 398–400\nresilient application, 394–398\ncharacteristics of, 369\ncommand-line tool, 377\ndefined, 13\nexample, 371–378\ncreating a group, 372–374\ndeleting a group, 378\ngroup membership, 372\njoining a group, 374–376\nlisting group members, 376–377\ninstalling and running, 370\ncommands, 371\nsetting up configuration file, 371\nstarting local ZooKeeper server, 371\nin production, 401\nconfiguration, 402\nresilience and performance, 401\nservice, 378–391\nconsistency, 386\ndata model, 379\nimplementation, 385\noperations, 380\nsessions, 388\nstates, 389\nuse in HBase, 346\nwebsite, descriptions of data structures and\nprotocols, 400\nzookeeper reserved word, 379\nzookeeper_mt library, 382\nzookeeper_st library, 382\nzxid, 387\nIndex | 501About the Author\nTom White has been an Apache Hadoop committer since February 2007, and is a\nmember of the Apache Software Foundation. He works for Cloudera, a company that\noffers Hadoop support and training. Previously, he was an independent Hadoop con-\nsultant, working with companies to set up, use, and extend Hadoop. He has written\nnumerous articles for oreilly.com, java.net, and IBM’s developerWorks, and has spo-\nken about Hadoop at several conferences. Tom has a B.A. from the University of Cam-\nbridge and an M.A. in philosophy of science from the University of Leeds, UK He lives\nin Powys, Wales, with his family.\nColophon\nThe animal on the cover of Hadoop: The Definitive Guide is an African elephant. They\nare the largest land animals on earth (slightly larger than their cousin, the Asian ele-\nphant) and can be identified by their ears, which have been said to look somewhat like\nthe continent of Asia. Males stand 12 feet tall at the shoulder and weigh 12,000 pounds,\nbut they can get as big as 15,000 pounds, whereas females stand 10 feet tall and weigh\n8,000–11,000 pounds.\nThey have four molars; each weighs about 11 pounds and measures about 12 inches\nlong. As the front pair wears down and drops out in pieces, the back pair shifts forward,\nand two new molars emerge in the back of the mouth. They replace their teeth six times\nthroughout their lives, and between 40–60 years of age, they will lose all of their teeth\nand likely die of starvation (a common cause of death). Their tusks are teeth—actually\nit is the second set of incisors that becomes the tusks, which they use for digging for\nroots and stripping the bark off trees for food, fighting each other during mating season,\nand defending themselves against predators. Their tusks weigh between 50–100\npounds and are between 5–8 feet long.\nAfrican elephants live throughout sub-Saharan Africa. Most of the continent’s ele-\nphants live on savannas and in dry woodlands. In some regions, they can be found in\ndesert areas; in others, they are found in mountains.\nElephants are fond of water. They shower by sucking water into their trunks and\nspraying it all over themselves; afterward, they spray their skin with a protective coating\nof dust. An elephant’s trunk is actually a long nose used for smelling, breathing, trum-\npeting, drinking, and grabbing things, especially food. The trunk alone contains about\n100,000 different muscles. African elephants have two finger-like features on the end\nof their trunks that they can use to grab small items. They feed on roots, grass, fruit,\nand bark. An adult elephant can consume up to 300 pounds of food in a single day.\nThese hungry animals do not sleep much—they roam great distances while foraging\nfor the large quantities of food that they require to sustain their massive bodies.Having a baby elephant is a serious commitment. Elephants have longer pregnancies\nthan any other mammal: almost 22 months. At birth, elephants already weigh approx-\nimately 200 pounds and stand about 3 feet tall.\nThis species plays an important role in the forest and savanna ecosystems in which they\nlive. Many plant species are dependent on passing through an elephant’s digestive tract\nbefore they can germinate; it is estimated that at least a third of tree species in west\nAfrican forests rely on elephants in this way. Elephants grazing on vegetation also affect\nthe structure of habitats and influence bush fire patterns. For example, under natural\nconditions, elephants make gaps through the rainforest, enabling the sunlight to enter,\nwhich allows the growth of various plant species. This in turn facilitates a more abun-\ndant and more diverse fauna of smaller animals. As a result of the influence elephants\nhave over many plants and animals, they are often referred to as a keystone species\nbecause they are vital to the long-term survival of the ecosystems in which they live.\nThe cover image is from the Dover Pictorial Archive. The cover font is Adobe ITC\nGaramond. The text font is Linotype Birka; the heading font is Adobe Myriad Con-\ndensed; and the code font is LucasFont’s TheSansMonoCondensed.\n----------------------------------- SOURCE END -------------------------------------",1369115548268
error,log,,,"Problems occurred when invoking code from plug-in: ""org.eclipse.jdt.ui"".",1369115548270
error,log,,,"Error in JDT Core during AST creation",1369115548270
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115548854
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115557124
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.paste",1369115558131
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.undo",1369115572848
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.undo",1369115573316
executed,command,org.eclipse.ui,3.7.0.v20110928-1505,"org.eclipse.ui.edit.undo",1369115573485
deactivated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115583779
activated,workbench,org.eclipse.ui.workbench,3.7.1.v20120104-1859,"",1369115586216
